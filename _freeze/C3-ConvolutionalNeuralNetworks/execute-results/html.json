{
  "hash": "b4a6c91b3e42fd0fc2ed4e0c57980d32",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Convolutional Neural Networks (CNN)\n\n\n\n\n\nThe main purpose of convolutional neural networks is image recognition. (Sound can be understood as an image as well!) In a convolutional neural network, we have at least one convolution layer, additional to the normal, fully connected deep neural network layers.\n\nNeurons in a convolution layer are connected only to a small spatially contiguous area of the input layer (*receptive field*). We use this structure (*feature map*) to scan the **entire** features / neurons (e.g. picture). Think of the feature map as a *kernel* or *filter* (or imagine a sliding window with weighted pixels) that is used to scan the image. As the name is already indicating, this operation is a convolution in mathematics. The kernel weights are optimized, but we use the same weights across the entire input neurons (*shared weights*).\n\nThe resulting (hidden) convolutional layer after training is called a *feature map*. You can think of the feature map as a map that shows you where the \"shapes\" expressed by the kernel appear in the input. One kernel / feature map will not be enough, we typically have many shapes that we want to recognize. Thus, the input layer is typically connected to several feature maps, which can be aggregated and followed by a second layer of feature maps, and so on.\n\nYou get one convolution map/layer for each kernel of one convolutional layer.\n\n## Example MNIST\n\nWe will show the use of convolutional neural networks with the MNIST data set. This data set is maybe one of the most famous image data sets. It consists of 60,000 handwritten digits from 0-9.\n\nTo do so, we define a few helper functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nrotate = function(x){ t(apply(x, 2, rev)) }\n\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE,\n     main = paste0(\"Label: \", as.character(title)))\n}\n```\n:::\n\n\nThe MNIST data set is so famous that there is an automatic download function in Keras:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n```\n:::\n\n\nLet's visualize a few digits:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noldpar = par(mfrow = c(1, 3))\n.n = sapply(1:3, function(x) imgPlot(train$x[x,,], train$y[x]))\n```\n\n::: {.cell-output-display}\n![](C3-ConvolutionalNeuralNetworks_files/figure-html/chunk_chapter5_11-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(oldpar)\n```\n:::\n\n\nSimilar to the normal machine learning workflow, we have to scale the pixels (from 0-255) to the range of $[0, 1]$ and one hot encode the response. For scaling the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2D array/tensor is typically called a matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\ntest_y = to_categorical(test$y, 10)\n```\n:::\n\n\nThe last dimension denotes the number of channels in the image. In our case we have only one channel because the images are black and white.\n\nMost times, we would have at least 3 color channels, for example RGB (red, green, blue) or HSV (hue, saturation, value), sometimes with several additional dimensions like transparency.\n\nTo build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size $2\\times2$. These are 2D kernels because our images are 2D. For movies for example, one would use 3D kernels (the third dimension would correspond to time and not to the color channels).\n\n::: panelset\n::: panel\n[Keras]{.panel-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = keras_model_sequential()\nmodel %>%\n layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n layer_max_pooling_2d() %>%\n layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n layer_max_pooling_2d() %>%\n layer_flatten() %>%\n layer_dense(100L, activation = \"relu\") %>%\n layer_dense(10L, activation = \"softmax\")\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 27, 27, 16)              80          \n max_pooling2d_1 (MaxPooling2D)     (None, 13, 13, 16)              0           \n conv2d (Conv2D)                    (None, 11, 11, 16)              2320        \n max_pooling2d (MaxPooling2D)       (None, 5, 5, 16)                0           \n flatten (Flatten)                  (None, 400)                     0           \n dense_1 (Dense)                    (None, 100)                     40100       \n dense (Dense)                      (None, 10)                      1010        \n================================================================================\nTotal params: 43,510\nTrainable params: 43,510\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n:::\n\n::: panel\n[Torch]{.panel-name}\n\n```         \nPrepare/download data:\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(torchvision)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntrain_ds = mnist_dataset(\n  \".\",\n  download = TRUE,\n  train = TRUE,\n  transform = transform_to_tensor\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nProcessing...\nDone!\n```\n:::\n\n```{.r .cell-code}\ntest_ds = mnist_dataset(\n  \".\",\n  download = TRUE,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n```\n:::\n\n\nBuild dataloader:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_dl = dataloader(train_ds, batch_size = 32, shuffle = TRUE)\ntest_dl = dataloader(test_ds, batch_size = 32)\nfirst_batch = train_dl$.iter()\ndf = first_batch$.next()\n\ndf$x$size()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32  1 28 28\n```\n:::\n:::\n\n\nBuild convolutional neural network: We have here to calculate the shapes of our layers on our own:\n\n**We start with our input of shape (batch_size, 1, 28, 28)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample = df$x\nsample$size()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32  1 28 28\n```\n:::\n:::\n\n\n**First convolutional layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconv1 = nn_conv2d(1, 16L, 2L, stride = 1L)\n(sample %>% conv1)$size()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32 16 27 27\n```\n:::\n:::\n\n\nOutput: batch_size = 32, number of feature maps = 16, dimensions of each feature map = $(27 , 27)$ Wit a kernel size of two and stride = 1 we will lose one pixel in each dimension... Questions:\n\n-   What happens if we increase the stride?\n-   What happens if we increase the kernel size?\n\n**Pooling layer summarizes each feature map**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32 16 13 13\n```\n:::\n:::\n\n\nkernel_size = 2L and stride = 2L halfs the pixel dimensions of our image.\n\n**Fully connected layer**\n\nNow we have to flatten our final output of the convolutional neural network model to use a normal fully connected layer, but to do so we have to calculate the number of inputs for the fully connected layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndims = (sample %>% conv1 %>%\n          nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()\n# Without the batch size of course.\nfinal = prod(dims[-1]) \nprint(final)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2704\n```\n:::\n\n```{.r .cell-code}\nfc = nn_linear(final, 10L)\n(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L)\n  %>% torch_flatten(start_dim = 2L) %>% fc)$size()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32 10\n```\n:::\n:::\n\n\nBuild the network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet = nn_module(\n  \"mnist\",\n  initialize = function(){\n    self$conv1 = nn_conv2d(1, 16L, 2L)\n    self$conv2 = nn_conv2d(16L, 16L, 3L)\n    self$fc1 = nn_linear(400L, 100L)\n    self$fc2 = nn_linear(100L, 10L)\n  },\n  forward = function(x){\n    x %>%\n      self$conv1() %>%\n      nnf_relu() %>%\n      nnf_max_pool2d(2) %>%\n      self$conv2() %>%\n      nnf_relu() %>%\n      nnf_max_pool2d(2) %>%\n      torch_flatten(start_dim = 2) %>%\n      self$fc1() %>%\n      nnf_relu() %>%\n      self$fc2()\n  }\n)\n```\n:::\n\n:::\n:::\n\nWe additionally used a pooling layer for downsizing the resulting feature maps. Without further specification, a $2\\times2$ pooling layer is taken automatically. Pooling layers take the input feature map and divide it into (in our case) parts of $2\\times2$ size. Then the respective pooling operation is executed. For every input map/layer, you get one (downsized) output map/layer.\n\nAs we are using the max pooling layer (there are sever other methods like the mean pooling), only the maximum value of these 4 parts is taken and forwarded further. Example input:\n\n```         \n1   2   |   5   8   |   3   6\n6   5   |   2   4   |   8   1\n------------------------------\n9   4   |   3   7   |   2   5\n0   3   |   2   7   |   4   9\n```\n\nWe use max pooling for every field:\n\n```         \nmax(1, 2, 6, 5)   |   max(5, 8, 2, 4)   |   max(3, 6, 8, 1)\n-----------------------------------------------------------\nmax(9, 4, 0, 3)   |   max(3, 7, 2, 7)   |   max(2, 5, 4, 9)\n```\n\nSo the resulting pooled information is:\n\n```         \n6   |   8   |   8\n------------------\n9   |   7   |   9\n```\n\nIn this example, a $4\\times6$ layer was transformed to a $2\\times3$ layer and thus downsized. This is similar to the biological process called *lateral inhibition* where active neurons inhibit the activity of neighboring neurons. It's a loss of information but often very useful for aggregating information and prevent overfitting.\n\nAfter another convolutional and pooling layer we flatten the output. That means the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all weights from the last feature maps). You can imagine that like reshaping a matrix (2D) to a simple 1D vector. Then the full vector is used. Having flattened the layer, we can simply use our typical output layer.\n\n::: panelset\n::: panel\n[Keras]{.panel-name}\n\nThe rest is as usual: First we compile the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  keras::compile(\n      optimizer = keras::optimizer_adamax(0.01),\n      loss = loss_categorical_crossentropy\n  )\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 27, 27, 16)              80          \n max_pooling2d_1 (MaxPooling2D)     (None, 13, 13, 16)              0           \n conv2d (Conv2D)                    (None, 11, 11, 16)              2320        \n max_pooling2d (MaxPooling2D)       (None, 5, 5, 16)                0           \n flatten (Flatten)                  (None, 400)                     0           \n dense_1 (Dense)                    (None, 100)                     40100       \n dense (Dense)                      (None, 10)                      1010        \n================================================================================\nTotal params: 43,510\nTrainable params: 43,510\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\nThen, we train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nepochs = 5L\nbatch_size = 32L\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n```\n:::\n\n:::\n\n::: panel\n[Torch]{.panel-name}\n\nTrain model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = net()\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nfor(e in 1:3){\n  losses = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]], reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      losses = c(losses, loss$item())\n    }\n  )\n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", e, mean(losses)))\n}\n```\n:::\n\n\nEvaluation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch$eval()\n\ntest_losses = c()\ntotal = 0\ncorrect = 0\n\ncoro::loop(\n  for(batch in test_dl){\n    output = model_torch(batch[[1]])\n    labels = batch[[2]]\n    loss = nnf_cross_entropy(output, labels)\n    test_losses = c(test_losses, loss$item())\n    predicted = torch_max(output$data(), dim = 2)[[2]]\n    total = total + labels$size(1)\n    correct = correct + (predicted == labels)$sum()$item()\n  }\n)\n\nmean(test_losses)\ntest_accuracy =  correct/total\ntest_accuracy\n```\n:::\n\n:::\n:::\n\n## Example CIFAR\n\nCIFAR10 is another famous dataset. It consists of ten classes with colored images (see https://www.cs.toronto.edu/\\~kriz/cifar.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\ndata = keras::dataset_cifar10()\ntrain = data$train\ntest = data$test\nimage = train$x[1,,,]\nimage %>% \n image_to_array() %>%\n `/`(., 255) %>%\n as.raster() %>%\n plot()\n## normalize pixel to 0-1\ntrain_x = array(train$x/255, c(dim(train$x)))\ntest_x = array(test$x/255, c(dim(test$x)))\ntrain_y = to_categorical(train$y, 10)\ntest_y = to_categorical(test$y, 10)\nmodel = keras_model_sequential()\nmodel %>% \n layer_conv_2d(input_shape = c(32L, 32L,3L),filters = 16L, kernel_size = c(2L,2L), activation = \"relu\") %>% \n layer_max_pooling_2d() %>% \n layer_dropout(0.3) %>% \n layer_conv_2d(filters = 16L, kernel_size = c(3L,3L), activation = \"relu\") %>% \n layer_max_pooling_2d() %>% \n layer_flatten() %>% \n layer_dense(10, activation = \"softmax\")\nsummary(model)\nmodel %>% \n compile(\n optimizer = optimizer_adamax(),\n loss = loss_categorical_crossentropy\n )\nearly = callback_early_stopping(patience = 5L)\nepochs = 1L\nbatch_size =20L\nmodel %>% fit(\n x = train_x, \n y = train_y,\n epochs = epochs,\n batch_size = batch_size,\n shuffle = TRUE,\n validation_split = 0.2,\n callbacks = c(early)\n)\n```\n:::\n\n\n## Exercise\n\n\n```{=html}\n  <hr/>\n  <strong><span style=\"color: #0011AA; font-size:18px;\">Task</span></strong><br/>\n```\n\nThe next exercise is on the flower data set in the Ecodata package.\n\nFollow the steps, we did above and build your own convolutional neural network.\n\nIn the end, submit your predictions to the submission server. If you have extra time, have a look at kaggle and find the flower data set challenge for specific architectures tailored for this data set.\n\n**Tip:** Take a look at the dataset chapter.\n\n\n```{=html}\n  <details>\n    <summary>\n      <strong><span style=\"color: #0011AA; font-size:18px;\">Solution</span></strong>\n    </summary>\n    <p>\n```\n\nThe following code shows different behavior in the context of data augmentation and model complexity.\n\nThe topic of overfitting can be seen cleary: Compare the simple model and its performance on the training and the test data. Then compare the more complex or even the regularized models and their performance on training and test data.\n\nYou see that the very simple models tend to overfit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  # Prepare training and test sets:\n\n  ## Outer split -> dataset$train and dataset$test, dataset$labels are the labels for dataset$train\n  dataset = EcoData::dataset_flower()\n\n  ## Inner split:\n  train = dataset$train/255\n  indicesTrain = sample.int(nrow(train), 0.8 * nrow(train))\n  test = train[-indicesTrain,,,]\n  train = train[indicesTrain,,,]\n  \n  labelsTrain = dataset$labels\n  labelsTest = labelsTrain[-indicesTrain]\n  labelsTrain = labelsTrain[indicesTrain]\n```\n:::\n\n\n\n\n<!-- **Even more complex model:** -->\n\n<!-- The following snippet offers a solution for data generation with oversampling and undersampling, because the distribution of classes is not equal in the flower data set. -->\n\n\n\n\n\n<!-- Read in for example the following way: -->\n\n\n\n\n\n<!-- <h2>Be careful, this exercise uses A LOT OF MEMORY!! If you have a SSD, you might want to turn pagefile / swap off. If you don't have at least 8 GB RAM, don't try to run this exercise, it won't work.</h2> -->\n\n<!-- To avoid crashes of your system, you might want to do some memory management, like: -->\n\n<!-- * After model training, unload the training set and load the test set. -->\n\n<!-- * Generally remove data that is used no longer. -->\n\n<!-- * Lazy load new images (not shown here) out of a generator (later shown in section @ref(gan), but with manually written training loop). -->\n\n\n\n\n\n<!-- As you can see, the network works in principle (76% accuracy for training data). Mind, that this is not a binary classification problem and we are expecting roughly 20% accuracy by chance. -->\n\n<!-- Just a little hint: More complex networks are not always better. This won't be shown explicitly (as it is very computing-intensive). You can try for example 64 filter kernels per layer or copy one of the convolutional layers (including pooling layer) to see what happens. -->\n\n<!-- Now, we are training without holdouts to get the most power. -->\n\n\n\n\n\n<!-- Maybe you can find (much?) better networks, that fit already better than 84% on the training data. -->\n\n<!-- Mind, that there are 5 classes. If your model predicts only 3 or 4 of them, is is not surprising, that the accuracy is low. -->\n\n\n```{=html}\n    </p>\n  </details>\n  <br/><hr/>\n```",
    "supporting": [
      "C3-ConvolutionalNeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}