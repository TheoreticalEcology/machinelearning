{
  "hash": "de834c68ccd032fe0e164569843becff",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Machine learning pipeline {#workflow}\n\nThe Standard Machine Learning Pipeline at the example of the Titanic Data set\n\nBefore we specialize on any tuning, it is important to understand that machine learning always consists of a pipeline of actions.\n\nThe typical machine learning workflow consist of:\n\n-   Data cleaning and exploration (EDA = explorative data analysis) for example with tidyverse.\n-   Preprocessing and feature selection.\n-   Splitting data set into training and test set for evaluation.\n-   Model fitting.\n-   Model evaluation.\n-   New predictions.\n\nHere is an (optional) video that explains the entire pipeline from a slightly different perspective:\n\n\n<iframe width=\"560\" height=\"315\" \n  src=\"https://www.youtube.com/embed/nKW8Ndu7Mjw\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media;\n  gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\nIn the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics can be found here :\n\n\n<iframe width=\"560\" height=\"315\" \n  src=\"https://www.youtube.com/embed/nRtp7wSEtJA\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media;\n  gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\nAnother good reference is \"**R for data science**\" by Hadley Wickham: <a href=\"https://r4ds.had.co.nz/\" target=\"_blank\" rel=\"noopener\"></a>.\n\nFor this lecture you need the Titanic data set provided by us. You can find it in GRIPS (datasets.RData in the data set and submission section) or at <a href=\"http://rhsbio6.uni-regensburg.de:8500\" target=\"_blank\" rel=\"noopener\">http://rhsbio6.uni-regensburg.de:8500</a>.\n\nWe have split the data set already into training and test/prediction data sets (the test/prediction split has one column less than the train split, as the result is not known a priori).\n\n## Data preparation\n\nLoad necessary libraries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\nLoad data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\ndata(titanic_ml)\ndata = titanic_ml\n```\n:::\n\n\nStandard summaries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t1309 obs. of  14 variables:\n $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n```\n:::\n\n```{.r .cell-code}\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n```\n:::\n:::\n\n\nThe name variable consists of 1309 unique factors (there are 1309 observations...) and could be now transformed. If you are interested in how to do that, take a look at the following box.\n\n::: {.callout-tip collapse=\"true\"}\n## Feature engineering of the name variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(unique(data$name))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1307\n```\n:::\n:::\n\n\nHowever, there is a title in each name. Let's extract the titles:\n\n1.  We will extract all names and split each name after each comma \",\".\n2.  We will split the second split of the name after a point \".\" and extract the titles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_split = sapply(data$name,\n                     function(x) stringr::str_split(x, pattern = \",\")[[1]][2])\ntitles = sapply(first_split,\n                function(x) strsplit(x, \".\",fixed = TRUE)[[1]][1])\n```\n:::\n\n\nWe get 18 unique titles:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(titles)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntitles\n         Capt           Col           Don          Dona            Dr \n            1             4             1             1             8 \n     Jonkheer          Lady         Major        Master          Miss \n            1             1             2            61           260 \n         Mlle           Mme            Mr           Mrs            Ms \n            2             1           757           197             2 \n          Rev           Sir  the Countess \n            8             1             1 \n```\n:::\n:::\n\n\nA few titles have a very low occurrence rate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles = stringr::str_trim((titles))\ntitles %>%\n fct_count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 × 2\n   f                n\n   <fct>        <int>\n 1 Capt             1\n 2 Col              4\n 3 Don              1\n 4 Dona             1\n 5 Dr               8\n 6 Jonkheer         1\n 7 Lady             1\n 8 Major            2\n 9 Master          61\n10 Miss           260\n11 Mlle             2\n12 Mme              1\n13 Mr             757\n14 Mrs            197\n15 Ms               2\n16 Rev              8\n17 Sir              1\n18 the Countess     1\n```\n:::\n:::\n\n\nWe will combine titles with low occurrences into one title, which we can easily do with the forcats package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles2 =\n  forcats::fct_collapse(titles,\n                        officer = c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\"),\n                        royal = c(\"Jonkheer\", \"Don\", \"Sir\",\n                                  \"the Countess\", \"Dona\", \"Lady\"),\n                        miss = c(\"Miss\", \"Mlle\"),\n                        mrs = c(\"Mrs\", \"Mme\", \"Ms\")\n                        )\n```\n:::\n\n\nWe can count titles again to see the new number of titles:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles2 %>%  \n   fct_count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  f           n\n  <fct>   <int>\n1 officer    23\n2 royal       6\n3 Master     61\n4 miss      262\n5 mrs       200\n6 Mr        757\n```\n:::\n:::\n\n\nAdd new title variable to data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata =\n  data %>%\n    mutate(title = titles2)\n```\n:::\n\n:::\n\n### Imputation\n\nNAs are a common problem in ML. For example, the age variable has 20% NAs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1667 21.0000 28.0000 29.8811 39.0000 80.0000     263 \n```\n:::\n\n```{.r .cell-code}\nsum(is.na(data$age)) / nrow(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2009167\n```\n:::\n:::\n\n\nEither we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups. In tidyverse we can easily \"group\" the data, i.e. we will nest the observations (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age....)) will be done within the specified groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata =\n  data %>%\n    select(survived, sex, age, fare, pclass) %>% \n    group_by(sex, pclass) %>%\n    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %>%\n    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %>%\n    ungroup()\n```\n:::\n\n\n### Preprocessing and Feature Selection\n\nLater (tomorrow), we want to use Keras in our example, but it cannot handle factors and requires the data to be scaled.\n\nNormally, one would do this for all predictors, but as we only show the pipeline here, we have sub-selected a bunch of predictors and do this only for them. We first scale the numeric predictors and change the factors with only two groups/levels into integers (this can be handled by Keras).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_sub =\n  data %>%\n    select(survived, sex, age2, fare2, pclass) %>%\n    mutate(age2 = scales::rescale(age2, c(0, 1)),\n           fare2 = scales::rescale(fare2, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Transforming factors with more than two levels\n\nFactors with more than two levels should be **one hot encoded** (Make columns for every different factor level and write 1 in the respective column for every taken feature value and 0 else. For example: $\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}$):\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_title = model.matrix(~0+as.factor(title), data = data)\ncolnames(one_title) = levels(data$title)\n\none_sex = model.matrix(~0+as.factor(sex), data = data)\ncolnames(one_sex) = levels(data$sex)\n\none_pclass = model.matrix(~0+as.factor(pclass), data = data)\ncolnames(one_pclass) = paste0(\"pclass\", 1:length(unique(data$pclass)))\n```\n:::\n\n\nAnd we have to add the dummy encoded variables to the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = cbind(data.frame(survived= data$survived),\n                 one_title, one_sex, age = data$age2,\n                 fare = data$fare2, one_pclass)\nhead(data)\n```\n:::\n\n:::\n\n\n## Modelling\n\n### Split data for final predictions\nTo tune our hyperparameters and evaluate our models, we need to split the data as we learned in the CV section. Before doing so, however, we must split off the new observations in the data set :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data_sub$survived)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.0000  0.0000  0.3853  1.0000  1.0000     655 \n```\n:::\n:::\n\n\n655 observations have NAs in our response variable, these are the observations for which we want to make predictions at the end of our pipeline (we have no information about their actual values!).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_new = data_sub[!is.na(data_sub$survived),]\ndata_obs = data_sub[is.na(data_sub$survived),]\n```\n:::\n\n\n### Training and evaluation\n\nNow, we can do a simple 10xCV with the observed_data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nlibrary(glmnetUtils)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 10\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = 1:cv,\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = 0.2)\n  results[i, 2] = Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                            alpha = 0.2,\n                                                            s = 0.01,\n                                                            type = \"response\"))\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   set       AUC\n1    1 0.8650794\n2    2 0.8723197\n3    3 0.7632114\n4    4 0.7541463\n5    5 0.8984221\n6    6 0.7200000\n7    7 0.7829268\n8    8 0.8701737\n9    9 0.8787879\n10  10 0.7867647\n```\n:::\n\n```{.r .cell-code}\nprint(mean(results$AUC))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8191832\n```\n:::\n:::\n\n\n\n### Hyperparameter optimization\n\nWe did a simple 10xCV to evaluate our model but we didn't tune our hyperparameters ($\\lambda$ and $\\alpha$). If we want to tune them, we need do another CV within each split of the model evaulation CV, which is called nested CV.\n\nWe used only one split (the split for the submission server doesn't count) to evaluate the performance of the model before we made the final predictions. If we test many different hyperparameter combinations, how do we ensure that a certain hyperparameter is not only good for our training dataset but also good for the new data (our outer split on the submission server)? You may have guessed it already, we need to do another CV within the previous CV to check whether a certain hyperparameter solution generalizes to the whole data. To tune $\\lambda$, we would need to split the data another time (called nested CV).\n\nWhy is it important to tune hyperparameters? Hyperparameters (configuration parameters of our ML algorithms that (mostly) control their complexity) are usually tuned (optimized) in an automatic / systematic way. A common procedure, called random search, is to sample random configuration combinations from the set of hyperparameters and test for each combination the prediction error.\n\nLet's implement manually a nested CV to tune the $\\alpha$. Let's start with a 5CVx5CV and 20x different alpha values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(20,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  # inner split\n  for(j in 1:cv_inner) {\n    inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n    train_inner = train_outer[inner_split != j, ]\n    test_inner = train_outer[inner_split == j, ]\n    \n    tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = 0.01,\n                                                         type = \"response\")))\n      })\n    best_alpha = hyper_alpha[which.max(tuning_results_inner)]\n  }\n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = 0.01, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set     alpha       AUC\n1   1 0.9148060 0.8715686\n2   2 0.1346666 0.7532354\n3   3 0.1346666 0.8175466\n4   4 0.6417455 0.8243832\n5   5 0.1174874 0.8171263\n```\n:::\n:::\n\n\n\n## Predictions and Submission\n\nWhen we are satisfied with the performance of our model, we will create predictions for the new observations on the submission server. But first we will train now our model on the full observed dataset:\n\n$\\alpha = 0.915$ has the highest AUC, let's use it to train the model on the full dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = 0.915)\n```\n:::\n\n\n\nWe cannot assess the performance for the new observations because the true survival ratio is unknown, however, we can now submit our predictions to the submission server at <a href=\"http://rhsbio7.uni-regensburg.de:8500\" target=\"_blank\" rel=\"noopener\">http://rhsbio7.uni-regensburg.de:8500</a>.\n\nFor the submission it is critical to change the predictions into a data.frame, select the second column (the probability to survive), and save it with the write.csv function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_new = data_sub[is.na(data_sub$survived),]\nwrite.csv(data.frame(y = predict(model, data_new, alpha = 0.915, s = 0.01, type = \"response\")[,1] ), file = \"Max_1.csv\")\n```\n:::\n\n\nWe have now used the $\\alpha$ value with the highest AUC here, but our tuning has shown that the best value of $\\alpha$ depends on the partitioning, so it would probably be better to build ten models and combine their predictions (e.g., by averaging the predictions):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(results$alpha, function(alpha) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = alpha)\n    return(predict(model, data_new, alpha = alpha, s = 0.01, type = \"response\")[,1])\n  })\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean), file = \"Max_1.csv\"))\n```\n:::\n\n\n\n## Exercises\n\n::: {.callout-caution icon=\"false\"}\n#### Task: Tuning $\\alpha$ and $\\lambda$\n\n1.  Extend the code from above and tune $\\alpha$ and $\\lambda$\n\n2.  Train the model with best set of hyperparameters and submit your predictions\n\n3.  Compare the predictive performance from the single best model with the ensemble model\n\nSubmit both predictions (http://rhsbio7.uni-regensburg.de:8500/), which model has a higher AUC?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\n\n# missRanger uses a random forest to impute NAs (RF is trained on the data to predict values for the NAs)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[!is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[is.na(data_sub$survived),] # data with known response\n```\n:::\n\n\nBonus: \n\n-   Try different features\n-   Try cito\n-   Try different datasets (see @sec-datasets)\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for the single model</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(30,0, 1)\nhyper_lambda = runif(30,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  # inner split\n  for(j in 1:cv_inner) {\n    inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n    train_inner = train_outer[inner_split != j, ]\n    test_inner = train_outer[inner_split == j, ]\n    \n    tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\")))\n      })\n    best_alpha = hyper_alpha[which.max(tuning_results_inner)]\n    best_lambda = hyper_lambda[which.max(tuning_results_inner)]\n  }\n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = best_lambda\n  results[i, 4] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = 0.01, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set     alpha      lambda       AUC\n1   1 0.7365883 0.007334147 0.8735294\n2   2 0.6417455 0.003948339 0.7559731\n3   3 0.6417455 0.003948339 0.8196170\n4   4 0.6417455 0.003948339 0.8258345\n5   5 0.6417455 0.003948339 0.8142235\n```\n:::\n:::\n\n\nPredictions:\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = results$alpha[i])\n    return(predict(model, data_new, alpha = results$alpha[i], s = results$lambda[i], type = \"response\")[,1])\n  })\n\n# Single predictions from the model with the highest AUC:\nwrite.csv(data.frame(y = prediction_ensemble[,which.max(results$AUC)], file = \"Max_titanic_best_model.csv\"))\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean), file = \"Max_titanic_ensemble.csv\"))\n```\n:::\n\n\n\n\n</div>\n\n\n:::\n\n## Machine learning frameworks\n\nAs we have seen today, many of the machine learning algorithms are distributed over several packages but the general machine learning pipeline is very similar for all models: feature engineering, feature selection, hyperparameter tuning and cross-validation.\n\nMachine learning frameworks such as `mlr3` or `tidymodels` provide a general interface for the ML pipeline, in particular the training and the hyperparameter tuning with nested CV. They support most ML packages/algorithms.\n\n### mlr3 {#mlr}\n\nThe key features of mlr3 are:\n\n-   All common machine learning packages are integrated into mlr3, you can easily switch between different machine learning algorithms.\n-   A common 'language'/workflow to specify machine learning pipelines.\n-   Support for different cross-validation strategies.\n-   Hyperparameter tuning for all supported machine learning algorithms.\n-   Ensemble models.\n\nUseful links:\n\n-   <a href=\"https://mlr3book.mlr-org.com/\" target=\"_blank\" rel=\"noopener\">mlr3-book</a> (still in work)\n-   <a href=\"https://mlr3.mlr-org.com/\" target=\"_blank\" rel=\"noopener\">mlr3 website</a>\n-   <a href=\"https://cheatsheets.mlr-org.com/mlr3.pdf\" target=\"_blank\" rel=\"noopener\">mlr3 cheatsheet</a>\n\n#### mlr3 - The Basic Workflow\n\nThe mlr3 package actually consists of several packages for different tasks (e.g. mlr3tuning for hyperparameter tuning, mlr3pipelines for data preparation pipes). But let's start with the basic workflow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t4687 obs. of  40 variables:\n $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n $ Orbital.Period              : num  419 1040 682 427 503 ...\n $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n```\n:::\n:::\n\n\nLet's drop time, name and ID variable and create a classification task:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = nasa %>% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\n```\n:::\n\n\nCreate a generic pipeline of data transformation (imputation $\\rightarrow$ scaling $\\rightarrow$ encoding of categorical variables):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   Hazardous           Absolute.Magnitude \n                        4187                            0 \n               Aphelion.Dist           Asc.Node.Longitude \n                           0                            0 \n                Eccentricity    Epoch.Date.Close.Approach \n                           0                            0 \n            Epoch.Osculation         Est.Dia.in.Feet.max. \n                           0                            0 \n        Est.Dia.in.Feet.min.           Est.Dia.in.KM.max. \n                           0                            0 \n          Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n                           0                            0 \n           Est.Dia.in.M.min.        Est.Dia.in.Miles.max. \n                           0                            0 \n       Est.Dia.in.Miles.min.                  Inclination \n                           0                            0 \n Jupiter.Tisserand.Invariant                 Mean.Anomaly \n                           0                            0 \n                 Mean.Motion               Miles.per.hour \n                           0                            0 \n  Minimum.Orbit.Intersection     Miss.Dist..Astronomical. \n                           0                            0 \n      Miss.Dist..kilometers.            Miss.Dist..lunar. \n                           0                            0 \n           Miss.Dist..miles.                     Orbit.ID \n                           0                            0 \n          Orbit.Uncertainity               Orbital.Period \n                           0                            0 \n              Perihelion.Arg          Perihelion.Distance \n                           0                            0 \n             Perihelion.Time  Relative.Velocity.km.per.hr \n                           0                            0 \nRelative.Velocity.km.per.sec              Semi.Major.Axis \n                           0                            0 \n               Equinox.J2000             Equinox..MISSING \n                           0                            0 \n         Orbiting.Body.Earth       Orbiting.Body..MISSING \n                           0                            0 \n```\n:::\n:::\n\n\nWe can even visualize the preprocessing graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreprocessing$plot()\n```\n\n::: {.cell-output-display}\n![](A4-MLpipeline_files/figure-html/chunk_chapter4_68-1.png){width=672}\n:::\n:::\n\n\nTo test our model (glmnet) with 10-fold cross-validated, we will do:\n\n-   Specify the missing target rows as validation so that they will be ignored.\n-   Specify the cross-validation, the learner (the machine learning model we want to use), and the measurement (AUC).\n-   Run (benchmark) our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ntransformed_task$data()[1,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity\n1:         0         -0.8132265    -0.3804201          -1.140837    -0.315606\n   Epoch.Date.Close.Approach Epoch.Osculation Est.Dia.in.Feet.max.\n1:                 -4.792988        0.1402677            0.2714179\n   Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max.\n1:            0.3134076          0.3007134          0.2565687         0.2710953\n   Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n1:         0.2916245             0.2620443              0.258651   0.5442288\n   Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n1:                   0.3840868    -1.028761   0.3193953     -0.2541306\n   Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers.\n1:                  -5.459119                -7.076926              0.2512296\n   Miss.Dist..lunar. Miss.Dist..miles.  Orbit.ID Orbit.Uncertainity\n1:         0.2398625         0.2381077 -9.651472          -1.007087\n   Orbital.Period Perihelion.Arg Perihelion.Distance Perihelion.Time\n1:     -0.3013135      -1.170536         -0.01831583       0.1052611\n   Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec Semi.Major.Axis\n1:                  -0.2816782                   -0.2841407      -0.2791037\n   Equinox.J2000 Equinox..MISSING Orbiting.Body.Earth Orbiting.Body..MISSING\n1:             1                0                   1                      0\n```\n:::\n\n```{.r .cell-code}\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresult = mlr3::resample(transformed_task,\n                        EN, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n```\n:::\n\n\nVery cool! Preprocessing + 10-fold cross-validation model evaluation in a few lines of code!\n\nLet's create the final predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n```\n:::\n\n\nYou could now submit the predictions <a href=\"http://rhsbio7.uni-regensburg.de:8500\" target=\"_blank\" rel=\"noopener\">here</a>.\n\nBut we are still not happy with the results, let's do some hyperparameter tuning!\n\n#### mlr3 - Hyperparameter Tuning\n\nWith mlr3, we can easily extend the above example to do hyperparameter tuning within nested cross-validation (the tuning has its own inner cross-validation).\n\nPrint the hyperparameter space of our glmnet learner:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEN$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n                      id    class lower upper nlevels        default parents\n 1:                alpha ParamDbl     0     1     Inf              1        \n 2:                  big ParamDbl  -Inf   Inf     Inf        9.9e+35        \n 3:               devmax ParamDbl     0     1     Inf          0.999        \n 4:                dfmax ParamInt     0   Inf     Inf <NoDefault[3]>        \n 5:                  eps ParamDbl     0     1     Inf          1e-06        \n 6:                epsnr ParamDbl     0     1     Inf          1e-08        \n 7:                exact ParamLgl    NA    NA       2          FALSE        \n 8:              exclude ParamInt     1   Inf     Inf <NoDefault[3]>        \n 9:                 exmx ParamDbl  -Inf   Inf     Inf            250        \n10:                 fdev ParamDbl     0     1     Inf          1e-05        \n11:                gamma ParamDbl  -Inf   Inf     Inf              1   relax\n12:            intercept ParamLgl    NA    NA       2           TRUE        \n13:               lambda ParamUty    NA    NA     Inf <NoDefault[3]>        \n14:     lambda.min.ratio ParamDbl     0     1     Inf <NoDefault[3]>        \n15:         lower.limits ParamUty    NA    NA     Inf <NoDefault[3]>        \n16:                maxit ParamInt     1   Inf     Inf         100000        \n17:                mnlam ParamInt     1   Inf     Inf              5        \n18:                 mxit ParamInt     1   Inf     Inf            100        \n19:               mxitnr ParamInt     1   Inf     Inf             25        \n20:            newoffset ParamUty    NA    NA     Inf <NoDefault[3]>        \n21:              nlambda ParamInt     1   Inf     Inf            100        \n22:               offset ParamUty    NA    NA     Inf                       \n23:       penalty.factor ParamUty    NA    NA     Inf <NoDefault[3]>        \n24:                 pmax ParamInt     0   Inf     Inf <NoDefault[3]>        \n25:                 pmin ParamDbl     0     1     Inf          1e-09        \n26:                 prec ParamDbl  -Inf   Inf     Inf          1e-10        \n27:                relax ParamLgl    NA    NA       2          FALSE        \n28:                    s ParamDbl     0   Inf     Inf           0.01        \n29:          standardize ParamLgl    NA    NA       2           TRUE        \n30: standardize.response ParamLgl    NA    NA       2          FALSE        \n31:               thresh ParamDbl     0   Inf     Inf          1e-07        \n32:             trace.it ParamInt     0     1       2              0        \n33:        type.gaussian ParamFct    NA    NA       2 <NoDefault[3]>        \n34:        type.logistic ParamFct    NA    NA       2 <NoDefault[3]>        \n35:     type.multinomial ParamFct    NA    NA       2 <NoDefault[3]>        \n36:         upper.limits ParamUty    NA    NA     Inf <NoDefault[3]>        \n                      id    class lower upper nlevels        default parents\n    value\n 1:      \n 2:      \n 3:      \n 4:      \n 5:      \n 6:      \n 7:      \n 8:      \n 9:      \n10:      \n11:      \n12:      \n13:      \n14:      \n15:      \n16:      \n17:      \n18:      \n19:      \n20:      \n21:      \n22:      \n23:      \n24:      \n25:      \n26:      \n27:      \n28:      \n29:      \n30:      \n31:      \n32:      \n33:      \n34:      \n35:      \n36:      \n    value\n```\n:::\n:::\n\n\nDefine the hyperparameter space of the random forest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(paradox)\n\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\nprint(EN_pars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n       id    class lower upper nlevels        default value\n1:  alpha ParamDbl     0   1.0     Inf <NoDefault[3]>      \n2: lambda ParamDbl     0   0.5     Inf <NoDefault[3]>      \n```\n:::\n:::\n\n\nTo set up the tuning pipeline we need:\n\n-   Inner cross-validation resampling object.\n-   Tuning criterion (e.g. AUC).\n-   Tuning method (e.g. random or block search).\n-   Tuning terminator (When should we stop tuning? E.g. after $n$ iterations).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<AutoTuner:classif.glmnet.tuned>\n* Model: list\n* Search Space:\n<ParamSet>\n       id    class lower upper nlevels        default value\n1:  alpha ParamDbl     0   1.0     Inf <NoDefault[3]>      \n2: lambda ParamDbl     0   0.5     Inf <NoDefault[3]>      \n* Packages: mlr3, mlr3tuning, mlr3learners, glmnet\n* Predict Type: prob\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass, weights\n```\n:::\n:::\n\n\nNow we can wrap it normally into the 10-fold cross-validated setup as done previously:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.auc \n  0.6767554 \n```\n:::\n:::\n\n\nLet's create the final predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n```\n:::\n\n\n## Exercises\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Use mlr3 for the titanic dataset\n\n1.  Use `mlr3` to tune glmnet for the titanic dataset using nested CV\n2.  Submit single predictions and multiple predictions\n\nIf you need help, take a look at the solution, go through it line by line and try to understand it.\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nPrepare data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = titanic_ml %>% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n survived       age      boat     cabin  embarked      fare home.dest     parch \n      655       263         0         0         0         0         0         0 \n   pclass       sex     sibsp \n        0         0         0 \n```\n:::\n\n```{.r .cell-code}\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n```\n:::\n\n\nHyperparameter tuning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\n\n\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = cv10, store_models = TRUE)\n```\n:::\n\n\nEvaluation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasurement =  msr(\"classif.auc\")\nresult$aggregate(measurement)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.auc \n  0.9924459 \n```\n:::\n:::\n\n\nPredictions:\n\nWe can extract a learner with optimized hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = result$learners[[1]]$learner$clone()\nmodel$param_set$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$alpha\n[1] 0.2353052\n\n$lambda\n[1] 0.03832512\n```\n:::\n:::\n\n\nAnd we can fit it then on the full data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$train(transformed_task)\npredictions = model$predict(transformed_task, row_ids = transformed_task$row_roles$holdout)\npredictions = predictions$prob[,1]\nhead(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.94428932 0.07481128 0.21730284 0.86504280 0.94653467 0.95414457\n```\n:::\n:::\n\n\nAnd submit to http://rhsbio7.uni-regensburg.de:8500\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(data.frame(y = predictions), file = \"glmnet.csv\")\n```\n:::\n\n\n\n</div>\n\n:::\n",
    "supporting": [
      "A4-MLpipeline_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}