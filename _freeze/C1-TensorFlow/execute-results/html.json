{
  "hash": "f57b33d090e31693bfe01c47e999473b",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n\n# Introduction to TensorFlow and Keras {#tensorflowintro}\n\n## Introduction to TensorFlow\n\nOne of the most commonly used frameworks for machine learning is **TensorFlow**. TensorFlow is an open source <a href=\"https://en.wikipedia.org/wiki/Linear_algebra\" target=\"_blank\" rel=\"noopener\">linear algebra</a> library with focus on neural networks, published by Google in 2015. TensorFlow supports several interesting features, in particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization. \n\nThese advantages are nicely explained in the following video: \n  \n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/MotG3XI2qSs\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media;\n  gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\nTo sum up the most important points of the video: \n  \n* TensorFlow is a math library which is highly optimized for neural networks.\n* If a GPU is available, computations can be easily run on the GPU but even on a CPU TensorFlow is still very fast.\n* The \"backend\" (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for NVIDIA GPUs).\n* The interface (the part of TensorFlow we use) is written in Python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend.\n\nAll operations in TensorFlow are written in C++ and are highly optimized. But don't worry, we don’t have to use C++ to use TensorFlow because there are several bindings for other languages. TensorFlow officially supports a Python API, but meanwhile there are several community carried APIs for other languages:\n  \n* R\n* Go\n* Rust\n* Swift\n* JavaScript\n\nIn this course we will use TensorFlow with the <a href=\"https://tensorflow.rstudio.com/\" target=\"_blank\" rel=\"noopener\">https://tensorflow.rstudio.com/</a> binding, that was developed and published 2017 by the RStudio team. First, they developed an R package (reticulate) for calling Python in R. Actually, we are using the Python TensorFlow module in R (more about this later).\n\nTensorFlow offers different levels of API. We could implement a neural network completely by ourselves or we could use Keras which is provided as a submodule by TensorFlow. Keras is a powerful module for building and training neural networks. It allows us building and training neural networks in a few lines of codes. Since the end of 2018, Keras and TensorFlow are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras for neural networks but also how we can use the TensorFlow’s automatic differenation for using complex objective functions.\n\n\nUseful links:\n\n* <a href=\"https://www.tensorflow.org/api_docs/python/tf\" target=\"_blank\" rel=\"noopener\">TensorFlow documentation</a> (This is for the Python API, but just replace the \".\" with \"$\".)\n* <a href=\"https://tensorflow.rstudio.com/\" target=\"_blank\" rel=\"noopener\">Rstudio TensorFlow website</a>\n\n\n### Data Containers\n\nTensorFlow has two data containers (structures):\n  \n* constant (tf$constant): Creates a constant (immutable) value in the computation graph.\n* variable (tf$Variable): Creates a mutable value in the computation graph (used as parameter/weight in models).\n\nTo get started with TensorFlow, we have to load the library and check if the installation worked. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nimmutable = tf$constant(5.0)\nmutable = tf$constant(5.0)\n```\n:::\n\n\nDon't worry about weird messages (they will only appear once at the start of the session).\n\n\n### Basic Operations\n\nWe now can define the variables and do some math with them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(5.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(10.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nc = tf$add(a, b)\nprint(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(15.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\nk_print_tensor(c) # Comes out of Keras!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(15.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nNormal R methods such as print() are provided by the R package \"tensorflow\". \n\nThe TensorFlow library (created by the RStudio team) built R methods for all common operations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\nk_print_tensor(a+b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(15.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nTheir operators also automatically transform R numbers into constant tensors when attempting to add a tensor to an R number:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(20.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nTensorFlow containers are objects, what means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object).\nFor instance, there is a method to transform the tensor object back to an R object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"tensorflow.tensor\"                               \n[2] \"tensorflow.python.framework.ops.EagerTensor\"     \n[3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n[4] \"tensorflow.python.framework.ops.Tensor\"          \n[5] \"tensorflow.python.types.internal.NativeObject\"   \n[6] \"tensorflow.python.types.core.Tensor\"             \n[7] \"python.builtin.object\"                           \n```\n:::\n\n```{.r .cell-code}\nclass(d$numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"numeric\"\n```\n:::\n:::\n\n\n\n### Data Types\n\nR uses dynamic typing, what means you can assign a number, character, function or whatever to a variable and the the type is automatically inferred.\nIn other languages you have to state the type explicitly, e.g. in C:\n\n\n::: {.cell}\n\n```{.c .cell-code}\nint a = 5;\nfloat a = 5.0;\nchar a = \"a\";\n```\n:::\n\n\nWhile TensorFlow tries to infer the type dynamically, you must often state it explicitly.\nCommon important types: \n\n* float32 (floating point number with 32 bits, \"single precision\")\n* float64 (floating point number with 64 bits, \"double precision\")\n* int8 (integer with 8 bits)\n\nThe reason why TensorFlow is so explicit about types is that many GPUs (e.g. the NVIDIA GeForces) can handle only up to 32 bit numbers! (you do not need high precision in graphical modeling)\n\nBut let us see in practice what we have to do with these types and how to specifcy them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\n```\n:::\n\n\nSo what went wrong here? We tried to divide a float32 by a float64 number, but we can only divide numbers of the same type!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\n```\n:::\n\n\nWe can also specify the type of the object by providing an object e.g. tf$float64.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\n```\n:::\n\n\nIn TensorFlow, arguments often require exact/explicit data types:\nTensorFlow often expects integers as arguments. In R however an integer is normally saved as float. \nThus, we have to use an \"L\" after an integer to tell the R interpreter that it should be treated as an integer:\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()\n```\n:::\n\n\nSkipping the \"L\" is one of the most common errors when using R-TensorFlow!\n\n### Exercises\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: TensorFlow Operations\n\nTo run TensorFlow from R, note that you can access the different mathematical operations in TensorFlow via tf\\$..., e.g. there is a tf\\$math\\$... for all common math operations or the tf\\$linalg\\$... for different linear algebra operations.\nTip: type tf\\$ and then hit the tab key to list all available options (sometimes you have to do this directly in the console).\n\nAn example: How to get the maximum value of a vector?\n\nAn example: How to get the maximum value of a vector?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\nmax(y)  # Float!\ntf$math$reduce_max(y) # Float!\n```\n:::\n\n\nRewrite the following expressions (a to g) in TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# b)\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\n# c) Tip: Use Google!\nwhich.max(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# d) \nwhich.min(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n\n```{.r .cell-code}\n# e) Tip: Use Google! \norder(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n```\n:::\n\n```{.r .cell-code}\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n```\n:::\n\n```{.r .cell-code}\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)\t# Test for equality.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(1, shape=(), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$math$reduce_min(y) # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(1.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\ntf$math$reduce_mean(x)  # Integer!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(50, shape=(), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$math$reduce_mean(y)  # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(50.5, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\n# c)    which.max(x)\ntf$argmax(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(0, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\ntf$argmax(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(0, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\n# d)    which.min(x)\ntf$argmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(99, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\n# e)    order(x)\ntf$argsort(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n  3  2  1  0], shape=(100), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n  10.433675  10.103772   9.608109   8.582045 ]\n [11.311237  11.200746  11.076511  10.934624  10.769221  10.570932\n  10.323349   9.993557   9.498147   8.473241 ]\n [11.186988  11.076511  10.952296  10.810434  10.645068  10.446829\n  10.199324   9.869672   9.374583   8.351139 ]\n [11.045079  10.934624  10.810434  10.668607  10.503285  10.305112\n  10.05771    9.728241   9.233568   8.212026 ]\n [10.87965   10.769221  10.645068  10.503285  10.338026  10.139942\n   9.892679   9.563459   9.069353   8.0503845]\n [10.68132   10.570932  10.446829  10.305112  10.139942   9.941987\n   9.694924   9.366061   8.872767   7.857481 ]\n [10.433675  10.323349  10.199324  10.05771    9.892679   9.694924\n   9.448175   9.119868   8.62784    7.6182513]\n [10.103772   9.993557   9.869672   9.728241   9.563459   9.366061\n   9.119868   8.79255    8.302762   7.30317  ]\n [ 9.608109   9.498147   9.374583   9.233568   9.069353   8.872767\n   8.62784    8.302762   7.818028   6.8405466]\n [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n   7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(True, shape=(), dtype=bool)\n```\n:::\n:::\n\n\n\n</div>\n\n\n:::\n\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Runtime\n\nThis exercise compares the speed of R to TensorFlow.\nThe first exercise is to rewrite the following function in TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n\nHere, we provide a skeleton for a TensorFlow function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n```\n:::\n\n\nWe can compare the speed using the Microbenchmark package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n```\n:::\n\n\nTry different matrix sizes for the test matrix and compare the speed.\n\nTip: Have a look at the the tf.reduce_mean documentation and the \"axis\" argument.\n\n<br/>\n\nCompare the following with different matrix sizes:\n\n* test = matrix(0.0, 1000L, 500L)\n* testTF = tf\\$constant(test)\n\nAlso try the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\n```\n:::\n\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in microbenchmark::microbenchmark(do_something_R(test),\ndo_something_TF(test)): less accurate nanosecond times to avoid potential\ninteger overflows\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                  expr     min       lq     mean   median       uq      max\n  do_something_R(test) 261.334 281.0345 307.6406 303.6050 313.5270 1085.680\n do_something_TF(test) 602.659 626.2750 695.9266 657.4965 676.0285 3533.708\n neval cld\n   100  a \n   100   b\n```\n:::\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                  expr      min       lq     mean   median       uq       max\n  do_something_R(test) 5.164442 5.704391 7.705723 6.723959 8.316830 67.702521\n do_something_TF(test) 1.294575 1.577004 1.871683 1.734915 1.975093  5.039146\n neval cld\n   100  a \n   100   b\n```\n:::\n:::\n\n\nWhy is R faster (the first time)?\n\n* a) The R functions we used (apply, mean, \"-\") are also implemented in C.\n* b) The problem is not large enough and TensorFlow has an overhead.\n\n<br/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                                    expr       min         lq       mean\n tf$matmul(testTF, tf$transpose(testTF))   7.07619   7.375777   8.019691\n                        test %*% t(test) 161.22077 162.531503 164.182029\n     median         uq       max neval cld\n   7.577148   8.507972  11.41071   100  a \n 163.291725 164.431669 226.62451   100   b\n```\n:::\n:::\n\n\n\n</div>\n\n\n:::\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Linear Algebra\n\n\nGoogle to find out how to write the following expressions in TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n```\n:::\n\n```{.r .cell-code}\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3\n```\n:::\n\n```{.r .cell-code}\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n```\n:::\n\n```{.r .cell-code}\n# l)\neigen(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n```\n:::\n\n```{.r .cell-code}\n# m)\ndet(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.          0.         -0.66666667]\n [-1.          0.5        -0.16666667]\n [ 0.          0.          0.33333333]], shape=(3, 3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# j)    diag(A)\ntf$linalg$diag_part(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 0. 0.]\n [0. 2. 0.]\n [0. 0. 3.]], shape=(3, 3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# l)    eigen(A)\ntf$linalg$eigh(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntf.Tensor([-0.56155281  3.          3.56155281], shape=(3), dtype=float64)\n\n[[2]]\ntf.Tensor(\n[[-0.78820544  0.         -0.61541221]\n [ 0.61541221  0.         -0.78820544]\n [ 0.          1.         -0.        ]], shape=(3, 3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# m)    det(A)\ntf$linalg$det(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6.0, shape=(), dtype=float64)\n```\n:::\n:::\n\n\n\n</div>\n\n\n:::\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Automatic differentation\n\n\nTensorFlow supports automatic differentiation (analytical and not numerical!). \nLet's have a look at the function $f(x) = 5 x^2 + 3$ with derivative $f'(x) = 10x$.\nSo for $f'(5)$ we will get $10$.\n\nLet's do this in TensorFlow. Define the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\n```\n:::\n\n\nWe want to calculate the derivative for $x = 2.0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = tf$constant(2.0)\n```\n:::\n\n\nTo do automatic differentiation, we have to forward $x$ through the function within the tf\\$GradientTape() environment. We have also have to tell TensorFlow which value to \"watch\":\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n```\n:::\n\n\nTo print the gradient:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(tape$gradient(y, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(20.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nWe can also calculate the second order derivative $f''(x) = 10$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(10.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nWhat is happening here? Think about and discuss it.\n\n\nA more advanced example: *Linear regression*\n\nIn this case we first simulate data following $\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}$  ($\\boldsymbol{\\epsilon}$ follows a normal distribution == error).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n```\n:::\n\n\nIn R we would do the following to fit a linear regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y~x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   <2e-16 ***\nx2          2.741690   0.025328 108.249   <2e-16 ***\nx3          1.179181   0.023644  49.872   <2e-16 ***\nx4          0.591873   0.025154  23.530   <2e-16 ***\nx5          2.302417   0.022575 101.991   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,\tAdjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLet's build our own model in TensorFlow.\nHere, we use now the variable data container type (remember they are mutable and we need this type for the weights ($\\boldsymbol{w}$) of the regression model). We want our model to learn these weights.\n\nThe input (predictors, independent variables or features, $\\boldsymbol{X}$) and the observed (response, $\\boldsymbol{y}$) are constant and will not be learned/optimized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(5, 1) dtype=float64, numpy=\narray([[2.19290567],\n       [2.74534135],\n       [1.17146559],\n       [0.58811305],\n       [2.30174941]])>\n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  2.217 2.719 1.165 0.593 2.303 \n```\n:::\n:::\n\n\nDiscuss the code, go through the code line by line and try to understand it.\n\nAdditional exercise:\n\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, list(wTF, interceptTF))\n  optimizer$apply_gradients(purrr::transpose(list(grads, list(wTF, interceptTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(3, 1) dtype=float64, numpy=\narray([[2.48089253],\n       [2.47586968],\n       [1.00278615]])>\n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  2.47 2.465 1.003 \n```\n:::\n\n```{.r .cell-code}\nk_print_tensor(interceptTF, message = \"Resulting intercept:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(1, 1) dtype=float64, numpy=array([[4.15394202]])>\n```\n:::\n\n```{.r .cell-code}\ncat(\"Original intercept: \", intercept, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal intercept:  4.09 \n```\n:::\n:::\n\n\n\n</div>\n\n\n:::\n\n\n\n## Introduction to PyTorch\n  \nPyTorch is another famous library for deep learning. Like TensorFlow, Torch itself is written in C++ with an API for Python. In 2020, the RStudio team released R-Torch, and while R-TensorFlow calls the Python API in the background, the R-Torch API is built directly on the C++ Torch library! \n  \nUseful links:\n\n* <a href=\"https://pytorch.org/docs/stable/index.html\" target=\"_blank\" rel=\"noopener\">PyTorch documentation</a> (This is for the Python API, bust just replace the \".\" with \"$\".)\n* <a href=\"https://torch.mlverse.org/\" target=\"_blank\" rel=\"noopener\">R-Torch website</a>\n\n\nTo get started with Torch, we have to load the library and check if the installation worked. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n```\n:::\n\n\n\n### Data Containers\n\nUnlike TensorFlow, Torch doesn't have two data containers for mutable and immutable variables. All variables are initialized via the torch_tensor function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = torch_tensor(1.)\n```\n:::\n\n\nTo mark variables as mutable (and to track their operations for automatic differentiation) we have to set the argument 'requires_grad' to true in the torch_tensor function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)\n```\n:::\n\n\n\n### Basic Operations\n\nWe now can define the variables and do some math with them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nc = a$add(b)\nprint(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nThe R-Torch package provides all common R methods (an advantage over TensorFlow).\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nprint(a/b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 0.5000\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nprint(a*b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 50\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nTheir operators also automatically transform R numbers into tensors when attempting to add a tensor to a R number:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nAs for TensorFlow, we have to explicitly transform the tensors back to R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"torch_tensor\" \"R7\"          \n```\n:::\n\n```{.r .cell-code}\nclass(as.numeric(d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"numeric\"\n```\n:::\n:::\n\n\n\n### Data Types\n\nSimilar to TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b \n```\n:::\n\n\nBut here's a difference! With TensorFlow we would get an error, but with R-Torch, m is automatically casted to a double (float64). However, this is still bad practice!\n  \nDuring the course we will try to provide the corresponding PyTorch code snippets for all Keras/TensorFlow examples.\n\n### Exercises\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Torch Operations\n\n\nRewrite the following expressions (a to g) in torch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# b)\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\n# c) Tip: Use Google!\nwhich.max(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# d) \nwhich.min(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n\n```{.r .cell-code}\n# e) Tip: Use Google! \norder(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n```\n:::\n\n```{.r .cell-code}\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n```\n:::\n\n```{.r .cell-code}\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)\t# Test for equality.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\ntorch_min(y) # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPUFloatType{} ]\n```\n:::\n\n```{.r .cell-code}\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n```\n:::\n\n```{.r .cell-code}\ntorch_mean(y)  # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n```\n:::\n\n```{.r .cell-code}\n# c)    which.max(x)\ntorch_argmax(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\ntorch_argmax(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\n# d)    which.min(x)\ntorch_argmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n100\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\n# e)    order(x)\ntorch_argsort(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 100\n  99\n  98\n  97\n  96\n  95\n  94\n  93\n  92\n  91\n  90\n  89\n  88\n  87\n  86\n  85\n  84\n  83\n  82\n  81\n  80\n  79\n  78\n  77\n  76\n  75\n  74\n  73\n  72\n  71\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{100} ]\n```\n:::\n\n```{.r .cell-code}\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n 4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n 4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n 4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n 4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n 3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n 3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n 3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n 2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n 2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n[ CPUFloatType{10,10} ]\n```\n:::\n\n```{.r .cell-code}\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1\n[ CPUBoolType{1} ]\n```\n:::\n:::\n\n\n</div>\n\n\n:::\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Runtime\n\n1.  What is the meaning of \"An effect is not significant\"?\n2.  Is an effect with three \\*\\*\\* more significant / certain than an effect with one \\*?\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\nThis exercise compares the speed of R to torch\nThe first exercise is to rewrite the following function in torch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n\nHere, we provide a skeleton for a TensorFlow function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n```\n:::\n\n\nWe can compare the speed using the Microbenchmark package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n```\n:::\n\n\nTry different matrix sizes for the test matrix and compare the speed.\n\nTip: Have a look at the the torch_mean documentation and the \"dim\" argument.\n\n<br/>\n\nCompare the following with different matrix sizes:\n\n* test = matrix(0.0, 1000L, 500L)\n* testTorch = torch_tensor(test)\n\nAlso try the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n```\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                     expr     min      lq     mean   median       uq      max\n     do_something_R(test) 260.473 276.627 294.3796 283.3510 291.4280 1100.932\n do_something_torch(test)  63.591  70.602 125.7531  75.4195  83.7425 3764.866\n neval cld\n   100  a \n   100   b\n```\n:::\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                     expr      min       lq     mean   median       uq\n     do_something_R(test) 5463.250 5757.138 7600.090 5965.705 9320.612\n do_something_torch(test)  944.517 1308.412 1737.009 1451.626 1690.041\n       max neval cld\n 28450.515   100  a \n  8954.482   100   b\n```\n:::\n:::\n\n\nWhy is R faster (the first time)?\n\n* a) The R functions we used (apply, mean, \"-\") are also implemented in C.\n* b) The problem is not large enough and torch has an overhead.\n\n<br/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                                   expr        min         lq       mean\n torch_matmul(testTorch, testTorch$t())   1.053618   1.250398   1.690949\n                       test %*% t(test) 163.971054 164.814608 169.907867\n     median         uq        max neval cld\n   1.384242   1.884298   5.823066   100  a \n 166.669326 169.065325 247.873946   100   b\n```\n:::\n:::\n\n\n\n</div>\n\n\n:::\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Linear Algebra\n\nGoogle to find out how to write the following tasks in torch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n```\n:::\n\n```{.r .cell-code}\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3\n```\n:::\n\n```{.r .cell-code}\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n```\n:::\n\n```{.r .cell-code}\n# l)\neigen(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n```\n:::\n\n```{.r .cell-code}\n# m)\ndet(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1.0000 -0.0000 -0.6667\n-1.0000  0.5000 -0.1667\n 0.0000  0.0000  0.3333\n[ CPUFloatType{3,3} ]\n```\n:::\n\n```{.r .cell-code}\n# j)    diag(A)\ntorch_diag(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1\n 2\n 3\n[ CPUFloatType{3} ]\n```\n:::\n\n```{.r .cell-code}\n# k)    diag(diag(A))\ntorch_diag(A)$diag()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1  0  0\n 0  2  0\n 0  0  3\n[ CPUFloatType{3,3} ]\n```\n:::\n\n```{.r .cell-code}\n# l)    eigen(A)\nlinalg_eigh(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntorch_tensor\n-0.5616\n 3.0000\n 3.5616\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n-0.7882  0.0000  0.6154\n 0.6154  0.0000  0.7882\n 0.0000  1.0000  0.0000\n[ CPUFloatType{3,3} ]\n```\n:::\n\n```{.r .cell-code}\n# m)    det(A)\nlinalg_det(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n6\n[ CPUFloatType{} ]\n```\n:::\n:::\n\n\n\n</div>\n\n\n:::\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Automatic differentation\n\nTorch supports automatic differentiation (analytical and not numerical!). \nLet's have a look at the function $f(x) = 5 x^2 + 3$ with derivative $f'(x) = 10x$.\nSo for $f'(5)$ we will get $10$.\n\nLet's do this in torch Define the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\n```\n:::\n\n\nWe want to calculate the derivative for $x = 2.0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = torch_tensor(2.0, requires_grad = TRUE)\n```\n:::\n\n\nTo do automatic differentiation, we have to forward $x$ through the function and call the \\$backward() method of the result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = f(x)\ny$backward(retain_graph=TRUE )\n```\n:::\n\n\nTo print the gradient:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 20\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nWe can also calculate the second order derivative $f''(x) = 10$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 10\n[ CPUFloatType{1} ][ grad_fn = <MulBackward0> ]\n```\n:::\n:::\n\n\nWhat is happening here? Think about and discuss it.\n\n\nA more advanced example: *Linear regression*\n\nIn this case we first simulate data following $\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}$  ($\\boldsymbol{\\epsilon}$ follows a normal distribution == error).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n```\n:::\n\n\nIn R we would do the following to fit a linear regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y~x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   <2e-16 ***\nx2          2.741690   0.025328 108.249   <2e-16 ***\nx3          1.179181   0.023644  49.872   <2e-16 ***\nx4          0.591873   0.025154  23.530   <2e-16 ***\nx5          2.302417   0.022575 101.991   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,\tAdjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLet's build our own model in TensorFlow.\nHere, we use now the variable data container type (remember they are mutable and we need this type for the weights ($\\boldsymbol{w}$) of the regression model). We want our model to learn these weights.\n\nThe input (predictors, independent variables or features, $\\boldsymbol{X}$) and the observed (response, $\\boldsymbol{y}$) are constant and will not be learned/optimized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Loss: 4.4065318107605\"\n[1] \"Loss: 2.37926030158997\"\n[1] \"Loss: 0.901207685470581\"\n[1] \"Loss: 0.403193712234497\"\n[1] \"Loss: 0.296265542507172\"\n[1] \"Loss: 0.268377959728241\"\n[1] \"Loss: 0.232994809746742\"\n[1] \"Loss: 0.219554677605629\"\n[1] \"Loss: 0.215328559279442\"\n[1] \"Loss: 0.213282078504562\"\n```\n:::\n\n```{.r .cell-code}\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInferred weights:  0.701 3.089 1.801 1.123 3.452 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  0.67 3.085 1.787 1.121 3.455 \n```\n:::\n:::\n\n\nDiscuss the code, go through the code line by line and try to understand it.\n\nAdditional exercise:\n\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Loss: 3.51533484458923\"\n[1] \"Loss: 1.74870145320892\"\n[1] \"Loss: 0.41416934132576\"\n[1] \"Loss: 0.518697261810303\"\n[1] \"Loss: 0.293963462114334\"\n[1] \"Loss: 0.263338804244995\"\n[1] \"Loss: 0.258341491222382\"\n[1] \"Loss: 0.254723280668259\"\n[1] \"Loss: 0.252453774213791\"\n[1] \"Loss: 0.25116890668869\"\n```\n:::\n\n```{.r .cell-code}\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInferred weights:  3.118 -0.349 2.107 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  3.131 -0.353 2.11 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInferred intercept:  2.836 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Original intercept: \", intercept, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal intercept:  2.832 \n```\n:::\n:::\n\n\n\n</div>\n\n\n:::\n\n\n\n\n## Keras/Torch Framework\n\nWe have seen that we can use TensorFlow directly out of R, and we could use this knowledge to implement a neural network in TensorFlow directly in R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us with implementing the machine learning models in TensorFlow. The most common of those is Keras.\n\nKeras is a powerful framework for building and training neural networks with a few lines of codes. Since the end of 2018, Keras and TensorFlow are completely interoperable, allowing us to utilize the best of both. \n\nThe objective of this lesson is to familiarize yourself with Keras. If you have installed TensorFlow, Keras can be found within TensorFlow: tf.keras. However, the RStudio team has built an R package on top of tf.keras, and it is more convenient to use this. To load the Keras package, type\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n# or library(torch)\n```\n:::\n\n\n\n### Example workflow in Keras / Torch\n\nTo show how Keras works, we will now build a small classifier to predict the three species of the iris data set. Load the necessary packages and data sets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(torch)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\ndata(iris)\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n\nFor neural networks, it is beneficial to scale the predictors (scaling = centering and standardization, see ?scale).\nWe also split our data into predictors (X) and response (Y = the three species).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = scale(iris[,1:4])\nY = iris[,5]\n```\n:::\n\n\nAdditionally, Keras/TensorFlow cannot handle factors and we have to create contrasts (one-hot encoding).\nTo do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like Python and C++, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Keep this in mind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY = to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    1    0    0\n[3,]    1    0    0\n[4,]    1    0    0\n[5,]    1    0    0\n[6,]    1    0    0\n```\n:::\n:::\n\n\nAfter having prepared the data, we will now see a typical workflow to specify a model in Keras/Torch. \n\n**1. Initialize a sequential model in Keras:**\n\n\n::: {.panel-tabset}\n\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = keras_model_sequential()\n```\n:::\n\n\n## Torch\n\nTorch users can skip this step.\n\n:::\n\nA sequential Keras model is a higher order type of model within Keras and consists of one input and one output model. \n\n**2. Add hidden layers to the model (we will learn more about hidden layers during the next days).**\n\nWhen specifying the hidden layers, we also have to specify the shape and a so called _activation function_. \nYou can think of the activation function as decision for what is forwarded to the next neuron (but we will learn more about it later).\nIf you want to know this topic in even more depth, consider watching the videos presented in section \\@ref(basicMath).\n\nThe shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).\n\n::: {.panel-tabset}\n\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 3L, activation = \"softmax\") \n```\n:::\n\n\n## Torch\n  \nThe Torch syntax is very similar, we will give a list of layers to the \"nn_sequential\" function. Here, we have to specify the softmax activation function as an extra layer:\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\n```\n:::\n\n\n:::\n\n* softmax scales a potential multidimensional vector to the interval $(0, 1]$ for each component. The sum of all components equals 1. This might be very useful for example for handling probabilities. **Ensure ther the labels start at 0! Otherwise the softmax function does not work well!**\n\n\n\n**3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).**\n    \nWe will learn about other options later, so for now, do not worry about the \"**learning_rate**\" (\"**lr**\" in Torch or earlier in TensorFlow) argument, cross entropy or the optimizer.\n\n::: {.panel-tabset}\n\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_3 (Dense)                    (None, 20)                      100         \n dense_2 (Dense)                    (None, 20)                      420         \n dense_1 (Dense)                    (None, 20)                      420         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 1,003\nTrainable params: 1,003\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\n## Torch\n\nSpecify optimizer and the parameters which will be trained (in our case the parameters of the network):\n    \n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n```\n:::\n\n\n:::\n\n\n\n\n**4. Fit model in 30 iterations (epochs)**\n\n::: {.panel-tabset}\n\n## Keras\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nmodel_history =\n  model %>%\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\n```\n:::\n\n  \n## Torch\n\nIn Torch, we jump directly to the training loop, however, here we have to write our own training loop:\n\n1. Get a batch of data.\n2. Predict on batch.\n3. Ccalculate loss between predictions and true labels.\n4. Backpropagate error.\n5. Update weights.\n6. Go to step 1 and repeat.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n:::\n\n\n\n**5. Plot training history:**\n\n::: {.panel-tabset}\n\n## Keras\n \n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_72-1.png){width=672}\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_73-1.png){width=672}\n:::\n:::\n\n\n:::\n\n\n**6. Create predictions:**\n\n::: {.panel-tabset}\n\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = predict(model, X) # Probabilities for each class.\n```\n:::\n\n  \nGet probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predictions) # Quasi-probabilities for each species.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]        [,2]         [,3]\n[1,] 0.9915600 0.006817889 0.0016221496\n[2,] 0.9584184 0.037489697 0.0040918575\n[3,] 0.9910416 0.007848956 0.0011094128\n[4,] 0.9813542 0.016901711 0.0017440914\n[5,] 0.9949830 0.004031503 0.0009855649\n[6,] 0.9905725 0.006884387 0.0025430375\n```\n:::\n:::\n\n  \nFor each plant, we want to know for which species we got the highest probability:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds = apply(predictions, 1, which.max) \nprint(preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 2 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n:::\n:::\n\n    \n:::\n\n\n**7. Calculate Accuracy (how often we have been correct):**\n\n::: {.panel-tabset}\n\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9066667\n```\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds_torch == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.98\n```\n:::\n:::\n\n\n:::\n\n**8. Plot predictions, to see if we have done a good job:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_79-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(oldpar)   # Reset par.\n```\n:::\n\n\nSo you see, building a neural network is very easy with Keras or Torch and you can already do it on your own.\n\n### Exercises\n\n\n```{=html}\n  <hr/>\n  <strong><span style=\"color: #0011AA; font-size:18px;\">1. Task</span></strong><br/>\n```\n\n\n\nWe will now build a regression for the airquality data set with Keras/Torch. We want to predict the variable \"Ozone\". \n\nTasks:\n1. Complete the steps and the code chunk so that the model is successfully trained! \n2. Try different learning rates and neural network sizes (increase/decrease number of hidden layers and neurons (units) in each layer). What happes?\n\n\n::::: {.panelset}\n\n::: {.panel}\n[Keras]{.panel-name}\n\n0. Load and prepare the data set:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\ndata = airquality\n```\n:::\n\n\nExplore the data with summary() and plot():\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n:::\n\n```{.r .cell-code}\nplot(data)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_27-1.png){width=672}\n:::\n:::\n\n\n1. There are NAs in the data, which we have to remove because Keras cannot handle NAs.\nIf you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").\n\n2. Split the data in predictors ($\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.\n\n3. Build a sequential Keras model.\n\n4. Add hidden layers (input and output layer are already specified, you have to add hidden layers between them):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\n* Why do we use 5L as input shape?\n* Why only one output node and \"linear\" activation layer?\n\n5. Compile model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n```\n:::\n\n\nWhat is the \"mean_squared_error\" loss?\n\n6. Fit model:\n\nTip: Only matrices are accepted for $\\boldsymbol{X}$ and $\\boldsymbol{y}$ by Keras. R often drops a one column matrix into a vector (change it back to a matrix!)\n\n7. Plot training history.\n\n8. Create predictions.\n\n9. Compare your Keras model with a linear model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\nprint(rmse_keras)\n```\n:::\n\n\n:::\n\n::: {.panel}\n[Torch]{.panel-name}\n\n0. Load and prepare the data set:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\n```\n:::\n\n\nExplore the data with summary() and plot():\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n:::\n\n```{.r .cell-code}\nplot(data)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_27_torch-1.png){width=672}\n:::\n:::\n\n\n1. There are NAs in the data, which we have to remove because Keras cannot handle NAs.\nIf you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").\n\n2. Split the data in predictors ($\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.\n\n3. Pass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    ...\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\n* Why do we use 5L as input shape?\n* Why only one output node and no activation layer?\n\n4. Create optimizer\n\nWe have to pass the network's parameters to the optimizer (how is this different to keras?)\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n```\n:::\n\n\n5. Fit model\n\nIn torch we have to write the trainings loop on our own. Complete the trainings loop:\n\nTips: \n\n- Number of training $ steps = Number of rows / batchsize * Epochs $\n- Search torch::nnf_... for the correct loss function (mse...)\n- Make sure that X_torch and Y_torch have the same data type! (you can set the dtype via torch_tensor(..., dtype = ...))\n_ Check the dimension of Y_torch, we need a matrix!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = ...\nbatch_size = 32\nsteps = ...\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, ...) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = ...\n  Y_batch = ...\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = ...\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n6. Plot training history.\n\n7. Create predictions.\n\n8. Compare your Torch model with a linear model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\nprint(rmse_torch)\n```\n:::\n\n\n:::\n\n:::::\n\n\n\n```{=html}\n  <details>\n    <summary>\n      <strong><span style=\"color: #0011AA; font-size:18px;\">Solution</span></strong>\n    </summary>\n    <p>\n```\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = airquality\n```\n:::\n\n\n**1. There are NAs in the data, which we have to remove because Keras and Torch cannot handle NAs!**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n```\n:::\n:::\n\n\n**2. Split the data in predictors and response and scale the matrix.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = scale(data[,2:6])\ny = data[,1]\n```\n:::\n\n\n::::: {.panelset}\n\n::: {.panel}\n[Keras]{.panel-name}\n\n\n**3. Build sequential Keras model.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nmodel = keras_model_sequential()\n```\n:::\n\n\n**4. Add hidden layers (input and output layer are already specified, you have to add hidden layers between them).**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\nWe use 5L as input shape, because we have 5 predictors. Analogously, we use 1L for our 1d response.\nBecause we do not want any compression, dilation or other nonlinear effects, we use the simple linear layer (equal to no activation function at all). For more about activation functions, look for example <a href=\"https://en.wikipedia.org/wiki/Activation_function\" target=\"_blank\" rel=\"noopener\">here</a>. Or wait for the next days.\nYou may also have seen the previously shown link <a href=\"https://mlfromscratch.com/activation-functions-explained/#/\" target=\"_blank\" rel=\"noopener\">about activation functions in more detail</a>.\n\n**5. Compile model.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n```\n:::\n\n\nThe mean_squared_error is the ordinary least squares approach in regression analysis.\n\n**6. Fit model.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n```\n:::\n\n\n**7. Plot training history.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_38-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel %>%\n  evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss \n147.5745 \n```\n:::\n:::\n\n\n**8. Create predictions.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_keras = predict(model, x)\n```\n:::\n\n\n**9. Compare Keras model with a linear model.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.067499\n```\n:::\n:::\n\n\n:::\n\n::: {.panel}\n[Torch]{.panel-name}\n\n**3. Pass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\nWe use 5L as input shape, because we have 5 predictors. Analogously, we use 1L for our 1d response.\nBecause we do not want any compression, dilation or other nonlinear effects, we use the simple linear layer (equal to no activation function at all). For more about activation functions, look for example <a href=\"https://en.wikipedia.org/wiki/Activation_function\" target=\"_blank\" rel=\"noopener\">here</a>. Or wait for the next days.\nYou may also have seen the previously shown link <a href=\"https://mlfromscratch.com/activation-functions-explained/#/\" target=\"_blank\" rel=\"noopener\">about activation functions in more detail</a>.\n\n**4. Create optimizer**\n\nWe have to pass the network's parameters to the optimizer (how is this different to keras?)\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n```\n:::\n\n\nIn keras we use the compile function to pass a optimizer and a loss function to the model whereas in torch we have to pass the network's parameters to the optimizer.\n\n**5. Fit model**\n\nIn torch we have to write the trainings loop on our own. Complete the trainings loop:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n**6. Plot training history.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n```\n:::\n\n\n**7. Create predictions.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n```\n:::\n\n\n\n**8. Compare your Torch model with a linear model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\nprint(rmse_torch)\n```\n:::\n\n\n\n:::\n\n:::::\n\n**Look at this slightly more complex model and compare the loss plot and the accuracy in contrast to the former.**\n\n::::: {.panelset}\n\n::: {.panel}\n[Keras]{.panel-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_41-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel %>%\n  evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss \n210.4453 \n```\n:::\n\n```{.r .cell-code}\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.51122\n```\n:::\n:::\n\n\n:::\n\n::: {.panel}\n[Torch]{.panel-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_41_torch-1.png){width=672}\n:::\n\n```{.r .cell-code}\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.12053\n```\n:::\n:::\n\n\n\n:::\n:::::\n\n\nYou see, the more complex model works better, because it can learn the functional form between the features and the response better (if necessary).\nBut keep the overfitting problem in mind!\n\n**Look at the little change in learning rates for the next 2 models and compare the loss plot and the accuracy in contrast to the former.**\n\n\n::::: {.panelset}\n\n::: {.panel}\n[Keras]{.panel-name}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_42-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel %>%\n  evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss \n56.70872 \n```\n:::\n\n```{.r .cell-code}\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.661808\n```\n:::\n:::\n\n\n\n:::\n\n::: {.panel}\n[Torch]{.panel-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.1)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_43_torch-1.png){width=672}\n:::\n\n```{.r .cell-code}\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.461597\n```\n:::\n:::\n\n\n\n\n:::\n:::::\n\n\nYou can see, the higher learning rate yields a little bit worse results. The optimum is jumped over.\n\n::::: {.panelset}\n\n::: {.panel}\n[Keras]{.panel-name}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_43-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel %>%\n  evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    loss \n340.8205 \n```\n:::\n\n```{.r .cell-code}\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13.18632\n```\n:::\n:::\n\n\n:::\n\n::: {.panel}\n[Torch]{.panel-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_442_torch-1.png){width=672}\n:::\n\n```{.r .cell-code}\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 12.48754\n```\n:::\n:::\n\n\n\n:::\n\n:::::\n\nYou can see, that for the lower learning rate, the optimum (compared to the run with learning rate 0.05) is not yet reached (to few epochs have gone by).\nBut also here, mind the overfitting problem. For too many epochs, things might get worse!\n\n\n```{=html}\n    </p>\n  </details>\n  <br/>\n```\n\n```{=html}\n  <hr/>\n  <strong><span style=\"color: #0011AA; font-size:18px;\">2. Task</span></strong><br/>\n```\n\n\n\nThe next task differs for Torch and Keras users. Keras users will learn more about the inner working of training while Torch users will learn how to simplify and generalize the training loop.\n\n::::: {.panelset}\n\n::: {.panel}\n[Keras]{.panel-name}\n\nSimilar to Torch, here we will write the training loop ourselves in the following. The training loop consists of several steps:\n\n1. Sample batches of X and Y data\n2. Open the gradientTape to create a computational graph (autodiff)\n3. Make predictions and calculate loss\n4. Update parameters based on the gradients at the loss (go back to 1. and repeat)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$bX\n        Solar.R        Wind        Temp      Month        Day\n87  -1.13877323 -0.37654514  0.44147123 -0.1467431  1.1546835\n117  0.58361881 -1.83815816  0.33653910  0.5319436  1.0398360\n129 -1.01809608  1.56290291  0.65133550  1.2106304 -1.1422676\n121  0.44100036 -2.14734553  1.70065685  0.5319436  1.4992262\n91   0.74817856 -0.71384046  0.54640337 -0.1467431  1.6140738\n137 -1.76410028  0.26993754 -0.71278225  1.2106304 -0.2234871\n21  -1.93963068 -0.06735777 -1.97196786 -1.5041165  0.5804458\n141 -1.73118833  0.10128988 -0.18812157  1.2106304  0.2359031\n78   0.97856221  0.10128988  0.44147123 -0.1467431  0.1210555\n15  -1.31430363  0.91642022 -2.07689999 -1.5041165 -0.1086396\n38  -0.63412333 -0.06735777  0.44147123 -0.8254298 -1.0274200\n49  -1.62148183 -0.20789749 -1.34237505 -0.8254298  0.2359031\n123  0.03508631 -1.02302783  1.70065685  0.5319436  1.7289213\n136  0.58361881 -1.02302783 -0.08318944  1.2106304 -0.3383347\n120  0.19964606 -0.06735777  2.01545325  0.5319436  1.3843787\n114 -1.63245248  1.22560759 -0.60785011  0.5319436  0.6952933\n145 -1.87380678 -0.20789749 -0.71278225  1.2106304  0.6952933\n140  0.43002971  1.08506788 -1.13251078  1.2106304  0.1210555\n64   0.56167751 -0.20789749  0.33653910 -0.1467431 -1.4868103\n118  0.33129386 -0.54519280  0.86119977  0.5319436  1.1546835\n128 -0.98518413 -0.71384046  0.96613190  1.2106304 -1.2571152\n62   0.92370896 -1.64140257  0.65133550 -0.1467431 -1.7165054\n125  0.13382216 -1.36032314  1.49079258  1.2106304 -1.6016578\n4    1.40641756  0.43858520 -1.65717146 -1.5041165 -1.3719627\n79   1.09923936 -1.02302783  0.65133550 -0.1467431  0.2359031\n82  -1.95060133 -0.85438017 -0.39798584 -0.1467431  0.5804458\n149  0.08993956 -0.85438017 -0.81771438  1.2106304  1.1546835\n17   1.34059366  0.57912491 -1.23744292 -1.5041165  0.1210555\n48   1.08826871  3.02451593 -0.60785011 -0.8254298  0.1210555\n130  0.73720791  0.26993754  0.23160696  1.2106304 -1.0274200\n132  0.49585361  0.26993754 -0.29305371  1.2106304 -0.7977249\n30   0.41905906 -1.19167548  0.12667483 -1.5041165  1.6140738\n\n$bY\n [1]  20 168  32 118  64   9   1  13  35  18  29  20  85  28  76   9  23  18  32\n[20]  73  47 135  78  18  61  16  30  34  37  20  21 115\n```\n:::\n\n```{.r .cell-code}\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss:  1444.033 \nLoss:  488.953 \nLoss:  270.0465 \nLoss:  450.0282 \nLoss:  138.2488 \nLoss:  227.6001 \nLoss:  216.2361 \nLoss:  109.9781 \nLoss:  352.7486 \nLoss:  239.2065 \nLoss:  234.0703 \nLoss:  224.0462 \nLoss:  227.475 \nLoss:  336.5538 \nLoss:  348.1582 \nLoss:  158.787 \nLoss:  209.5738 \nLoss:  321.0661 \nLoss:  232.6139 \nLoss:  289.6932 \n```\n:::\n:::\n\n\n:::\n\n::: {.panel}\n[Torch]{.panel-name}\n\nKeras and Torch use dataloaders to generate the data batches. Dataloaders are objects that return batches of data infinetly. Keras create the dataloader object automatically in the fit function, in Torch we have to write them ourselves:\n\n1. Define a dataset object. This object informs the dataloader function about the inputs, outputs, length (nrow), and how to sample from it. \n2. Create an instance of the dataset object by calling it and passing the actual data to it\n3. Pass the initiated dataset to the dataloader function \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = matrix(data[,1], ncol = 1L)\n\n\ntorch_dataset = torch::dataset(\n    name = \"airquality\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n```\n:::\n\n\nOur dataloader is again an object which has to be initiated. The initiated object returns a list of two elements, batch x and batch y. The initated object stops returning batches when the dataset was completly transversed (no worries, we don't have to all of this ourselves).\n\nOur training has also changed now:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = nn_sequential(\n  nn_linear(5L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 1L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 10: 387.950073\nLoss at epoch 20: 282.698288\nLoss at epoch 30: 257.855043\nLoss at epoch 40: 244.420750\nLoss at epoch 50: 217.362108\n```\n:::\n\n```{.r .cell-code}\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_45_torch-1.png){width=672}\n:::\n:::\n\n\nThe previous sampling wasn't ideal, why?\n\n:::\n\n:::::\n\n\nNow change the code from above for the iris data set.\nTip: In tf\\$keras\\$losses\\$... you can find various loss functions.\n\n\n```{=html}\n  <details>\n    <summary>\n      <strong><span style=\"color: #0011AA; font-size:18px;\">Solution</span></strong>\n    </summary>\n    <p>\n```\n\n\n::::: {.panelset}\n\n::: {.panel}\n[Keras]{.panel-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss:  0.002633849 \nLoss:  0.0005500487 \nLoss:  0.001006462 \nLoss:  0.0001315936 \nLoss:  0.0004843124 \nLoss:  0.0004023896 \nLoss:  0.0004356128 \nLoss:  0.000235351 \nLoss:  4.823796e-05 \nLoss:  0.0001512702 \nLoss:  0.0002624761 \nLoss:  0.0001274793 \nLoss:  7.111725e-05 \nLoss:  0.0001509234 \nLoss:  0.0002024032 \nLoss:  0.0001532886 \nLoss:  9.489701e-05 \nLoss:  0.0001040314 \nLoss:  7.334561e-05 \nLoss:  2.743953e-05 \nLoss:  9.655961e-05 \nLoss:  2.361947e-05 \nLoss:  6.918395e-05 \nLoss:  1.603245e-05 \nLoss:  1.772152e-05 \nLoss:  2.512357e-05 \n```\n:::\n:::\n\n\n\n:::\n\n::: {.panel}\n[Torch]{.panel-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = as.integer(iris$Species)\n\n\ntorch_dataset = torch::dataset(\n    name = \"iris\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(4L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 3L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 10: 16.298814\nLoss at epoch 20: 8.492696\nLoss at epoch 30: 5.744957\nLoss at epoch 40: 4.344102\nLoss at epoch 50: 3.493563\n```\n:::\n\n```{.r .cell-code}\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C1-TensorFlow_files/figure-html/chunk_chapter3_task_47_torch-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n:::::\n\n\nRemarks:\n\n* Mind the different input and output layer numbers.\n* The loss function increases randomly, because different subsets of the data were drawn.\nThis is a downside of stochastic gradient descent.\n* A positive thing about stochastic gradient descent is, that local valleys or hills may be left and global ones can be found instead.\n\n\n\n```{=html}\n    </p>\n  </details>\n  <br/><hr/>\n```\n\n\n\n\n## Underlying mathematical concepts - optional {#basicMath}\n\nIf are not yet familiar with the underlying concepts of neural networks and want to know more about that, it is suggested to read / view the following videos / sites. Consider the Links and videos with descriptions in parentheses as optional bonus.\n\n_**This might be useful to understand the further concepts in more depth.**_\n\n* (<a href=\"https://en.wikipedia.org/wiki/Newton%27s_method#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Newton%27s_method#Description</a> (Especially the animated graphic is interesting).)\n\n* <a href=\"https://en.wikipedia.org/wiki/Gradient_descent#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Gradient_descent#Description</a>\n\n* <a href=\"https://mlfromscratch.com/neural-networks-explained/#/\" target=\"_blank\" rel=\"noopener\">Neural networks (Backpropagation, etc.)</a>.\n\n* <a href=\"https://mlfromscratch.com/activation-functions-explained/#/\" target=\"_blank\" rel=\"noopener\">Activation functions in detail</a> (requires the above as prerequisite).\n\n_**Videos about the topic**_:\n\n* **Gradient descent explained**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/sDv4f4s2SB8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n* (Stochastic gradient descent explained)\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/vMh0zPT0tLI\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n* (Entropy explained)\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/YtebGVx-Fxw\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n* **Short explanation of entropy, cross entropy and Kullback–Leibler divergence**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/ErfnhcEV1O8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n* **Deep Learning (chapter 1)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/aircAruvnKk\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n* **How neural networks learn - Deep Learning (chapter 2)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/IHZwWFHWa-w\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n* **Backpropagation - Deep Learning (chapter 3)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n* **Another video about backpropagation (extends the previous one) - Deep Learning (chapter 4)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/tIeHLnjs5U8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n### Caveats of neural network optimization\n\nDepending on activation functions, it might occur that the network won't get updated, even with high learning rates (called *vanishing gradient*, especially for \"sigmoid\" functions).\nFurthermore, updates might overshoot (called *exploding gradients*) or activation functions will result in many zeros (especially for \"relu\", *dying relu*).\n\nIn general, the first layers of a network tend to learn (much) more slowly than subsequent ones.\n\n\n",
    "supporting": [
      "C1-TensorFlow_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}