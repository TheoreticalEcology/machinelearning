{
  "hash": "a33bd12ea79dd5669a0454a5f6a6bd9d",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n# Introduction to TensorFlow and Keras {#sec-tensorflowintro}\n\n## Introduction to TensorFlow\n\nOne of the most commonly used frameworks for machine learning is **TensorFlow**. TensorFlow is an open source <a href=\"https://en.wikipedia.org/wiki/Linear_algebra\" target=\"_blank\" rel=\"noopener\">linear algebra</a> library with focus on neural networks, published by Google in 2015. TensorFlow supports several interesting features, in particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization.\n\nThese advantages are nicely explained in the following video:\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/MotG3XI2qSs\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media;\n  gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\nTo sum up the most important points of the video:\n\n-   TensorFlow is a math library which is highly optimized for neural networks.\n-   If a GPU is available, computations can be easily run on the GPU but even on a CPU TensorFlow is still very fast.\n-   The \"backend\" (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for NVIDIA GPUs).\n-   The interface (the part of TensorFlow we use) is written in Python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend.\n\nAll operations in TensorFlow are written in C++ and are highly optimized. But don't worry, we don't have to use C++ to use TensorFlow because there are several bindings for other languages. TensorFlow officially supports a Python API, but meanwhile there are several community carried APIs for other languages:\n\n-   R\n-   Go\n-   Rust\n-   Swift\n-   JavaScript\n\nIn this course we will use TensorFlow with the <a href=\"https://tensorflow.rstudio.com/\" target=\"_blank\" rel=\"noopener\">https://tensorflow.rstudio.com/</a> binding, that was developed and published 2017 by the RStudio team. First, they developed an R package (reticulate) for calling Python in R. Actually, we are using the Python TensorFlow module in R (more about this later).\n\nTensorFlow offers different levels of API. We could implement a neural network completely by ourselves or we could use Keras which is provided as a submodule by TensorFlow. Keras is a powerful module for building and training neural networks. It allows us building and training neural networks in a few lines of codes. Since the end of 2018, Keras and TensorFlow are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras for neural networks but also how we can use the TensorFlow's automatic differenation for using complex objective functions.\n\nUseful links:\n\n-   <a href=\"https://www.tensorflow.org/api_docs/python/tf\" target=\"_blank\" rel=\"noopener\">TensorFlow documentation</a> (This is for the Python API, but just replace the \".\" with \"\\$\".)\n-   <a href=\"https://tensorflow.rstudio.com/\" target=\"_blank\" rel=\"noopener\">Rstudio TensorFlow website</a>\n\n### Data Containers\n\nTensorFlow has two data containers (structures):\n\n-   constant (tf\\$constant): Creates a constant (immutable) value in the computation graph.\n-   variable (tf\\$Variable): Creates a mutable value in the computation graph (used as parameter/weight in models).\n\nTo get started with TensorFlow, we have to load the library and check if the installation worked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nimmutable = tf$constant(5.0)\nmutable = tf$Variable(5.0)\n```\n:::\n\n\nDon't worry about weird messages (they will only appear once at the start of the session).\n\n### Basic Operations\n\nWe now can define the variables and do some math with them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(5.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(10.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nc = tf$add(a, b)\nprint(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(15.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\n```\n:::\n\n\nNormal R methods such as print() are provided by the R package \"tensorflow\".\n\nThe TensorFlow library (created by the RStudio team) built R methods for all common operations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\n(a+b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(15.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nTheir operators also automatically transform R numbers into constant tensors when attempting to add a tensor to an R number:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(20.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nTensorFlow containers are objects, what means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object). For instance, there is a method to transform the tensor object back to an R object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"tensorflow.tensor\"                               \n[2] \"tensorflow.python.framework.ops.EagerTensor\"     \n[3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n[4] \"tensorflow.python.framework.tensor.Tensor\"       \n[5] \"tensorflow.python.types.internal.NativeObject\"   \n[6] \"tensorflow.python.types.core.Symbol\"             \n[7] \"tensorflow.python.types.core.Value\"              \n[8] \"tensorflow.python.types.core.Tensor\"             \n[9] \"python.builtin.object\"                           \n```\n:::\n\n```{.r .cell-code}\nclass(d$numpy())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"numeric\"\n```\n:::\n\n```{.r .cell-code}\nclass(as.matrix(d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n\n### Data Types\n\nR uses dynamic typing, what means you can assign a number, character, function or whatever to a variable and the the type is automatically inferred. In other languages you have to state the type explicitly, e.g. in C:\n\n\n::: {.cell}\n\n```{.c .cell-code}\nint a = 5;\nfloat a = 5.0;\nchar a = \"a\";\n```\n:::\n\n\nWhile TensorFlow tries to infer the type dynamically, you must often state it explicitly. Common important types:\n\n-   float32 (floating point number with 32 bits, \"single precision\")\n-   float64 (floating point number with 64 bits, \"double precision\")\n-   int8 (integer with 8 bits)\n\nThe reason why TensorFlow is so explicit about types is that many GPUs (e.g. the NVIDIA GeForces) can handle only up to 32 bit numbers! (you do not need high precision in graphical modeling)\n\nBut let us see in practice what we have to do with these types and how to specifcy them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\n```\n:::\n\n\nSo what went wrong here? We tried to divide a float32 by a float64 number, but we can only divide numbers of the same type!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\n```\n:::\n\n\nWe can also specify the type of the object by providing an object e.g. tf\\$float64.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\n```\n:::\n\n\nIn TensorFlow, arguments often require exact/explicit data types: TensorFlow often expects integers as arguments. In R however an integer is normally saved as float. Thus, we have to use an \"L\" after an integer to tell the R interpreter that it should be treated as an integer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()\n```\n:::\n\n\nSkipping the \"L\" is one of the most common errors when using R-TensorFlow!\n\n### Exercises\n\n::: {.callout-caution icon=\"false\"}\n#### Question: TensorFlow Operations\n\nTo run TensorFlow from R, note that you can access the different mathematical operations in TensorFlow via tf\\$..., e.g. there is a tf\\$math\\$... for all common math operations or the tf\\$linalg\\$... for different linear algebra operations. Tip: type tf\\$ and then hit the tab key to list all available options (sometimes you have to do this directly in the console).\n\nAn example: How to get the maximum value of a vector?\n\nAn example: How to get the maximum value of a vector?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\nmax(y)  # Float!\ntf$math$reduce_max(y) # Float!\n```\n:::\n\n\nRewrite the following expressions (a to g) in TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# b)\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\n# c) Tip: Use Google!\nwhich.max(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# d) \nwhich.min(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n\n```{.r .cell-code}\n# e) Tip: Use Google! \norder(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n```\n:::\n\n```{.r .cell-code}\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n```\n:::\n\n```{.r .cell-code}\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)\t# Test for equality.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 methods overwritten by 'keras':\n  method                               from  \n  as.data.frame.keras_training_history keras3\n  plot.keras_training_history          keras3\n  print.keras_training_history         keras3\n  r_to_py.R6ClassGenerator             keras3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'keras'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:keras3':\n\n    %<-active%, %py_class%, activation_elu, activation_exponential,\n    activation_gelu, activation_hard_sigmoid, activation_linear,\n    activation_relu, activation_selu, activation_sigmoid,\n    activation_softmax, activation_softplus, activation_softsign,\n    activation_tanh, adapt, application_densenet121,\n    application_densenet169, application_densenet201,\n    application_efficientnet_b0, application_efficientnet_b1,\n    application_efficientnet_b2, application_efficientnet_b3,\n    application_efficientnet_b4, application_efficientnet_b5,\n    application_efficientnet_b6, application_efficientnet_b7,\n    application_inception_resnet_v2, application_inception_v3,\n    application_mobilenet, application_mobilenet_v2,\n    application_mobilenet_v3_large, application_mobilenet_v3_small,\n    application_nasnetlarge, application_nasnetmobile,\n    application_resnet101, application_resnet101_v2,\n    application_resnet152, application_resnet152_v2,\n    application_resnet50, application_resnet50_v2, application_vgg16,\n    application_vgg19, application_xception, bidirectional,\n    callback_backup_and_restore, callback_csv_logger,\n    callback_early_stopping, callback_lambda,\n    callback_learning_rate_scheduler, callback_model_checkpoint,\n    callback_reduce_lr_on_plateau, callback_remote_monitor,\n    callback_tensorboard, clone_model, constraint_maxnorm,\n    constraint_minmaxnorm, constraint_nonneg, constraint_unitnorm,\n    count_params, custom_metric, dataset_boston_housing,\n    dataset_cifar10, dataset_cifar100, dataset_fashion_mnist,\n    dataset_imdb, dataset_imdb_word_index, dataset_mnist,\n    dataset_reuters, dataset_reuters_word_index, freeze_weights,\n    from_config, get_config, get_file, get_layer, get_vocabulary,\n    get_weights, image_array_save, image_dataset_from_directory,\n    image_load, image_to_array, imagenet_decode_predictions,\n    imagenet_preprocess_input, initializer_constant,\n    initializer_glorot_normal, initializer_glorot_uniform,\n    initializer_he_normal, initializer_he_uniform,\n    initializer_identity, initializer_lecun_normal,\n    initializer_lecun_uniform, initializer_ones,\n    initializer_orthogonal, initializer_random_normal,\n    initializer_random_uniform, initializer_truncated_normal,\n    initializer_variance_scaling, initializer_zeros, install_keras,\n    keras, keras_model, keras_model_sequential, Layer,\n    layer_activation, layer_activation_elu,\n    layer_activation_leaky_relu, layer_activation_parametric_relu,\n    layer_activation_relu, layer_activation_softmax,\n    layer_activity_regularization, layer_add, layer_additive_attention,\n    layer_alpha_dropout, layer_attention, layer_average,\n    layer_average_pooling_1d, layer_average_pooling_2d,\n    layer_average_pooling_3d, layer_batch_normalization,\n    layer_category_encoding, layer_center_crop, layer_concatenate,\n    layer_conv_1d, layer_conv_1d_transpose, layer_conv_2d,\n    layer_conv_2d_transpose, layer_conv_3d, layer_conv_3d_transpose,\n    layer_conv_lstm_1d, layer_conv_lstm_2d, layer_conv_lstm_3d,\n    layer_cropping_1d, layer_cropping_2d, layer_cropping_3d,\n    layer_dense, layer_depthwise_conv_1d, layer_depthwise_conv_2d,\n    layer_discretization, layer_dot, layer_dropout, layer_embedding,\n    layer_flatten, layer_gaussian_dropout, layer_gaussian_noise,\n    layer_global_average_pooling_1d, layer_global_average_pooling_2d,\n    layer_global_average_pooling_3d, layer_global_max_pooling_1d,\n    layer_global_max_pooling_2d, layer_global_max_pooling_3d,\n    layer_gru, layer_hashing, layer_input, layer_integer_lookup,\n    layer_lambda, layer_layer_normalization, layer_lstm, layer_masking,\n    layer_max_pooling_1d, layer_max_pooling_2d, layer_max_pooling_3d,\n    layer_maximum, layer_minimum, layer_multi_head_attention,\n    layer_multiply, layer_normalization, layer_permute,\n    layer_random_brightness, layer_random_contrast, layer_random_crop,\n    layer_random_flip, layer_random_rotation, layer_random_translation,\n    layer_random_zoom, layer_repeat_vector, layer_rescaling,\n    layer_reshape, layer_resizing, layer_rnn, layer_separable_conv_1d,\n    layer_separable_conv_2d, layer_simple_rnn,\n    layer_spatial_dropout_1d, layer_spatial_dropout_2d,\n    layer_spatial_dropout_3d, layer_string_lookup, layer_subtract,\n    layer_text_vectorization, layer_unit_normalization,\n    layer_upsampling_1d, layer_upsampling_2d, layer_upsampling_3d,\n    layer_zero_padding_1d, layer_zero_padding_2d,\n    layer_zero_padding_3d, learning_rate_schedule_cosine_decay,\n    learning_rate_schedule_cosine_decay_restarts,\n    learning_rate_schedule_exponential_decay,\n    learning_rate_schedule_inverse_time_decay,\n    learning_rate_schedule_piecewise_constant_decay,\n    learning_rate_schedule_polynomial_decay, loss_binary_crossentropy,\n    loss_categorical_crossentropy, loss_categorical_hinge,\n    loss_cosine_similarity, loss_hinge, loss_huber, loss_kl_divergence,\n    loss_mean_absolute_error, loss_mean_absolute_percentage_error,\n    loss_mean_squared_error, loss_mean_squared_logarithmic_error,\n    loss_poisson, loss_sparse_categorical_crossentropy,\n    loss_squared_hinge, mark_active, metric_auc,\n    metric_binary_accuracy, metric_binary_crossentropy,\n    metric_categorical_accuracy, metric_categorical_crossentropy,\n    metric_categorical_hinge, metric_cosine_similarity,\n    metric_false_negatives, metric_false_positives, metric_hinge,\n    metric_mean, metric_mean_absolute_error,\n    metric_mean_absolute_percentage_error, metric_mean_iou,\n    metric_mean_squared_error, metric_mean_squared_logarithmic_error,\n    metric_mean_wrapper, metric_poisson, metric_precision,\n    metric_precision_at_recall, metric_recall,\n    metric_recall_at_precision, metric_root_mean_squared_error,\n    metric_sensitivity_at_specificity,\n    metric_sparse_categorical_accuracy,\n    metric_sparse_categorical_crossentropy,\n    metric_sparse_top_k_categorical_accuracy,\n    metric_specificity_at_sensitivity, metric_squared_hinge,\n    metric_sum, metric_top_k_categorical_accuracy,\n    metric_true_negatives, metric_true_positives, new_callback_class,\n    new_layer_class, new_learning_rate_schedule_class, new_loss_class,\n    new_metric_class, new_model_class, normalize, optimizer_adadelta,\n    optimizer_adagrad, optimizer_adam, optimizer_adamax,\n    optimizer_ftrl, optimizer_nadam, optimizer_rmsprop, optimizer_sgd,\n    pad_sequences, pop_layer, predict_on_batch, regularizer_l1,\n    regularizer_l1_l2, regularizer_l2, regularizer_orthogonal,\n    set_vocabulary, set_weights, shape, test_on_batch,\n    text_dataset_from_directory, time_distributed,\n    timeseries_dataset_from_array, to_categorical, train_on_batch,\n    unfreeze_weights, use_backend, with_custom_object_scope, zip_lists\n```\n:::\n\n```{.r .cell-code}\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(1, shape=(), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$math$reduce_min(y) # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(1.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\ntf$math$reduce_mean(x)  # Integer!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(50, shape=(), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$math$reduce_mean(y)  # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(50.5, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\n# c)    which.max(x)\ntf$argmax(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(0, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\ntf$argmax(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(0, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\n# d)    which.min(x)\ntf$argmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(99, shape=(), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\n# e)    order(x)\ntf$argsort(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n  3  2  1  0], shape=(100), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n  10.433675  10.103772   9.608109   8.582045 ]\n [11.311237  11.200746  11.076511  10.934624  10.769221  10.570932\n  10.323349   9.993557   9.498147   8.473241 ]\n [11.186988  11.076511  10.952296  10.810434  10.645068  10.446829\n  10.199324   9.869672   9.374583   8.351139 ]\n [11.045079  10.934624  10.810434  10.668607  10.503285  10.305112\n  10.05771    9.728241   9.233568   8.212026 ]\n [10.87965   10.769221  10.645068  10.503285  10.338026  10.139942\n   9.892679   9.563459   9.069353   8.0503845]\n [10.68132   10.570932  10.446829  10.305112  10.139942   9.941987\n   9.694924   9.366061   8.872767   7.857481 ]\n [10.433675  10.323349  10.199324  10.05771    9.892679   9.694924\n   9.448175   9.119868   8.62784    7.6182513]\n [10.103772   9.993557   9.869672   9.728241   9.563459   9.366061\n   9.119868   8.79255    8.302762   7.30317  ]\n [ 9.608109   9.498147   9.374583   9.233568   9.069353   8.872767\n   8.62784    8.302762   7.818028   6.8405466]\n [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n   7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(True, shape=(), dtype=bool)\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Runtime\n\nThis exercise compares the speed of R to TensorFlow. The first exercise is to rewrite the following function in TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n\nHere, we provide a skeleton for a TensorFlow function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n```\n:::\n\n\nWe can compare the speed using the Microbenchmark package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n```\n:::\n\n\nTry different matrix sizes for the test matrix and compare the speed.\n\nTip: Have a look at the the tf.reduce_mean documentation and the \"axis\" argument.\n\n<br/>\n\nCompare the following with different matrix sizes:\n\n-   test = matrix(0.0, 1000L, 500L)\n-   testTF = tf\\$constant(test)\n\nAlso try the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\n```\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in microbenchmark::microbenchmark(do_something_R(test),\ndo_something_TF(test)): less accurate nanosecond times to avoid potential\ninteger overflows\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                  expr     min       lq     mean  median      uq     max neval\n  do_something_R(test) 260.883 283.7200 345.7067 287.533 293.068 5184.86   100\n do_something_TF(test) 410.000 431.5045 482.5749 437.798 452.517 3045.48   100\n cld\n  a \n   b\n```\n:::\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                  expr      min       lq     mean   median       uq      max\n  do_something_R(test) 5.556566 5.930588 7.141658 6.755857 8.193850 9.817450\n do_something_TF(test) 1.206671 1.397116 1.686352 1.569336 1.910313 3.199148\n neval cld\n   100  a \n   100   b\n```\n:::\n:::\n\n\nWhy is R faster (the first time)?\n\n-   \n\n    a)  The R functions we used (apply, mean, \"-\") are also implemented in C.\n\n-   \n\n    b)  The problem is not large enough and TensorFlow has an overhead.\n\n<br/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                                    expr        min         lq       mean\n tf$matmul(testTF, tf$transpose(testTF))   6.703623   7.189944   7.699726\n                        test %*% t(test) 164.064780 164.709710 166.357264\n     median         uq       max neval cld\n   7.445702   8.182411  10.08735   100  a \n 165.302508 166.990745 224.77701   100   b\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Linear Algebra\n\nGoogle to find out how to write the following expressions in TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n```\n:::\n\n```{.r .cell-code}\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3\n```\n:::\n\n```{.r .cell-code}\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n```\n:::\n\n```{.r .cell-code}\n# l)\neigen(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n```\n:::\n\n```{.r .cell-code}\n# m)\ndet(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.          0.         -0.66666667]\n [-1.          0.5        -0.16666667]\n [ 0.          0.          0.33333333]], shape=(3, 3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# j)    diag(A)\ntf$linalg$diag_part(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 0. 0.]\n [0. 2. 0.]\n [0. 0. 3.]], shape=(3, 3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# l)    eigen(A)\ntf$linalg$eigh(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntf.Tensor([-0.56155281  3.          3.56155281], shape=(3), dtype=float64)\n\n[[2]]\ntf.Tensor(\n[[-0.78820544  0.         -0.61541221]\n [ 0.61541221  0.         -0.78820544]\n [ 0.          1.         -0.        ]], shape=(3, 3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# m)    det(A)\ntf$linalg$det(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6.0, shape=(), dtype=float64)\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Automatic differentation\n\nTensorFlow supports automatic differentiation (analytical and not numerical!). Let's have a look at the function $f(x) = 5 x^2 + 3$ with derivative $f'(x) = 10x$. So for $f'(5)$ we will get $10$.\n\nLet's do this in TensorFlow. Define the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\n```\n:::\n\n\nWe want to calculate the derivative for $x = 2.0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = tf$constant(2.0)\n```\n:::\n\n\nTo do automatic differentiation, we have to forward $x$ through the function within the tf\\$GradientTape() environment. We have also have to tell TensorFlow which value to \"watch\":\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n```\n:::\n\n\nTo print the gradient:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(tape$gradient(y, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(20.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nWe can also calculate the second order derivative $f''(x) = 10$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(10.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nWhat is happening here? Think about and discuss it.\n\nA more advanced example: *Linear regression*\n\nIn this case we first simulate data following $\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}$ ($\\boldsymbol{\\epsilon}$ follows a normal distribution == error).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n```\n:::\n\n\nIn R we would do the following to fit a linear regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y~x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58138 -0.18050  0.00713  0.12863  0.63903 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.01834    0.02636   0.696    0.488    \nx1           2.43877    0.02423 100.655  < 2e-16 ***\nx2           2.99780    0.02235 134.141  < 2e-16 ***\nx3          -0.10623    0.02514  -4.226 5.51e-05 ***\nx4           0.64236    0.02602  24.691  < 2e-16 ***\nx5           1.75014    0.02284  76.614  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2584 on 94 degrees of freedom\nMultiple R-squared:  0.9975,\tAdjusted R-squared:  0.9974 \nF-statistic:  7609 on 5 and 94 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLet's build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights ($\\boldsymbol{w}$) of the regression model). We want our model to learn these weights.\n\nThe input (predictors, independent variables or features, $\\boldsymbol{X}$) and the observed (response, $\\boldsymbol{y}$) are constant and will not be learned/optimized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ print(as.numeric(loss), message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.864504\n[1] 0.8102491\n[1] 0.5625316\n[1] 0.29904\n[1] 0.2843287\n[1] 0.2622374\n[1] 0.2553701\n[1] 0.2544637\n[1] 0.2540109\n[1] 0.2536058\n```\n:::\n\n```{.r .cell-code}\nprint(as.matrix(wTF), message = \"Resulting weights:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 2.2504456\n[2,] 2.3225314\n[3,] 1.9442997\n[4,] 0.3335486\n[5,] 1.9999003\n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  2.261 2.343 1.944 0.35 2.015 \n```\n:::\n:::\n\n\nDiscuss the code, go through the code line by line and try to understand it.\n\nAdditional exercise:\n\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ print(as.numeric(loss), message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, list(wTF, interceptTF))\n  optimizer$apply_gradients(purrr::transpose(list(grads, list(wTF, interceptTF))))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.680687\n[1] 1.744586\n[1] 1.046664\n[1] 0.6064231\n[1] 0.2717477\n[1] 0.2921552\n[1] 0.2609245\n[1] 0.2525084\n[1] 0.252449\n[1] 0.250573\n```\n:::\n\n```{.r .cell-code}\nprint(as.matrix(wTF), message = \"Resulting weights:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]\n[1,] 1.443409\n[2,] 1.435141\n[3,] 1.172537\n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  1.442 1.429 1.176 \n```\n:::\n\n```{.r .cell-code}\nprint(as.numeric(interceptTF), message = \"Resulting intercept:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.506111\n```\n:::\n\n```{.r .cell-code}\ncat(\"Original intercept: \", intercept, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal intercept:  2.5 \n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\n## Introduction to PyTorch\n\nPyTorch is another famous library for deep learning. Like TensorFlow, Torch itself is written in C++ with an API for Python. In 2020, the RStudio team released R-Torch, and while R-TensorFlow calls the Python API in the background, the R-Torch API is built directly on the C++ Torch library!\n\nUseful links:\n\n-   <a href=\"https://pytorch.org/docs/stable/index.html\" target=\"_blank\" rel=\"noopener\">PyTorch documentation</a> (This is for the Python API, bust just replace the \".\" with \"\\$\".)\n-   <a href=\"https://torch.mlverse.org/\" target=\"_blank\" rel=\"noopener\">R-Torch website</a>\n\nTo get started with Torch, we have to load the library and check if the installation worked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'torch'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:keras3':\n\n    as_iterator\n```\n:::\n:::\n\n\n### Data Containers\n\nUnlike TensorFlow, Torch doesn't have two data containers for mutable and immutable variables. All variables are initialized via the torch_tensor function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = torch_tensor(1.)\n```\n:::\n\n\nTo mark variables as mutable (and to track their operations for automatic differentiation) we have to set the argument 'requires_grad' to true in the torch_tensor function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)\n```\n:::\n\n\n### Basic Operations\n\nWe now can define the variables and do some math with them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nc = a$add(b)\nprint(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nThe R-Torch package provides all common R methods (an advantage over TensorFlow).\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nprint(a/b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 0.5000\n[ CPUFloatType{1} ]\n```\n:::\n\n```{.r .cell-code}\nprint(a*b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 50\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nTheir operators also automatically transform R numbers into tensors when attempting to add a tensor to a R number:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nAs for TensorFlow, we have to explicitly transform the tensors back to R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"torch_tensor\" \"R7\"          \n```\n:::\n\n```{.r .cell-code}\nclass(as.numeric(d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"numeric\"\n```\n:::\n:::\n\n\n### Data Types\n\nSimilar to TensorFlow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b \n```\n:::\n\n\nBut here's a difference! With TensorFlow we would get an error, but with R-Torch, m is automatically casted to a double (float64). However, this is still bad practice!\n\nDuring the course we will try to provide the corresponding PyTorch code snippets for all Keras/TensorFlow examples.\n\n### Exercises\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Torch Operations\n\nRewrite the following expressions (a to g) in torch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# b)\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\n# c) Tip: Use Google!\nwhich.max(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# d) \nwhich.min(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n\n```{.r .cell-code}\n# e) Tip: Use Google! \norder(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n```\n:::\n\n```{.r .cell-code}\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n```\n:::\n\n```{.r .cell-code}\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)\t# Test for equality.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\ntorch_min(y) # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPUFloatType{} ]\n```\n:::\n\n```{.r .cell-code}\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50.5\n```\n:::\n\n```{.r .cell-code}\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n```\n:::\n\n```{.r .cell-code}\ntorch_mean(y)  # Float!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n```\n:::\n\n```{.r .cell-code}\n# c)    which.max(x)\ntorch_argmax(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\ntorch_argmax(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n1\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\n# d)    which.min(x)\ntorch_argmin(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n100\n[ CPULongType{} ]\n```\n:::\n\n```{.r .cell-code}\n# e)    order(x)\ntorch_argsort(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 100\n  99\n  98\n  97\n  96\n  95\n  94\n  93\n  92\n  91\n  90\n  89\n  88\n  87\n  86\n  85\n  84\n  83\n  82\n  81\n  80\n  79\n  78\n  77\n  76\n  75\n  74\n  73\n  72\n  71\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{100} ]\n```\n:::\n\n```{.r .cell-code}\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n 4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n 4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n 4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n 4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n 3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n 3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n 3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n 2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n 2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n[ CPUFloatType{10,10} ]\n```\n:::\n\n```{.r .cell-code}\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1\n[ CPUBoolType{1} ]\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Runtime\n\n1.  What is the meaning of \"An effect is not significant\"?\n2.  Is an effect with three \\*\\*\\* more significant / certain than an effect with one \\*?\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n This exercise compares the speed of R to torch The first exercise is to rewrite the following function in torch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n\nHere, we provide a skeleton for a TensorFlow function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n```\n:::\n\n\nWe can compare the speed using the Microbenchmark package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n```\n:::\n\n\nTry different matrix sizes for the test matrix and compare the speed.\n\nTip: Have a look at the the torch_mean documentation and the \"dim\" argument.\n\n<br/>\n\nCompare the following with different matrix sizes:\n\n-   test = matrix(0.0, 1000L, 500L)\n-   testTorch = torch_tensor(test)\n\nAlso try the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n```\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                     expr     min       lq      mean  median       uq      max\n     do_something_R(test) 261.908 281.0755 295.29307 284.909 291.2230 1141.071\n do_something_torch(test)  55.555  61.9510  93.54683  64.206  70.7455 1672.349\n neval cld\n   100  a \n   100   b\n```\n:::\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                     expr      min       lq     mean   median       uq\n     do_something_R(test) 5509.785 5678.172 7001.044 5937.169 8000.432\n do_something_torch(test)  910.897 1224.875 1328.881 1328.031 1413.065\n       max neval cld\n 12527.181   100  a \n  3075.697   100   b\n```\n:::\n:::\n\n\nWhy is R faster (the first time)?\n\n-   \n\n    a)  The R functions we used (apply, mean, \"-\") are also implemented in C.\n\n-   \n\n    b)  The problem is not large enough and torch has an overhead.\n\n<br/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                                   expr        min         lq       mean\n torch_matmul(testTorch, testTorch$t())   1.005238   1.241234   1.631383\n                       test %*% t(test) 164.589293 167.845821 190.496715\n     median         uq        max neval cld\n   1.437911   1.678602   6.608872   100  a \n 175.203742 195.195916 423.898262   100   b\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Linear Algebra\n\nGoogle to find out how to write the following tasks in torch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n```\n:::\n\n```{.r .cell-code}\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3\n```\n:::\n\n```{.r .cell-code}\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n```\n:::\n\n```{.r .cell-code}\n# l)\neigen(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n```\n:::\n\n```{.r .cell-code}\n# m)\ndet(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1.0000 -0.0000 -0.6667\n-1.0000  0.5000 -0.1667\n 0.0000  0.0000  0.3333\n[ CPUFloatType{3,3} ]\n```\n:::\n\n```{.r .cell-code}\n# j)    diag(A)\ntorch_diag(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1\n 2\n 3\n[ CPUFloatType{3} ]\n```\n:::\n\n```{.r .cell-code}\n# k)    diag(diag(A))\ntorch_diag(A)$diag()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 1  0  0\n 0  2  0\n 0  0  3\n[ CPUFloatType{3,3} ]\n```\n:::\n\n```{.r .cell-code}\n# l)    eigen(A)\nlinalg_eigh(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntorch_tensor\n-0.5616\n 3.0000\n 3.5616\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n-0.7882  0.0000  0.6154\n 0.6154  0.0000  0.7882\n 0.0000  1.0000  0.0000\n[ CPUFloatType{3,3} ]\n```\n:::\n\n```{.r .cell-code}\n# m)    det(A)\nlinalg_det(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n6\n[ CPUFloatType{} ]\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Automatic differentation\n\nTorch supports automatic differentiation (analytical and not numerical!). Let's have a look at the function $f(x) = 5 x^2 + 3$ with derivative $f'(x) = 10x$. So for $f'(5)$ we will get $10$.\n\nLet's do this in torch Define the function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\n```\n:::\n\n\nWe want to calculate the derivative for $x = 2.0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = torch_tensor(2.0, requires_grad = TRUE)\n```\n:::\n\n\nTo do automatic differentiation, we have to forward $x$ through the function and call the \\$backward() method of the result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = f(x)\ny$backward(retain_graph=TRUE )\n```\n:::\n\n\nTo print the gradient:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 20\n[ CPUFloatType{1} ]\n```\n:::\n:::\n\n\nWe can also calculate the second order derivative $f''(x) = 10$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch_tensor\n 10\n[ CPUFloatType{1} ][ grad_fn = <MulBackward0> ]\n```\n:::\n:::\n\n\nWhat is happening here? Think about and discuss it.\n\nA more advanced example: *Linear regression*\n\nIn this case we first simulate data following $\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}$ ($\\boldsymbol{\\epsilon}$ follows a normal distribution == error).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n```\n:::\n\n\nIn R we would do the following to fit a linear regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y~x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   <2e-16 ***\nx2          2.741690   0.025328 108.249   <2e-16 ***\nx3          1.179181   0.023644  49.872   <2e-16 ***\nx4          0.591873   0.025154  23.530   <2e-16 ***\nx5          2.302417   0.022575 101.991   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,\tAdjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLet's build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights ($\\boldsymbol{w}$) of the regression model). We want our model to learn these weights.\n\nThe input (predictors, independent variables or features, $\\boldsymbol{X}$) and the observed (response, $\\boldsymbol{y}$) are constant and will not be learned/optimized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Loss: 4.4065318107605\"\n[1] \"Loss: 2.37926030158997\"\n[1] \"Loss: 0.901207685470581\"\n[1] \"Loss: 0.403193712234497\"\n[1] \"Loss: 0.296265542507172\"\n[1] \"Loss: 0.268377959728241\"\n[1] \"Loss: 0.232994809746742\"\n[1] \"Loss: 0.219554677605629\"\n[1] \"Loss: 0.215328559279442\"\n[1] \"Loss: 0.213282078504562\"\n```\n:::\n\n```{.r .cell-code}\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInferred weights:  0.701 3.089 1.801 1.123 3.452 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  0.67 3.085 1.787 1.121 3.455 \n```\n:::\n:::\n\n\nDiscuss the code, go through the code line by line and try to understand it.\n\nAdditional exercise:\n\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Loss: 3.51533484458923\"\n[1] \"Loss: 1.74870145320892\"\n[1] \"Loss: 0.41416934132576\"\n[1] \"Loss: 0.518697261810303\"\n[1] \"Loss: 0.293963462114334\"\n[1] \"Loss: 0.263338804244995\"\n[1] \"Loss: 0.258341491222382\"\n[1] \"Loss: 0.254723280668259\"\n[1] \"Loss: 0.252453774213791\"\n[1] \"Loss: 0.25116890668869\"\n```\n:::\n\n```{.r .cell-code}\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInferred weights:  3.118 -0.349 2.107 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Original weights: \", w, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal weights:  3.131 -0.353 2.11 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInferred intercept:  2.836 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Original intercept: \", intercept, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal intercept:  2.832 \n```\n:::\n:::\n\n\n\n</div>\n\n:::\n",
    "supporting": [
      "C1-TensorFlow_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}