{
  "hash": "7924f8e0553a5f2d8281392836d5ff9c",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Exercise - ML Pipeline {.unnumbered}\n\n## Exercise - Tuning Regularization\n\n::: {.callout-warning}\n\n#### Task: Tuning $\\alpha$ and $\\lambda$\n\n1.  Extend the code from above and tune $\\alpha$ and $\\lambda$ (Nested-CV or via a simple CV)\n\n2.  Train the model with best set of hyperparameters and submit your predictions\n\n3.  Compare the predictive performance from the single best model with the ensemble model\n\nSubmit both predictions (<http://rhsbio7.uni-regensburg.de:8500/>), which model has a higher AUC?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(missRanger)\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-8\n```\n:::\n\n```{.r .cell-code}\nlibrary(glmnetUtils)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'glmnetUtils'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:glmnet':\n\n    cv.glmnet, glmnet\n```\n:::\n\n```{.r .cell-code}\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\n\n# missRanger uses a random forest to impute NAs (RF is trained on the data to predict values for the NAs)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n```\n:::\n\n\nBonus:\n\n-   Try different features\n-   Try cito\n-   Try different datasets (see @sec-datasets)\n\nCode template for a simple CV (only $\\alpha$ is tuned, add the tuning for $\\lambda$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nlibrary(glmnetUtils)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\nhyper_alpha = runif(20,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                         alpha = hyper_alpha[k],\n                                                         s = 0.01,\n                                                         type = \"response\")))\n      })\n  best_alpha = hyper_alpha[which.max(tuning_results)]\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set     alpha       AUC\n1   1 0.4577418 0.8750000\n2   2 0.9148060 0.7574664\n3   3 0.8304476 0.8201346\n4   4 0.1346666 0.8272859\n5   5 0.7365883 0.8144654\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for the single model</button>\n\n\nNested CV:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(30,0, 1)\nhyper_lambda = runif(30,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  \n  tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n          best_alpha = NULL\n          best_lambda = NULL\n          best_auc = NULL  \n          \n          auc_inner = NULL\n        \n          for(j in 1:cv_inner) {\n            inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n            train_inner = train_outer[inner_split != j, ]\n            test_inner = train_outer[inner_split == j, ]\n        \n            model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n            \n            \n            auc_inner[j]= Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\"))\n            \n          }\n        return(mean(auc_inner))\n      })\n  \n  \n  best_alpha = hyper_alpha[which.max(tuning_results_inner)]\n  best_lambda = hyper_lambda[which.max(tuning_results_inner)]\n  best_auc = max(tuning_results_inner)\n  \n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  \n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = best_lambda\n  results[i, 4] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = best_lambda, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set      alpha      lambda       AUC\n1   1 0.08243756 0.038936491 0.8723039\n2   2 0.64174552 0.003948339 0.7572175\n3   3 0.73658831 0.007334147 0.8190994\n4   4 0.73658831 0.007334147 0.8255926\n5   5 0.08243756 0.038936491 0.8122883\n```\n:::\n:::\n\n\nSimple CV:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\nhyper_alpha = runif(20,0, 1)\nhyper_lambda = runif(20, 0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\")))\n      })\n  best_alpha = hyper_alpha[which.max(tuning_results)]\n  best_lambda = hyper_lambda[which.max(tuning_results)]\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = best_lambda\n  results[i, 4] = max(tuning_results)\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set     alpha      lambda       AUC\n1   1 0.4622928 0.003948339 0.8740196\n2   2 0.4622928 0.003948339 0.7572175\n3   3 0.1174874 0.207658973 0.8237578\n4   4 0.9782264 0.007334147 0.8258345\n5   5 0.9782264 0.007334147 0.8144654\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = results$alpha[i])\n    return(predict(model, data_new, alpha = results$alpha[i], s = results$lambda[i], type = \"response\")[,1])\n  })\n\n# Single predictions from the model with the highest AUC:\nwrite.csv(data.frame(y = prediction_ensemble[,which.max(results$AUC)]), file = \"Max_titanic_best_model.csv\")\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n```\n:::\n\n\n\n</div>\n\n:::",
    "supporting": [
      "Exercise_pipeline_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}