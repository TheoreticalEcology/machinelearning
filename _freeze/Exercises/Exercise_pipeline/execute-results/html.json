{
  "hash": "b77dc7a05546866b3608affb327405b7",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Exercise - ML Pipeline {.unnumbered}\n\n## Exercise - Tuning Regularization\n\n::: {.callout-warning}\n\n#### Task: Tuning $\\alpha$ and $\\lambda$\n\n1.  Extend the code from above (@sec-nested) and tune $\\alpha$ and $\\lambda$ (Nested-CV or via a simple CV)\n\n2.  Train the model with best set of hyperparameters and submit your predictions\n\n3.  Compare the predictive performance from the single best model with the ensemble model\n\nSubmit both predictions (<http://rhsbio7.uni-regensburg.de:8500/>), which model has a higher AUC?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(missRanger)\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-8\n```\n:::\n\n```{.r .cell-code}\nlibrary(glmnetUtils)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'glmnetUtils'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:glmnet':\n\n    cv.glmnet, glmnet\n```\n:::\n\n```{.r .cell-code}\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\n\n# missRanger uses a random forest to impute NAs (RF is trained on the data to predict values for the NAs)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n```\n:::\n\n\nBonus:\n\n-   Try different features\n-   Try cito\n-   Try different datasets (see @sec-datasets)\n\nCode template for a simple CV (only $\\alpha$ is tuned, add the tuning for $\\lambda$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nlibrary(glmnetUtils)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\nhyper_alpha = runif(20,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                         alpha = hyper_alpha[k],\n                                                         s = 0.01,\n                                                         type = \"response\")))\n      })\n  best_alpha = hyper_alpha[which.max(tuning_results)]\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set     alpha       AUC\n1   1 0.2861395 0.8747549\n2   2 0.9148060 0.7567198\n3   3 0.8304476 0.8203934\n4   4 0.1346666 0.8275278\n5   5 0.9370754 0.8137397\n```\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for the single model</button>\n\n\nNested CV:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(30,0, 1)\nhyper_lambda = runif(30,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  \n  tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n          best_alpha = NULL\n          best_lambda = NULL\n          best_auc = NULL  \n          \n          auc_inner = NULL\n        \n          for(j in 1:cv_inner) {\n            inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n            train_inner = train_outer[inner_split != j, ]\n            test_inner = train_outer[inner_split == j, ]\n        \n            model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n            \n            \n            auc_inner[j]= Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\"))\n            \n          }\n        return(mean(auc_inner))\n      })\n  \n  \n  best_alpha = hyper_alpha[which.max(tuning_results_inner)]\n  best_lambda = hyper_lambda[which.max(tuning_results_inner)]\n  best_auc = max(tuning_results_inner)\n  \n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  \n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = best_lambda\n  results[i, 4] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = best_lambda, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set      alpha      lambda       AUC\n1   1 0.08243756 0.038936491 0.8713235\n2   2 0.73658831 0.007334147 0.7564709\n3   3 0.64174552 0.003948339 0.8188406\n4   4 0.73658831 0.007334147 0.8258345\n5   5 0.08243756 0.038936491 0.8120464\n```\n:::\n:::\n\n\nSimple CV:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\nhyper_alpha = runif(20,0, 1)\nhyper_lambda = runif(20, 0, 1)\ncv_outer = 10\n\ntuning_results = \n      sapply(1:length(hyper_alpha), function(k) {\n          best_alpha = NULL\n          best_lambda = NULL\n          best_auc = NULL  \n          \n          auc_inner = NULL\n        \n          for(j in 1:cv_outer) {\n            inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_outer))\n            train = train_outer[inner_split != j, ]\n            test = train_outer[inner_split == j, ]\n        \n            model = glmnet(survived~.,data = train, family = \"binomial\",alpha = hyper_alpha[k])\n            \n            \n            auc_inner[j]= Metrics::auc(test$survived, predict(model, test, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\"))\n            \n          }\n        return(mean(auc_inner))\n      })\nresults = data.frame(alpha = hyper_alpha, lambda = hyper_lambda, AUC = tuning_results)\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       alpha      lambda       AUC\n1  0.9148060 0.904031387 0.5000000\n2  0.9370754 0.138710168 0.7480513\n3  0.2861395 0.988891729 0.5000000\n4  0.8304476 0.946668233 0.5000000\n5  0.6417455 0.082437558 0.8026639\n6  0.5190959 0.514211784 0.5000000\n7  0.7365883 0.390203467 0.5000000\n8  0.1346666 0.905738131 0.8057416\n9  0.6569923 0.446969628 0.5000000\n10 0.7050648 0.836004260 0.5000000\n11 0.4577418 0.737595618 0.5000000\n12 0.7191123 0.811055141 0.5000000\n13 0.9346722 0.388108283 0.5000000\n14 0.2554288 0.685169729 0.7506481\n15 0.4622928 0.003948339 0.8176582\n16 0.9400145 0.832916080 0.5000000\n17 0.9782264 0.007334147 0.8165926\n18 0.1174874 0.207658973 0.8128947\n19 0.4749971 0.906601408 0.5000000\n20 0.5603327 0.611778643 0.5000000\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = results$alpha[i])\n    return(predict(model, data_new, alpha = results$alpha[i], s = results$lambda[i], type = \"response\")[,1])\n  })\n\n# Single predictions from the model with the highest AUC:\nwrite.csv(data.frame(y = prediction_ensemble[,which.max(results$AUC)]), file = \"Max_titanic_best_model.csv\")\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n```\n:::\n\n\n\n</div>\n\n:::\n\n\n\n## Exercise - ML pipeline with mlr3\n\n::: {.callout-warning}\n#### Question: Use mlr3 for the titanic dataset\n\n1.  Use `mlr3` to tune glmnet for the titanic dataset using nested CV\n2.  Submit single predictions and multiple predictions\n\nIf you need help, take a look at the solution, go through it line by line and try to understand it.\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nPrepare data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mlr3\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\ndata = titanic_ml %>% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n survived       age      boat     cabin  embarked      fare home.dest     parch \n      655       263         0         0         0         0         0         0 \n   pclass       sex     sibsp \n        0         0         0 \n```\n:::\n\n```{.r .cell-code}\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n```\n:::\n\n\nHyperparameter tuning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\n\n\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = cv10, store_models = TRUE)\n```\n:::\n\n\nEvaluation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasurement =  msr(\"classif.auc\")\nresult$aggregate(measurement)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.auc \n   0.993747 \n```\n:::\n:::\n\n\nPredictions:\n\nWe can extract a learner with optimized hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = result$learners[[1]]$learner$clone()\nmodel$param_set$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$alpha\n[1] 0.03991976\n\n$lambda\n[1] 0.3433614\n```\n:::\n:::\n\n\nAnd we can fit it then on the full data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$train(transformed_task)\npredictions = model$predict(transformed_task, row_ids = transformed_task$row_roles$holdout)\npredictions = predictions$prob[,1]\nhead(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8054736 0.2484122 0.4228709 0.6407894 0.7942164 0.7994930\n```\n:::\n:::\n\n\nAnd submit to http://rhsbio7.uni-regensburg.de:8500\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(data.frame(y = predictions), file = \"glmnet.csv\")\n```\n:::\n\n\n\n</div>\n\n:::\n",
    "supporting": [
      "Exercise_pipeline_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}