{
  "hash": "1bb759524bbddf6fda0f1facc73093e9",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Exercise - Unsupervised learning {.unnumbered}\n\n\n\n\n::: {.callout-caution icon=\"false\"}\n#### Task\n\nGo through the 4(5) unsupervised algorithms from the supervised chapter @sec-unsupervised, and check if they are sensitive (i.e. if results change) if you scale the input features (= predictors), instead of using the raw data. Discuss in your group: Which is more appropriate for this analysis and/or in general: Scaling or not scaling?\n\n\n<div class='webex-solution'><button>Click here to see the solution for hierarchical clustering</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)\n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n\n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n\n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output-display}\n![](Exercise_unsupervised_files/figure-html/chunk_chapter3_task_0-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercise_unsupervised_files/figure-html/chunk_chapter3_task_0-2.png){width=672}\n:::\n:::\n\n\nIt seems that scaling is harmful for hierarchical clustering. But this might be a deception. **Be careful:** If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for K-means</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n\n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n\n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n\n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output-display}\n![](Exercise_unsupervised_files/figure-html/chunk_chapter3_task_1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercise_unsupervised_files/figure-html/chunk_chapter3_task_1-2.png){width=672}\n:::\n:::\n\n\nIt seems that scaling is harmful for K-means clustering. But this might be a deception. <strong>*Be careful:*</strong> If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for density-based clustering</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\nLevels: 1 2 3\n```\n:::\n\n```{.r .cell-code}\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n\n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n\n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNot scaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     3 47  0  0  0\n      2     5  0 38  3  4\n      3    17  0  0 33  0\n\nCorrect classified points:  118  /  150\nSum of noise points:  25 \n```\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     9 41  0  0  0\n      2    14  0 36  0  0\n      3    36  0  1  4  9\n\nCorrect classified points:  81  /  150\nSum of noise points:  59 \n```\n:::\n:::\n\n\nIt seems that scaling is harmful for density based clustering. But this might be a deception. <strong>*Be careful:*</strong> If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for model-based clustering</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  mb3 = Mclust(traits, 3)\n\n  tbl = table(iris$Species, mb3$classification)\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNot scaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n```\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n```\n:::\n:::\n\n\nFor model based clustering, scaling does not matter.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for ordination</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n```\n\n::: {.cell-output-display}\n![](Exercise_unsupervised_files/figure-html/chunk_chapter3_task_4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n```\n\n::: {.cell-output-display}\n![](Exercise_unsupervised_files/figure-html/chunk_chapter3_task_4-2.png){width=672}\n:::\n\n```{.r .cell-code}\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")\n```\n\n::: {.cell-output-display}\n![](Exercise_unsupervised_files/figure-html/chunk_chapter3_task_4-3.png){width=672}\n:::\n:::\n\n\nFor PCA ordination, scaling matters. Because we are interested in directions of maximal variance, all parameters should be scaled, or the one with the highest values might dominate all others.\n\n\n</div>\n\n:::",
    "supporting": [
      "Exercise_unsupervised_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}