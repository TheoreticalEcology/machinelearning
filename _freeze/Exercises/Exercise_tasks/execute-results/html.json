{
  "hash": "f7a8770854384379b24641b018a5b291",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Exercise - ML Tasks {.unnumbered}\n\n\n## Exercise - Unsupervised learning\n\n::: {.callout-warning}\n#### Task\n\nGo through the 4(5) unsupervised algorithms from the supervised chapter @sec-unsupervised, and check\n\n-   if they are sensitive (i.e. if results change) \n-   if you scale the input features (= predictors), instead of using the raw data. \n\nDiscuss in your group: Which is more appropriate for this analysis and/or in general: Scaling or not scaling?\n\n\n<div class='webex-solution'><button>Click here to see the solution for hierarchical clustering</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)\n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n\n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n\n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/chunk_chapter3_task_0-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/chunk_chapter3_task_0-2.png){width=672}\n:::\n:::\n\n\nIt seems that scaling is harmful for hierarchical clustering. But this might be a deception. **Be careful:** If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for K-means</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n\n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n\n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n\n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/chunk_chapter3_task_1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/chunk_chapter3_task_1-2.png){width=672}\n:::\n:::\n\n\nIt seems that scaling is harmful for K-means clustering. But this might be a deception. <strong>*Be careful:*</strong> If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for density-based clustering</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\nLevels: 1 2 3\n```\n:::\n\n```{.r .cell-code}\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n\n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n\n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNot scaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     3 47  0  0  0\n      2     5  0 38  3  4\n      3    17  0  0 33  0\n\nCorrect classified points:  118  /  150\nSum of noise points:  25 \n```\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     9 41  0  0  0\n      2    14  0 36  0  0\n      3    36  0  1  4  9\n\nCorrect classified points:  81  /  150\nSum of noise points:  59 \n```\n:::\n:::\n\n\nIt seems that scaling is harmful for density based clustering. But this might be a deception. <strong>*Be careful:*</strong> If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for model-based clustering</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  mb3 = Mclust(traits, 3)\n\n  tbl = table(iris$Species, mb3$classification)\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNot scaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n```\n:::\n\n```{.r .cell-code}\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n```\n:::\n:::\n\n\nFor model based clustering, scaling does not matter.\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for ordination</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/chunk_chapter3_task_4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/chunk_chapter3_task_4-2.png){width=672}\n:::\n\n```{.r .cell-code}\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/chunk_chapter3_task_4-3.png){width=672}\n:::\n:::\n\n\nFor PCA ordination, scaling matters. Because we are interested in directions of maximal variance, all parameters should be scaled, or the one with the highest values might dominate all others.\n\n\n</div>\n\n:::\n\n\n\n\n\n## Exercise - Supervised Learning\n\n\nUsing a random forest on the iris dataset, which parameter would be more important (remember there is a function to check this) to predict Petal.Width? <div class='webex-radiogroup' id='radio_OSNCJRVKET'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OSNCJRVKET\" value=\"answer\"></input> <span>Species.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_OSNCJRVKET\" value=\"\"></input> <span>Sepal.Width.</span></label></div>\n\n\n::: {.callout-warning}\n#### Task: First deep neural network\n\nDeep neural networks are currently the state of the art in unsupervised learning. Their ability to model different types of data (e.g. graphs, images) is one of the reasons for their rise in recent years. However, their use beyond tabular data (tabular data == features have specific meanings) requires extensive (programming) knowledge of the underlying deep learning frameworks (e.g. TensorFlow or PyTorch), which we will teach you in two days. For tabular data, we can use packages like cito, which work similarly to regression functions like lm and allow us to train deep neural networks in one line of code.\n\nA demonstration with the iris dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\n\n# always scale your features when using DNNs\niris_scaled = iris\niris_scaled[,1:4] = scale(iris_scaled[,1:4])\n\n# the default architecture is 3 hidden layers, each with 10 hidden nodes (we will talk on Wednesday more about the architecture)\n# Similar to a lm/glm we have to specify the response/loss family, for multi-target (3 species) we use the softmax loss function\nmodel = dnn(Species~., lr = 0.1,data = iris_scaled, loss = \"softmax\", verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nDNNs are not interpretable, i.e. no coefficients (slopes) that tell us how the features affect the response, however, similar to the RF, we can calculate a 'variable importance' which is similar to an anova:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n      variable importance_1\n1 Sepal.Length     2.651392\n2  Sepal.Width     7.497025\n3 Petal.Length    73.110128\n4  Petal.Width    60.538016\n\nAverage Conditional Effects:\n               Response_1  Response_2  Response_3\nSepal.Length -0.001174799  0.03160960 -0.03043478\nSepal.Width   0.002223916  0.04358233 -0.04580625\nPetal.Length -0.002362458 -0.16797119  0.17033363\nPetal.Width  -0.001566329 -0.13929280  0.14085912\n\nStandard Deviation of Conditional Effects:\n              Response_1 Response_2 Response_3\nSepal.Length 0.004778670 0.09946603  0.0997009\nSepal.Width  0.008728511 0.16728159  0.1664983\nPetal.Length 0.009840439 0.58161090  0.5809248\nPetal.Width  0.006367108 0.46785845  0.4674118\n```\n:::\n:::\n\n\nPredictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(model, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        setosa   versicolor    virginica\n[1,] 0.9999228 7.721816e-05 4.341381e-14\n[2,] 0.9993097 6.903210e-04 3.786510e-13\n[3,] 0.9998496 1.503872e-04 8.224155e-14\n[4,] 0.9997148 2.851895e-04 2.097471e-13\n[5,] 0.9999589 4.115722e-05 2.574190e-14\n[6,] 0.9999557 4.429019e-05 7.816093e-14\n```\n:::\n:::\n\n\nWe get three columns, one for each species, and they are probabilities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(iris$Sepal.Length, iris$Sepal.Width, col = apply(predict(model, type = \"response\"), 1, which.max))\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nPerformance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(apply(predict(model), 1, which.max), as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n     1  2  3\n  1 50  0  0\n  2  0 49  1\n  3  0  1 49\n```\n:::\n:::\n\n\n\n**Task:**\n\n-   predict `Sepal.Length` instead of `Species` (classification -\\> regression)\n-   Use the 'mse' loss function\n-   Plot predicted vs observed\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nRegression:\n\nlosses such as \"mse\" (mean squared error) or the \"msa\" (mean absolute error) are used for regression tasks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Sepal.Length~., lr = 0.1,data = iris_scaled, loss = \"mse\")\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n      variable importance_1\n1  Sepal.Width     2.298429\n2 Petal.Length    28.875459\n3  Petal.Width     1.803035\n4      Species     1.900405\n\nAverage Conditional Effects:\n             Response_1\nSepal.Width   0.2855176\nPetal.Length  1.5969565\nPetal.Width  -0.2672007\n\nStandard Deviation of Conditional Effects:\n             Response_1\nSepal.Width   0.1077870\nPetal.Length  0.5844340\nPetal.Width   0.1256384\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(iris_scaled$Sepal.Length, predict(model))\n```\n\n::: {.cell-output-display}\n![](Exercise_tasks_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nCalculate $R^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(iris_scaled$Sepal.Length, predict(model))**2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.8855536\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n",
    "supporting": [
      "Exercise_tasks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}