{
  "hash": "e3e910ce2cfa32f799146e08ce2d8e08",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Exercise - Algorithms {.unnumbered}\n\n## Exercise - Trees\n\n\n::: {.callout-warning}\n#### Question: Regression Trees\n\nWe will use the following code snippet to understand the hyperparameter mincut and thus the predictive performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nset.seed(123)\n\ndata = airquality\nrt = tree(Ozone~., data = data,\n          control = tree.control(mincut = 1L, nobs = nrow(data)))\n\nplot(rt)\ntext(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n```\n:::\n\n\nTry different mincut parameters and see what happens. (Compare the root mean squared error for different mincut parameters and explain what you see. Compare predictions for different mincut parameters and explain what happens.) What was wrong in the snippet above?\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\ndoTask = function(mincut){\n  rt = tree(Ozone~., data = data,\n            control = tree.control(mincut = mincut, nobs = nrow(data)))\n\n  pred = predict(rt, data)\n  plot(data$Temp, data$Ozone,\n       main = paste0(\n         \"mincut: \", mincut,\n         \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n  )\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\nfor(i in c(1, 2, 3, 5, 10, 15, 25, 50, 54, 55, 56, 57, 75, 100)){ doTask(i) }\n```\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_22-14.png){width=672}\n:::\n:::\n\n\nApproximately at mincut = 15, prediction is the best (mind overfitting). After mincut = 56, the prediction has no information at all and the RMSE stays constant.\n\nMind the complete cases of the airquality data set, that was the error. \n</div>\n\n:::\n\n::: {.callout-warning}\n#### Question: Random forest\n\nWe will use the following code snippet to explore a random forest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        IncNodePurity\nSolar.R      17969.59\nWind         31978.36\nTemp         34176.71\nMonth        10753.73\nDay          15436.47\n```\n:::\n\n```{.r .cell-code}\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE:  9.507848 \n```\n:::\n\n```{.r .cell-code}\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_23-1.png){width=672}\n:::\n:::\n\n\nTry different values for the nodesize describe how the predictions depend on this parameter.\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\nfor(nodesize in c(1, 15, 50, 100)){\n  for(mtry in c(1, 3, 5)){\n    rf = randomForest(Ozone~., data = data, nodesize = nodesize)\n    \n    pred = predict(rf, data)\n    \n    plot(data$Temp, data$Ozone, main = paste0(\n        \"    nodesize: \", nodesize,\n        \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n    )\n    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n  }\n}\n```\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/chunk_chapter4_task_24-12.png){width=672}\n:::\n:::\n\n\nNodesize affects the complexity. In other words: The bigger the nodesize, the smaller the trees and the more bias/less variance.\n\n\n</div>\n\n:::\n\n::: {.callout-warning}\n#### Question: Boosted regression trees\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\n\nmodel = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                         nrounds = 500L, verbose = 0L)\npred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n               ntreelimit = 10L)\n\nsaveGIF(\n  {\n    for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n      pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                     ntreelimit = i)\n      image(matrix(pred, 100, 100), main = paste0(\"Trees: \", i),\n            axes = FALSE, las = 2)\n      axis(1, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1))\n      axis(2, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n    }\n  },\n  movie.name = \"boosting.gif\", autobrowse = FALSE\n)\n```\n:::\n\n\n![](./images/boosting.gif){width=\"370\"}\n\nRun the code above and play with different values for **max_depth** and describe what you see!\n\nTip: have a look at the boosting.gif.\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\nfor(max_depth in c(3, 6, 10, 20)){\n  model = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                           max_depth = max_depth,\n                           nrounds = 500, verbose = 0L)\n\n  saveGIF(\n    {\n      for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n        pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                       ntreelimit = i)\n        image(matrix(pred, 100, 100),\n              main = paste0(\"eta: \", eta,\n                            \"    max_depth: \", max_depth,\n                            \"    Trees: \", i),\n              axes = FALSE, las = 2)\n        axis(1, at = seq(0, 1, length.out = 10),\n             labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0, 1, length.out = 10),\n             labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    },\n    movie.name = paste0(\"boosting_\", max_depth, \"_\", eta, \".gif\"),\n    autobrowse = FALSE\n  )\n}\n```\n:::\n\n\nWe see that for high values of max_depth, the predictions \"smooth out\" faster. On the other hand, with a low max_depth (low complexity of the individual trees), more trees are required in the ensemble to achieve a smooth prediction surface.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?xgboost::xgboost\n```\n:::\n\n\nJust some examples:\n\n-   ![](./images/boosting_3_0.1.gif){width=\"370\"}\n-   ![](./images/boosting_6_0.7.gif){width=\"370\"}\n-   ![](./images/boosting_20_0.9.gif){width=\"370\"}\n\n\n\n</div>\n\n:::\n\n\n::: column-margin\n\n| Hyperparameter    | Explanation                                                                                                                              |\n|--------------|----------------------------------------------------------|\n| mtry              | Subset of features randomly selected in each node (from which the algorithm can select the feature that will be used to split the data). |\n| minimum node size | Minimal number of observations allowed in a node (before the branching is canceled)                                                      |\n| max depth         | Maximum number of tree depth                                                                                                             |\n\n:::\n\n::: {.callout-warning}\n#### Question: Hyperparameter tuning of random forest\n\nCombing back to the titanic dataset from the morning, we want to optimize min node size in our RF using a simple CV.\n\nPrepare the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:randomForest':\n\n    combine\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n```\n:::\n\n\n**Hints:**\n\n-   adjust the '`type`' argument in the `predict(…)` method (the default is to predict classes)\n-   when predicting probabilities, the randomForest will return a matrix, a column for each class, we are interested in the probability of surviving (so the second column)\n\n**Bonus:**\n\n-   tune also mtry\n-   use more features\n\n::: {.callout-tip collapse=\"true\" appearance=\"minimal\"}\n## Code template\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_minnodesize = ...\n\nresults = data.frame(\n  set = rep(NA, cv),\n  minnodesize = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_minnodesize), function(k) {\n        model = randomForest(as.factor(survived)~., data = train_outer, nodesize = ... )\n        return(Metrics::auc(test_outer$survived, predict(model, newdata = test_outer, type = \"prob\")[,2]))\n      })\n  best_minnodesize = hyper_minnodesize[which.max(tuning_results)]\n  \n  results[i, 1] = i\n  results[i, 2] = best_minnodesize\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n```\n:::\n\n:::\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_minnodesize = sample(300, 20)\n\nresults = data.frame(\n  set = rep(NA, cv),\n  minnodesize = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_minnodesize), function(k) {\n        model = randomForest(as.factor(survived)~., data = train_outer, nodesize = hyper_minnodesize[k] )\n        return(Metrics::auc(test_outer$survived, predict(model, newdata = test_outer, type = \"prob\")[,2]))\n      })\n  best_minnodesize = hyper_minnodesize[which.max(tuning_results)]\n  \n  results[i, 1] = i\n  results[i, 2] = best_minnodesize\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set minnodesize       AUC\n1   1          74 0.8116560\n2   2          20 0.8409981\n3   3          20 0.8513631\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n  model = randomForest(as.factor(survived)~., data = data_obs, nodesize = results$minnodesize[i] )\n    return(predict(model, data_obs, type = \"prob\")[,2])\n  })\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n```\n:::\n\n\n\n</div>\n\n:::\n\n\n::: column-margin\n\nImportant hyperparameters:\n\n| Hyperparameter | Explanation                                                                  |\n|----------------|--------------------------------------------------------|\n| eta            | learning rate (weighting of the sequential trees)                            |\n| max depth      | maximal depth in the trees (small = low complexity, large = high complexity) |\n| subsample      | subsample ratio of the data (bootstrap ratio)                                |\n| lambda         | regularization strength of the individual trees                              |\n| max tree       | maximal number of trees in the ensemble                                      |\n\n\n:::\n\n::: {.callout-warning}\n#### Question: Hyperparameter tuning of boosted regression trees\n\nCombing back to the titanic dataset from the morning, we want to optimize max depth and the eta parameter in xgboost.\n\nPrepare the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\" appearance=\"minimal\"}\n## Code template\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_depth = ...\nhyper_eta = ...\n\nresults = data.frame(\n  set = rep(NA, cv),\n  depth = rep(NA, cv),\n  eta = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_depth), function(k) {\n        \n        # Cast data to xgboost data types\n        data_xg = xgb.DMatrix(data = as.matrix(train_outer[,-1]), label = train_outer$survived)\n        model = xgboost(data_xg, nrounds = 16L, eta = hyper_eta[k], max_depth = hyper_depth[k])\n        predictions = predict(model, newdata = as.matrix(test_outer)[,-1])\n        \n        return(Metrics::auc(test_outer$survived, predictions)))\n      })\n  \n  results[i, 1] = i\n  results[i, 2] = hyper_depth[which.max(tuning_results)]\n  results[i, 3] = hyper_eta[which.max(tuning_results)]  \n  results[i, 4] = max(tuning_results)\n}\n\nprint(results)\n```\n:::\n\n:::\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xgboost'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n:::\n\n```{.r .cell-code}\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_depth = sample(200, 20)\nhyper_eta = runif(20, 0, 1)\n\nresults = data.frame(\n  set = rep(NA, cv),\n  depth = rep(NA, cv),\n  eta = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_depth), function(k) {\n        \n        # Cast data to xgboost data types\n        data_xg = xgb.DMatrix(data = as.matrix(train_outer[,-1]), label = train_outer$survived)\n        model = xgboost(data_xg, nrounds = 16L, eta = hyper_eta[k], max_depth = hyper_depth[k])\n        predictions = predict(model, newdata = as.matrix(test_outer)[,-1])\n        \n        return(Metrics::auc(test_outer$survived, predictions))\n      })\n  \n  results[i, 1] = i\n  results[i, 2] = hyper_depth[which.max(tuning_results)]\n  results[i, 3] = hyper_eta[which.max(tuning_results)]  \n  results[i, 4] = max(tuning_results)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\ttrain-rmse:0.175827 \n[2]\ttrain-rmse:0.109441 \n[3]\ttrain-rmse:0.092795 \n[4]\ttrain-rmse:0.088512 \n[5]\ttrain-rmse:0.087381 \n[6]\ttrain-rmse:0.087107 \n[7]\ttrain-rmse:0.087027 \n[8]\ttrain-rmse:0.087007 \n[9]\ttrain-rmse:0.087003 \n[10]\ttrain-rmse:0.087002 \n[11]\ttrain-rmse:0.087002 \n[12]\ttrain-rmse:0.087002 \n[13]\ttrain-rmse:0.087002 \n[14]\ttrain-rmse:0.087002 \n[15]\ttrain-rmse:0.087002 \n[16]\ttrain-rmse:0.087002 \n[1]\ttrain-rmse:0.182387 \n[2]\ttrain-rmse:0.112793 \n[3]\ttrain-rmse:0.094016 \n[4]\ttrain-rmse:0.088861 \n[5]\ttrain-rmse:0.087508 \n[6]\ttrain-rmse:0.087140 \n[7]\ttrain-rmse:0.087039 \n[8]\ttrain-rmse:0.087011 \n[9]\ttrain-rmse:0.087004 \n[10]\ttrain-rmse:0.087002 \n[11]\ttrain-rmse:0.087002 \n[12]\ttrain-rmse:0.087002 \n[13]\ttrain-rmse:0.087002 \n[14]\ttrain-rmse:0.087002 \n[15]\ttrain-rmse:0.087002 \n[16]\ttrain-rmse:0.087002 \n[1]\ttrain-rmse:0.467591 \n[2]\ttrain-rmse:0.437246 \n[3]\ttrain-rmse:0.409265 \n[4]\ttrain-rmse:0.383782 \n[5]\ttrain-rmse:0.360274 \n[6]\ttrain-rmse:0.337716 \n[7]\ttrain-rmse:0.316400 \n[8]\ttrain-rmse:0.296867 \n[9]\ttrain-rmse:0.278947 \n[10]\ttrain-rmse:0.262474 \n[11]\ttrain-rmse:0.246994 \n[12]\ttrain-rmse:0.232921 \n[13]\ttrain-rmse:0.220217 \n[14]\ttrain-rmse:0.208335 \n[15]\ttrain-rmse:0.197668 \n[16]\ttrain-rmse:0.187673 \n[1]\ttrain-rmse:0.305459 \n[2]\ttrain-rmse:0.203594 \n[3]\ttrain-rmse:0.145842 \n[4]\ttrain-rmse:0.116749 \n[5]\ttrain-rmse:0.101977 \n[6]\ttrain-rmse:0.094731 \n[7]\ttrain-rmse:0.091107 \n[8]\ttrain-rmse:0.089156 \n[9]\ttrain-rmse:0.088162 \n[10]\ttrain-rmse:0.087629 \n[11]\ttrain-rmse:0.087345 \n[12]\ttrain-rmse:0.087189 \n[13]\ttrain-rmse:0.087103 \n[14]\ttrain-rmse:0.087057 \n[15]\ttrain-rmse:0.087032 \n[16]\ttrain-rmse:0.087018 \n[1]\ttrain-rmse:0.350164 \n[2]\ttrain-rmse:0.252016 \n[3]\ttrain-rmse:0.186831 \n[4]\ttrain-rmse:0.147834 \n[5]\ttrain-rmse:0.123078 \n[6]\ttrain-rmse:0.108796 \n[7]\ttrain-rmse:0.100220 \n[8]\ttrain-rmse:0.095171 \n[9]\ttrain-rmse:0.092074 \n[10]\ttrain-rmse:0.090201 \n[11]\ttrain-rmse:0.088995 \n[12]\ttrain-rmse:0.088264 \n[13]\ttrain-rmse:0.087806 \n[14]\ttrain-rmse:0.087516 \n[15]\ttrain-rmse:0.087331 \n[16]\ttrain-rmse:0.087211 \n[1]\ttrain-rmse:0.190103 \n[2]\ttrain-rmse:0.116106 \n[3]\ttrain-rmse:0.095590 \n[4]\ttrain-rmse:0.089478 \n[5]\ttrain-rmse:0.087726 \n[6]\ttrain-rmse:0.087214 \n[7]\ttrain-rmse:0.087064 \n[8]\ttrain-rmse:0.087020 \n[9]\ttrain-rmse:0.087006 \n[10]\ttrain-rmse:0.087003 \n[11]\ttrain-rmse:0.087003 \n[12]\ttrain-rmse:0.087003 \n[13]\ttrain-rmse:0.087003 \n[14]\ttrain-rmse:0.087003 \n[15]\ttrain-rmse:0.087003 \n[16]\ttrain-rmse:0.087003 \n[1]\ttrain-rmse:0.329438 \n[2]\ttrain-rmse:0.229970 \n[3]\ttrain-rmse:0.165347 \n[4]\ttrain-rmse:0.130049 \n[5]\ttrain-rmse:0.110895 \n[6]\ttrain-rmse:0.100268 \n[7]\ttrain-rmse:0.094529 \n[8]\ttrain-rmse:0.091396 \n[9]\ttrain-rmse:0.089568 \n[10]\ttrain-rmse:0.088477 \n[11]\ttrain-rmse:0.087870 \n[12]\ttrain-rmse:0.087513 \n[13]\ttrain-rmse:0.087307 \n[14]\ttrain-rmse:0.087184 \n[15]\ttrain-rmse:0.087110 \n[16]\ttrain-rmse:0.087066 \n[1]\ttrain-rmse:0.205885 \n[2]\ttrain-rmse:0.121338 \n[3]\ttrain-rmse:0.097982 \n[4]\ttrain-rmse:0.090629 \n[5]\ttrain-rmse:0.088183 \n[6]\ttrain-rmse:0.087395 \n[7]\ttrain-rmse:0.087133 \n[8]\ttrain-rmse:0.087045 \n[9]\ttrain-rmse:0.087016 \n[10]\ttrain-rmse:0.087006 \n[11]\ttrain-rmse:0.087003 \n[12]\ttrain-rmse:0.087003 \n[13]\ttrain-rmse:0.087003 \n[14]\ttrain-rmse:0.087003 \n[15]\ttrain-rmse:0.087003 \n[16]\ttrain-rmse:0.087003 \n[1]\ttrain-rmse:0.232610 \n[2]\ttrain-rmse:0.136983 \n[3]\ttrain-rmse:0.105745 \n[4]\ttrain-rmse:0.094020 \n[5]\ttrain-rmse:0.089683 \n[6]\ttrain-rmse:0.088056 \n[7]\ttrain-rmse:0.087410 \n[8]\ttrain-rmse:0.087154 \n[9]\ttrain-rmse:0.087060 \n[10]\ttrain-rmse:0.087024 \n[11]\ttrain-rmse:0.087010 \n[12]\ttrain-rmse:0.087005 \n[13]\ttrain-rmse:0.087003 \n[14]\ttrain-rmse:0.087003 \n[15]\ttrain-rmse:0.087003 \n[16]\ttrain-rmse:0.087003 \n[1]\ttrain-rmse:0.212223 \n[2]\ttrain-rmse:0.126366 \n[3]\ttrain-rmse:0.100020 \n[4]\ttrain-rmse:0.091146 \n[5]\ttrain-rmse:0.088439 \n[6]\ttrain-rmse:0.087504 \n[7]\ttrain-rmse:0.087178 \n[8]\ttrain-rmse:0.087062 \n[9]\ttrain-rmse:0.087022 \n[10]\ttrain-rmse:0.087008 \n[11]\ttrain-rmse:0.087004 \n[12]\ttrain-rmse:0.087003 \n[13]\ttrain-rmse:0.087003 \n[14]\ttrain-rmse:0.087003 \n[15]\ttrain-rmse:0.087003 \n[16]\ttrain-rmse:0.087003 \n[1]\ttrain-rmse:0.350936 \n[2]\ttrain-rmse:0.253277 \n[3]\ttrain-rmse:0.189156 \n[4]\ttrain-rmse:0.148701 \n[5]\ttrain-rmse:0.123730 \n[6]\ttrain-rmse:0.109281 \n[7]\ttrain-rmse:0.100498 \n[8]\ttrain-rmse:0.095362 \n[9]\ttrain-rmse:0.092203 \n[10]\ttrain-rmse:0.090257 \n[11]\ttrain-rmse:0.089029 \n[12]\ttrain-rmse:0.088286 \n[13]\ttrain-rmse:0.087820 \n[14]\ttrain-rmse:0.087527 \n[15]\ttrain-rmse:0.087338 \n[16]\ttrain-rmse:0.087215 \n[1]\ttrain-rmse:0.248468 \n[2]\ttrain-rmse:0.145743 \n[3]\ttrain-rmse:0.110049 \n[4]\ttrain-rmse:0.096179 \n[5]\ttrain-rmse:0.090720 \n[6]\ttrain-rmse:0.088565 \n[7]\ttrain-rmse:0.087669 \n[8]\ttrain-rmse:0.087290 \n[9]\ttrain-rmse:0.087122 \n[10]\ttrain-rmse:0.087052 \n[11]\ttrain-rmse:0.087023 \n[12]\ttrain-rmse:0.087010 \n[13]\ttrain-rmse:0.087005 \n[14]\ttrain-rmse:0.087004 \n[15]\ttrain-rmse:0.087004 \n[16]\ttrain-rmse:0.087004 \n[1]\ttrain-rmse:0.498441 \n[2]\ttrain-rmse:0.496901 \n[3]\ttrain-rmse:0.495367 \n[4]\ttrain-rmse:0.493828 \n[5]\ttrain-rmse:0.492281 \n[6]\ttrain-rmse:0.490754 \n[7]\ttrain-rmse:0.489233 \n[8]\ttrain-rmse:0.487703 \n[9]\ttrain-rmse:0.486194 \n[10]\ttrain-rmse:0.484701 \n[11]\ttrain-rmse:0.483189 \n[12]\ttrain-rmse:0.481697 \n[13]\ttrain-rmse:0.480212 \n[14]\ttrain-rmse:0.478716 \n[15]\ttrain-rmse:0.477242 \n[16]\ttrain-rmse:0.475773 \n[1]\ttrain-rmse:0.206652 \n[2]\ttrain-rmse:0.121658 \n[3]\ttrain-rmse:0.098195 \n[4]\ttrain-rmse:0.090556 \n[5]\ttrain-rmse:0.088205 \n[6]\ttrain-rmse:0.087406 \n[7]\ttrain-rmse:0.087138 \n[8]\ttrain-rmse:0.087045 \n[9]\ttrain-rmse:0.087015 \n[10]\ttrain-rmse:0.087006 \n[11]\ttrain-rmse:0.087003 \n[12]\ttrain-rmse:0.087002 \n[13]\ttrain-rmse:0.087002 \n[14]\ttrain-rmse:0.087002 \n[15]\ttrain-rmse:0.087002 \n[16]\ttrain-rmse:0.087002 \n[1]\ttrain-rmse:0.497105 \n[2]\ttrain-rmse:0.494254 \n[3]\ttrain-rmse:0.491404 \n[4]\ttrain-rmse:0.488575 \n[5]\ttrain-rmse:0.485738 \n[6]\ttrain-rmse:0.482949 \n[7]\ttrain-rmse:0.480181 \n[8]\ttrain-rmse:0.477404 \n[9]\ttrain-rmse:0.474695 \n[10]\ttrain-rmse:0.471927 \n[11]\ttrain-rmse:0.469235 \n[12]\ttrain-rmse:0.466506 \n[13]\ttrain-rmse:0.463826 \n[14]\ttrain-rmse:0.461135 \n[15]\ttrain-rmse:0.458464 \n[16]\ttrain-rmse:0.455845 \n[1]\ttrain-rmse:0.419005 \n[2]\ttrain-rmse:0.352627 \n[3]\ttrain-rmse:0.297627 \n[4]\ttrain-rmse:0.253814 \n[5]\ttrain-rmse:0.217581 \n[6]\ttrain-rmse:0.189113 \n[7]\ttrain-rmse:0.166227 \n[8]\ttrain-rmse:0.148174 \n[9]\ttrain-rmse:0.134004 \n[10]\ttrain-rmse:0.123090 \n[11]\ttrain-rmse:0.114698 \n[12]\ttrain-rmse:0.108291 \n[13]\ttrain-rmse:0.103426 \n[14]\ttrain-rmse:0.099694 \n[15]\ttrain-rmse:0.096845 \n[16]\ttrain-rmse:0.094662 \n[1]\ttrain-rmse:0.189928 \n[2]\ttrain-rmse:0.116045 \n[3]\ttrain-rmse:0.095563 \n[4]\ttrain-rmse:0.089466 \n[5]\ttrain-rmse:0.087722 \n[6]\ttrain-rmse:0.087214 \n[7]\ttrain-rmse:0.087063 \n[8]\ttrain-rmse:0.087019 \n[9]\ttrain-rmse:0.087007 \n[10]\ttrain-rmse:0.087003 \n[11]\ttrain-rmse:0.087003 \n[12]\ttrain-rmse:0.087003 \n[13]\ttrain-rmse:0.087003 \n[14]\ttrain-rmse:0.087003 \n[15]\ttrain-rmse:0.087003 \n[16]\ttrain-rmse:0.087003 \n[1]\ttrain-rmse:0.272092 \n[2]\ttrain-rmse:0.163696 \n[3]\ttrain-rmse:0.120192 \n[4]\ttrain-rmse:0.101641 \n[5]\ttrain-rmse:0.093493 \n[6]\ttrain-rmse:0.089993 \n[7]\ttrain-rmse:0.088384 \n[8]\ttrain-rmse:0.087659 \n[9]\ttrain-rmse:0.087310 \n[10]\ttrain-rmse:0.087144 \n[11]\ttrain-rmse:0.087068 \n[12]\ttrain-rmse:0.087032 \n[13]\ttrain-rmse:0.087016 \n[14]\ttrain-rmse:0.087008 \n[15]\ttrain-rmse:0.087004 \n[16]\ttrain-rmse:0.087003 \n[1]\ttrain-rmse:0.354093 \n[2]\ttrain-rmse:0.257314 \n[3]\ttrain-rmse:0.192974 \n[4]\ttrain-rmse:0.151840 \n[5]\ttrain-rmse:0.126063 \n[6]\ttrain-rmse:0.110879 \n[7]\ttrain-rmse:0.101735 \n[8]\ttrain-rmse:0.096267 \n[9]\ttrain-rmse:0.092827 \n[10]\ttrain-rmse:0.090699 \n[11]\ttrain-rmse:0.089337 \n[12]\ttrain-rmse:0.088477 \n[13]\ttrain-rmse:0.087941 \n[14]\ttrain-rmse:0.087602 \n[15]\ttrain-rmse:0.087392 \n[16]\ttrain-rmse:0.087254 \n[1]\ttrain-rmse:0.333494 \n[2]\ttrain-rmse:0.234593 \n[3]\ttrain-rmse:0.170822 \n[4]\ttrain-rmse:0.133994 \n[5]\ttrain-rmse:0.113482 \n[6]\ttrain-rmse:0.102127 \n[7]\ttrain-rmse:0.095877 \n[8]\ttrain-rmse:0.092217 \n[9]\ttrain-rmse:0.090080 \n[10]\ttrain-rmse:0.088836 \n[11]\ttrain-rmse:0.088098 \n[12]\ttrain-rmse:0.087662 \n[13]\ttrain-rmse:0.087401 \n[14]\ttrain-rmse:0.087243 \n[15]\ttrain-rmse:0.087148 \n[16]\ttrain-rmse:0.087090 \n[1]\ttrain-rmse:0.188921 \n[2]\ttrain-rmse:0.126826 \n[3]\ttrain-rmse:0.111578 \n[4]\ttrain-rmse:0.107653 \n[5]\ttrain-rmse:0.106672 \n[6]\ttrain-rmse:0.106413 \n[7]\ttrain-rmse:0.106345 \n[8]\ttrain-rmse:0.106328 \n[9]\ttrain-rmse:0.106325 \n[10]\ttrain-rmse:0.106325 \n[11]\ttrain-rmse:0.106325 \n[12]\ttrain-rmse:0.106325 \n[13]\ttrain-rmse:0.106325 \n[14]\ttrain-rmse:0.106325 \n[15]\ttrain-rmse:0.106325 \n[16]\ttrain-rmse:0.106325 \n[1]\ttrain-rmse:0.195457 \n[2]\ttrain-rmse:0.129858 \n[3]\ttrain-rmse:0.112563 \n[4]\ttrain-rmse:0.107986 \n[5]\ttrain-rmse:0.106780 \n[6]\ttrain-rmse:0.106446 \n[7]\ttrain-rmse:0.106357 \n[8]\ttrain-rmse:0.106331 \n[9]\ttrain-rmse:0.106327 \n[10]\ttrain-rmse:0.106324 \n[11]\ttrain-rmse:0.106324 \n[12]\ttrain-rmse:0.106324 \n[13]\ttrain-rmse:0.106324 \n[14]\ttrain-rmse:0.106324 \n[15]\ttrain-rmse:0.106324 \n[16]\ttrain-rmse:0.106324 \n[1]\ttrain-rmse:0.468748 \n[2]\ttrain-rmse:0.439602 \n[3]\ttrain-rmse:0.412375 \n[4]\ttrain-rmse:0.387024 \n[5]\ttrain-rmse:0.363245 \n[6]\ttrain-rmse:0.341305 \n[7]\ttrain-rmse:0.321214 \n[8]\ttrain-rmse:0.302495 \n[9]\ttrain-rmse:0.285540 \n[10]\ttrain-rmse:0.269621 \n[11]\ttrain-rmse:0.254977 \n[12]\ttrain-rmse:0.241989 \n[13]\ttrain-rmse:0.229524 \n[14]\ttrain-rmse:0.218100 \n[15]\ttrain-rmse:0.207744 \n[16]\ttrain-rmse:0.198353 \n[1]\ttrain-rmse:0.313094 \n[2]\ttrain-rmse:0.208954 \n[3]\ttrain-rmse:0.157890 \n[4]\ttrain-rmse:0.132285 \n[5]\ttrain-rmse:0.119543 \n[6]\ttrain-rmse:0.113201 \n[7]\ttrain-rmse:0.109925 \n[8]\ttrain-rmse:0.108229 \n[9]\ttrain-rmse:0.107343 \n[10]\ttrain-rmse:0.106871 \n[11]\ttrain-rmse:0.106623 \n[12]\ttrain-rmse:0.106485 \n[13]\ttrain-rmse:0.106412 \n[14]\ttrain-rmse:0.106371 \n[15]\ttrain-rmse:0.106348 \n[16]\ttrain-rmse:0.106336 \n[1]\ttrain-rmse:0.355868 \n[2]\ttrain-rmse:0.259663 \n[3]\ttrain-rmse:0.200152 \n[4]\ttrain-rmse:0.162609 \n[5]\ttrain-rmse:0.139389 \n[6]\ttrain-rmse:0.125898 \n[7]\ttrain-rmse:0.118142 \n[8]\ttrain-rmse:0.113540 \n[9]\ttrain-rmse:0.110768 \n[10]\ttrain-rmse:0.109049 \n[11]\ttrain-rmse:0.108048 \n[12]\ttrain-rmse:0.107412 \n[13]\ttrain-rmse:0.107015 \n[14]\ttrain-rmse:0.106758 \n[15]\ttrain-rmse:0.106599 \n[16]\ttrain-rmse:0.106498 \n[1]\ttrain-rmse:0.202989 \n[2]\ttrain-rmse:0.132188 \n[3]\ttrain-rmse:0.113915 \n[4]\ttrain-rmse:0.108559 \n[5]\ttrain-rmse:0.106982 \n[6]\ttrain-rmse:0.106521 \n[7]\ttrain-rmse:0.106381 \n[8]\ttrain-rmse:0.106339 \n[9]\ttrain-rmse:0.106327 \n[10]\ttrain-rmse:0.106325 \n[11]\ttrain-rmse:0.106325 \n[12]\ttrain-rmse:0.106325 \n[13]\ttrain-rmse:0.106325 \n[14]\ttrain-rmse:0.106325 \n[15]\ttrain-rmse:0.106325 \n[16]\ttrain-rmse:0.106325 \n[1]\ttrain-rmse:0.336020 \n[2]\ttrain-rmse:0.239423 \n[3]\ttrain-rmse:0.178614 \n[4]\ttrain-rmse:0.145721 \n[5]\ttrain-rmse:0.128115 \n[6]\ttrain-rmse:0.118376 \n[7]\ttrain-rmse:0.113218 \n[8]\ttrain-rmse:0.110247 \n[9]\ttrain-rmse:0.108582 \n[10]\ttrain-rmse:0.107646 \n[11]\ttrain-rmse:0.107094 \n[12]\ttrain-rmse:0.106781 \n[13]\ttrain-rmse:0.106591 \n[14]\ttrain-rmse:0.106481 \n[15]\ttrain-rmse:0.106416 \n[16]\ttrain-rmse:0.106377 \n[1]\ttrain-rmse:0.218171 \n[2]\ttrain-rmse:0.138561 \n[3]\ttrain-rmse:0.116578 \n[4]\ttrain-rmse:0.109665 \n[5]\ttrain-rmse:0.107391 \n[6]\ttrain-rmse:0.106672 \n[7]\ttrain-rmse:0.106438 \n[8]\ttrain-rmse:0.106361 \n[9]\ttrain-rmse:0.106335 \n[10]\ttrain-rmse:0.106328 \n[11]\ttrain-rmse:0.106326 \n[12]\ttrain-rmse:0.106325 \n[13]\ttrain-rmse:0.106325 \n[14]\ttrain-rmse:0.106325 \n[15]\ttrain-rmse:0.106325 \n[16]\ttrain-rmse:0.106325 \n[1]\ttrain-rmse:0.243659 \n[2]\ttrain-rmse:0.151452 \n[3]\ttrain-rmse:0.122570 \n[4]\ttrain-rmse:0.112393 \n[5]\ttrain-rmse:0.108622 \n[6]\ttrain-rmse:0.107210 \n[7]\ttrain-rmse:0.106669 \n[8]\ttrain-rmse:0.106458 \n[9]\ttrain-rmse:0.106375 \n[10]\ttrain-rmse:0.106343 \n[11]\ttrain-rmse:0.106330 \n[12]\ttrain-rmse:0.106327 \n[13]\ttrain-rmse:0.106325 \n[14]\ttrain-rmse:0.106324 \n[15]\ttrain-rmse:0.106324 \n[16]\ttrain-rmse:0.106324 \n[1]\ttrain-rmse:0.224229 \n[2]\ttrain-rmse:0.143881 \n[3]\ttrain-rmse:0.118744 \n[4]\ttrain-rmse:0.110552 \n[5]\ttrain-rmse:0.107774 \n[6]\ttrain-rmse:0.106820 \n[7]\ttrain-rmse:0.106494 \n[8]\ttrain-rmse:0.106382 \n[9]\ttrain-rmse:0.106343 \n[10]\ttrain-rmse:0.106330 \n[11]\ttrain-rmse:0.106326 \n[12]\ttrain-rmse:0.106325 \n[13]\ttrain-rmse:0.106325 \n[14]\ttrain-rmse:0.106325 \n[15]\ttrain-rmse:0.106325 \n[16]\ttrain-rmse:0.106325 \n[1]\ttrain-rmse:0.356608 \n[2]\ttrain-rmse:0.260611 \n[3]\ttrain-rmse:0.201036 \n[4]\ttrain-rmse:0.163319 \n[5]\ttrain-rmse:0.139903 \n[6]\ttrain-rmse:0.126250 \n[7]\ttrain-rmse:0.118378 \n[8]\ttrain-rmse:0.113702 \n[9]\ttrain-rmse:0.110875 \n[10]\ttrain-rmse:0.109121 \n[11]\ttrain-rmse:0.108097 \n[12]\ttrain-rmse:0.107444 \n[13]\ttrain-rmse:0.107040 \n[14]\ttrain-rmse:0.106780 \n[15]\ttrain-rmse:0.106610 \n[16]\ttrain-rmse:0.106506 \n[1]\ttrain-rmse:0.258755 \n[2]\ttrain-rmse:0.161024 \n[3]\ttrain-rmse:0.127393 \n[4]\ttrain-rmse:0.114806 \n[5]\ttrain-rmse:0.109861 \n[6]\ttrain-rmse:0.107794 \n[7]\ttrain-rmse:0.106926 \n[8]\ttrain-rmse:0.106577 \n[9]\ttrain-rmse:0.106431 \n[10]\ttrain-rmse:0.106368 \n[11]\ttrain-rmse:0.106342 \n[12]\ttrain-rmse:0.106330 \n[13]\ttrain-rmse:0.106326 \n[14]\ttrain-rmse:0.106325 \n[15]\ttrain-rmse:0.106324 \n[16]\ttrain-rmse:0.106324 \n[1]\ttrain-rmse:0.498496 \n[2]\ttrain-rmse:0.497004 \n[3]\ttrain-rmse:0.495503 \n[4]\ttrain-rmse:0.494016 \n[5]\ttrain-rmse:0.492527 \n[6]\ttrain-rmse:0.491058 \n[7]\ttrain-rmse:0.489582 \n[8]\ttrain-rmse:0.488116 \n[9]\ttrain-rmse:0.486651 \n[10]\ttrain-rmse:0.485198 \n[11]\ttrain-rmse:0.483749 \n[12]\ttrain-rmse:0.482301 \n[13]\ttrain-rmse:0.480863 \n[14]\ttrain-rmse:0.479433 \n[15]\ttrain-rmse:0.478008 \n[16]\ttrain-rmse:0.476554 \n[1]\ttrain-rmse:0.218904 \n[2]\ttrain-rmse:0.140297 \n[3]\ttrain-rmse:0.117255 \n[4]\ttrain-rmse:0.109975 \n[5]\ttrain-rmse:0.107500 \n[6]\ttrain-rmse:0.106715 \n[7]\ttrain-rmse:0.106452 \n[8]\ttrain-rmse:0.106366 \n[9]\ttrain-rmse:0.106337 \n[10]\ttrain-rmse:0.106327 \n[11]\ttrain-rmse:0.106325 \n[12]\ttrain-rmse:0.106324 \n[13]\ttrain-rmse:0.106324 \n[14]\ttrain-rmse:0.106324 \n[15]\ttrain-rmse:0.106324 \n[16]\ttrain-rmse:0.106324 \n[1]\ttrain-rmse:0.497207 \n[2]\ttrain-rmse:0.494446 \n[3]\ttrain-rmse:0.491677 \n[4]\ttrain-rmse:0.488942 \n[5]\ttrain-rmse:0.486212 \n[6]\ttrain-rmse:0.483530 \n[7]\ttrain-rmse:0.480842 \n[8]\ttrain-rmse:0.478182 \n[9]\ttrain-rmse:0.475485 \n[10]\ttrain-rmse:0.472820 \n[11]\ttrain-rmse:0.470169 \n[12]\ttrain-rmse:0.467528 \n[13]\ttrain-rmse:0.464914 \n[14]\ttrain-rmse:0.462323 \n[15]\ttrain-rmse:0.459744 \n[16]\ttrain-rmse:0.457167 \n[1]\ttrain-rmse:0.421967 \n[2]\ttrain-rmse:0.356180 \n[3]\ttrain-rmse:0.303317 \n[4]\ttrain-rmse:0.261215 \n[5]\ttrain-rmse:0.226402 \n[6]\ttrain-rmse:0.200018 \n[7]\ttrain-rmse:0.179061 \n[8]\ttrain-rmse:0.162063 \n[9]\ttrain-rmse:0.149095 \n[10]\ttrain-rmse:0.138970 \n[11]\ttrain-rmse:0.131664 \n[12]\ttrain-rmse:0.125849 \n[13]\ttrain-rmse:0.121325 \n[14]\ttrain-rmse:0.117884 \n[15]\ttrain-rmse:0.115363 \n[16]\ttrain-rmse:0.113489 \n[1]\ttrain-rmse:0.202819 \n[2]\ttrain-rmse:0.132252 \n[3]\ttrain-rmse:0.113920 \n[4]\ttrain-rmse:0.108616 \n[5]\ttrain-rmse:0.106990 \n[6]\ttrain-rmse:0.106518 \n[7]\ttrain-rmse:0.106380 \n[8]\ttrain-rmse:0.106339 \n[9]\ttrain-rmse:0.106327 \n[10]\ttrain-rmse:0.106325 \n[11]\ttrain-rmse:0.106324 \n[12]\ttrain-rmse:0.106324 \n[13]\ttrain-rmse:0.106324 \n[14]\ttrain-rmse:0.106324 \n[15]\ttrain-rmse:0.106324 \n[16]\ttrain-rmse:0.106324 \n[1]\ttrain-rmse:0.281255 \n[2]\ttrain-rmse:0.177293 \n[3]\ttrain-rmse:0.136263 \n[4]\ttrain-rmse:0.119489 \n[5]\ttrain-rmse:0.112259 \n[6]\ttrain-rmse:0.109022 \n[7]\ttrain-rmse:0.107610 \n[8]\ttrain-rmse:0.106922 \n[9]\ttrain-rmse:0.106606 \n[10]\ttrain-rmse:0.106456 \n[11]\ttrain-rmse:0.106385 \n[12]\ttrain-rmse:0.106352 \n[13]\ttrain-rmse:0.106336 \n[14]\ttrain-rmse:0.106329 \n[15]\ttrain-rmse:0.106326 \n[16]\ttrain-rmse:0.106326 \n[1]\ttrain-rmse:0.359633 \n[2]\ttrain-rmse:0.264519 \n[3]\ttrain-rmse:0.204909 \n[4]\ttrain-rmse:0.165767 \n[5]\ttrain-rmse:0.142295 \n[6]\ttrain-rmse:0.128040 \n[7]\ttrain-rmse:0.119535 \n[8]\ttrain-rmse:0.114573 \n[9]\ttrain-rmse:0.111464 \n[10]\ttrain-rmse:0.109553 \n[11]\ttrain-rmse:0.108353 \n[12]\ttrain-rmse:0.107599 \n[13]\ttrain-rmse:0.107136 \n[14]\ttrain-rmse:0.106842 \n[15]\ttrain-rmse:0.106655 \n[16]\ttrain-rmse:0.106536 \n[1]\ttrain-rmse:0.339902 \n[2]\ttrain-rmse:0.243940 \n[3]\ttrain-rmse:0.182473 \n[4]\ttrain-rmse:0.148246 \n[5]\ttrain-rmse:0.129837 \n[6]\ttrain-rmse:0.119676 \n[7]\ttrain-rmse:0.114036 \n[8]\ttrain-rmse:0.110838 \n[9]\ttrain-rmse:0.108953 \n[10]\ttrain-rmse:0.107867 \n[11]\ttrain-rmse:0.107235 \n[12]\ttrain-rmse:0.106866 \n[13]\ttrain-rmse:0.106645 \n[14]\ttrain-rmse:0.106515 \n[15]\ttrain-rmse:0.106438 \n[16]\ttrain-rmse:0.106392 \n[1]\ttrain-rmse:0.172884 \n[2]\ttrain-rmse:0.114327 \n[3]\ttrain-rmse:0.097112 \n[4]\ttrain-rmse:0.092596 \n[5]\ttrain-rmse:0.091285 \n[6]\ttrain-rmse:0.090974 \n[7]\ttrain-rmse:0.090897 \n[8]\ttrain-rmse:0.090876 \n[9]\ttrain-rmse:0.090872 \n[10]\ttrain-rmse:0.090871 \n[11]\ttrain-rmse:0.090871 \n[12]\ttrain-rmse:0.090871 \n[13]\ttrain-rmse:0.090871 \n[14]\ttrain-rmse:0.090871 \n[15]\ttrain-rmse:0.090871 \n[16]\ttrain-rmse:0.090871 \n[1]\ttrain-rmse:0.180143 \n[2]\ttrain-rmse:0.119365 \n[3]\ttrain-rmse:0.099512 \n[4]\ttrain-rmse:0.093257 \n[5]\ttrain-rmse:0.091529 \n[6]\ttrain-rmse:0.091051 \n[7]\ttrain-rmse:0.090931 \n[8]\ttrain-rmse:0.090886 \n[9]\ttrain-rmse:0.090874 \n[10]\ttrain-rmse:0.090871 \n[11]\ttrain-rmse:0.090871 \n[12]\ttrain-rmse:0.090871 \n[13]\ttrain-rmse:0.090871 \n[14]\ttrain-rmse:0.090871 \n[15]\ttrain-rmse:0.090871 \n[16]\ttrain-rmse:0.090871 \n[1]\ttrain-rmse:0.467837 \n[2]\ttrain-rmse:0.438604 \n[3]\ttrain-rmse:0.411015 \n[4]\ttrain-rmse:0.385333 \n[5]\ttrain-rmse:0.362395 \n[6]\ttrain-rmse:0.340771 \n[7]\ttrain-rmse:0.320978 \n[8]\ttrain-rmse:0.302839 \n[9]\ttrain-rmse:0.285958 \n[10]\ttrain-rmse:0.269559 \n[11]\ttrain-rmse:0.254379 \n[12]\ttrain-rmse:0.240419 \n[13]\ttrain-rmse:0.227989 \n[14]\ttrain-rmse:0.216288 \n[15]\ttrain-rmse:0.205268 \n[16]\ttrain-rmse:0.195270 \n[1]\ttrain-rmse:0.306273 \n[2]\ttrain-rmse:0.207907 \n[3]\ttrain-rmse:0.150508 \n[4]\ttrain-rmse:0.122639 \n[5]\ttrain-rmse:0.107649 \n[6]\ttrain-rmse:0.099862 \n[7]\ttrain-rmse:0.095745 \n[8]\ttrain-rmse:0.093491 \n[9]\ttrain-rmse:0.092295 \n[10]\ttrain-rmse:0.091648 \n[11]\ttrain-rmse:0.091298 \n[12]\ttrain-rmse:0.091104 \n[13]\ttrain-rmse:0.090998 \n[14]\ttrain-rmse:0.090940 \n[15]\ttrain-rmse:0.090908 \n[16]\ttrain-rmse:0.090890 \n[1]\ttrain-rmse:0.350994 \n[2]\ttrain-rmse:0.256770 \n[3]\ttrain-rmse:0.192391 \n[4]\ttrain-rmse:0.152738 \n[5]\ttrain-rmse:0.128474 \n[6]\ttrain-rmse:0.113825 \n[7]\ttrain-rmse:0.105039 \n[8]\ttrain-rmse:0.099728 \n[9]\ttrain-rmse:0.096563 \n[10]\ttrain-rmse:0.094503 \n[11]\ttrain-rmse:0.093173 \n[12]\ttrain-rmse:0.092334 \n[13]\ttrain-rmse:0.091808 \n[14]\ttrain-rmse:0.091472 \n[15]\ttrain-rmse:0.091260 \n[16]\ttrain-rmse:0.091121 \n[1]\ttrain-rmse:0.188462 \n[2]\ttrain-rmse:0.121134 \n[3]\ttrain-rmse:0.100574 \n[4]\ttrain-rmse:0.093793 \n[5]\ttrain-rmse:0.091724 \n[6]\ttrain-rmse:0.091118 \n[7]\ttrain-rmse:0.090942 \n[8]\ttrain-rmse:0.090891 \n[9]\ttrain-rmse:0.090876 \n[10]\ttrain-rmse:0.090872 \n[11]\ttrain-rmse:0.090872 \n[12]\ttrain-rmse:0.090872 \n[13]\ttrain-rmse:0.090872 \n[14]\ttrain-rmse:0.090872 \n[15]\ttrain-rmse:0.090872 \n[16]\ttrain-rmse:0.090872 \n[1]\ttrain-rmse:0.330286 \n[2]\ttrain-rmse:0.233028 \n[3]\ttrain-rmse:0.171402 \n[4]\ttrain-rmse:0.136800 \n[5]\ttrain-rmse:0.116880 \n[6]\ttrain-rmse:0.105968 \n[7]\ttrain-rmse:0.099698 \n[8]\ttrain-rmse:0.096035 \n[9]\ttrain-rmse:0.093901 \n[10]\ttrain-rmse:0.092672 \n[11]\ttrain-rmse:0.091943 \n[12]\ttrain-rmse:0.091513 \n[13]\ttrain-rmse:0.091258 \n[14]\ttrain-rmse:0.091101 \n[15]\ttrain-rmse:0.091007 \n[16]\ttrain-rmse:0.090952 \n[1]\ttrain-rmse:0.205089 \n[2]\ttrain-rmse:0.126427 \n[3]\ttrain-rmse:0.103381 \n[4]\ttrain-rmse:0.095116 \n[5]\ttrain-rmse:0.092337 \n[6]\ttrain-rmse:0.091355 \n[7]\ttrain-rmse:0.091028 \n[8]\ttrain-rmse:0.090923 \n[9]\ttrain-rmse:0.090887 \n[10]\ttrain-rmse:0.090876 \n[11]\ttrain-rmse:0.090873 \n[12]\ttrain-rmse:0.090872 \n[13]\ttrain-rmse:0.090872 \n[14]\ttrain-rmse:0.090872 \n[15]\ttrain-rmse:0.090872 \n[16]\ttrain-rmse:0.090872 \n[1]\ttrain-rmse:0.232649 \n[2]\ttrain-rmse:0.140287 \n[3]\ttrain-rmse:0.109680 \n[4]\ttrain-rmse:0.098320 \n[5]\ttrain-rmse:0.093795 \n[6]\ttrain-rmse:0.092004 \n[7]\ttrain-rmse:0.091305 \n[8]\ttrain-rmse:0.091040 \n[9]\ttrain-rmse:0.090936 \n[10]\ttrain-rmse:0.090895 \n[11]\ttrain-rmse:0.090879 \n[12]\ttrain-rmse:0.090875 \n[13]\ttrain-rmse:0.090872 \n[14]\ttrain-rmse:0.090872 \n[15]\ttrain-rmse:0.090872 \n[16]\ttrain-rmse:0.090872 \n[1]\ttrain-rmse:0.211676 \n[2]\ttrain-rmse:0.130184 \n[3]\ttrain-rmse:0.104742 \n[4]\ttrain-rmse:0.095784 \n[5]\ttrain-rmse:0.092631 \n[6]\ttrain-rmse:0.091497 \n[7]\ttrain-rmse:0.091087 \n[8]\ttrain-rmse:0.090944 \n[9]\ttrain-rmse:0.090896 \n[10]\ttrain-rmse:0.090878 \n[11]\ttrain-rmse:0.090874 \n[12]\ttrain-rmse:0.090872 \n[13]\ttrain-rmse:0.090872 \n[14]\ttrain-rmse:0.090872 \n[15]\ttrain-rmse:0.090872 \n[16]\ttrain-rmse:0.090872 \n[1]\ttrain-rmse:0.351765 \n[2]\ttrain-rmse:0.257726 \n[3]\ttrain-rmse:0.193300 \n[4]\ttrain-rmse:0.153479 \n[5]\ttrain-rmse:0.129025 \n[6]\ttrain-rmse:0.114251 \n[7]\ttrain-rmse:0.105334 \n[8]\ttrain-rmse:0.099924 \n[9]\ttrain-rmse:0.096689 \n[10]\ttrain-rmse:0.094583 \n[11]\ttrain-rmse:0.093217 \n[12]\ttrain-rmse:0.092362 \n[13]\ttrain-rmse:0.091826 \n[14]\ttrain-rmse:0.091482 \n[15]\ttrain-rmse:0.091265 \n[16]\ttrain-rmse:0.091124 \n[1]\ttrain-rmse:0.248807 \n[2]\ttrain-rmse:0.150750 \n[3]\ttrain-rmse:0.114629 \n[4]\ttrain-rmse:0.100943 \n[5]\ttrain-rmse:0.095022 \n[6]\ttrain-rmse:0.092671 \n[7]\ttrain-rmse:0.091637 \n[8]\ttrain-rmse:0.091212 \n[9]\ttrain-rmse:0.091015 \n[10]\ttrain-rmse:0.090932 \n[11]\ttrain-rmse:0.090895 \n[12]\ttrain-rmse:0.090880 \n[13]\ttrain-rmse:0.090874 \n[14]\ttrain-rmse:0.090873 \n[15]\ttrain-rmse:0.090873 \n[16]\ttrain-rmse:0.090873 \n[1]\ttrain-rmse:0.498454 \n[2]\ttrain-rmse:0.496913 \n[3]\ttrain-rmse:0.495379 \n[4]\ttrain-rmse:0.493850 \n[5]\ttrain-rmse:0.492327 \n[6]\ttrain-rmse:0.490810 \n[7]\ttrain-rmse:0.489318 \n[8]\ttrain-rmse:0.487812 \n[9]\ttrain-rmse:0.486312 \n[10]\ttrain-rmse:0.484818 \n[11]\ttrain-rmse:0.483348 \n[12]\ttrain-rmse:0.481865 \n[13]\ttrain-rmse:0.480387 \n[14]\ttrain-rmse:0.478934 \n[15]\ttrain-rmse:0.477468 \n[16]\ttrain-rmse:0.476007 \n[1]\ttrain-rmse:0.205887 \n[2]\ttrain-rmse:0.126584 \n[3]\ttrain-rmse:0.103145 \n[4]\ttrain-rmse:0.095044 \n[5]\ttrain-rmse:0.092488 \n[6]\ttrain-rmse:0.091395 \n[7]\ttrain-rmse:0.091042 \n[8]\ttrain-rmse:0.090927 \n[9]\ttrain-rmse:0.090888 \n[10]\ttrain-rmse:0.090875 \n[11]\ttrain-rmse:0.090872 \n[12]\ttrain-rmse:0.090872 \n[13]\ttrain-rmse:0.090872 \n[14]\ttrain-rmse:0.090872 \n[15]\ttrain-rmse:0.090872 \n[16]\ttrain-rmse:0.090872 \n[1]\ttrain-rmse:0.497128 \n[2]\ttrain-rmse:0.494277 \n[3]\ttrain-rmse:0.491445 \n[4]\ttrain-rmse:0.488669 \n[5]\ttrain-rmse:0.485877 \n[6]\ttrain-rmse:0.483105 \n[7]\ttrain-rmse:0.480352 \n[8]\ttrain-rmse:0.477654 \n[9]\ttrain-rmse:0.474939 \n[10]\ttrain-rmse:0.472245 \n[11]\ttrain-rmse:0.469603 \n[12]\ttrain-rmse:0.466945 \n[13]\ttrain-rmse:0.464307 \n[14]\ttrain-rmse:0.461687 \n[15]\ttrain-rmse:0.459094 \n[16]\ttrain-rmse:0.456486 \n[1]\ttrain-rmse:0.419566 \n[2]\ttrain-rmse:0.354595 \n[3]\ttrain-rmse:0.302976 \n[4]\ttrain-rmse:0.261853 \n[5]\ttrain-rmse:0.227510 \n[6]\ttrain-rmse:0.198239 \n[7]\ttrain-rmse:0.174989 \n[8]\ttrain-rmse:0.156728 \n[9]\ttrain-rmse:0.142304 \n[10]\ttrain-rmse:0.131013 \n[11]\ttrain-rmse:0.122163 \n[12]\ttrain-rmse:0.115350 \n[13]\ttrain-rmse:0.110198 \n[14]\ttrain-rmse:0.106096 \n[15]\ttrain-rmse:0.103045 \n[16]\ttrain-rmse:0.100497 \n[1]\ttrain-rmse:0.188274 \n[2]\ttrain-rmse:0.121075 \n[3]\ttrain-rmse:0.100476 \n[4]\ttrain-rmse:0.093772 \n[5]\ttrain-rmse:0.091719 \n[6]\ttrain-rmse:0.091116 \n[7]\ttrain-rmse:0.090941 \n[8]\ttrain-rmse:0.090890 \n[9]\ttrain-rmse:0.090876 \n[10]\ttrain-rmse:0.090872 \n[11]\ttrain-rmse:0.090872 \n[12]\ttrain-rmse:0.090872 \n[13]\ttrain-rmse:0.090872 \n[14]\ttrain-rmse:0.090872 \n[15]\ttrain-rmse:0.090872 \n[16]\ttrain-rmse:0.090872 \n[1]\ttrain-rmse:0.272716 \n[2]\ttrain-rmse:0.168770 \n[3]\ttrain-rmse:0.125837 \n[4]\ttrain-rmse:0.106769 \n[5]\ttrain-rmse:0.098221 \n[6]\ttrain-rmse:0.094337 \n[7]\ttrain-rmse:0.092501 \n[8]\ttrain-rmse:0.091659 \n[9]\ttrain-rmse:0.091242 \n[10]\ttrain-rmse:0.091046 \n[11]\ttrain-rmse:0.090953 \n[12]\ttrain-rmse:0.090909 \n[13]\ttrain-rmse:0.090888 \n[14]\ttrain-rmse:0.090878 \n[15]\ttrain-rmse:0.090875 \n[16]\ttrain-rmse:0.090873 \n[1]\ttrain-rmse:0.354916 \n[2]\ttrain-rmse:0.261668 \n[3]\ttrain-rmse:0.197618 \n[4]\ttrain-rmse:0.157084 \n[5]\ttrain-rmse:0.131749 \n[6]\ttrain-rmse:0.116237 \n[7]\ttrain-rmse:0.106711 \n[8]\ttrain-rmse:0.100926 \n[9]\ttrain-rmse:0.097294 \n[10]\ttrain-rmse:0.094981 \n[11]\ttrain-rmse:0.093500 \n[12]\ttrain-rmse:0.092565 \n[13]\ttrain-rmse:0.091972 \n[14]\ttrain-rmse:0.091582 \n[15]\ttrain-rmse:0.091333 \n[16]\ttrain-rmse:0.091172 \n[1]\ttrain-rmse:0.334342 \n[2]\ttrain-rmse:0.237612 \n[3]\ttrain-rmse:0.175374 \n[4]\ttrain-rmse:0.139616 \n[5]\ttrain-rmse:0.118774 \n[6]\ttrain-rmse:0.107187 \n[7]\ttrain-rmse:0.100534 \n[8]\ttrain-rmse:0.096630 \n[9]\ttrain-rmse:0.094311 \n[10]\ttrain-rmse:0.092937 \n[11]\ttrain-rmse:0.092117 \n[12]\ttrain-rmse:0.091627 \n[13]\ttrain-rmse:0.091330 \n[14]\ttrain-rmse:0.091151 \n[15]\ttrain-rmse:0.091040 \n[16]\ttrain-rmse:0.090973 \n```\n:::\n\n```{.r .cell-code}\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set depth         eta       AUC\n1   1   110 0.007334147 0.7782072\n2   2   153 0.082437558 0.8068937\n3   3    89 0.003948339 0.8102082\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n\n      data_xg = xgb.DMatrix(data = as.matrix(data_obs[,-1]), label = data_obs$survived)\n      model = xgboost(data_xg, nrounds = 16L, eta = results$eta[i], max_depth = results$depth[i])\n      predictions = predict(model, newdata = as.matrix(data_new)[,-1])\n    return(predictions)\n  })\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n```\n:::\n\n\n\n</div>\n\n:::\n\n::: {.callout-warning}\n#### Bonus: Implement a BRT on your own!\n\nYou can easily implement a BRT or boosted linear model using the rpart package or the lm function.\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nGo through the code line by line and try to understand it. Ask, if you have any questions you cannot solve.\n\n\n::: {.cell}\n\n:::\n\n\nLet's try it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = model.matrix(~. , data = airquality)\n\nmodel = get_boosting_model(x = data[,-2], y = data[,2], n_trees = 5L )\npred = predict(model, newdata = data[,-2])\nplot(data[,2], pred, xlab = \"observed\", ylab = \"predicted\")\n```\n\n::: {.cell-output-display}\n![](Exercise_algorithms_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n</div>\n\n:::\n",
    "supporting": [
      "Exercise_algorithms_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}