{
  "hash": "c598a130cc7333467f6f197eb632970a",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n# Causal Inference and Machine Learning {#sec-causalInference}\n\n\n\n\n\n\n\n\n\nxAI aims at explaining how predictions are being made. In general, xAI != causality. xAI methods measure which variables are used for predictions by the algorithm, or how far variables improve predictions. The important point to note here: If a variable causes something, we could also expect that it helps predicting the very thing. The opposite, however, is not generally true - very often it is possible that a variable that doesn't cause anything can predict something.\n\nIn statistics courses (in particular our course: Advanced Biostatistics), we discuss the issue of causality at full length. Here, we don't want to go into the details, but again, you should in general resist to interpret indicators of importance in xAI as causal effects. They tell you something about what's going on in the algorithm, not about what's going on in reality.\n\n## Causal Inference on Static Data {#causalInference}\n\nMethods for causal inference depend on whether we have dynamic or static data. The latter is the more common case. With static data, the problem is confounding. If you have several correlated predictors, you can get spurious correlations between a given predictor and the response, although there is no causal effect in general.\n\nMultiple regression and few other methods are able to correct for other predictors and thus isolate the causal effect. The same is not necessarily true for machine learning algorithms and xAI methods. This is not a bug, but a feature - for making good predictions, it is often no problem, but rather an advantage to also use non-causal predictors.\n\nHere an example for the indicators of variable importance in the random forest algorithm. The purpose of this script is to show that random forest variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are separable and would be correctly separated by an lm / ANOVA.\n\nWe first simulate a data set with 2 predictors that are strongly correlated, but only one of them has an effect on the response.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(123)\n\n# Simulation parameters.\nn = 1000\ncol = 0.7\n\n# Create collinear predictors.\nx1 = runif(n)\nx2 = col * x1 + (1-col) * runif(n)\n\n# Response is only influenced by x1.\ny = x1 + rnorm(n)\n```\n:::\n\n\n\n\nlm / anova correctly identify $x1$ as causal variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(y ~ x1 + x2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0709 -0.6939  0.0102  0.6976  3.3373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.02837    0.08705   0.326 0.744536    \nx1           1.07383    0.27819   3.860 0.000121 ***\nx2          -0.04547    0.37370  -0.122 0.903186    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 997 degrees of freedom\nMultiple R-squared:  0.08104,\tAdjusted R-squared:  0.0792 \nF-statistic: 43.96 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nFit random forest and show variable importance:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nfit = randomForest(y ~ x1 + x2, importance = TRUE)\nvarImpPlot(fit)\n```\n\n::: {.cell-output-display}\n![](D1-causality_files/figure-html/chunk_chapter6_12-1.png){width=672}\n:::\n:::\n\n\n\n\nVariable importance is now split nearly evenly.\n\nTask: understand why this is - remember:\n\n-   How the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built.\n-   Remember that as $x1 \\propto x2$, we can use $x2$ as a replacement for $x1$.\n-   Remember that the variable importance measures the average contributions of the different variables in the trees of the forest.\n\n::: callout-note\n\nWe found that (D)NN can better separate collinearity than the other classical ML algorithms. Probably, this is one of the advantages of (D)NN over RF or BRT for tabular data for two reasons:\n\n- We can trust DNN more \"causally\" than the other algorithms, in the sense that an effect in the DNN is not just a consequence of collinearity.\n- Causality (or the ability to correctly estimate causal effects) is important for predicting out-of-distribution, e.g., changing collinearity structures\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nnn.fit = dnn(y~x1+x2, loss = \"mse\", data = data.frame(y = y, x1=x1, x2=x2), verbose = F, plot = F)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 methods overwritten by 'reformulas':\n  method       from\n  head.call    cito\n  head.formula cito\n  head.name    cito\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(nn.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1       x1   0.13864303\n2       x2   0.00346932\n\nAverage Conditional Effects:\n   Response_1\nx1  0.9445299\nx2  0.1204029\n\nStandard Deviation of Conditional Effects:\n   Response_1\nx1 0.04281256\nx2 0.06451134\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n<!-- ## Structural Equation Models -->\n\n<!-- If causal relationships get more complicated, it will not be possible to adjust correctly with a simple lm. In this case, in statistics, we will usually use structural equation models (SEMs). Structural equation models are designed to estimate entire causal diagrams. There are two main SEM packages in R: For anything that is non-normal, you will currently have to estimate the directed acyclic graph (that depicts causal relations) piece-wise with CRAN package piecewiseSEM. Example for a vegetation data set: -->\n\n<!-- ```{r chunk_chapter6_13, results='hide', message=FALSE, warning=FALSE, eval=FALSE} -->\n\n<!-- library(piecewiseSEM) -->\n\n<!-- mod = psem( -->\n\n<!--  lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, -->\n\n<!--     data = keeley), -->\n\n<!--  lm(firesev ~ elev + age + cover, data = keeley), -->\n\n<!--  lm(cover ~ age + elev + hetero + abiotic, data = keeley) -->\n\n<!-- ) -->\n\n<!-- summary(mod) -->\n\n<!-- plot(mod) -->\n\n<!-- ``` -->\n\n<!-- For linear structural equation models, we can estimate the entire directed acyclic graph at once. This also allows having unobserved variables in the directed acyclic graph. One of the most popular packages for this is lavaan. -->\n\n<!-- ```{r chunk_chapter6_14, message=FALSE, warning=FALSE, eval=FALSE} -->\n\n<!-- library(lavaan) -->\n\n<!-- mod = \" -->\n\n<!--  rich ~ distance + elev + abiotic + age + hetero + firesev + cover -->\n\n<!--  firesev ~ elev + age + cover -->\n\n<!--  cover ~ age + elev + abiotic -->\n\n<!-- \" -->\n\n<!-- fit = sem(mod, data = keeley) -->\n\n<!-- summary(fit) -->\n\n<!-- ``` -->\n\n<!-- The default plot options are not so nice as before. -->\n\n<!-- ```{r chunk_chapter6_15, eval=FALSE} -->\n\n<!-- library(lavaanPlot) -->\n\n<!-- lavaanPlot(model = fit) -->\n\n<!-- ``` -->\n\n<!-- Another plotting option is using semPlot. -->\n\n<!-- ```{r chunk_chapter6_16, eval=FALSE} -->\n\n<!-- library(semPlot) -->\n\n<!-- semPaths(fit) -->\n\n<!-- ``` -->\n\n## Automatic Causal Discovery\n\nBut how to get the causal graph? In statistics, it is common to \"guess\" it and afterwards do residual checks, in the same way as we guess the structure of a regression. For more complicated problems, however, this is unsatisfying. Some groups therefore work on so-called causal discovery algorithms, i.e. algorithms that automatically generate causal graphs from data. One of the most classic algorithms of this sort is the *PC algorithm*. Here an example using the pcalg package:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pcalg)\n```\n:::\n\n\n\n\nLoading the data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"gmG\", package = \"pcalg\") # Loads data sets gmG and gmG8.\n\nsuffStat = list(C = cor(gmG8$x), n = nrow(gmG8$x))\n\nvarNames = gmG8$g@nodes\n```\n:::\n\n\n\n\nFirst, the skeleton algorithm creates a basic graph without connections (a skeleton of the graph).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskel.gmG8 = skeleton(suffStat, indepTest = gaussCItest,\n\nlabels = varNames, alpha = 0.01)\n\nRgraphviz::plot(skel.gmG8@graph)\n```\n:::\n\n\n\n\nWhat is missing here is the direction of the errors. The PC algorithm now makes tests for conditional independence, which allows fixing a part (but typically not all) of the directions of the causal arrows.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc.gmG8 = pc(suffStat, indepTest = gaussCItest,\n\nlabels = varNames, alpha = 0.01)\n\nRgraphviz::plot(pc.gmG8@graph )\n```\n:::\n\n\n\n\n## Causal Inference on Dynamic Data\n\nWhen working with dynamic data, we can use an additional piece of information - the cause usually precedes the effect, which means that we can test for a time-lag between cause and effect to determine the direction of causality. This way of testing for causality is known as *Granger causality*, or Granger methods. Here an example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest)\n\n## What came first: the chicken or the egg?\ndata(ChickEgg)\ngrangertest(egg ~ chicken, order = 3, data = ChickEgg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGranger causality test\n\nModel 1: egg ~ Lags(egg, 1:3) + Lags(chicken, 1:3)\nModel 2: egg ~ Lags(egg, 1:3)\n  Res.Df Df      F Pr(>F)\n1     44                 \n2     47 -3 0.5916 0.6238\n```\n\n\n:::\n\n```{.r .cell-code}\ngrangertest(chicken ~ egg, order = 3, data = ChickEgg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGranger causality test\n\nModel 1: chicken ~ Lags(chicken, 1:3) + Lags(egg, 1:3)\nModel 2: chicken ~ Lags(chicken, 1:3)\n  Res.Df Df     F   Pr(>F)   \n1     44                     \n2     47 -3 5.405 0.002966 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n## Outlook for Machine Learning\n\nAs we have seen, there are already a few methods / algorithms for discovering causality from large data sets, but the systematic transfer of these concepts to machine learning, in particular deep learning, is still at its infancy. At the moment, this field is actively researched and changes extremely fast, so we recommend using Google to see what is currently going on. Particular in business and industry, there is a large interest in learning about causal effect from large data sets. In our opinion, a great topic for young scientists to specialize on.\n",
    "supporting": [
      "D1-causality_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}