{
  "hash": "bbd2f9167963d736f55ce868427fd9fd",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n# Autoencoder\n\n\n\n\n\n\n\n\n\n<!-- Autoencoders (AE) can be used for unsupervised learning. The idea is similar to data compression: The first part of the network compresses (encodes) the data to a low dimensional space (e.g. 2-4 dimensions) and the second part of the network decompresses (decodes) and learns to reconstruct the data (think of a hourglass). -->\n\n<!-- Why is this useful? The method is similar to a dimension reduction technique (e.g. PCA) but with the advantage that we don't have to make any distributional assumptions (but see PCA). For instance, we could first train an autoencoder on genomic expression data with thousands of features, compress them into 2-4 dimensions, and then use them for clustering. -->\n\n<!-- ## Autoencoder - Deep Neural Network Flower -->\n\n<!-- We now will write an autoencoder for the MNIST data set. -->\n\n<!-- Let's start with the (usual) MNIST example: -->\n\n<!-- ``` r -->\n\n<!-- library(keras) -->\n\n<!-- library(tensorflow) -->\n\n<!-- data = keras::dataset_mnist() -->\n\n<!-- ``` -->\n\n<!-- We don't need the labels here, our images will be the inputs and at the same time the outputs of our final autoencoder. -->\n\n<!-- ``` r -->\n\n<!-- rotate = function(x){ t(apply(x, 2, rev)) } -->\n\n<!-- imgPlot = function(img, title = \"\"){ -->\n\n<!--   col = grey.colors(255) -->\n\n<!--   if(title != \"\"){ main = paste0(\"Label: \", as.character(title)) } -->\n\n<!--   else{ main = \"\" } -->\n\n<!--   image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE, main = main) -->\n\n<!-- } -->\n\n<!-- train = data[[1]] -->\n\n<!-- test = data[[2]] -->\n\n<!-- train_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L)) -->\n\n<!-- test_x = array(test[[1]]/255, c(dim(test[[1]])[1], 784L)) -->\n\n<!-- ``` -->\n\n<!-- Our encoder: image (784 dimensions) $\\rightarrow$ 2 dimensions -->\n\n<!-- ``` r -->\n\n<!-- down_size_model = keras_model_sequential() -->\n\n<!-- down_size_model %>%  -->\n\n<!--   layer_dense(units = 100L, input_shape = c(784L), activation = \"relu\") %>%  -->\n\n<!--   layer_dense(units = 20L, activation = \"relu\") %>%  -->\n\n<!--   layer_dense(units = 2L, activation = \"linear\") -->\n\n<!-- ``` -->\n\n<!-- Our decoder: 2 dimensions $\\rightarrow$ 784 dimensions (our image) -->\n\n<!-- ``` r -->\n\n<!-- up_size_model = keras_model_sequential() -->\n\n<!-- up_size_model %>%  -->\n\n<!--   layer_dense(units = 20L, input_shape = c(2L), activation = \"relu\") %>%  -->\n\n<!--   layer_dense(units = 100L, activation = \"relu\") %>%  -->\n\n<!--   layer_dense(units = 784L, activation = \"sigmoid\") -->\n\n<!-- ``` -->\n\n<!-- We can use the non-sequential model type to connect the two models. (We did the same in the transfer learning chapter.) -->\n\n<!-- ``` r -->\n\n<!-- autoencoder = keras_model(inputs = down_size_model$input,  -->\n\n<!--                           outputs = up_size_model(down_size_model$output)) -->\n\n<!-- autoencoder$compile(loss = loss_binary_crossentropy, -->\n\n<!--                     optimizer = optimizer_adamax(0.01)) -->\n\n<!-- summary(autoencoder) -->\n\n<!-- ``` -->\n\n<!-- ```{r, eval = TRUE,echo = FALSE, results = TRUE} -->\n\n<!-- cat(\" -->\n\n<!-- Model: -->\n\n<!-- __________________________________________________________________________________________ -->\n\n<!--  Layer (type)                           Output Shape                        Param #        -->\n\n<!-- ========================================================================================== -->\n\n<!--  dense_2_input (InputLayer)             [(None, 784)]                       0              -->\n\n<!--  dense_2 (Dense)                        (None, 100)                         78500          -->\n\n<!--  dense_1 (Dense)                        (None, 20)                          2020           -->\n\n<!--  dense (Dense)                          (None, 2)                           42             -->\n\n<!--  sequential_1 (Sequential)              (None, 784)                         81344          -->\n\n<!-- ========================================================================================== -->\n\n<!-- Total params: 161,906 -->\n\n<!-- Trainable params: 161,906 -->\n\n<!-- Non-trainable params: 0 -->\n\n<!-- __________________________________________________________________________________________     -->\n\n<!--     \") -->\n\n<!-- ``` -->\n\n<!-- We will now show an example of an image before and after the unfitted autoencoder, so we see that we have to train the autoencoder. -->\n\n<!-- ``` r -->\n\n<!-- image = autoencoder(train_x[1,,drop = FALSE]) -->\n\n<!-- imgPlot(array(train_x[1,,drop = FALSE], c(28, 28)), title = \"Before\") -->\n\n<!-- imgPlot(array(image$numpy(), c(28, 28)), title = \"After\") -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_6__AEmnistoutput-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- Fit the autoencoder (inputs == outputs!): -->\n\n<!-- ``` r -->\n\n<!-- library(tensorflow) -->\n\n<!-- library(keras) -->\n\n<!-- set_random_seed(123L, disable_gpu = FALSE)  # Already sets R's random seed. -->\n\n<!-- autoencoder %>%  -->\n\n<!--   fit(x = train_x, y = train_x, epochs = 5L, batch_size = 128L) -->\n\n<!-- ``` -->\n\n<!-- Visualization of the latent variables: -->\n\n<!-- ``` r -->\n\n<!-- pred_dim = down_size_model(test_x) -->\n\n<!-- reconstr_pred = up_size_model(pred_dim) -->\n\n<!-- imgPlot(array(reconstr_pred[10,]$numpy(), dim = c(28L, 28L)), title = \"\") -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_8__AEvisualization-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- ``` r -->\n\n<!-- ownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\", -->\n\n<!--               \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\") -->\n\n<!-- oldpar = par(mfrow = c(1, 1)) -->\n\n<!-- plot(pred_dim$numpy()[,1], pred_dim$numpy()[,2], col = ownColors[test[[2]]+1L]) -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_9__AEvisualizationContinuation-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- The picture above shows the 2-dimensional encoded values of the numbers in the MNIST data set and the number they are depicting via the respective color. -->\n\nSimple autoencoder of the iris dataset:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\n\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nautoencoder = dnn(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, hidden = c(10L, 5L, 2L, 5L, 10L), data = df, lr = 0.1, verbose = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 methods overwritten by 'reformulas':\n  method       from\n  head.call    cito\n  head.formula cito\n  head.name    cito\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](E1-Autoencoder_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoencoder$net\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `nn_module` containing 236 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• 0: <nn_linear> #50 parameters\n• 1: <nn_selu> #0 parameters\n• 2: <nn_linear> #55 parameters\n• 3: <nn_selu> #0 parameters\n• 4: <nn_linear> #12 parameters\n• 5: <nn_selu> #0 parameters\n• 6: <nn_linear> #15 parameters\n• 7: <nn_selu> #0 parameters\n• 8: <nn_linear> #60 parameters\n• 9: <nn_selu> #0 parameters\n• 10: <nn_linear> #44 parameters\n```\n\n\n:::\n\n```{.r .cell-code}\npredictions = \nas.matrix(df[,1:4]) %>% \n  torch_tensor() %>% \n  autoencoder$net$`0`() %>% \n  autoencoder$net$`1`() %>% \n  autoencoder$net$`2`() %>% \n  autoencoder$net$`3`() %>% \n  autoencoder$net$`4`() %>% \n  as.matrix()\n\n\nplot(predictions, col = iris$Species)\n```\n\n::: {.cell-output-display}\n![](E1-Autoencoder_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n\n\n## Autoencoder - Convolutional Neural Networks for Flower\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(coro)\n\nFlatten = \n  nn_module(\n    forward = function(input) return(input$view(list(input$size(1L), -1)))\n  )\n\nUnFlatten = \n  nn_module(\n    forward = function(input, size=1152) return( input$view(list(input$size(1L), size, 1L, 1L)))\n  )\n\nAE = nn_module(\n  initialize = function(image_channels = 3L, h_dim=1152, z_dim=2L) {\n    self$encoder = nn_sequential(\n      nn_conv2d(image_channels, 16, kernel_size=4, stride=2),\n      nn_relu(),\n      nn_conv2d(16, 32, kernel_size=4, stride=2),\n      nn_relu(),\n      nn_conv2d(32, 64, kernel_size=4, stride=2),\n      nn_relu(),\n      nn_conv2d(64, 128, kernel_size=4, stride=2),\n      nn_relu(),\n      Flatten()\n    )\n    self$fc1 = nn_linear(h_dim, z_dim)\n    self$fc2 = nn_linear(z_dim, h_dim)\n    \n    self$decoder = nn_sequential(\n      UnFlatten(),\n      nn_conv_transpose2d(h_dim, 128, kernel_size=6, stride=2),\n      nn_relu(),\n      nn_conv_transpose2d(128, 64, kernel_size=6, stride=2),\n      nn_relu(),\n      nn_conv_transpose2d(64, 32, kernel_size=7, stride=2),\n      nn_relu(),\n      nn_conv_transpose2d(32, image_channels, kernel_size=8, stride=2),\n      nn_sigmoid(),\n    )\n  },\n  \n  \n  encode = function( x) {\n    h = self$encoder(x)\n    return(self$fc1(h))\n  },\n  \n  decode = function(z) {\n    z = self$fc2(z)\n    z = self$decoder(z)\n    return(z)\n  },\n  \n  forward = function(input) {\n    results = self$encode(input)\n    z = self$decode(results)\n    return(z)\n  }\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nae = AE()\n\ndevice = \"cuda:0\"\n\ndata = EcoData::dataset_flower()\ntrain = data$train/255\nlabels = data$labels\n\n\nae$to(device = device)\nae$encoder$to(device = device)\nae$decoder$to(device = device)\nae$fc1$to(device = device)\nae$fc2$to(device = device)\n\n\n\ntrain = aperm(train, perm = c(1, 4, 2, 3))\ndataset = torch::tensor_dataset(torch_tensor(train))\ndataLoader = torch::dataloader(dataset, batch_size = 50L, shuffle = TRUE, pin_memory = TRUE)\n\noptimizer = optim_adam(ae$parameters, lr=0.01) \n\n\nfor(e in 1:200) {\n  batch_losses = NULL\n  counter = 1\n  coro::loop(for (b in dataLoader) {\n    optimizer$zero_grad()\n    batch = b[[1]]$to(device = device)\n    pred = ae(batch)\n    loss = nnf_binary_cross_entropy(pred, batch)\n    loss$backward()\n    optimizer$step()\n    batch_losses[counter] <- loss$item()\n    counter = counter + 1\n  })\n  cat(\"Epoch: \", e, \" loss: \", mean(batch_losses), \"\\n\")\n}\n```\n:::\n\n\n\n\n<!-- We can also use convolutional neural networks instead or on the side of deep neural networks: Prepare data: -->\n\n<!-- ``` r -->\n\n<!-- data = tf$keras$datasets$mnist$load_data() -->\n\n<!-- train = data[[1]] -->\n\n<!-- train_x = array(train[[1]]/255, c(dim(train[[1]]), 1L)) -->\n\n<!-- test_x = array(data[[2]][[1]]/255, c(dim(data[[2]][[1]]/255), 1L)) -->\n\n<!-- ``` -->\n\n<!-- Then define the downsize model: -->\n\n<!-- ``` r -->\n\n<!-- down_size_model = keras_model_sequential() -->\n\n<!-- down_size_model %>%  -->\n\n<!--   layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L, 3L), input_shape = c(28L, 28L, 1L), padding = \"same\") %>%  -->\n\n<!--   layer_max_pooling_2d(, padding = \"same\") %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_max_pooling_2d(, padding = \"same\") %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_max_pooling_2d(, padding = \"same\") %>%  -->\n\n<!--   layer_flatten() %>%  -->\n\n<!--   layer_dense(units = 2L, activation = \"linear\") -->\n\n<!-- ``` -->\n\n<!-- Define the upsize model: -->\n\n<!-- ``` r -->\n\n<!-- up_size_model = keras_model_sequential() -->\n\n<!-- up_size_model %>%  -->\n\n<!--   layer_dense(units = 128L, activation = \"relu\", input_shape = c(2L)) %>%  -->\n\n<!--   layer_reshape(target_shape = c(4L, 4L, 8L)) %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_upsampling_2d() %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_upsampling_2d() %>%  -->\n\n<!--   layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L,3L)) %>%  -->\n\n<!--   layer_upsampling_2d() %>%  -->\n\n<!--   layer_conv_2d(filters = 1, activation = \"sigmoid\", kernel_size = c(3L,3L), padding = \"same\") -->\n\n<!-- ``` -->\n\n<!-- Combine the two models and fit it: -->\n\n<!-- ``` r -->\n\n<!-- library(tensorflow) -->\n\n<!-- library(keras) -->\n\n<!-- set_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed. -->\n\n<!-- autoencoder = tf$keras$models$Model(inputs = down_size_model$input, -->\n\n<!--                                     outputs = up_size_model(down_size_model$output)) -->\n\n<!-- autoencoder %>% compile(loss = loss_binary_crossentropy, -->\n\n<!--                     optimizer = optimizer_rmsprop(0.001)) -->\n\n<!-- autoencoder %>%  fit(x = tf$constant(train_x), y = tf$constant(train_x), -->\n\n<!--                       epochs = 50L, batch_size = 64L) -->\n\n<!-- ``` -->\n\n<!-- Test it: -->\n\n<!-- ``` r -->\n\n<!-- pred_dim = down_size_model(tf$constant(test_x, \"float32\")) -->\n\n<!-- reconstr_pred = autoencoder(tf$constant(test_x, \"float32\")) -->\n\n<!-- imgPlot(reconstr_pred[10,,,]$numpy()[,,1]) -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_14-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- ``` r -->\n\n<!-- ownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\", -->\n\n<!--               \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\") -->\n\n<!-- plot(pred_dim[,1]$numpy(), pred_dim[,2]$numpy(), col = ownColors[test[[2]]+1L]) -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_14-2.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- ``` r -->\n\n<!-- ## Generate new images! -->\n\n<!-- new = matrix(c(10, 10), 1, 2) -->\n\n<!-- imgPlot(array(up_size_model(new)$numpy(), c(28L, 28L))) -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_14-3.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- ``` r -->\n\n<!-- new = matrix(c(5, 5), 1, 2) -->\n\n<!-- imgPlot(array(up_size_model(new)$numpy(), c(28L, 28L))) -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_14-4.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n## Variational Autoencoder (VAE) {#sec-VAE}\n\nThe difference between a variational and a normal autoencoder is that a variational autoencoder assumes a distribution for the latent variables (latent variables cannot be observed and are composed of other variables) and the parameters of this distribution are learned. Thus new objects can be generated by inserting valid (!) (with regard to the assumed distribution) \"seeds\" to the decoder. To achieve the property that more or less randomly chosen points in the low dimensional latent space are meaningful and yield suitable results after decoding, the latent space/training process must be regularized. In this process, the input to the VAE is encoded to a distribution in the latent space rather than a single point.\n\nHelper functions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = EcoData::dataset_flower()\ntrain = data$train/255\nlabels = data$labels\ntrain = aperm(train, perm = c(1, 4, 2, 3))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(coro)\n\nFlatten = \n  nn_module(\n    forward = function(input) return(input$view(list(input$size(1L), -1)))\n  )\n\nUnFlatten = \n  nn_module(\n    forward = function(input, size=1152) return( input$view(list(input$size(1L), size, 1L, 1L)))\n  )\n\nVAE = nn_module(\n  initialize = function(image_channels = 3L, h_dim=1152, z_dim=30L) {\n    self$encoder = nn_sequential(\n      nn_conv2d(image_channels, 16, kernel_size=4, stride=2),\n      nn_relu(),\n      nn_conv2d(16, 32, kernel_size=4, stride=2),\n      nn_relu(),\n      nn_conv2d(32, 64, kernel_size=4, stride=2),\n      nn_relu(),\n      nn_conv2d(64, 128, kernel_size=4, stride=2),\n      nn_relu(),\n      Flatten()\n    )\n    self$fc1 = nn_linear(h_dim, z_dim)\n    self$fc2 = nn_linear(h_dim, z_dim)\n    self$fc3 = nn_linear(z_dim, h_dim)\n    \n    self$decoder = nn_sequential(\n      UnFlatten(),\n      nn_conv_transpose2d(h_dim, 128, kernel_size=6, stride=2),\n      nn_relu(),\n      nn_conv_transpose2d(128, 64, kernel_size=6, stride=2),\n      nn_relu(),\n      nn_conv_transpose2d(64, 32, kernel_size=7, stride=2),\n      nn_relu(),\n      nn_conv_transpose2d(32, image_channels, kernel_size=8, stride=2),\n      nn_sigmoid(),\n    )\n  },\n  \n  sample_from_normal = function(mu, logvar) {\n    std = (logvar$mul(0.5)$exp_())$to(device = mu$device)\n    esp = torch_randn(mu$size())$to(device = mu$device)\n    z = mu + std * esp\n    return(z)  \n  },\n  \n  get_mu_and_sample = function(h) {\n    mu = self$fc1(h)\n    logvar = self$fc2(h)\n    samples = self$sample_from_normal(mu, logvar)\n    return(list(samples, mu, logvar))\n  },\n  \n  encode = function( x) {\n    h = self$encoder(x)\n    results = self$get_mu_and_sample(h)\n    return(results) #z, mu, logvar\n  },\n  \n  decode = function(z) {\n    z = self$fc3(z)\n    z = self$decoder(z)\n    return(z)\n  },\n  \n  forward = function(input) {\n    results = self$encode(input)\n    z = results[[1]]\n    mu = results[[2]]\n    logvar = results[[3]]\n    z = self$decode(z)\n    return(list(z, mu, logvar))\n  }\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvae = VAE()\ndevice = \"cpu\"\nvae$to(device = device)\nvae$encoder$to(device = device)\nvae$decoder$to(device = device)\nvae$fc1$to(device = device)\nvae$fc2$to(device = device)\nvae$fc3$to(device = device)\n```\n:::\n\n\n\n\nTry untrained VAE:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreditions = as_array(vae(torch_tensor(train[1:5,,,]))[[1]])\npreditions = aperm(preditions, c(1, 3, 4, 2))\npreditions[1,,,]  %>%\n  keras3::image_to_array() %>% \n  as.raster() %>%\n  plot()\n```\n:::\n\n\n\n\nTrain VAE:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loss function:\nloss_function = function(reconstructed, x, mu, logvar){\n  loss_bce = nnf_binary_cross_entropy(reconstructed, x, reduction = \"sum\")\n  # Kullback–Leibler divergence / Normal prior on our latent dimensions!\n  KLD = -0.5 * torch_mean(1 + logvar - mu$pow(2) - logvar$exp())\n  return(loss_bce+KLD)\n}\n\ndataset = torch::tensor_dataset(torch_tensor(train))\ndataLoader = torch::dataloader(dataset, batch_size = 50L, shuffle = TRUE, pin_memory = TRUE)\n\noptimizer = optim_adam(vae$parameters, lr=0.001) \n\nfor(e in 1:200) {\n  batch_losses = NULL\n  counter = 1\n  coro::loop(for (b in dataLoader) {\n    optimizer$zero_grad()\n    batch = b[[1]]$to(device = device)\n    pred = vae(batch)\n    loss = loss_function(pred[[1]], batch, pred[[2]], pred[[3]])\n    loss$backward()\n    optimizer$step()\n    batch_losses[counter] <- loss$item()\n    counter = counter + 1\n  })\n  cat(\"Epoch: \", e, \" loss: \", mean(batch_losses), \"\\n\")\n}\n```\n:::\n\n\n\n\nSample from our Decoder:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimages = vae$decode(torch_randn(c(20L, 30L))$to( device = device)) $cpu()\nimages = as_array(images$cpu())\nimages = aperm(images, c(1, 3, 4, 2))\nimages[1,,,]  %>%\n  as.raster() %>%\n  plot()\n```\n:::\n\n\n\n\nExample images (from the above trained VAE):\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](E1-Autoencoder_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n<!-- ``` r -->\n\n<!-- library(tfprobability) -->\n\n<!-- data = tf$keras$datasets$mnist$load_data() -->\n\n<!-- train = data[[1]] -->\n\n<!-- train_x = array(train[[1]]/255, c(dim(train[[1]]), 1L)) -->\n\n<!-- ``` -->\n\n<!-- We will use TensorFlow probability to define priors for our latent variables. -->\n\n<!-- ``` r -->\n\n<!-- library(tfprobability) -->\n\n<!-- tfp = reticulate::import(\"tensorflow_probability\") -->\n\n<!-- ``` -->\n\n<!-- Build the two networks: -->\n\n<!-- ``` r -->\n\n<!-- encoded = 2L -->\n\n<!-- prior = tfd_independent(tfd_normal(c(0.0, 0.0), 1.0), 1L) -->\n\n<!-- up_size_model = keras_model_sequential() -->\n\n<!-- up_size_model %>%  -->\n\n<!--   layer_dense(units = 128L, activation = \"relu\", input_shape = c(2L)) %>%  -->\n\n<!--   layer_reshape(target_shape = c(4L, 4L, 8L)) %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_upsampling_2d() %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_upsampling_2d() %>%  -->\n\n<!--   layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L,3L)) %>%  -->\n\n<!--   layer_upsampling_2d() %>%  -->\n\n<!--   layer_conv_2d(filters = 1, activation = \"sigmoid\", kernel_size = c(3L,3L), padding = \"same\") -->\n\n<!-- down_size_model = keras_model_sequential() -->\n\n<!-- down_size_model %>%  -->\n\n<!--   layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L, 3L), input_shape = c(28L, 28L, 1L), padding = \"same\") %>%  -->\n\n<!--   layer_max_pooling_2d(, padding = \"same\") %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_max_pooling_2d(, padding = \"same\") %>%  -->\n\n<!--   layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>%  -->\n\n<!--   layer_max_pooling_2d(, padding = \"same\") %>%  -->\n\n<!--   layer_flatten() %>%  -->\n\n<!--   layer_dense(units = 4L, activation = \"linear\") %>%  -->\n\n<!--   layer_independent_normal(2L, -->\n\n<!--                            activity_regularizer = -->\n\n<!--                              tfp$layers$KLDivergenceRegularizer(distribution_b = prior)) -->\n\n<!-- VAE = keras_model(inputs = down_size_model$inputs, -->\n\n<!--                   outputs = up_size_model(down_size_model$outputs)) -->\n\n<!-- ``` -->\n\n<!-- Compile and fit model: -->\n\n<!-- ``` r -->\n\n<!-- library(tensorflow) -->\n\n<!-- library(keras) -->\n\n<!-- set_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed. -->\n\n<!-- loss_binary = function(true, pred){ -->\n\n<!--   return(loss_binary_crossentropy(true, pred) * 28.0 * 28.0) -->\n\n<!-- } -->\n\n<!-- VAE %>% compile(loss = loss_binary, optimizer = optimizer_adamax()) -->\n\n<!-- VAE %>% fit(train_x, train_x, epochs = 50L) -->\n\n<!-- ``` -->\n\n<!-- And show that it works: -->\n\n<!-- ``` r -->\n\n<!-- dist = down_size_model(train_x[1:2000,,,,drop = FALSE]) -->\n\n<!-- images = up_size_model(dist$sample()[1:5,]) -->\n\n<!-- ownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\", -->\n\n<!--               \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\") -->\n\n<!-- oldpar = par(mfrow = c(1, 1)) -->\n\n<!-- imgPlot(images[1,,,1]$numpy()) -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_19__VAEmnist-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- ``` r -->\n\n<!-- plot(dist$mean()$numpy()[,1], dist$mean()$numpy()[,2], col = ownColors[train[[2]]+1L]) -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_19__VAEmnist-2.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- ## Exercise -->\n\n<!-- ::: {.callout-caution icon=\"false\"} -->\n\n<!-- #### Question -->\n\n<!-- Read section @sec-VAE on variational autoencoders and try to transfer the examples with MNIST to our flower data set (so from black-white images to colored images). -->\n\n<!-- \n<div class='webex-solution'><button>Click here to see the solution</button>\n -->\n\n<!-- Split the data: -->\n\n<!-- ``` r -->\n\n<!-- library(keras) -->\n\n<!-- library(tensorflow) -->\n\n<!-- library(tfprobability) -->\n\n<!-- set_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed. -->\n\n<!-- data = EcoData::dataset_flower() -->\n\n<!-- test = data$test/255 -->\n\n<!-- train = data$train/255 -->\n\n<!-- rm(data) -->\n\n<!-- ``` -->\n\n<!-- Build the variational autoencoder: -->\n\n<!-- ``` r -->\n\n<!-- encoded = 10L -->\n\n<!-- prior = tfp$distributions$Independent( -->\n\n<!--   tfp$distributions$Normal(loc=tf$zeros(encoded), scale = 1.), -->\n\n<!--   reinterpreted_batch_ndims = 1L -->\n\n<!-- ) -->\n\n<!-- down_size_model = tf$keras$models$Sequential(list( -->\n\n<!--   tf$keras$layers$InputLayer(input_shape = c(80L, 80L, 3L)), -->\n\n<!--   tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu, -->\n\n<!--                          kernel_size = 5L, strides = 1L), -->\n\n<!--   tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu, -->\n\n<!--                          kernel_size = 5L, strides = 2L), -->\n\n<!--   tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu, -->\n\n<!--                          kernel_size = 5L, strides = 1L), -->\n\n<!--   tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu, -->\n\n<!--                          kernel_size = 5L, strides = 2L), -->\n\n<!--   tf$keras$layers$Conv2D(filters = 128L, activation = tf$nn$leaky_relu, -->\n\n<!--                          kernel_size = 7L, strides = 1L), -->\n\n<!--   tf$keras$layers$Flatten(), -->\n\n<!--   tf$keras$layers$Dense(units = tfp$layers$MultivariateNormalTriL$params_size(encoded), -->\n\n<!--                         activation = NULL), -->\n\n<!--   tfp$layers$MultivariateNormalTriL( -->\n\n<!--     encoded,  -->\n\n<!--     activity_regularizer = tfp$layers$KLDivergenceRegularizer(prior, weight = 0.0002) -->\n\n<!--   ) -->\n\n<!-- )) -->\n\n<!-- up_size_model = tf$keras$models$Sequential(list( -->\n\n<!--   tf$keras$layers$InputLayer(input_shape = encoded), -->\n\n<!--   tf$keras$layers$Dense(units = 8192L, activation = \"relu\"), -->\n\n<!--   tf$keras$layers$Reshape(target_shape =  c(8L, 8L, 128L)), -->\n\n<!--   tf$keras$layers$Conv2DTranspose(filters = 128L, kernel_size = 7L, -->\n\n<!--                                   activation = tf$nn$leaky_relu, strides = 1L, -->\n\n<!--                                   use_bias = FALSE), -->\n\n<!--   tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L, -->\n\n<!--                                   activation = tf$nn$leaky_relu, strides = 2L, -->\n\n<!--                                   use_bias = FALSE), -->\n\n<!--   tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L, -->\n\n<!--                                   activation = tf$nn$leaky_relu, strides = 1L, -->\n\n<!--                                   use_bias = FALSE), -->\n\n<!--   tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L, -->\n\n<!--                                   activation = tf$nn$leaky_relu, strides = 2L, -->\n\n<!--                                   use_bias = FALSE), -->\n\n<!--   tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L, -->\n\n<!--                                   activation = tf$nn$leaky_relu, strides = 1L, -->\n\n<!--                                   use_bias = FALSE), -->\n\n<!--   tf$keras$layers$Conv2DTranspose(filters = 3L, kernel_size = c(4L, 4L), -->\n\n<!--                                   activation = \"sigmoid\", strides = c(1L, 1L), -->\n\n<!--                                   use_bias = FALSE) -->\n\n<!-- )) -->\n\n<!-- VAE = tf$keras$models$Model(inputs = down_size_model$inputs,  -->\n\n<!--                             outputs = up_size_model(down_size_model$outputs)) -->\n\n<!-- ``` -->\n\n<!-- ```{r, eval = TRUE,echo = FALSE, results = TRUE} -->\n\n<!-- cat(' -->\n\n<!-- summary(VAE) -->\n\n<!-- Model: \"model_3\" -->\n\n<!-- __________________________________________________________________________________________ -->\n\n<!--  Layer (type)                           Output Shape                        Param #        -->\n\n<!-- ========================================================================================== -->\n\n<!--  input_1 (InputLayer)                   [(None, 80, 80, 3)]                 0              -->\n\n<!--  conv2d_14 (Conv2D)                     (None, 76, 76, 32)                  2432           -->\n\n<!--  conv2d_15 (Conv2D)                     (None, 36, 36, 32)                  25632          -->\n\n<!--  conv2d_16 (Conv2D)                     (None, 32, 32, 64)                  51264          -->\n\n<!--  conv2d_17 (Conv2D)                     (None, 14, 14, 64)                  102464         -->\n\n<!--  conv2d_18 (Conv2D)                     (None, 8, 8, 128)                   401536         -->\n\n<!--  flatten_2 (Flatten)                    (None, 8192)                        0              -->\n\n<!--  dense_10 (Dense)                       (None, 65)                          532545         -->\n\n<!--  multivariate_normal_tri_l (Multivariat  ((None, 10),                       0              -->\n\n<!--  eNormalTriL)                            (None, 10))                                       -->\n\n<!--  sequential_7 (Sequential)              (None, 80, 80, 3)                   1278464        -->\n\n<!-- ========================================================================================== -->\n\n<!-- Total params: 2,394,337 -->\n\n<!-- Trainable params: 2,394,337 -->\n\n<!-- Non-trainable params: 0 -->\n\n<!-- __________________________________________________________________________________________     -->\n\n<!--     ') -->\n\n<!-- ``` -->\n\n<!-- Compile and train model: -->\n\n<!-- ``` r -->\n\n<!-- be = function(true, pred){ -->\n\n<!--   return(tf$losses$binary_crossentropy(true, pred) * 80.0 * 80.0) -->\n\n<!-- } -->\n\n<!-- VAE$compile(loss = be, -->\n\n<!--             optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.0003)) -->\n\n<!-- VAE$fit(x = train, y = train, epochs = 50L, shuffle = TRUE, batch_size = 20L) -->\n\n<!-- dist = down_size_model(train[1:10,,,]) -->\n\n<!-- images = up_size_model( dist$sample()[1:5,] ) -->\n\n<!-- oldpar = par(mfrow = c(3, 1), mar = rep(1, 4)) -->\n\n<!-- scales::rescale(images[1,,,]$numpy(), to = c(0, 255)) %>%  -->\n\n<!--   image_to_array() %>% -->\n\n<!--   `/`(., 255) %>% -->\n\n<!--   as.raster() %>% -->\n\n<!--   plot() -->\n\n<!-- scales::rescale(images[2,,,]$numpy(), to = c(0, 255)) %>%  -->\n\n<!--   image_to_array() %>% -->\n\n<!--   `/`(., 255) %>% -->\n\n<!--   as.raster() %>% -->\n\n<!--   plot() -->\n\n<!-- scales::rescale(images[3,,,]$numpy(), to = c(0, 255)) %>%  -->\n\n<!--   image_to_array() %>% -->\n\n<!--   `/`(., 255) %>% -->\n\n<!--   as.raster() %>% -->\n\n<!--   plot() -->\n\n<!-- ``` -->\n\n<!-- <img src=\"09-GAN_files/figure-html/chunk_chapter7_task_2__VAEflower-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/> -->\n\n<!-- \n</div>\n -->\n\n<!-- ::: -->\n",
    "supporting": [
      "E1-Autoencoder_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}