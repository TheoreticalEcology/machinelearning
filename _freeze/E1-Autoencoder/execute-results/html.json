{
  "hash": "553263c98588fe0471ef108cded2b5f5",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Autoencoder\n\n\n\n\n\nAn autoencoder (AE) is a type of artificial neural network for unsupervised learning. The idea is similar to data compression: The first part of the network compresses (encodes) the data to a low dimensional space (e.g. 2-4 dimensions) and the second part of the network decompresses (reverses the encoding) and learns to reconstruct the data (think of a hourglass).\n\nWhy is this useful? The method is similar to a dimension reduction technique (e.g. PCA) but with the advantage that we don't have to make any distributional assumptions (but see PCA). For instance, we could first train an autoencoder on genomic expression data with thousands of features, compress them into 2-4 dimensions, and then use them for clustering.\n\n## Autoencoder - Deep Neural Network MNIST\n\nWe now will write an autoencoder for the MNIST data set.\n\nLet's start with the (usual) MNIST example:\n\n``` r\nlibrary(keras)\nlibrary(tensorflow)\n\ndata = keras::dataset_mnist()\n```\n\nWe don't need the labels here, our images will be the inputs and at the same time the outputs of our final autoencoder.\n\n``` r\nrotate = function(x){ t(apply(x, 2, rev)) }\n\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  if(title != \"\"){ main = paste0(\"Label: \", as.character(title)) }\n  else{ main = \"\" }\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE, main = main)\n}\n\ntrain = data[[1]]\ntest = data[[2]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L))\ntest_x = array(test[[1]]/255, c(dim(test[[1]])[1], 784L))\n```\n\nOur encoder: image (784 dimensions) $\\rightarrow$ 2 dimensions\n\n``` r\ndown_size_model = keras_model_sequential()\ndown_size_model %>% \n  layer_dense(units = 100L, input_shape = c(784L), activation = \"relu\") %>% \n  layer_dense(units = 20L, activation = \"relu\") %>% \n  layer_dense(units = 2L, activation = \"linear\")\n```\n\nOur decoder: 2 dimensions $\\rightarrow$ 784 dimensions (our image)\n\n``` r\nup_size_model = keras_model_sequential()\nup_size_model %>% \n  layer_dense(units = 20L, input_shape = c(2L), activation = \"relu\") %>% \n  layer_dense(units = 100L, activation = \"relu\") %>% \n  layer_dense(units = 784L, activation = \"sigmoid\")\n```\n\nWe can use the non-sequential model type to connect the two models. (We did the same in the transfer learning chapter.)\n\n``` r\nautoencoder = keras_model(inputs = down_size_model$input, \n                          outputs = up_size_model(down_size_model$output))\nautoencoder$compile(loss = loss_binary_crossentropy,\n                    optimizer = optimizer_adamax(0.01))\nsummary(autoencoder)\n#> Model: \"model\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_2_input (InputLayer)             [(None, 784)]                       0             \n#>  dense_2 (Dense)                        (None, 100)                         78500         \n#>  dense_1 (Dense)                        (None, 20)                          2020          \n#>  dense (Dense)                          (None, 2)                           42            \n#>  sequential_1 (Sequential)              (None, 784)                         81344         \n#> ==========================================================================================\n#> Total params: 161,906\n#> Trainable params: 161,906\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n```\n\nWe will now show an example of an image before and after the unfitted autoencoder, so we see that we have to train the autoencoder.\n\n``` r\nimage = autoencoder(train_x[1,,drop = FALSE])\noldpar = par(mfrow = c(1, 2))\nimgPlot(array(train_x[1,,drop = FALSE], c(28, 28)), title = \"Before\")\nimgPlot(array(image$numpy(), c(28, 28)), title = \"After\")\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_6__AEmnistoutput-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\npar(oldpar)\n```\n\nFit the autoencoder (inputs == outputs!):\n\n``` r\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(123L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nautoencoder %>% \n  fit(x = train_x, y = train_x, epochs = 5L, batch_size = 128L)\n```\n\nVisualization of the latent variables:\n\n``` r\npred_dim = down_size_model(test_x)\nreconstr_pred = up_size_model(pred_dim)\nimgPlot(array(reconstr_pred[10,]$numpy(), dim = c(28L, 28L)), title = \"\")\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_8__AEvisualization-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\noldpar = par(mfrow = c(1, 1))\nplot(pred_dim$numpy()[,1], pred_dim$numpy()[,2], col = ownColors[test[[2]]+1L])\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_9__AEvisualizationContinuation-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\npar(oldpar)\n```\n\nThe picture above shows the 2-dimensional encoded values of the numbers in the MNIST data set and the number they are depicting via the respective color.\n\n## Autoencoder - MNIST Convolutional Neural Networks\n\nWe can also use convolutional neural networks instead or on the side of deep neural networks: Prepare data:\n\n``` r\ndata = tf$keras$datasets$mnist$load_data()\ntrain = data[[1]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]]), 1L))\ntest_x = array(data[[2]][[1]]/255, c(dim(data[[2]][[1]]/255), 1L))\n```\n\nThen define the downsize model:\n\n``` r\ndown_size_model = keras_model_sequential()\ndown_size_model %>% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L, 3L), input_shape = c(28L, 28L, 1L), padding = \"same\") %>% \n  layer_max_pooling_2d(, padding = \"same\") %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_max_pooling_2d(, padding = \"same\") %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_max_pooling_2d(, padding = \"same\") %>% \n  layer_flatten() %>% \n  layer_dense(units = 2L, activation = \"linear\")\n```\n\nDefine the upsize model:\n\n``` r\nup_size_model = keras_model_sequential()\nup_size_model %>% \n  layer_dense(units = 128L, activation = \"relu\", input_shape = c(2L)) %>% \n  layer_reshape(target_shape = c(4L, 4L, 8L)) %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_upsampling_2d() %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_upsampling_2d() %>% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L,3L)) %>% \n  layer_upsampling_2d() %>% \n  layer_conv_2d(filters = 1, activation = \"sigmoid\", kernel_size = c(3L,3L), padding = \"same\")\n```\n\nCombine the two models and fit it:\n\n``` r\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nautoencoder = tf$keras$models$Model(inputs = down_size_model$input,\n                                    outputs = up_size_model(down_size_model$output))\n\nautoencoder %>% compile(loss = loss_binary_crossentropy,\n                    optimizer = optimizer_rmsprop(0.001))\n\nautoencoder %>%  fit(x = tf$constant(train_x), y = tf$constant(train_x),\n                      epochs = 50L, batch_size = 64L)\n```\n\nTest it:\n\n``` r\npred_dim = down_size_model(tf$constant(test_x, \"float32\"))\nreconstr_pred = autoencoder(tf$constant(test_x, \"float32\"))\nimgPlot(reconstr_pred[10,,,]$numpy()[,,1])\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_14-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\n\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\nplot(pred_dim[,1]$numpy(), pred_dim[,2]$numpy(), col = ownColors[test[[2]]+1L])\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_14-2.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\n\n## Generate new images!\nnew = matrix(c(10, 10), 1, 2)\nimgPlot(array(up_size_model(new)$numpy(), c(28L, 28L)))\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_14-3.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\n\nnew = matrix(c(5, 5), 1, 2)\nimgPlot(array(up_size_model(new)$numpy(), c(28L, 28L)))\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_14-4.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n## Variational Autoencoder (VAE) {#sec-VAE}\n\nThe difference between a variational and a normal autoencoder is that a variational autoencoder assumes a distribution for the latent variables (latent variables cannot be observed and are composed of other variables) and the parameters of this distribution are learned. Thus new objects can be generated by inserting valid (!) (with regard to the assumed distribution) \"seeds\" to the decoder. To achieve the property that more or less randomly chosen points in the low dimensional latent space are meaningful and yield suitable results after decoding, the latent space/training process must be regularized. In this process, the input to the VAE is encoded to a distribution in the latent space rather than a single point.\n\nFor building variational autoencoders, we will use TensorFlow probability, but first, we need to split the data again.\n\n``` r\nlibrary(tfprobability)\n\ndata = tf$keras$datasets$mnist$load_data()\ntrain = data[[1]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]]), 1L))\n```\n\nWe will use TensorFlow probability to define priors for our latent variables.\n\n``` r\nlibrary(tfprobability)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ntfp = reticulate::import(\"tensorflow_probability\")\n```\n\nBuild the two networks:\n\n``` r\nencoded = 2L\nprior = tfd_independent(tfd_normal(c(0.0, 0.0), 1.0), 1L)\n\nup_size_model = keras_model_sequential()\nup_size_model %>% \n  layer_dense(units = 128L, activation = \"relu\", input_shape = c(2L)) %>% \n  layer_reshape(target_shape = c(4L, 4L, 8L)) %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_upsampling_2d() %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_upsampling_2d() %>% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L,3L)) %>% \n  layer_upsampling_2d() %>% \n  layer_conv_2d(filters = 1, activation = \"sigmoid\", kernel_size = c(3L,3L), padding = \"same\")\n\ndown_size_model = keras_model_sequential()\ndown_size_model %>% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L, 3L), input_shape = c(28L, 28L, 1L), padding = \"same\") %>% \n  layer_max_pooling_2d(, padding = \"same\") %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_max_pooling_2d(, padding = \"same\") %>% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %>% \n  layer_max_pooling_2d(, padding = \"same\") %>% \n  layer_flatten() %>% \n  layer_dense(units = 4L, activation = \"linear\") %>% \n  layer_independent_normal(2L,\n                           activity_regularizer =\n                             tfp$layers$KLDivergenceRegularizer(distribution_b = prior))\n\nVAE = keras_model(inputs = down_size_model$inputs,\n                  outputs = up_size_model(down_size_model$outputs))\n```\n\nCompile and fit model:\n\n``` r\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nloss_binary = function(true, pred){\n  return(loss_binary_crossentropy(true, pred) * 28.0 * 28.0)\n}\nVAE %>% compile(loss = loss_binary, optimizer = optimizer_adamax())\n\nVAE %>% fit(train_x, train_x, epochs = 50L)\n```\n\nAnd show that it works:\n\n``` r\ndist = down_size_model(train_x[1:2000,,,,drop = FALSE])\nimages = up_size_model(dist$sample()[1:5,])\n\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\noldpar = par(mfrow = c(1, 1))\nimgPlot(images[1,,,1]$numpy())\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_19__VAEmnist-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\nplot(dist$mean()$numpy()[,1], dist$mean()$numpy()[,2], col = ownColors[train[[2]]+1L])\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_19__VAEmnist-2.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\npar(oldpar)\n```\n\n## Exercise\n\n::: {.callout-caution icon=\"false\"}\n#### Question\n\nRead section @sec-VAE on variational autoencoders and try to transfer the examples with MNIST to our flower data set (so from black-white images to colored images).\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nSplit the data:\n\n``` r\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(tfprobability)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = EcoData::dataset_flower()\ntest = data$test/255\ntrain = data$train/255\nrm(data)\n```\n\nBuild the variational autoencoder:\n\n``` r\nencoded = 10L\nprior = tfp$distributions$Independent(\n  tfp$distributions$Normal(loc=tf$zeros(encoded), scale = 1.),\n  reinterpreted_batch_ndims = 1L\n)\n\ndown_size_model = tf$keras$models$Sequential(list(\n  tf$keras$layers$InputLayer(input_shape = c(80L, 80L, 3L)),\n  tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 1L),\n  tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 2L),\n  tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 1L),\n  tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 2L),\n  tf$keras$layers$Conv2D(filters = 128L, activation = tf$nn$leaky_relu,\n                         kernel_size = 7L, strides = 1L),\n  tf$keras$layers$Flatten(),\n  tf$keras$layers$Dense(units = tfp$layers$MultivariateNormalTriL$params_size(encoded),\n                        activation = NULL),\n  tfp$layers$MultivariateNormalTriL(\n    encoded, \n    activity_regularizer = tfp$layers$KLDivergenceRegularizer(prior, weight = 0.0002)\n  )\n))\n\nup_size_model = tf$keras$models$Sequential(list(\n  tf$keras$layers$InputLayer(input_shape = encoded),\n  tf$keras$layers$Dense(units = 8192L, activation = \"relu\"),\n  tf$keras$layers$Reshape(target_shape =  c(8L, 8L, 128L)),\n  tf$keras$layers$Conv2DTranspose(filters = 128L, kernel_size = 7L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 2L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 2L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 3L, kernel_size = c(4L, 4L),\n                                  activation = \"sigmoid\", strides = c(1L, 1L),\n                                  use_bias = FALSE)\n))\n\nVAE = tf$keras$models$Model(inputs = down_size_model$inputs, \n                            outputs = up_size_model(down_size_model$outputs))\nsummary(VAE)\n#> Model: \"model_3\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  input_1 (InputLayer)                   [(None, 80, 80, 3)]                 0             \n#>  conv2d_14 (Conv2D)                     (None, 76, 76, 32)                  2432          \n#>  conv2d_15 (Conv2D)                     (None, 36, 36, 32)                  25632         \n#>  conv2d_16 (Conv2D)                     (None, 32, 32, 64)                  51264         \n#>  conv2d_17 (Conv2D)                     (None, 14, 14, 64)                  102464        \n#>  conv2d_18 (Conv2D)                     (None, 8, 8, 128)                   401536        \n#>  flatten_2 (Flatten)                    (None, 8192)                        0             \n#>  dense_10 (Dense)                       (None, 65)                          532545        \n#>  multivariate_normal_tri_l (Multivariat  ((None, 10),                       0             \n#>  eNormalTriL)                            (None, 10))                                      \n#>  sequential_7 (Sequential)              (None, 80, 80, 3)                   1278464       \n#> ==========================================================================================\n#> Total params: 2,394,337\n#> Trainable params: 2,394,337\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n```\n\nCompile and train model:\n\n``` r\nbe = function(true, pred){\n  return(tf$losses$binary_crossentropy(true, pred) * 80.0 * 80.0)\n}\n\nVAE$compile(loss = be,\n            optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.0003))\nVAE$fit(x = train, y = train, epochs = 50L, shuffle = TRUE, batch_size = 20L)\n#> <keras.callbacks.History object at 0x7fcefde64b90>\n\ndist = down_size_model(train[1:10,,,])\nimages = up_size_model( dist$sample()[1:5,] )\n\noldpar = par(mfrow = c(3, 1), mar = rep(1, 4))\nscales::rescale(images[1,,,]$numpy(), to = c(0, 255)) %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\nscales::rescale(images[2,,,]$numpy(), to = c(0, 255)) %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\nscales::rescale(images[3,,,]$numpy(), to = c(0, 255)) %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_task_2__VAEflower-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n``` r\npar(oldpar)\n```\n\n\n</div>\n\n\n:::\n",
    "supporting": [
      "E1-Autoencoder_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}