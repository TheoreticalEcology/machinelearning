{
  "hash": "e787bbf1a46a1bd4fbdf8a4f50c57049",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Machine learning framework - mlr3 {#workflow-mlr3}\n\n\nAs we have seen today, many of the machine learning algorithms are distributed over several packages but the general machine learning pipeline is very similar for all models: feature engineering, feature selection, hyperparameter tuning and cross-validation.\n\nMachine learning frameworks such as `mlr3` or `tidymodels` provide a general interface for the ML pipeline, in particular the training and the hyperparameter tuning with nested CV. They support most ML packages/algorithms.\n\n## mlr3 {#sec-mlr}\n\nThe key features of mlr3 are:\n\n-   All common machine learning packages are integrated into mlr3, you can easily switch between different machine learning algorithms.\n-   A common 'language'/workflow to specify machine learning pipelines.\n-   Support for different cross-validation strategies.\n-   Hyperparameter tuning for all supported machine learning algorithms.\n-   Ensemble models.\n\nUseful links:\n\n-   <a href=\"https://mlr3book.mlr-org.com/\" target=\"_blank\" rel=\"noopener\">mlr3-book</a> (still in work)\n-   <a href=\"https://mlr3.mlr-org.com/\" target=\"_blank\" rel=\"noopener\">mlr3 website</a>\n-   <a href=\"https://cheatsheets.mlr-org.com/mlr3.pdf\" target=\"_blank\" rel=\"noopener\">mlr3 cheatsheet</a>\n\n### mlr3 - The Basic Workflow\n\nThe mlr3 package actually consists of several packages for different tasks (e.g. mlr3tuning for hyperparameter tuning, mlr3pipelines for data preparation pipes). But let's start with the basic workflow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(cito)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t4687 obs. of  40 variables:\n $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n $ Orbital.Period              : num  419 1040 682 427 503 ...\n $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n```\n:::\n:::\n\n\nLet's drop time, name and ID variable and create a classification task:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = nasa %>% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\n```\n:::\n\n\nCreate a generic pipeline of data transformation (imputation $\\rightarrow$ scaling $\\rightarrow$ encoding of categorical variables):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   Hazardous           Absolute.Magnitude \n                        4187                            0 \n               Aphelion.Dist           Asc.Node.Longitude \n                           0                            0 \n                Eccentricity    Epoch.Date.Close.Approach \n                           0                            0 \n            Epoch.Osculation         Est.Dia.in.Feet.max. \n                           0                            0 \n        Est.Dia.in.Feet.min.           Est.Dia.in.KM.max. \n                           0                            0 \n          Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n                           0                            0 \n           Est.Dia.in.M.min.        Est.Dia.in.Miles.max. \n                           0                            0 \n       Est.Dia.in.Miles.min.                  Inclination \n                           0                            0 \n Jupiter.Tisserand.Invariant                 Mean.Anomaly \n                           0                            0 \n                 Mean.Motion               Miles.per.hour \n                           0                            0 \n  Minimum.Orbit.Intersection     Miss.Dist..Astronomical. \n                           0                            0 \n      Miss.Dist..kilometers.            Miss.Dist..lunar. \n                           0                            0 \n           Miss.Dist..miles.                     Orbit.ID \n                           0                            0 \n          Orbit.Uncertainity               Orbital.Period \n                           0                            0 \n              Perihelion.Arg          Perihelion.Distance \n                           0                            0 \n             Perihelion.Time  Relative.Velocity.km.per.hr \n                           0                            0 \nRelative.Velocity.km.per.sec              Semi.Major.Axis \n                           0                            0 \n               Equinox.J2000             Equinox..MISSING \n                           0                            0 \n         Orbiting.Body.Earth       Orbiting.Body..MISSING \n                           0                            0 \n```\n:::\n:::\n\n\nWe can even visualize the preprocessing graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreprocessing$plot()\n```\n\n::: {.cell-output-display}\n![](A4-MLpipeline-mlr3_files/figure-html/chunk_chapter4_68-1.png){width=672}\n:::\n:::\n\n\nTo test our model (glmnet) with 10-fold cross-validated, we will do:\n\n-   Specify the missing target rows as validation so that they will be ignored.\n-   Specify the cross-validation, the learner (the machine learning model we want to use), and the measurement (AUC).\n-   Run (benchmark) our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ntransformed_task$data()[1,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity\n      <fctr>              <num>         <num>              <num>        <num>\n1:         0         -0.8132265    -0.3804201          -1.140837    -0.315606\n   Epoch.Date.Close.Approach Epoch.Osculation Est.Dia.in.Feet.max.\n                       <num>            <num>                <num>\n1:                 -4.792988        0.1402677            0.2714179\n   Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max.\n                  <num>              <num>              <num>             <num>\n1:            0.3134076          0.3007134          0.2565687         0.2710953\n   Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n               <num>                 <num>                 <num>       <num>\n1:         0.2916245             0.2620443              0.258651   0.5442288\n   Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n                         <num>        <num>       <num>          <num>\n1:                   0.3840868    -1.028761   0.3193953     -0.2541306\n   Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers.\n                        <num>                    <num>                  <num>\n1:                  -5.459119                -7.076926              0.2512296\n   Miss.Dist..lunar. Miss.Dist..miles.  Orbit.ID Orbit.Uncertainity\n               <num>             <num>     <num>              <num>\n1:         0.2398625         0.2381077 -9.651472          -1.007087\n   Orbital.Period Perihelion.Arg Perihelion.Distance Perihelion.Time\n            <num>          <num>               <num>           <num>\n1:     -0.3013135      -1.170536         -0.01831583       0.1052611\n   Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec Semi.Major.Axis\n                         <num>                        <num>           <num>\n1:                  -0.2816782                   -0.2841407      -0.2791037\n   Equinox.J2000 Equinox..MISSING Orbiting.Body.Earth Orbiting.Body..MISSING\n           <num>            <num>               <num>                  <num>\n1:             1                0                   1                      0\n```\n:::\n\n```{.r .cell-code}\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresult = mlr3::resample(transformed_task,\n                        EN, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n```\n:::\n\n\nVery cool! Preprocessing + 10-fold cross-validation model evaluation in a few lines of code!\n\nLet's create the final predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n```\n:::\n\n\nYou could now submit the predictions <a href=\"http://rhsbio7.uni-regensburg.de:8500\" target=\"_blank\" rel=\"noopener\">here</a>.\n\nBut we are still not happy with the results, let's do some hyperparameter tuning!\n\n### mlr3 - Hyperparameter Tuning\n\nWith mlr3, we can easily extend the above example to do hyperparameter tuning within nested cross-validation (the tuning has its own inner cross-validation).\n\nPrint the hyperparameter space of our glmnet learner:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEN$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Unknown argument 'on' has been passed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nKey: <id>\n                      id    class lower upper nlevels\n                  <char>   <char> <num> <num>   <num>\n 1:                alpha ParamDbl     0     1     Inf\n 2:                  big ParamDbl  -Inf   Inf     Inf\n 3:               devmax ParamDbl     0     1     Inf\n 4:                dfmax ParamInt     0   Inf     Inf\n 5:                  eps ParamDbl     0     1     Inf\n 6:                epsnr ParamDbl     0     1     Inf\n 7:                exact ParamLgl    NA    NA       2\n 8:              exclude ParamInt     1   Inf     Inf\n 9:                 exmx ParamDbl  -Inf   Inf     Inf\n10:                 fdev ParamDbl     0     1     Inf\n11:                gamma ParamDbl  -Inf   Inf     Inf\n12:            intercept ParamLgl    NA    NA       2\n13:               lambda ParamUty    NA    NA     Inf\n14:     lambda.min.ratio ParamDbl     0     1     Inf\n15:         lower.limits ParamUty    NA    NA     Inf\n16:                maxit ParamInt     1   Inf     Inf\n17:                mnlam ParamInt     1   Inf     Inf\n18:                 mxit ParamInt     1   Inf     Inf\n19:               mxitnr ParamInt     1   Inf     Inf\n20:            newoffset ParamUty    NA    NA     Inf\n21:              nlambda ParamInt     1   Inf     Inf\n22:               offset ParamUty    NA    NA     Inf\n23:       penalty.factor ParamUty    NA    NA     Inf\n24:                 pmax ParamInt     0   Inf     Inf\n25:                 pmin ParamDbl     0     1     Inf\n26:                 prec ParamDbl  -Inf   Inf     Inf\n27:                relax ParamLgl    NA    NA       2\n28:                    s ParamDbl     0   Inf     Inf\n29:          standardize ParamLgl    NA    NA       2\n30: standardize.response ParamLgl    NA    NA       2\n31:               thresh ParamDbl     0   Inf     Inf\n32:             trace.it ParamInt     0     1       2\n33:        type.gaussian ParamFct    NA    NA       2\n34:        type.logistic ParamFct    NA    NA       2\n35:     type.multinomial ParamFct    NA    NA       2\n36:         upper.limits ParamUty    NA    NA     Inf\n                      id    class lower upper nlevels\n                                                                                      default\n                                                                                       <list>\n 1:                                                                                         1\n 2:                                                                                   9.9e+35\n 3:                                                                                     0.999\n 4: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n 5:                                                                                     1e-06\n 6:                                                                                     1e-08\n 7:                                                                                     FALSE\n 8: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n 9:                                                                                       250\n10:                                                                                     1e-05\n11:                                                                                         1\n12:                                                                                      TRUE\n13: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n14: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n15: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n16:                                                                                    100000\n17:                                                                                         5\n18:                                                                                       100\n19:                                                                                        25\n20: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n21:                                                                                       100\n22:                                                                                          \n23: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n24: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n25:                                                                                     1e-09\n26:                                                                                     1e-10\n27:                                                                                     FALSE\n28:                                                                                      0.01\n29:                                                                                      TRUE\n30:                                                                                     FALSE\n31:                                                                                     1e-07\n32:                                                                                         0\n33: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n34: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n35: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n36: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n                                                                                      default\n    parents  value\n     <list> <list>\n 1:               \n 2:               \n 3:               \n 4:               \n 5:               \n 6:               \n 7:               \n 8:               \n 9:               \n10:               \n11:   relax       \n12:               \n13:               \n14:               \n15:               \n16:               \n17:               \n18:               \n19:               \n20:               \n21:               \n22:               \n23:               \n24:               \n25:               \n26:               \n27:               \n28:               \n29:               \n30:               \n31:               \n32:               \n33:               \n34:               \n35:               \n36:               \n    parents  value\n```\n:::\n:::\n\n\nDefine the hyperparameter space of the random forest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(paradox)\n\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\nprint(EN_pars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n       id    class lower upper nlevels\n   <char>   <char> <num> <num>   <num>\n1:  alpha ParamDbl     0   1.0     Inf\n2: lambda ParamDbl     0   0.5     Inf\n                                                                                     default\n                                                                                      <list>\n1: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n2: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n    value\n   <list>\n1:       \n2:       \n```\n:::\n:::\n\n\nTo set up the tuning pipeline we need:\n\n-   Inner cross-validation resampling object.\n-   Tuning criterion (e.g. AUC).\n-   Tuning method (e.g. random or block search).\n-   Tuning terminator (When should we stop tuning? E.g. after $n$ iterations).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<AutoTuner:classif.glmnet.tuned>\n* Model: list\n* Search Space:\n<ParamSet>\n       id    class lower upper nlevels\n   <char>   <char> <num> <num>   <num>\n1:  alpha ParamDbl     0   1.0     Inf\n2: lambda ParamDbl     0   0.5     Inf\n                                                                                     default\n                                                                                      <list>\n1: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n2: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n    value\n   <list>\n1:       \n2:       \n* Packages: mlr3, mlr3tuning, mlr3learners, glmnet\n* Predict Type: prob\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass, weights\n```\n:::\n:::\n\n\nNow we can wrap it normally into the 10-fold cross-validated setup as done previously:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.auc \n  0.6767554 \n```\n:::\n:::\n\n\nLet's create the final predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n```\n:::\n\n\n## Exercises\n\n### Tuning Regularization\n\n::: callout-warning\n#### Question: Hyperparameter tuning - Titanic dataset\n\nTune architecture\n\n-   Tune training parameters (learning rate, batch size) and regularization\n\n**Hints**\n\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\n-   passing `tune(...)` to a hyperparameter will tell cito to tune this specific hyperparameter\n-   the `tuning = config_tuning(...)` let you specify the cross-validation strategy and the number of hyperparameters that should be tested (steps = number of hyperparameter combinations that should be tried)\n-   after tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~., \n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L)\n                  )\n\n# tuning results\nmodel_tuned$tuning\n\n\n# model_tuned is now already the best model!\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\n\nmodel = dnn(survived~., \n          hidden = c(10L, 10L), # change\n          activation = c(\"selu\", \"selu\"), # change\n          loss = \"binomial\", \n          lr = 0.05, #change\n          validation = 0.2,\n          lambda = 0.001, # change\n          alpha = 0.1, # change\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = data_obs, epochs = 40L, verbose = TRUE, plot= TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 1: training: 0.720, validation: 0.673, lr: 0.05000\n```\n:::\n\n::: {.cell-output-display}\n![](A4-MLpipeline-mlr3_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 2: training: 0.658, validation: 0.811, lr: 0.05000\nLoss at epoch 3: training: 0.654, validation: 0.600, lr: 0.05000\nLoss at epoch 4: training: 0.647, validation: 0.824, lr: 0.05000\nLoss at epoch 5: training: 0.641, validation: 0.586, lr: 0.05000\nLoss at epoch 6: training: 0.611, validation: 0.563, lr: 0.05000\nLoss at epoch 7: training: 0.639, validation: 0.729, lr: 0.05000\nLoss at epoch 8: training: 0.618, validation: 0.565, lr: 0.05000\nLoss at epoch 9: training: 0.604, validation: 0.754, lr: 0.05000\nLoss at epoch 10: training: 0.602, validation: 0.566, lr: 0.05000\nLoss at epoch 11: training: 0.571, validation: 0.510, lr: 0.05000\nLoss at epoch 12: training: 0.593, validation: 0.529, lr: 0.05000\nLoss at epoch 13: training: 0.594, validation: 0.499, lr: 0.05000\nLoss at epoch 14: training: 0.564, validation: 0.560, lr: 0.05000\nLoss at epoch 15: training: 0.575, validation: 0.484, lr: 0.05000\nLoss at epoch 16: training: 0.559, validation: 0.503, lr: 0.05000\nLoss at epoch 17: training: 0.539, validation: 0.457, lr: 0.05000\nLoss at epoch 18: training: 0.537, validation: 0.553, lr: 0.05000\nLoss at epoch 19: training: 0.590, validation: 0.565, lr: 0.05000\nLoss at epoch 20: training: 0.592, validation: 0.549, lr: 0.05000\nLoss at epoch 21: training: 0.535, validation: 0.962, lr: 0.05000\nLoss at epoch 22: training: 0.515, validation: 0.461, lr: 0.05000\nLoss at epoch 23: training: 0.564, validation: 0.453, lr: 0.05000\nLoss at epoch 24: training: 0.514, validation: 0.416, lr: 0.05000\nLoss at epoch 25: training: 0.517, validation: 0.648, lr: 0.05000\nLoss at epoch 26: training: 0.483, validation: 0.573, lr: 0.05000\nLoss at epoch 27: training: 0.492, validation: 0.406, lr: 0.05000\nLoss at epoch 28: training: 0.501, validation: 0.458, lr: 0.05000\nLoss at epoch 29: training: 0.466, validation: 0.491, lr: 0.05000\nLoss at epoch 30: training: 0.594, validation: 0.423, lr: 0.05000\nLoss at epoch 31: training: 0.525, validation: 0.726, lr: 0.05000\nLoss at epoch 32: training: 0.482, validation: 0.397, lr: 0.05000\nLoss at epoch 33: training: 0.524, validation: 0.965, lr: 0.05000\nLoss at epoch 34: training: 0.525, validation: 0.885, lr: 0.05000\nLoss at epoch 35: training: 0.520, validation: 0.430, lr: 0.05000\nLoss at epoch 36: training: 0.546, validation: 0.577, lr: 0.05000\nLoss at epoch 37: training: 0.493, validation: 0.570, lr: 0.05000\nLoss at epoch 38: training: 0.465, validation: 1.023, lr: 0.05000\nLoss at epoch 39: training: 0.470, validation: 0.493, lr: 0.05000\nLoss at epoch 40: training: 0.482, validation: 0.392, lr: 0.05000\n```\n:::\n\n```{.r .cell-code}\n# Predictions:\n\npredictions = predict(model, newdata = data_new, type = \"response\") # change prediction type to response so that cito predicts probabilities\n\nwrite.csv(data.frame(y = predictions[,1]), file = \"Max_titanic_dnn.csv\")\n```\n:::\n\n:::\n\n<!-- ::: {.callout-warning} -->\n\n<!-- #### Task: Tuning $\\alpha$ and $\\lambda$ -->\n\n<!-- 1.  Extend the code from above and tune $\\alpha$ and $\\lambda$ (via 10xCV) -->\n\n<!-- 2.  Train the model with best set of hyperparameters and submit your predictions -->\n\n<!-- Submit your predictions (<http://rhsbio7.uni-regensburg.de:8500/>), which model has a higher AUC?. -->\n\n<!-- **Important**: -->\n\n<!-- Submissions only work if your preditions are probabilities and a data.frame (which you can create via `write.csv(data.frame(y = predictions ), file = \"Max_1.csv\")` -->\n\n<!--  ) -->\n\n<!-- ```{r} -->\n\n<!-- library(EcoData) -->\n\n<!-- library(dplyr) -->\n\n<!-- library(missRanger) -->\n\n<!-- library(glmnet) -->\n\n<!-- library(glmnetUtils) -->\n\n<!-- data(titanic_ml) -->\n\n<!-- data = titanic_ml -->\n\n<!-- data =  -->\n\n<!--   data %>% select(survived, sex, age, fare, pclass) -->\n\n<!-- # missRanger uses a random forest to impute NAs (RF is trained on the data to predict values for the NAs) -->\n\n<!-- data[,-1] = missRanger(data[,-1], verbose = 0) -->\n\n<!-- data_sub = -->\n\n<!--   data %>% -->\n\n<!--     mutate(age = scales::rescale(age, c(0, 1)), -->\n\n<!--            fare = scales::rescale(fare, c(0, 1))) %>% -->\n\n<!--     mutate(sex = as.integer(sex) - 1L, -->\n\n<!--            pclass = as.integer(pclass - 1L)) -->\n\n<!-- data_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end -->\n\n<!-- data_obs = data_sub[!is.na(data_sub$survived),] # data with known response -->\n\n<!-- ``` -->\n\n<!-- Bonus: -->\n\n<!-- -   Try different features -->\n\n<!-- -   Try cito -->\n\n<!-- -   Try different datasets (see @sec-datasets) -->\n\n<!-- Code template for a simple CV (for tuning $\\lambda$): -->\n\n<!-- - Extend the following code so that $\\alpha$ is also tuned! -->\n\n<!-- ```{r} -->\n\n<!-- set.seed(42) -->\n\n<!-- cv = 5 -->\n\n<!-- hyper_lambda = runif(20,0, 0.2) -->\n\n<!-- tuning_results =  -->\n\n<!--     sapply(1:length(hyper_lambda), function(k) { -->\n\n<!--         auc_inner = NULL -->\n\n<!--         for(j in 1:cv) { -->\n\n<!--           inner_split = as.integer(cut(1:nrow(data_obs), breaks = cv)) -->\n\n<!--           train_inner = data_obs[inner_split != j, ] -->\n\n<!--           test_inner = data_obs[inner_split == j, ] -->\n\n<!--           model = glmnet(survived~.,data = train_inner, lambda = hyper_lambda[k], family = \"binomial\") -->\n\n<!--           auc_inner[j]= Metrics::auc(test_inner$survived, predict(model, test_inner, type = \"response\")) -->\n\n<!--         } -->\n\n<!--       return(mean(auc_inner)) -->\n\n<!--     }) -->\n\n<!-- results = data.frame(lambda = hyper_lambda, AUC = tuning_results) -->\n\n<!-- print(results) -->\n\n<!-- ``` -->\n\n<!-- ::: -->\n\n<!-- \n<div class='webex-solution'><button>Click here to see the solution</button>\n -->\n\n<!-- ```{r} -->\n\n<!-- set.seed(42) -->\n\n<!-- cv = 5 -->\n\n<!-- hyper_lambda = runif(20,0, 0.2) -->\n\n<!-- hyper_alpha = runif(20, 0, 1) -->\n\n<!-- tuning_results =  -->\n\n<!--     sapply(1:length(hyper_alpha), function(k) { -->\n\n<!--         auc_inner = NULL -->\n\n<!--         for(j in 1:cv) { -->\n\n<!--           inner_split = as.integer(cut(1:nrow(data_obs), breaks = cv)) -->\n\n<!--           train_inner = data_obs[inner_split != j, ] -->\n\n<!--           test_inner = data_obs[inner_split == j, ] -->\n\n<!--           model = glmnet(survived~.,data = train_inner, family = \"binomial\", alpha = hyper_alpha[k], lambda = hyper_lambda[k]) -->\n\n<!--           auc_inner[j]= Metrics::auc(test_inner$survived, predict(model, test_inner, type = \"response\")) -->\n\n<!--         } -->\n\n<!--       return(mean(auc_inner)) -->\n\n<!--     }) -->\n\n<!-- results = data.frame(lambda = hyper_lambda, alpha = hyper_alpha,  AUC = tuning_results) -->\n\n<!-- print(results) -->\n\n<!-- print(results[which.max(results$AUC),]) -->\n\n<!-- ``` -->\n\n<!-- Predictions: -->\n\n<!-- ```{r, results='hide', message=FALSE, warning=FALSE} -->\n\n<!-- model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = results[which.max(results$AUC),2]) -->\n\n<!-- predictions = predict(model, data_new, alpha = results$alpha[i], s = results[which.max(results$AUC),1], type = \"response\")[,1] -->\n\n<!-- write.csv(data.frame(y = predictions, file = \"Max_titanic_best_model.csv\")) -->\n\n<!-- ``` -->\n\n<!-- \n</div>\n -->\n\n### Bonus: mlr3\n\n::: callout-warning\n#### Task: Use mlr3 for the titanic dataset\n\n1.  Use `mlr3` to tune glmnet for the titanic dataset using nested CV\n2.  Submit single predictions and multiple predictions\n\nIf you need help, take a look at the solution, go through it line by line and try to understand it.\n:::\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nPrepare data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = titanic_ml %>% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n survived       age      boat     cabin  embarked      fare home.dest     parch \n      655       263         0         0         0         0         0         0 \n   pclass       sex     sibsp \n        0         0         0 \n```\n:::\n\n```{.r .cell-code}\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n```\n:::\n\n\nHyperparameter tuning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\n\n\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = cv10, store_models = TRUE)\n```\n:::\n\n\nEvaluation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasurement =  msr(\"classif.auc\")\nresult$aggregate(measurement)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.auc \n  0.9939211 \n```\n:::\n:::\n\n\nPredictions:\n\nWe can extract a learner with optimized hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = result$learners[[1]]$learner$clone()\nmodel$param_set$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$alpha\n[1] 0.1832108\n\n$lambda\n[1] 0.1246408\n```\n:::\n:::\n\n\nAnd we can fit it then on the full data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$train(transformed_task)\npredictions = model$predict(transformed_task, row_ids = transformed_task$row_roles$holdout)\npredictions = predictions$prob[,1]\nhead(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8555042 0.1554276 0.3219091 0.7343347 0.8628420 0.8735773\n```\n:::\n:::\n\n\nAnd submit to http://rhsbio7.uni-regensburg.de:8500\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(data.frame(y = predictions), file = \"glmnet.csv\")\n```\n:::\n\n\n\n</div>\n\n",
    "supporting": [
      "A4-MLpipeline-mlr3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}