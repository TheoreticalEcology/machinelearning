{
  "hash": "ec459f36027d56d4afca66b6ffcad45e",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n# Deep Neural Networks (DNN)\n\n\n\n\n\n\n\n\n\nWe can use TensorFlow directly from R (see @sec-tensorflowintro for an introduction to TensorFlow), and we could use this knowledge to implement a neural network in TensorFlow directly in R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us implement the machine learning models in TensorFlow. The most common of these is Keras.\n\nKeras is a powerful framework for building and training neural networks with just a few lines of code. As of the end of 2018, Keras and TensorFlow are fully interoperable, allowing us to take advantage of the best of both.\n\nThe goal of this lesson is to familiarize you with Keras. If you have TensorFlow installed, you can find Keras inside TensorFlow: tf.keras. However, the RStudio team has built an R package on top of tf.keras that is more convenient to use. To load the Keras package, type\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras3)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'keras3'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tensorflow':\n\n    set_random_seed, shape\n```\n\n\n:::\n\n```{.r .cell-code}\n# or library(torch)\n```\n:::\n\n\n\n\n## Example workflow in Keras / Torch\n\nWe build a small classifier to predict the three species of the iris data set. Load the necessary packages and data sets:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras3)\nlibrary(tensorflow)\nlibrary(torch)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'torch'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:keras3':\n\n    as_iterator\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(iris)\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n\n\n\nFor neural networks, it is beneficial to scale the predictors (scaling = centering and standardization, see ?scale). We also split our data into predictors (X) and response (Y = the three species).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = scale(iris[,1:4])\nY = iris[,5]\n```\n:::\n\n\n\n\nAdditionally, Keras/TensorFlow cannot handle factors and we have to create contrasts (one-hot encoding). To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like Python and C++, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Keep this in mind.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY = keras3::to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    1    0    0\n[3,]    1    0    0\n[4,]    1    0    0\n[5,]    1    0    0\n[6,]    1    0    0\n```\n\n\n:::\n:::\n\n\n\n\nAfter having prepared the data, we start now with the typical workflow in keras.\n\n**1. Initialize a sequential model in Keras:**\n\n::: panel-tabset\n## Keras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = keras_model_sequential(shape(4L))\n```\n:::\n\n\n\n\n## Torch\n\nTorch users can skip this step.\n:::\n\nA sequential Keras model is a higher order type of model within Keras and consists of one input and one output model.\n\n**2. Add hidden layers to the model (we will learn more about hidden layers during the next days).**\n\nWhen specifying the hidden layers, we also have to specify the shape and a so called *activation function*. You can think of the activation function as decision for what is forwarded to the next neuron (but we will learn more about it later). If you want to know this topic in even more depth, consider watching the videos presented in section \\@ref(basicMath).\n\nThe shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).\n\n::: panel-tabset\n## Keras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n  layer_dense(units = 3L, activation = \"softmax\") \n```\n:::\n\n\n\n\n## Torch\n\nThe Torch syntax is very similar, we will give a list of layers to the \"nn_sequential\" function. Here, we have to specify the softmax activation function as an extra layer:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\n```\n:::\n\n\n\n:::\n\n-   softmax scales a potential multidimensional vector to the interval $(0, 1]$ for each component. The sum of all components equals 1. This might be very useful for example for handling probabilities. **Ensure ther the labels start at 0! Otherwise the softmax function does not work well!**\n\n**3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).**\n\nWe will learn about other options later, so for now, do not worry about the \"**learning_rate**\" (\"**lr**\" in Torch or earlier in TensorFlow) argument, cross entropy or the optimizer.\n\n::: panel-tabset\n## Keras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  compile(loss = keras3::loss_categorical_crossentropy,\n          keras3::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ (None, 20)               │           100 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ (None, 20)               │           420 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ (None, 20)               │           420 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ (None, 3)                │            63 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 1,003 (3.92 KB)\n Trainable params: 1,003 (3.92 KB)\n Non-trainable params: 0 (0.00 B)\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_68-1.png){width=109}\n:::\n:::\n\n\n\n\n## Torch\n\nSpecify optimizer and the parameters which will be trained (in our case the parameters of the network):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n```\n:::\n\n\n\n:::\n\n**4. Fit model in 30 iterations (epochs)**\n\n::: panel-tabset\n## Keras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\n\nmodel_history =\n  model |>\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/30\n8/8 - 0s - 41ms/step - loss: 1.0091\nEpoch 2/30\n8/8 - 0s - 3ms/step - loss: 0.9684\nEpoch 3/30\n8/8 - 0s - 3ms/step - loss: 0.9349\nEpoch 4/30\n8/8 - 0s - 3ms/step - loss: 0.9041\nEpoch 5/30\n8/8 - 0s - 3ms/step - loss: 0.8767\nEpoch 6/30\n8/8 - 0s - 3ms/step - loss: 0.8500\nEpoch 7/30\n8/8 - 0s - 3ms/step - loss: 0.8258\nEpoch 8/30\n8/8 - 0s - 3ms/step - loss: 0.8019\nEpoch 9/30\n8/8 - 0s - 3ms/step - loss: 0.7805\nEpoch 10/30\n8/8 - 0s - 3ms/step - loss: 0.7610\nEpoch 11/30\n8/8 - 0s - 3ms/step - loss: 0.7437\nEpoch 12/30\n8/8 - 0s - 3ms/step - loss: 0.7281\nEpoch 13/30\n8/8 - 0s - 3ms/step - loss: 0.7127\nEpoch 14/30\n8/8 - 0s - 2ms/step - loss: 0.6998\nEpoch 15/30\n8/8 - 0s - 3ms/step - loss: 0.6874\nEpoch 16/30\n8/8 - 0s - 3ms/step - loss: 0.6759\nEpoch 17/30\n8/8 - 0s - 3ms/step - loss: 0.6645\nEpoch 18/30\n8/8 - 0s - 3ms/step - loss: 0.6536\nEpoch 19/30\n8/8 - 0s - 3ms/step - loss: 0.6428\nEpoch 20/30\n8/8 - 0s - 3ms/step - loss: 0.6326\nEpoch 21/30\n8/8 - 0s - 3ms/step - loss: 0.6222\nEpoch 22/30\n8/8 - 0s - 3ms/step - loss: 0.6116\nEpoch 23/30\n8/8 - 0s - 3ms/step - loss: 0.6005\nEpoch 24/30\n8/8 - 0s - 3ms/step - loss: 0.5894\nEpoch 25/30\n8/8 - 0s - 3ms/step - loss: 0.5779\nEpoch 26/30\n8/8 - 0s - 3ms/step - loss: 0.5672\nEpoch 27/30\n8/8 - 0s - 3ms/step - loss: 0.5556\nEpoch 28/30\n8/8 - 0s - 3ms/step - loss: 0.5439\nEpoch 29/30\n8/8 - 0s - 3ms/step - loss: 0.5325\nEpoch 30/30\n8/8 - 0s - 2ms/step - loss: 0.5207\n```\n\n\n:::\n:::\n\n\n\n\n## Torch\n\nIn Torch, we jump directly to the training loop which we have to write on our own:\n\n1.  Get a batch of data.\n2.  Predict on batch.\n3.  Ccalculate loss between predictions and true labels.\n4.  Backpropagate error.\n5.  Update weights.\n6.  Go to step 1 and repeat.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n\n:::\n\n**5. Plot training history:**\n\n::: panel-tabset\n## Keras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_72-1.png){width=672}\n:::\n:::\n\n\n\n\n## Torch\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_73-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n**6. Create predictions:**\n\n::: panel-tabset\n## Keras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = predict(model, X) # Probabilities for each class.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5/5 - 0s - 9ms/step\n```\n\n\n:::\n:::\n\n\n\n\nGet probabilities:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predictions) # Quasi-probabilities for each species.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]       [,2]        [,3]\n[1,] 0.9778623 0.01867981 0.003457977\n[2,] 0.9399117 0.05284167 0.007246608\n[3,] 0.9727422 0.02348044 0.003777272\n[4,] 0.9631907 0.03240500 0.004404361\n[5,] 0.9832864 0.01411382 0.002599736\n[6,] 0.9793835 0.01658982 0.004026664\n```\n\n\n:::\n:::\n\n\n\n\nFor each plant, we want to know for which species we got the highest probability:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds = apply(predictions, 1, which.max) \nprint(preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 3 2 3 2 2 2 2 3 2 3 2 3 3 2 2 2 3 2 2 2\n [75] 2 3 3 3 3 2 2 2 2 3 3 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n\n\n:::\n:::\n\n\n\n\n## Torch\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n\n\n:::\n:::\n\n\n\n:::\n\n**7. Calculate Accuracy (how often we have been correct):**\n\n::: panel-tabset\n## Keras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8533333\n```\n\n\n:::\n:::\n\n\n\n\n## Torch\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds_torch == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.96\n```\n\n\n:::\n:::\n\n\n\n:::\n\n**8. Plot predictions, to see if we have done a good job:**\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_79-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in par(oldpar): graphical parameter \"cin\" cannot be set\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in par(oldpar): graphical parameter \"cra\" cannot be set\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in par(oldpar): graphical parameter \"csi\" cannot be set\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in par(oldpar): graphical parameter \"cxy\" cannot be set\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in par(oldpar): graphical parameter \"din\" cannot be set\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in par(oldpar): graphical parameter \"page\" cannot be set\n```\n\n\n:::\n:::\n\n\n\n\n\nSo you see, building a neural network is very easy with Keras or Torch and you can already do it on your own.\n\n## Exercise\n\n:::: callout-warning\n#### Task: Regression with keras\n\nWe now build a regression for the airquality data set with Keras/Torch. We want to predict the variable \"Ozone\" (continuous).\n\nTasks:\n\n-   **Fill in the missing steps**\n\n::: panel-tabset\n## Keras\n\nBefore we start, load and prepare the data set:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\n\ndata = airquality\n\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n\n\n:::\n:::\n\n\n\n\n1.  **There are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n\n2.  **Split the data in features (**$\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = scale(data[,2:6])\ny = data[,1]\n```\n:::\n\n\n\n\n\n</div>\n\n\n3.  **Create a sequential Keras model.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\nmodel = keras_model_sequential(shape(5L))\n```\n:::\n\n\n\n\n\n</div>\n\n\n4.  **Add hidden layers (input and output layer are already specified, you have to add hidden layers between them):**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\n\n\n-   Why do we use 5L as input shape?\n-   Why only one output node and \"linear\" activation layer?\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\n\n\n\n</div>\n\n\n5.  **Compile the model**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  compile(loss = keras3::loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n```\n:::\n\n\n\n\nWhat is the \"mean_squared_error\" loss?\n\n6.  **Fit model:**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_history =\n  model |>\n  fit(x = x, y = as.numeric(y), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/100\n6/6 - 0s - 54ms/step - loss: 2498.1033\nEpoch 2/100\n6/6 - 0s - 4ms/step - loss: 901.0377\nEpoch 3/100\n6/6 - 0s - 3ms/step - loss: 465.5430\nEpoch 4/100\n6/6 - 0s - 4ms/step - loss: 390.5018\nEpoch 5/100\n6/6 - 0s - 4ms/step - loss: 349.1705\nEpoch 6/100\n6/6 - 0s - 4ms/step - loss: 334.8552\nEpoch 7/100\n6/6 - 0s - 4ms/step - loss: 328.4256\nEpoch 8/100\n6/6 - 0s - 4ms/step - loss: 328.2533\nEpoch 9/100\n6/6 - 0s - 4ms/step - loss: 317.9826\nEpoch 10/100\n6/6 - 0s - 4ms/step - loss: 318.7128\nEpoch 11/100\n6/6 - 0s - 4ms/step - loss: 306.6119\nEpoch 12/100\n6/6 - 0s - 4ms/step - loss: 312.1312\nEpoch 13/100\n6/6 - 0s - 3ms/step - loss: 298.1062\nEpoch 14/100\n6/6 - 0s - 4ms/step - loss: 308.5615\nEpoch 15/100\n6/6 - 0s - 4ms/step - loss: 300.3846\nEpoch 16/100\n6/6 - 0s - 3ms/step - loss: 313.3854\nEpoch 17/100\n6/6 - 0s - 3ms/step - loss: 299.6471\nEpoch 18/100\n6/6 - 0s - 3ms/step - loss: 306.5469\nEpoch 19/100\n6/6 - 0s - 3ms/step - loss: 309.6739\nEpoch 20/100\n6/6 - 0s - 3ms/step - loss: 302.1554\nEpoch 21/100\n6/6 - 0s - 3ms/step - loss: 319.6404\nEpoch 22/100\n6/6 - 0s - 3ms/step - loss: 296.3943\nEpoch 23/100\n6/6 - 0s - 3ms/step - loss: 304.6011\nEpoch 24/100\n6/6 - 0s - 3ms/step - loss: 305.2535\nEpoch 25/100\n6/6 - 0s - 3ms/step - loss: 296.4977\nEpoch 26/100\n6/6 - 0s - 3ms/step - loss: 321.8782\nEpoch 27/100\n6/6 - 0s - 3ms/step - loss: 286.9942\nEpoch 28/100\n6/6 - 0s - 3ms/step - loss: 307.0777\nEpoch 29/100\n6/6 - 0s - 3ms/step - loss: 289.0710\nEpoch 30/100\n6/6 - 0s - 3ms/step - loss: 291.0422\nEpoch 31/100\n6/6 - 0s - 3ms/step - loss: 294.8492\nEpoch 32/100\n6/6 - 0s - 3ms/step - loss: 296.9902\nEpoch 33/100\n6/6 - 0s - 3ms/step - loss: 274.3746\nEpoch 34/100\n6/6 - 0s - 3ms/step - loss: 314.2931\nEpoch 35/100\n6/6 - 0s - 3ms/step - loss: 284.9017\nEpoch 36/100\n6/6 - 0s - 3ms/step - loss: 289.7855\nEpoch 37/100\n6/6 - 0s - 3ms/step - loss: 287.2884\nEpoch 38/100\n6/6 - 0s - 3ms/step - loss: 279.1912\nEpoch 39/100\n6/6 - 0s - 4ms/step - loss: 299.3198\nEpoch 40/100\n6/6 - 0s - 4ms/step - loss: 298.3962\nEpoch 41/100\n6/6 - 0s - 4ms/step - loss: 296.8012\nEpoch 42/100\n6/6 - 0s - 4ms/step - loss: 288.2070\nEpoch 43/100\n6/6 - 0s - 4ms/step - loss: 306.7628\nEpoch 44/100\n6/6 - 0s - 3ms/step - loss: 285.8872\nEpoch 45/100\n6/6 - 0s - 3ms/step - loss: 285.1398\nEpoch 46/100\n6/6 - 0s - 3ms/step - loss: 304.4249\nEpoch 47/100\n6/6 - 0s - 3ms/step - loss: 273.3715\nEpoch 48/100\n6/6 - 0s - 3ms/step - loss: 280.0021\nEpoch 49/100\n6/6 - 0s - 3ms/step - loss: 273.6143\nEpoch 50/100\n6/6 - 0s - 3ms/step - loss: 272.2939\nEpoch 51/100\n6/6 - 0s - 3ms/step - loss: 276.5786\nEpoch 52/100\n6/6 - 0s - 3ms/step - loss: 271.6053\nEpoch 53/100\n6/6 - 0s - 3ms/step - loss: 268.0160\nEpoch 54/100\n6/6 - 0s - 3ms/step - loss: 274.5173\nEpoch 55/100\n6/6 - 0s - 3ms/step - loss: 263.0834\nEpoch 56/100\n6/6 - 0s - 3ms/step - loss: 269.8439\nEpoch 57/100\n6/6 - 0s - 3ms/step - loss: 264.0655\nEpoch 58/100\n6/6 - 0s - 4ms/step - loss: 265.5551\nEpoch 59/100\n6/6 - 0s - 4ms/step - loss: 272.8403\nEpoch 60/100\n6/6 - 0s - 3ms/step - loss: 266.6149\nEpoch 61/100\n6/6 - 0s - 3ms/step - loss: 272.6976\nEpoch 62/100\n6/6 - 0s - 3ms/step - loss: 269.5123\nEpoch 63/100\n6/6 - 0s - 3ms/step - loss: 266.6290\nEpoch 64/100\n6/6 - 0s - 3ms/step - loss: 264.7144\nEpoch 65/100\n6/6 - 0s - 3ms/step - loss: 277.7432\nEpoch 66/100\n6/6 - 0s - 3ms/step - loss: 258.9519\nEpoch 67/100\n6/6 - 0s - 3ms/step - loss: 265.8469\nEpoch 68/100\n6/6 - 0s - 3ms/step - loss: 271.1631\nEpoch 69/100\n6/6 - 0s - 3ms/step - loss: 254.8403\nEpoch 70/100\n6/6 - 0s - 3ms/step - loss: 265.5042\nEpoch 71/100\n6/6 - 0s - 3ms/step - loss: 256.5101\nEpoch 72/100\n6/6 - 0s - 3ms/step - loss: 262.5606\nEpoch 73/100\n6/6 - 0s - 4ms/step - loss: 251.6421\nEpoch 74/100\n6/6 - 0s - 4ms/step - loss: 267.1255\nEpoch 75/100\n6/6 - 0s - 3ms/step - loss: 242.8577\nEpoch 76/100\n6/6 - 0s - 3ms/step - loss: 270.4315\nEpoch 77/100\n6/6 - 0s - 3ms/step - loss: 262.1392\nEpoch 78/100\n6/6 - 0s - 3ms/step - loss: 244.3012\nEpoch 79/100\n6/6 - 0s - 3ms/step - loss: 252.9988\nEpoch 80/100\n6/6 - 0s - 3ms/step - loss: 251.2216\nEpoch 81/100\n6/6 - 0s - 3ms/step - loss: 244.5091\nEpoch 82/100\n6/6 - 0s - 3ms/step - loss: 255.4844\nEpoch 83/100\n6/6 - 0s - 3ms/step - loss: 236.8943\nEpoch 84/100\n6/6 - 0s - 3ms/step - loss: 241.2098\nEpoch 85/100\n6/6 - 0s - 3ms/step - loss: 253.1217\nEpoch 86/100\n6/6 - 0s - 3ms/step - loss: 237.4809\nEpoch 87/100\n6/6 - 0s - 3ms/step - loss: 257.5538\nEpoch 88/100\n6/6 - 0s - 3ms/step - loss: 239.2589\nEpoch 89/100\n6/6 - 0s - 3ms/step - loss: 232.0303\nEpoch 90/100\n6/6 - 0s - 3ms/step - loss: 237.4416\nEpoch 91/100\n6/6 - 0s - 4ms/step - loss: 228.2105\nEpoch 92/100\n6/6 - 0s - 3ms/step - loss: 238.8170\nEpoch 93/100\n6/6 - 0s - 3ms/step - loss: 229.8240\nEpoch 94/100\n6/6 - 0s - 3ms/step - loss: 225.7728\nEpoch 95/100\n6/6 - 0s - 3ms/step - loss: 225.3193\nEpoch 96/100\n6/6 - 0s - 3ms/step - loss: 215.8254\nEpoch 97/100\n6/6 - 0s - 3ms/step - loss: 220.8841\nEpoch 98/100\n6/6 - 0s - 4ms/step - loss: 219.9174\nEpoch 99/100\n6/6 - 0s - 3ms/step - loss: 221.7522\nEpoch 100/100\n6/6 - 0s - 3ms/step - loss: 208.1270\n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n\n7.  **Plot training history.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n</div>\n\n\n8.  **Create predictions.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_keras = predict(model, x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4/4 - 0s - 11ms/step\n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n\n9.  **Compare your Keras model to a linear model:**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.78897\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(rmse_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.24392\n```\n\n\n:::\n:::\n\n\n\n\n## Torch\n\nBefore we start, load and prepare the data set:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(data)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_task_26_torch-1.png){width=672}\n:::\n:::\n\n\n\n\n1.  **There are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n\n2.  **Split the data in features (**$\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = scale(data[,2:6])\ny = data[,1]\n```\n:::\n\n\n\n\n\n</div>\n\n\n3.  **Pass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    ...\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\n\n\n-   Why do we use 5L as input shape?\n-   Why only one output node and no activation layer?\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\n\n\n\n</div>\n\n\n4.  **Create optimizer**\n\nWe have to pass the network's parameters to the optimizer (how is this different to keras?)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n```\n:::\n\n\n\n\n5.  **Fit model**\n\nIn torch we write the trainings loop on our own. Complete the trainings loop:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = ...\nbatch_size = 32\nsteps = ...\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, ...) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = ...\n  Y_batch = ...\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = ...\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n\n\n\n</div>\n\n\nTips:\n\n-   Number of training \\$ steps = Number of rows / batchsize \\* Epochs \\$\n-   Search torch::nnf\\_... for the correct loss function (mse...)\n-   Make sure that X_torch and Y_torch have the same data type! (you can set the dtype via torch_tensor(..., dtype = ...)) \\_ Check the dimension of Y_torch, we need a matrix!\n\n6.  **Plot training history.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n</div>\n\n\n7.  **Create predictions.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n```\n:::\n\n\n\n\n\n</div>\n\n\n8.  **Compare your Torch model with a linear model:**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.78897\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(rmse_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.897065\n```\n\n\n:::\n:::\n\n\n\n:::\n::::\n\n::: callout-warning\n#### Task: Titanic dataset\n\nBuild a Keras DNN for the titanic dataset\n:::\n\n\n<div class='webex-solution'><button>Solution Torch</button>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(missRanger)\nlibrary(torch)\ndata(titanic_ml)\ndata = titanic_ml\ndata =\n  data %>% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\nXtorch = data_obs[,-1] |> as.matrix() |> torch_tensor()\nYtorch = data_obs[,1] |> as.matrix() |> torch_tensor(dtype=torch_float32())\nXtest = data_new[,-1] |> as.matrix() |> torch_tensor()\n```\n:::\n\n\n\n\nDataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_indices = 1:400\nval_indices = 401:nrow(Xtorch)\ndataset_train= torch::tensor_dataset(Xtorch[train_indices,], Ytorch[train_indices,])\ntrain_dl = torch::dataloader(dataset_train, batch_size = 20L, shuffle = TRUE)\n\ndataset_val= torch::tensor_dataset(Xtorch[val_indices,], Ytorch[val_indices,])\nval_dl = torch::dataloader(dataset_val, batch_size = 20L, shuffle = TRUE)\n\nfirst_batch = train_dl$.iter()\ndf = first_batch$.next()\n\ndf[[1]] |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 0.0000  0.2735  0.0151  2.0000\n 1.0000  0.3444  0.0152  2.0000\n 1.0000  0.2860  0.0154  2.0000\n 0.0000  0.2993  0.0282  2.0000\n 1.0000  0.3236  0.0169  2.0000\n 1.0000  0.1106  0.0401  2.0000\n[ CPUFloatType{6,4} ]\n```\n\n\n:::\n\n```{.r .cell-code}\ndf[[2]] |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1\n 0\n 0\n 0\n 0\n 1\n[ CPUFloatType{6,1} ]\n```\n\n\n:::\n:::\n\n\n\n\n\nModel:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# blueprint\nnet = nn_module(\n  # first function tells torch how to build the network\n  initialize = function(units = 50L, input_dim=4L, dropout_rate = 0.5) {\n    # self\n    self$layer1 = nn_linear(in_features = input_dim, out_features = units)\n    self$dropout1 = nn_dropout(p = dropout_rate)\n    self$layer2 = nn_linear(units, units)\n    self$dropout2 = nn_dropout(p = dropout_rate)\n    self$layer3 = nn_linear(units, 1L)\n  },\n  # forward tells torch the input data should be processed\n  forward = function(x) {\n    # x = feature tensor\n    x |> \n      self$layer1() |> \n      nnf_relu() |> \n      self$dropout1() |> \n      self$layer2() |> \n      nnf_relu() |> \n      self$dropout2() |> \n      self$layer3() |> \n      torch_sigmoid()\n  }\n)\n```\n:::\n\n\n\n\nTraining loop:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_dl = torch::dataloader(dataset_train, batch_size = 150L, shuffle = TRUE)\nval_dl = torch::dataloader(dataset_val, batch_size = 150L, shuffle = TRUE)\n\nmodel = net()\nopt = optim_adam(params = model$parameters, lr = 0.01)\nepochs = 500L\n\noverall_train_loss = overall_val_loss = c()\nalpha = 0.7\nlambda = 0.01\nfor(e in 1:epochs) {\n  losses = losses_val = c()\n  \n  model$train() # -> dropout is on\n  coro::loop(\n    for(batch in train_dl) {\n      x = batch[[1]] # Feature matrix/tensor\n      y = batch[[2]] # Response matrix/tensor\n      opt$zero_grad() # reset optimizer\n      \n      pred = model(x)\n      loss = nnf_binary_cross_entropy(pred, y)\n      \n      # add regularization loss, l2 -> sum((weights)**2)*lambda \n      loss = loss + (1-alpha)*(lambda*sum(model$parameters[[1]]**2))\n      \n      # l1 regularization: sum(abs(weights))*lambda\n      loss = loss + (alpha)*(lambda*sum(abs(model$parameters[[1]])))\n      \n      loss$backward()\n      opt$step() # update weights\n      \n      losses = c(losses, loss$item())\n    }\n  ) \n  \n  # calculate validation loss after each epoch\n  \n  model$eval() # dropout is off\n  coro::loop(\n    for(batch in val_dl) {\n      x = batch[[1]] # Feature matrix/tensor\n      y = batch[[2]] # Response matrix/tensor\n      \n      pred = model(x)\n      loss = nnf_binary_cross_entropy(pred, y)\n      \n      losses_val = c(losses_val, loss$item())\n    }\n  ) \n  \n  overall_train_loss = c(overall_train_loss, mean(losses))\n  overall_val_loss = c(overall_val_loss, mean(losses_val))\n  \n  cat(sprintf(\"Loss at epoch: %d train: %3f eval: %3f\\n\", e, mean(losses), mean(losses_val)))\n  \n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoss at epoch: 1 train: 1.061287 eval: 0.646822\nLoss at epoch: 2 train: 0.973283 eval: 0.652817\nLoss at epoch: 3 train: 0.932252 eval: 0.623445\nLoss at epoch: 4 train: 0.881706 eval: 0.612755\nLoss at epoch: 5 train: 0.838846 eval: 0.601039\nLoss at epoch: 6 train: 0.794143 eval: 0.578016\nLoss at epoch: 7 train: 0.788708 eval: 0.562789\nLoss at epoch: 8 train: 0.717368 eval: 0.540282\nLoss at epoch: 9 train: 0.723465 eval: 0.533487\nLoss at epoch: 10 train: 0.659200 eval: 0.522329\nLoss at epoch: 11 train: 0.664211 eval: 0.522649\nLoss at epoch: 12 train: 0.612222 eval: 0.514556\nLoss at epoch: 13 train: 0.613024 eval: 0.500222\nLoss at epoch: 14 train: 0.607520 eval: 0.487394\nLoss at epoch: 15 train: 0.612270 eval: 0.494097\nLoss at epoch: 16 train: 0.587178 eval: 0.499945\nLoss at epoch: 17 train: 0.554300 eval: 0.482750\nLoss at epoch: 18 train: 0.561841 eval: 0.488962\nLoss at epoch: 19 train: 0.586277 eval: 0.484681\nLoss at epoch: 20 train: 0.559775 eval: 0.480452\nLoss at epoch: 21 train: 0.561195 eval: 0.485779\nLoss at epoch: 22 train: 0.554978 eval: 0.486634\nLoss at epoch: 23 train: 0.566058 eval: 0.491321\nLoss at epoch: 24 train: 0.555117 eval: 0.482375\nLoss at epoch: 25 train: 0.550732 eval: 0.481965\nLoss at epoch: 26 train: 0.532506 eval: 0.478239\nLoss at epoch: 27 train: 0.541434 eval: 0.482824\nLoss at epoch: 28 train: 0.547886 eval: 0.490667\nLoss at epoch: 29 train: 0.543194 eval: 0.479447\nLoss at epoch: 30 train: 0.542096 eval: 0.481395\nLoss at epoch: 31 train: 0.569523 eval: 0.472467\nLoss at epoch: 32 train: 0.520432 eval: 0.482531\nLoss at epoch: 33 train: 0.551853 eval: 0.476946\nLoss at epoch: 34 train: 0.531947 eval: 0.478664\nLoss at epoch: 35 train: 0.526163 eval: 0.485173\nLoss at epoch: 36 train: 0.553211 eval: 0.486571\nLoss at epoch: 37 train: 0.524824 eval: 0.482460\nLoss at epoch: 38 train: 0.527222 eval: 0.471737\nLoss at epoch: 39 train: 0.535728 eval: 0.478461\nLoss at epoch: 40 train: 0.573437 eval: 0.472048\nLoss at epoch: 41 train: 0.510421 eval: 0.482229\nLoss at epoch: 42 train: 0.525551 eval: 0.480425\nLoss at epoch: 43 train: 0.539331 eval: 0.483444\nLoss at epoch: 44 train: 0.524326 eval: 0.480079\nLoss at epoch: 45 train: 0.529846 eval: 0.481549\nLoss at epoch: 46 train: 0.534244 eval: 0.472180\nLoss at epoch: 47 train: 0.522389 eval: 0.473462\nLoss at epoch: 48 train: 0.525819 eval: 0.478135\nLoss at epoch: 49 train: 0.537413 eval: 0.466751\nLoss at epoch: 50 train: 0.544840 eval: 0.459146\nLoss at epoch: 51 train: 0.528388 eval: 0.486819\nLoss at epoch: 52 train: 0.533555 eval: 0.465963\nLoss at epoch: 53 train: 0.546339 eval: 0.476492\nLoss at epoch: 54 train: 0.535490 eval: 0.480250\nLoss at epoch: 55 train: 0.537781 eval: 0.474570\nLoss at epoch: 56 train: 0.522829 eval: 0.473763\nLoss at epoch: 57 train: 0.533746 eval: 0.471530\nLoss at epoch: 58 train: 0.500943 eval: 0.480092\nLoss at epoch: 59 train: 0.550841 eval: 0.465479\nLoss at epoch: 60 train: 0.522388 eval: 0.465922\nLoss at epoch: 61 train: 0.516799 eval: 0.474731\nLoss at epoch: 62 train: 0.523272 eval: 0.474029\nLoss at epoch: 63 train: 0.532005 eval: 0.470254\nLoss at epoch: 64 train: 0.515423 eval: 0.478724\nLoss at epoch: 65 train: 0.511948 eval: 0.474977\nLoss at epoch: 66 train: 0.515090 eval: 0.473491\nLoss at epoch: 67 train: 0.533681 eval: 0.468289\nLoss at epoch: 68 train: 0.517115 eval: 0.479768\nLoss at epoch: 69 train: 0.543463 eval: 0.473255\nLoss at epoch: 70 train: 0.506904 eval: 0.471708\nLoss at epoch: 71 train: 0.515715 eval: 0.467660\nLoss at epoch: 72 train: 0.523478 eval: 0.495652\nLoss at epoch: 73 train: 0.520893 eval: 0.485845\nLoss at epoch: 74 train: 0.533017 eval: 0.473370\nLoss at epoch: 75 train: 0.517889 eval: 0.471389\nLoss at epoch: 76 train: 0.517371 eval: 0.477436\nLoss at epoch: 77 train: 0.540815 eval: 0.468353\nLoss at epoch: 78 train: 0.526070 eval: 0.461190\nLoss at epoch: 79 train: 0.529169 eval: 0.470275\nLoss at epoch: 80 train: 0.507615 eval: 0.476921\nLoss at epoch: 81 train: 0.527837 eval: 0.474002\nLoss at epoch: 82 train: 0.520113 eval: 0.472925\nLoss at epoch: 83 train: 0.513032 eval: 0.463895\nLoss at epoch: 84 train: 0.512073 eval: 0.470341\nLoss at epoch: 85 train: 0.536100 eval: 0.475027\nLoss at epoch: 86 train: 0.518713 eval: 0.476431\nLoss at epoch: 87 train: 0.515439 eval: 0.476523\nLoss at epoch: 88 train: 0.518799 eval: 0.475238\nLoss at epoch: 89 train: 0.509792 eval: 0.470036\nLoss at epoch: 90 train: 0.538611 eval: 0.468533\nLoss at epoch: 91 train: 0.520392 eval: 0.476044\nLoss at epoch: 92 train: 0.552518 eval: 0.473782\nLoss at epoch: 93 train: 0.521309 eval: 0.476037\nLoss at epoch: 94 train: 0.528442 eval: 0.483846\nLoss at epoch: 95 train: 0.504562 eval: 0.486744\nLoss at epoch: 96 train: 0.506207 eval: 0.480809\nLoss at epoch: 97 train: 0.517122 eval: 0.478596\nLoss at epoch: 98 train: 0.520612 eval: 0.478937\nLoss at epoch: 99 train: 0.513192 eval: 0.472131\nLoss at epoch: 100 train: 0.521539 eval: 0.477064\nLoss at epoch: 101 train: 0.542996 eval: 0.469449\nLoss at epoch: 102 train: 0.528102 eval: 0.483352\nLoss at epoch: 103 train: 0.556344 eval: 0.487392\nLoss at epoch: 104 train: 0.525589 eval: 0.481893\nLoss at epoch: 105 train: 0.507550 eval: 0.477635\nLoss at epoch: 106 train: 0.526855 eval: 0.488782\nLoss at epoch: 107 train: 0.512394 eval: 0.473428\nLoss at epoch: 108 train: 0.500325 eval: 0.486692\nLoss at epoch: 109 train: 0.539331 eval: 0.480352\nLoss at epoch: 110 train: 0.502052 eval: 0.479354\nLoss at epoch: 111 train: 0.503382 eval: 0.485309\nLoss at epoch: 112 train: 0.506593 eval: 0.456416\nLoss at epoch: 113 train: 0.513503 eval: 0.471576\nLoss at epoch: 114 train: 0.500376 eval: 0.465450\nLoss at epoch: 115 train: 0.495770 eval: 0.486544\nLoss at epoch: 116 train: 0.529493 eval: 0.465582\nLoss at epoch: 117 train: 0.529156 eval: 0.483786\nLoss at epoch: 118 train: 0.525348 eval: 0.483837\nLoss at epoch: 119 train: 0.524843 eval: 0.485120\nLoss at epoch: 120 train: 0.529329 eval: 0.478986\nLoss at epoch: 121 train: 0.514253 eval: 0.476671\nLoss at epoch: 122 train: 0.532410 eval: 0.471552\nLoss at epoch: 123 train: 0.549683 eval: 0.475882\nLoss at epoch: 124 train: 0.537721 eval: 0.470775\nLoss at epoch: 125 train: 0.532292 eval: 0.478319\nLoss at epoch: 126 train: 0.515394 eval: 0.479779\nLoss at epoch: 127 train: 0.538956 eval: 0.498512\nLoss at epoch: 128 train: 0.519865 eval: 0.474230\nLoss at epoch: 129 train: 0.531616 eval: 0.469329\nLoss at epoch: 130 train: 0.544439 eval: 0.474695\nLoss at epoch: 131 train: 0.515902 eval: 0.485917\nLoss at epoch: 132 train: 0.522327 eval: 0.472809\nLoss at epoch: 133 train: 0.516905 eval: 0.475261\nLoss at epoch: 134 train: 0.525487 eval: 0.478491\nLoss at epoch: 135 train: 0.495121 eval: 0.479740\nLoss at epoch: 136 train: 0.525603 eval: 0.481157\nLoss at epoch: 137 train: 0.533950 eval: 0.477023\nLoss at epoch: 138 train: 0.511709 eval: 0.479740\nLoss at epoch: 139 train: 0.513323 eval: 0.477410\nLoss at epoch: 140 train: 0.523308 eval: 0.476431\nLoss at epoch: 141 train: 0.535427 eval: 0.483635\nLoss at epoch: 142 train: 0.518885 eval: 0.479784\nLoss at epoch: 143 train: 0.515777 eval: 0.473884\nLoss at epoch: 144 train: 0.545930 eval: 0.475854\nLoss at epoch: 145 train: 0.515613 eval: 0.479400\nLoss at epoch: 146 train: 0.523143 eval: 0.477056\nLoss at epoch: 147 train: 0.531391 eval: 0.489133\nLoss at epoch: 148 train: 0.541232 eval: 0.478018\nLoss at epoch: 149 train: 0.515095 eval: 0.481644\nLoss at epoch: 150 train: 0.534159 eval: 0.474542\nLoss at epoch: 151 train: 0.529138 eval: 0.475298\nLoss at epoch: 152 train: 0.526667 eval: 0.480136\nLoss at epoch: 153 train: 0.532573 eval: 0.466376\nLoss at epoch: 154 train: 0.487993 eval: 0.475270\nLoss at epoch: 155 train: 0.537415 eval: 0.479676\nLoss at epoch: 156 train: 0.527781 eval: 0.477555\nLoss at epoch: 157 train: 0.513761 eval: 0.478965\nLoss at epoch: 158 train: 0.523775 eval: 0.471459\nLoss at epoch: 159 train: 0.500876 eval: 0.470136\nLoss at epoch: 160 train: 0.548869 eval: 0.472939\nLoss at epoch: 161 train: 0.526359 eval: 0.475895\nLoss at epoch: 162 train: 0.511202 eval: 0.470212\nLoss at epoch: 163 train: 0.529021 eval: 0.468295\nLoss at epoch: 164 train: 0.524786 eval: 0.482668\nLoss at epoch: 165 train: 0.529483 eval: 0.472731\nLoss at epoch: 166 train: 0.514303 eval: 0.469538\nLoss at epoch: 167 train: 0.525693 eval: 0.480762\nLoss at epoch: 168 train: 0.527319 eval: 0.475783\nLoss at epoch: 169 train: 0.510581 eval: 0.473204\nLoss at epoch: 170 train: 0.520810 eval: 0.464590\nLoss at epoch: 171 train: 0.528076 eval: 0.477770\nLoss at epoch: 172 train: 0.510208 eval: 0.468734\nLoss at epoch: 173 train: 0.523263 eval: 0.477500\nLoss at epoch: 174 train: 0.508092 eval: 0.470660\nLoss at epoch: 175 train: 0.511997 eval: 0.471968\nLoss at epoch: 176 train: 0.498540 eval: 0.467518\nLoss at epoch: 177 train: 0.511547 eval: 0.479472\nLoss at epoch: 178 train: 0.500742 eval: 0.471070\nLoss at epoch: 179 train: 0.525906 eval: 0.478152\nLoss at epoch: 180 train: 0.540614 eval: 0.477761\nLoss at epoch: 181 train: 0.541392 eval: 0.476630\nLoss at epoch: 182 train: 0.502372 eval: 0.471854\nLoss at epoch: 183 train: 0.539383 eval: 0.471883\nLoss at epoch: 184 train: 0.508666 eval: 0.482836\nLoss at epoch: 185 train: 0.521211 eval: 0.473917\nLoss at epoch: 186 train: 0.503343 eval: 0.489341\nLoss at epoch: 187 train: 0.508412 eval: 0.465929\nLoss at epoch: 188 train: 0.514390 eval: 0.469254\nLoss at epoch: 189 train: 0.495561 eval: 0.472431\nLoss at epoch: 190 train: 0.524247 eval: 0.460994\nLoss at epoch: 191 train: 0.533367 eval: 0.474138\nLoss at epoch: 192 train: 0.512015 eval: 0.471551\nLoss at epoch: 193 train: 0.519604 eval: 0.483742\nLoss at epoch: 194 train: 0.502653 eval: 0.479629\nLoss at epoch: 195 train: 0.534692 eval: 0.473837\nLoss at epoch: 196 train: 0.538500 eval: 0.476177\nLoss at epoch: 197 train: 0.519311 eval: 0.478281\nLoss at epoch: 198 train: 0.536126 eval: 0.474546\nLoss at epoch: 199 train: 0.503548 eval: 0.475096\nLoss at epoch: 200 train: 0.511502 eval: 0.472484\nLoss at epoch: 201 train: 0.511936 eval: 0.471478\nLoss at epoch: 202 train: 0.531843 eval: 0.478301\nLoss at epoch: 203 train: 0.528174 eval: 0.467283\nLoss at epoch: 204 train: 0.530655 eval: 0.469709\nLoss at epoch: 205 train: 0.526465 eval: 0.470500\nLoss at epoch: 206 train: 0.543161 eval: 0.475500\nLoss at epoch: 207 train: 0.517044 eval: 0.479633\nLoss at epoch: 208 train: 0.515619 eval: 0.475840\nLoss at epoch: 209 train: 0.509115 eval: 0.477979\nLoss at epoch: 210 train: 0.523525 eval: 0.466606\nLoss at epoch: 211 train: 0.511375 eval: 0.460718\nLoss at epoch: 212 train: 0.517891 eval: 0.457429\nLoss at epoch: 213 train: 0.504721 eval: 0.461188\nLoss at epoch: 214 train: 0.518254 eval: 0.477680\nLoss at epoch: 215 train: 0.508817 eval: 0.468159\nLoss at epoch: 216 train: 0.513196 eval: 0.468719\nLoss at epoch: 217 train: 0.520213 eval: 0.480611\nLoss at epoch: 218 train: 0.506910 eval: 0.476310\nLoss at epoch: 219 train: 0.506539 eval: 0.460607\nLoss at epoch: 220 train: 0.510304 eval: 0.455342\nLoss at epoch: 221 train: 0.515913 eval: 0.471227\nLoss at epoch: 222 train: 0.513890 eval: 0.484910\nLoss at epoch: 223 train: 0.529532 eval: 0.469285\nLoss at epoch: 224 train: 0.509295 eval: 0.477103\nLoss at epoch: 225 train: 0.510787 eval: 0.477365\nLoss at epoch: 226 train: 0.521620 eval: 0.470862\nLoss at epoch: 227 train: 0.552735 eval: 0.472625\nLoss at epoch: 228 train: 0.503338 eval: 0.466224\nLoss at epoch: 229 train: 0.511244 eval: 0.484182\nLoss at epoch: 230 train: 0.550620 eval: 0.480171\nLoss at epoch: 231 train: 0.518718 eval: 0.480032\nLoss at epoch: 232 train: 0.513876 eval: 0.478171\nLoss at epoch: 233 train: 0.524864 eval: 0.481846\nLoss at epoch: 234 train: 0.504239 eval: 0.466469\nLoss at epoch: 235 train: 0.500711 eval: 0.471786\nLoss at epoch: 236 train: 0.503917 eval: 0.470299\nLoss at epoch: 237 train: 0.520559 eval: 0.480239\nLoss at epoch: 238 train: 0.538831 eval: 0.481764\nLoss at epoch: 239 train: 0.520653 eval: 0.478511\nLoss at epoch: 240 train: 0.509144 eval: 0.481923\nLoss at epoch: 241 train: 0.509922 eval: 0.471383\nLoss at epoch: 242 train: 0.517133 eval: 0.474773\nLoss at epoch: 243 train: 0.509163 eval: 0.476303\nLoss at epoch: 244 train: 0.499063 eval: 0.479805\nLoss at epoch: 245 train: 0.533093 eval: 0.464295\nLoss at epoch: 246 train: 0.508919 eval: 0.465948\nLoss at epoch: 247 train: 0.516644 eval: 0.476382\nLoss at epoch: 248 train: 0.519940 eval: 0.475304\nLoss at epoch: 249 train: 0.523438 eval: 0.470279\nLoss at epoch: 250 train: 0.518099 eval: 0.477193\nLoss at epoch: 251 train: 0.517551 eval: 0.472268\nLoss at epoch: 252 train: 0.517131 eval: 0.464533\nLoss at epoch: 253 train: 0.508258 eval: 0.466979\nLoss at epoch: 254 train: 0.507807 eval: 0.479228\nLoss at epoch: 255 train: 0.497602 eval: 0.476629\nLoss at epoch: 256 train: 0.501601 eval: 0.483492\nLoss at epoch: 257 train: 0.536535 eval: 0.483428\nLoss at epoch: 258 train: 0.498128 eval: 0.477615\nLoss at epoch: 259 train: 0.508529 eval: 0.479367\nLoss at epoch: 260 train: 0.498868 eval: 0.477609\nLoss at epoch: 261 train: 0.525070 eval: 0.478738\nLoss at epoch: 262 train: 0.493277 eval: 0.477766\nLoss at epoch: 263 train: 0.527595 eval: 0.481134\nLoss at epoch: 264 train: 0.513322 eval: 0.484504\nLoss at epoch: 265 train: 0.497188 eval: 0.483946\nLoss at epoch: 266 train: 0.515891 eval: 0.472165\nLoss at epoch: 267 train: 0.515239 eval: 0.470791\nLoss at epoch: 268 train: 0.486143 eval: 0.475409\nLoss at epoch: 269 train: 0.514652 eval: 0.469806\nLoss at epoch: 270 train: 0.516380 eval: 0.465741\nLoss at epoch: 271 train: 0.517942 eval: 0.475236\nLoss at epoch: 272 train: 0.527605 eval: 0.478818\nLoss at epoch: 273 train: 0.505747 eval: 0.483225\nLoss at epoch: 274 train: 0.522335 eval: 0.474751\nLoss at epoch: 275 train: 0.526779 eval: 0.463789\nLoss at epoch: 276 train: 0.544610 eval: 0.479575\nLoss at epoch: 277 train: 0.497088 eval: 0.477863\nLoss at epoch: 278 train: 0.503466 eval: 0.477036\nLoss at epoch: 279 train: 0.520230 eval: 0.486980\nLoss at epoch: 280 train: 0.500994 eval: 0.479368\nLoss at epoch: 281 train: 0.508005 eval: 0.468456\nLoss at epoch: 282 train: 0.503601 eval: 0.474429\nLoss at epoch: 283 train: 0.497745 eval: 0.477840\nLoss at epoch: 284 train: 0.506749 eval: 0.475127\nLoss at epoch: 285 train: 0.518292 eval: 0.471714\nLoss at epoch: 286 train: 0.497848 eval: 0.478583\nLoss at epoch: 287 train: 0.510573 eval: 0.466624\nLoss at epoch: 288 train: 0.520358 eval: 0.475869\nLoss at epoch: 289 train: 0.502438 eval: 0.468359\nLoss at epoch: 290 train: 0.499502 eval: 0.468596\nLoss at epoch: 291 train: 0.511164 eval: 0.481586\nLoss at epoch: 292 train: 0.515578 eval: 0.473793\nLoss at epoch: 293 train: 0.540023 eval: 0.474243\nLoss at epoch: 294 train: 0.483646 eval: 0.483440\nLoss at epoch: 295 train: 0.499257 eval: 0.465031\nLoss at epoch: 296 train: 0.506763 eval: 0.476127\nLoss at epoch: 297 train: 0.502007 eval: 0.479268\nLoss at epoch: 298 train: 0.527459 eval: 0.470687\nLoss at epoch: 299 train: 0.503935 eval: 0.489158\nLoss at epoch: 300 train: 0.532213 eval: 0.477268\nLoss at epoch: 301 train: 0.515937 eval: 0.478095\nLoss at epoch: 302 train: 0.512991 eval: 0.486248\nLoss at epoch: 303 train: 0.529440 eval: 0.481875\nLoss at epoch: 304 train: 0.497512 eval: 0.474714\nLoss at epoch: 305 train: 0.518154 eval: 0.482828\nLoss at epoch: 306 train: 0.510891 eval: 0.479378\nLoss at epoch: 307 train: 0.511579 eval: 0.469754\nLoss at epoch: 308 train: 0.525084 eval: 0.465501\nLoss at epoch: 309 train: 0.515277 eval: 0.478518\nLoss at epoch: 310 train: 0.490920 eval: 0.488164\nLoss at epoch: 311 train: 0.498140 eval: 0.475063\nLoss at epoch: 312 train: 0.510525 eval: 0.463179\nLoss at epoch: 313 train: 0.514674 eval: 0.479161\nLoss at epoch: 314 train: 0.499929 eval: 0.476219\nLoss at epoch: 315 train: 0.495891 eval: 0.476772\nLoss at epoch: 316 train: 0.515579 eval: 0.461296\nLoss at epoch: 317 train: 0.513432 eval: 0.468931\nLoss at epoch: 318 train: 0.511828 eval: 0.467864\nLoss at epoch: 319 train: 0.526295 eval: 0.462116\nLoss at epoch: 320 train: 0.493885 eval: 0.474014\nLoss at epoch: 321 train: 0.515045 eval: 0.472723\nLoss at epoch: 322 train: 0.519315 eval: 0.477456\nLoss at epoch: 323 train: 0.515790 eval: 0.474233\nLoss at epoch: 324 train: 0.512640 eval: 0.472020\nLoss at epoch: 325 train: 0.516458 eval: 0.465115\nLoss at epoch: 326 train: 0.521442 eval: 0.475927\nLoss at epoch: 327 train: 0.529877 eval: 0.467917\nLoss at epoch: 328 train: 0.504019 eval: 0.478946\nLoss at epoch: 329 train: 0.515719 eval: 0.478433\nLoss at epoch: 330 train: 0.515022 eval: 0.471327\nLoss at epoch: 331 train: 0.526155 eval: 0.482792\nLoss at epoch: 332 train: 0.506914 eval: 0.479751\nLoss at epoch: 333 train: 0.515165 eval: 0.477726\nLoss at epoch: 334 train: 0.508698 eval: 0.481943\nLoss at epoch: 335 train: 0.503392 eval: 0.475020\nLoss at epoch: 336 train: 0.511191 eval: 0.469472\nLoss at epoch: 337 train: 0.516447 eval: 0.467252\nLoss at epoch: 338 train: 0.499755 eval: 0.472216\nLoss at epoch: 339 train: 0.500429 eval: 0.471359\nLoss at epoch: 340 train: 0.502073 eval: 0.479270\nLoss at epoch: 341 train: 0.512032 eval: 0.452738\nLoss at epoch: 342 train: 0.514965 eval: 0.472179\nLoss at epoch: 343 train: 0.499858 eval: 0.477576\nLoss at epoch: 344 train: 0.506614 eval: 0.481273\nLoss at epoch: 345 train: 0.539589 eval: 0.471980\nLoss at epoch: 346 train: 0.504980 eval: 0.465976\nLoss at epoch: 347 train: 0.522742 eval: 0.466492\nLoss at epoch: 348 train: 0.536209 eval: 0.475267\nLoss at epoch: 349 train: 0.516090 eval: 0.479547\nLoss at epoch: 350 train: 0.518387 eval: 0.472879\nLoss at epoch: 351 train: 0.509542 eval: 0.471686\nLoss at epoch: 352 train: 0.512149 eval: 0.474047\nLoss at epoch: 353 train: 0.521854 eval: 0.467641\nLoss at epoch: 354 train: 0.497158 eval: 0.475143\nLoss at epoch: 355 train: 0.507110 eval: 0.468417\nLoss at epoch: 356 train: 0.507705 eval: 0.476855\nLoss at epoch: 357 train: 0.506504 eval: 0.468806\nLoss at epoch: 358 train: 0.511705 eval: 0.466145\nLoss at epoch: 359 train: 0.486156 eval: 0.471627\nLoss at epoch: 360 train: 0.524531 eval: 0.464871\nLoss at epoch: 361 train: 0.537135 eval: 0.480881\nLoss at epoch: 362 train: 0.497421 eval: 0.478087\nLoss at epoch: 363 train: 0.505873 eval: 0.481051\nLoss at epoch: 364 train: 0.529919 eval: 0.469590\nLoss at epoch: 365 train: 0.507262 eval: 0.465545\nLoss at epoch: 366 train: 0.505488 eval: 0.473796\nLoss at epoch: 367 train: 0.511497 eval: 0.475992\nLoss at epoch: 368 train: 0.524693 eval: 0.464133\nLoss at epoch: 369 train: 0.523909 eval: 0.469205\nLoss at epoch: 370 train: 0.491022 eval: 0.474732\nLoss at epoch: 371 train: 0.530235 eval: 0.465720\nLoss at epoch: 372 train: 0.496792 eval: 0.474071\nLoss at epoch: 373 train: 0.522428 eval: 0.482350\nLoss at epoch: 374 train: 0.501861 eval: 0.483875\nLoss at epoch: 375 train: 0.518763 eval: 0.473002\nLoss at epoch: 376 train: 0.520727 eval: 0.469772\nLoss at epoch: 377 train: 0.520035 eval: 0.481091\nLoss at epoch: 378 train: 0.519716 eval: 0.484875\nLoss at epoch: 379 train: 0.516215 eval: 0.475415\nLoss at epoch: 380 train: 0.499082 eval: 0.475147\nLoss at epoch: 381 train: 0.529229 eval: 0.475681\nLoss at epoch: 382 train: 0.532435 eval: 0.485580\nLoss at epoch: 383 train: 0.531546 eval: 0.483242\nLoss at epoch: 384 train: 0.510418 eval: 0.476269\nLoss at epoch: 385 train: 0.515485 eval: 0.486933\nLoss at epoch: 386 train: 0.532891 eval: 0.469876\nLoss at epoch: 387 train: 0.499574 eval: 0.483714\nLoss at epoch: 388 train: 0.516007 eval: 0.473191\nLoss at epoch: 389 train: 0.511539 eval: 0.476372\nLoss at epoch: 390 train: 0.517493 eval: 0.472065\nLoss at epoch: 391 train: 0.530827 eval: 0.481969\nLoss at epoch: 392 train: 0.509783 eval: 0.477736\nLoss at epoch: 393 train: 0.510914 eval: 0.479178\nLoss at epoch: 394 train: 0.530239 eval: 0.486025\nLoss at epoch: 395 train: 0.501546 eval: 0.490109\nLoss at epoch: 396 train: 0.516433 eval: 0.477811\nLoss at epoch: 397 train: 0.512753 eval: 0.469816\nLoss at epoch: 398 train: 0.526281 eval: 0.467045\nLoss at epoch: 399 train: 0.530430 eval: 0.465997\nLoss at epoch: 400 train: 0.513842 eval: 0.479103\nLoss at epoch: 401 train: 0.509782 eval: 0.471138\nLoss at epoch: 402 train: 0.537638 eval: 0.476323\nLoss at epoch: 403 train: 0.541526 eval: 0.475983\nLoss at epoch: 404 train: 0.523840 eval: 0.490951\nLoss at epoch: 405 train: 0.513676 eval: 0.477369\nLoss at epoch: 406 train: 0.517969 eval: 0.477747\nLoss at epoch: 407 train: 0.525017 eval: 0.484733\nLoss at epoch: 408 train: 0.519244 eval: 0.471192\nLoss at epoch: 409 train: 0.492734 eval: 0.487100\nLoss at epoch: 410 train: 0.519094 eval: 0.468944\nLoss at epoch: 411 train: 0.504315 eval: 0.470885\nLoss at epoch: 412 train: 0.520118 eval: 0.477972\nLoss at epoch: 413 train: 0.489506 eval: 0.473217\nLoss at epoch: 414 train: 0.498978 eval: 0.480649\nLoss at epoch: 415 train: 0.509001 eval: 0.472903\nLoss at epoch: 416 train: 0.496523 eval: 0.472550\nLoss at epoch: 417 train: 0.515926 eval: 0.481268\nLoss at epoch: 418 train: 0.497877 eval: 0.469926\nLoss at epoch: 419 train: 0.523717 eval: 0.476545\nLoss at epoch: 420 train: 0.526472 eval: 0.467497\nLoss at epoch: 421 train: 0.516970 eval: 0.476736\nLoss at epoch: 422 train: 0.501430 eval: 0.472998\nLoss at epoch: 423 train: 0.532158 eval: 0.473279\nLoss at epoch: 424 train: 0.530787 eval: 0.478201\nLoss at epoch: 425 train: 0.524955 eval: 0.476005\nLoss at epoch: 426 train: 0.519830 eval: 0.474286\nLoss at epoch: 427 train: 0.520321 eval: 0.479192\nLoss at epoch: 428 train: 0.488042 eval: 0.481723\nLoss at epoch: 429 train: 0.496308 eval: 0.475191\nLoss at epoch: 430 train: 0.525220 eval: 0.468320\nLoss at epoch: 431 train: 0.516577 eval: 0.470414\nLoss at epoch: 432 train: 0.509879 eval: 0.469362\nLoss at epoch: 433 train: 0.493520 eval: 0.482091\nLoss at epoch: 434 train: 0.512467 eval: 0.464280\nLoss at epoch: 435 train: 0.510716 eval: 0.468429\nLoss at epoch: 436 train: 0.493728 eval: 0.467846\nLoss at epoch: 437 train: 0.489570 eval: 0.474102\nLoss at epoch: 438 train: 0.503926 eval: 0.477575\nLoss at epoch: 439 train: 0.524978 eval: 0.468184\nLoss at epoch: 440 train: 0.514782 eval: 0.467043\nLoss at epoch: 441 train: 0.511905 eval: 0.475220\nLoss at epoch: 442 train: 0.514798 eval: 0.473226\nLoss at epoch: 443 train: 0.518089 eval: 0.478857\nLoss at epoch: 444 train: 0.516903 eval: 0.483481\nLoss at epoch: 445 train: 0.505125 eval: 0.489821\nLoss at epoch: 446 train: 0.528740 eval: 0.478487\nLoss at epoch: 447 train: 0.477229 eval: 0.472500\nLoss at epoch: 448 train: 0.531543 eval: 0.475358\nLoss at epoch: 449 train: 0.523741 eval: 0.479963\nLoss at epoch: 450 train: 0.528408 eval: 0.470418\nLoss at epoch: 451 train: 0.528204 eval: 0.463587\nLoss at epoch: 452 train: 0.527615 eval: 0.481948\nLoss at epoch: 453 train: 0.536807 eval: 0.487509\nLoss at epoch: 454 train: 0.517664 eval: 0.478869\nLoss at epoch: 455 train: 0.510771 eval: 0.475032\nLoss at epoch: 456 train: 0.508355 eval: 0.475232\nLoss at epoch: 457 train: 0.502757 eval: 0.477478\nLoss at epoch: 458 train: 0.515725 eval: 0.462765\nLoss at epoch: 459 train: 0.536536 eval: 0.468692\nLoss at epoch: 460 train: 0.485713 eval: 0.477087\nLoss at epoch: 461 train: 0.509616 eval: 0.477641\nLoss at epoch: 462 train: 0.494418 eval: 0.471959\nLoss at epoch: 463 train: 0.510116 eval: 0.473534\nLoss at epoch: 464 train: 0.516791 eval: 0.475728\nLoss at epoch: 465 train: 0.520223 eval: 0.481416\nLoss at epoch: 466 train: 0.538791 eval: 0.475302\nLoss at epoch: 467 train: 0.489708 eval: 0.466824\nLoss at epoch: 468 train: 0.517756 eval: 0.469977\nLoss at epoch: 469 train: 0.500657 eval: 0.478755\nLoss at epoch: 470 train: 0.527623 eval: 0.479292\nLoss at epoch: 471 train: 0.503840 eval: 0.476414\nLoss at epoch: 472 train: 0.501803 eval: 0.478360\nLoss at epoch: 473 train: 0.529980 eval: 0.478686\nLoss at epoch: 474 train: 0.523608 eval: 0.486472\nLoss at epoch: 475 train: 0.525104 eval: 0.487407\nLoss at epoch: 476 train: 0.505346 eval: 0.473849\nLoss at epoch: 477 train: 0.524340 eval: 0.479403\nLoss at epoch: 478 train: 0.529387 eval: 0.461537\nLoss at epoch: 479 train: 0.518149 eval: 0.467916\nLoss at epoch: 480 train: 0.510534 eval: 0.478324\nLoss at epoch: 481 train: 0.527116 eval: 0.479129\nLoss at epoch: 482 train: 0.521791 eval: 0.468847\nLoss at epoch: 483 train: 0.520450 eval: 0.476467\nLoss at epoch: 484 train: 0.512662 eval: 0.463004\nLoss at epoch: 485 train: 0.505028 eval: 0.470153\nLoss at epoch: 486 train: 0.516295 eval: 0.483002\nLoss at epoch: 487 train: 0.515090 eval: 0.478547\nLoss at epoch: 488 train: 0.516906 eval: 0.480072\nLoss at epoch: 489 train: 0.504679 eval: 0.476964\nLoss at epoch: 490 train: 0.507341 eval: 0.481446\nLoss at epoch: 491 train: 0.503827 eval: 0.472731\nLoss at epoch: 492 train: 0.521697 eval: 0.469121\nLoss at epoch: 493 train: 0.511479 eval: 0.473363\nLoss at epoch: 494 train: 0.522605 eval: 0.464062\nLoss at epoch: 495 train: 0.515275 eval: 0.482088\nLoss at epoch: 496 train: 0.514939 eval: 0.477791\nLoss at epoch: 497 train: 0.525112 eval: 0.472561\nLoss at epoch: 498 train: 0.487629 eval: 0.474489\nLoss at epoch: 499 train: 0.496753 eval: 0.487747\nLoss at epoch: 500 train: 0.519756 eval: 0.476542\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmatplot(cbind(overall_train_loss, overall_val_loss), type = \"l\", lty = 1, col = c(\"#2262AA\", \"#F82211\"), xlab = \"epoch\", ylab = \"Loss\")\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\nPredictions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$eval()\npredictions = model(Xtest)\npredictions = as.numeric(predictions)\n```\n:::\n\n\n\n\n\n\n\n</div>\n\n\n\n::::: callout-warning\n#### Bonus Task: More details on the inner working of Keras\n\nThe next task differs for Torch and Keras users. Keras users will learn more about the inner working of training while Torch users will learn how to simplify and generalize the training loop.\n\nGo through the code and try to understand it.\n\n::: panel-tabset\n## Keras\n\nSimilar to Torch, here we will write the training loop ourselves in the following. The training loop consists of several steps:\n\n1.  Sample batches of X and Y data\n2.  Open the gradientTape to create a computational graph (autodiff)\n3.  Make predictions and calculate loss\n4.  Update parameters based on the gradients at the loss (go back to 1. and repeat)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$bX\n        Solar.R        Wind        Temp      Month          Day\n79   1.09923936 -1.02302783  0.65133550 -0.1467431  0.235903090\n153  0.41905906  0.43858520 -1.02757865  1.2106304  1.614073775\n92   0.75914921 -0.20789749  0.33653910 -0.1467431  1.728921332\n136  0.58361881 -1.02302783 -0.08318944  1.2106304 -0.338334695\n3   -0.39276904  0.74777257 -0.39798584 -1.5041165 -1.486810266\n40   1.16506326  1.08506788  1.28092831 -0.8254298 -0.797724924\n41   1.51612406  0.43858520  0.96613190 -0.8254298 -0.682877366\n151  0.06799826  1.22560759 -0.29305371  1.2106304  1.384378661\n31   1.03341546 -0.71384046 -0.18812157 -1.5041165  1.728921332\n1    0.05702761 -0.71384046 -1.13251078 -1.5041165 -1.716505380\n138 -0.79868308  0.43858520 -0.71278225  1.2106304 -0.108639581\n88  -1.12780258  0.57912491  0.86119977 -0.1467431  1.269531104\n70   0.95662091 -1.19167548  1.49079258 -0.1467431 -0.797724924\n82  -1.95060133 -0.85438017 -0.39798584 -0.1467431  0.580445762\n20  -1.54468728 -0.06735777 -1.65717146 -1.5041165  0.465598204\n122  0.57264816 -1.02302783  1.91052111  0.5319436  1.614073775\n12   0.78109051 -0.06735777 -0.92264652 -1.5041165 -0.453182252\n116  0.29838191 -0.06735777  0.12667483  0.5319436  0.924988433\n16   1.63680121  0.43858520 -1.44730719 -1.5041165  0.006207976\n81   0.38614711  0.43858520  0.75626764 -0.1467431  0.465598204\n68   1.00050351 -1.36032314  1.07106404 -0.1467431 -1.027420038\n74  -0.10753214  1.39425525  0.33653910 -0.1467431 -0.338334695\n147 -1.48983403  0.10128988 -0.92264652  1.2106304  0.924988433\n47   0.06799826  1.39425525 -0.08318944 -0.8254298  0.006207976\n13   1.15409261 -0.20789749 -1.23744292 -1.5041165 -0.338334695\n14   0.97856221  0.26993754 -1.02757865 -1.5041165 -0.223487138\n87  -1.13877323 -0.37654514  0.44147123 -0.1467431  1.154683547\n100  0.48488296  0.10128988  1.28092831  0.5319436 -0.912572481\n38  -0.63412333 -0.06735777  0.44147123 -0.8254298 -1.027420038\n133  0.81400246 -0.06735777 -0.50291798  1.2106304 -0.682877366\n22   1.48321211  1.87209028 -0.50291798 -1.5041165  0.695293319\n111  0.64944271  0.26993754  0.02174269  0.5319436  0.350750647\n\n$bY\n [1] 61 20 59 28 12 71 39 14 37 41 13 52 97 16 11 84 16 45 14 63 77 27  7 21 11\n[26] 14 20 89 29 24 11 31\n```\n\n\n:::\n\n```{.r .cell-code}\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mse(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoss:  1557.822 \nLoss:  535.293 \nLoss:  257.8709 \nLoss:  414.4994 \nLoss:  266.5392 \nLoss:  273.8187 \nLoss:  462.1133 \nLoss:  213.2619 \nLoss:  160.4091 \nLoss:  220.8277 \nLoss:  204.1039 \nLoss:  255.8575 \nLoss:  283.8352 \nLoss:  210.2289 \nLoss:  109.0514 \nLoss:  223.2329 \nLoss:  212.3341 \nLoss:  189.9011 \nLoss:  394.5446 \nLoss:  355.501 \n```\n\n\n:::\n:::\n\n\n\n\n## Torch\n\nKeras and Torch use dataloaders to generate the data batches. Dataloaders are objects that return batches of data infinetly. Keras create the dataloader object automatically in the fit function, in Torch we have to write them ourselves:\n\n1.  Define a dataset object. This object informs the dataloader function about the inputs, outputs, length (nrow), and how to sample from it.\n2.  Create an instance of the dataset object by calling it and passing the actual data to it\n3.  Pass the initiated dataset to the dataloader function\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = matrix(data[,1], ncol = 1L)\n\n\ntorch_dataset = torch::dataset(\n    name = \"airquality\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n```\n:::\n\n\n\n\nOur dataloader is again an object which has to be initiated. The initiated object returns a list of two elements, batch x and batch y. The initated object stops returning batches when the dataset was completly transversed (no worries, we don't have to all of this ourselves).\n\nOur training loop has changed:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = nn_sequential(\n  nn_linear(5L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 1L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoss at epoch 10: 345.274590\nLoss at epoch 20: 297.495075\nLoss at epoch 30: 262.274246\nLoss at epoch 40: 249.399567\nLoss at epoch 50: 175.385204\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\nNow change the code from above for the iris data set. Tip: In tf$keras$losses\\$... you can find various loss functions.\n\n::: panel-tabset\n## Keras\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras3::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoss:  0.002853865 \nLoss:  0.0003605809 \nLoss:  0.000210291 \nLoss:  0.0001443397 \nLoss:  0.0004835225 \nLoss:  0.0002404193 \nLoss:  6.717737e-05 \nLoss:  0.0001725789 \nLoss:  0.0001153304 \nLoss:  0.0001410306 \nLoss:  0.0001124292 \nLoss:  9.977348e-05 \nLoss:  0.0001150258 \nLoss:  8.43777e-05 \nLoss:  5.691138e-05 \nLoss:  3.515561e-05 \nLoss:  4.100632e-05 \nLoss:  4.900979e-05 \nLoss:  7.684245e-05 \nLoss:  3.479642e-05 \nLoss:  3.521714e-05 \nLoss:  5.605527e-05 \nLoss:  2.567601e-05 \nLoss:  4.744987e-05 \nLoss:  5.685146e-05 \nLoss:  2.848717e-05 \n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n\n## Torch\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = as.integer(iris$Species)\n\n\ntorch_dataset = torch::dataset(\n    name = \"iris\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(4L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 3L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLoss at epoch 10: 13.205854\nLoss at epoch 20: 6.885013\nLoss at epoch 30: 4.659601\nLoss at epoch 40: 3.523017\nLoss at epoch 50: 2.833099\n```\n\n\n:::\n:::\n\n\n\n\n\n</div>\n\n:::\n\nRemarks:\n\n-   Mind the different input and output layer numbers.\n-   The loss function increases randomly, because different subsets of the data were drawn. This is a downside of stochastic gradient descent.\n-   A positive thing about stochastic gradient descent is, that local valleys or hills may be left and global ones can be found instead.\n:::::\n\n## Underlying mathematical concepts - optional {#basicMath}\n\nIf are not yet familiar with the underlying concepts of neural networks and want to know more about that, it is suggested to read / view the following videos / sites. Consider the Links and videos with descriptions in parentheses as optional bonus.\n\n***This might be useful to understand the further concepts in more depth.***\n\n-   (<a href=\"https://en.wikipedia.org/wiki/Newton%27s_method#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Newton%27s_method#Description</a> (Especially the animated graphic is interesting).)\n\n-   <a href=\"https://en.wikipedia.org/wiki/Gradient_descent#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Gradient_descent#Description</a>\n\n-   <a href=\"https://mlfromscratch.com/neural-networks-explained/#/\" target=\"_blank\" rel=\"noopener\">Neural networks (Backpropagation, etc.)</a>.\n\n-   <a href=\"https://mlfromscratch.com/activation-functions-explained/#/\" target=\"_blank\" rel=\"noopener\">Activation functions in detail</a> (requires the above as prerequisite).\n\n***Videos about the topic***:\n\n-   **Gradient descent explained**\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/sDv4f4s2SB8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n-   (Stochastic gradient descent explained)\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/vMh0zPT0tLI\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n-   (Entropy explained)\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/YtebGVx-Fxw\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n-   **Short explanation of entropy, cross entropy and Kullback--Leibler divergence**\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/ErfnhcEV1O8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n-   **Deep Learning (chapter 1)**\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/aircAruvnKk\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n-   **How neural networks learn - Deep Learning (chapter 2)**\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/IHZwWFHWa-w\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n-   **Backpropagation - Deep Learning (chapter 3)**\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n-   **Another video about backpropagation (extends the previous one) - Deep Learning (chapter 4)**\n\n\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/tIeHLnjs5U8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n\n\n### Caveats of neural network optimization\n\nDepending on activation functions, it might occur that the network won't get updated, even with high learning rates (called *vanishing gradient*, especially for \"sigmoid\" functions). Furthermore, updates might overshoot (called *exploding gradients*) or activation functions will result in many zeros (especially for \"relu\", *dying relu*).\n\nIn general, the first layers of a network tend to learn (much) more slowly than subsequent ones.\n",
    "supporting": [
      "C2-DeepNeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}