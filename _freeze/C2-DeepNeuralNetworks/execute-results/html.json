{
  "hash": "344c96b8ce4a39ac4de499be64b427c6",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Deep Neural Networks (DNN)\n\n\n\n\n\nWe can use TensorFlow directly from R (see @sec-tensorflowintro for an introduction to TensorFlow), and we could use this knowledge to implement a neural network in TensorFlow directly in R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us implement the machine learning models in TensorFlow. The most common of these is Keras.\n\nKeras is a powerful framework for building and training neural networks with just a few lines of code. As of the end of 2018, Keras and TensorFlow are fully interoperable, allowing us to take advantage of the best of both.\n\nThe goal of this lesson is to familiarize you with Keras. If you have TensorFlow installed, you can find Keras inside TensorFlow: tf.keras. However, the RStudio team has built an R package on top of tf.keras that is more convenient to use. To load the Keras package, type\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n# or library(torch)\n```\n:::\n\n\n## Example workflow in Keras / Torch\n\nWe build a small classifier to predict the three species of the iris data set. Load the necessary packages and data sets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(torch)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\ndata(iris)\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n\nFor neural networks, it is beneficial to scale the predictors (scaling = centering and standardization, see ?scale). We also split our data into predictors (X) and response (Y = the three species).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = scale(iris[,1:4])\nY = iris[,5]\n```\n:::\n\n\nAdditionally, Keras/TensorFlow cannot handle factors and we have to create contrasts (one-hot encoding). To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like Python and C++, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Keep this in mind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY = to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    1    0    0\n[3,]    1    0    0\n[4,]    1    0    0\n[5,]    1    0    0\n[6,]    1    0    0\n```\n:::\n:::\n\n\nAfter having prepared the data, we start now with the typical workflow in keras.\n\n**1. Initialize a sequential model in Keras:**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = keras_model_sequential()\n```\n:::\n\n\n## Torch\n\nTorch users can skip this step.\n:::\n\nA sequential Keras model is a higher order type of model within Keras and consists of one input and one output model.\n\n**2. Add hidden layers to the model (we will learn more about hidden layers during the next days).**\n\nWhen specifying the hidden layers, we also have to specify the shape and a so called *activation function*. You can think of the activation function as decision for what is forwarded to the next neuron (but we will learn more about it later). If you want to know this topic in even more depth, consider watching the videos presented in section \\@ref(basicMath).\n\nThe shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 3L, activation = \"softmax\") \n```\n:::\n\n\n## Torch\n\nThe Torch syntax is very similar, we will give a list of layers to the \"nn_sequential\" function. Here, we have to specify the softmax activation function as an extra layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\n```\n:::\n\n:::\n\n-   softmax scales a potential multidimensional vector to the interval $(0, 1]$ for each component. The sum of all components equals 1. This might be very useful for example for handling probabilities. **Ensure ther the labels start at 0! Otherwise the softmax function does not work well!**\n\n**3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).**\n\nWe will learn about other options later, so for now, do not worry about the \"**learning_rate**\" (\"**lr**\" in Torch or earlier in TensorFlow) argument, cross entropy or the optimizer.\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_3 (Dense)                    (None, 20)                      100         \n dense_2 (Dense)                    (None, 20)                      420         \n dense_1 (Dense)                    (None, 20)                      420         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 1,003\nTrainable params: 1,003\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n:::\n\n\n## Torch\n\nSpecify optimizer and the parameters which will be trained (in our case the parameters of the network):\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n```\n:::\n\n:::\n\n**4. Fit model in 30 iterations (epochs)**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\nmodel_history =\n  model %>%\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\n```\n:::\n\n\n## Torch\n\nIn Torch, we jump directly to the training loop, however, here we have to write our own training loop:\n\n1.  Get a batch of data.\n2.  Predict on batch.\n3.  Ccalculate loss between predictions and true labels.\n4.  Backpropagate error.\n5.  Update weights.\n6.  Go to step 1 and repeat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n:::\n\n**5. Plot training history:**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_72-1.png){width=672}\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_73-1.png){width=672}\n:::\n:::\n\n:::\n\n**6. Create predictions:**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = predict(model, X) # Probabilities for each class.\n```\n:::\n\n\nGet probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predictions) # Quasi-probabilities for each species.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]        [,2]         [,3]\n[1,] 0.9915600 0.006817889 0.0016221496\n[2,] 0.9584184 0.037489697 0.0040918575\n[3,] 0.9910416 0.007848956 0.0011094128\n[4,] 0.9813542 0.016901711 0.0017440914\n[5,] 0.9949830 0.004031503 0.0009855649\n[6,] 0.9905725 0.006884387 0.0025430375\n```\n:::\n:::\n\n\nFor each plant, we want to know for which species we got the highest probability:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds = apply(predictions, 1, which.max) \nprint(preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 2 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n:::\n:::\n\n:::\n\n**7. Calculate Accuracy (how often we have been correct):**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9066667\n```\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds_torch == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9733333\n```\n:::\n:::\n\n:::\n\n**8. Plot predictions, to see if we have done a good job:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_79-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(oldpar)   # Reset par.\n```\n:::\n\n\nSo you see, building a neural network is very easy with Keras or Torch and you can already do it on your own.\n\n## Exercise\n\n::: {.callout-caution icon=\"false\"}\n#### Task: Regression with keras\n\nWe now build a regression for the airquality data set with Keras/Torch. We want to predict the variable \"Ozone\" (continuous).\n\nTasks:\n\n-   **Fill in the missing steps**\n\n::: panel-tabset\n## Keras\n\nBefore we start, load and prepare the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)\t# Already sets R's random seed.\n\ndata = airquality\n\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n:::\n:::\n\n\n1.  **There are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n```\n:::\n:::\n\n\n\n</div>\n\n\n2.  **Split the data in features ($\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = scale(data[,2:6])\ny = data[,1]\n```\n:::\n\n\n\n</div>\n\n\n3.  **Create a sequential Keras model.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nmodel = keras_model_sequential()\n```\n:::\n\n\n\n</div>\n\n\n4.  **Add hidden layers (input and output layer are already specified, you have to add hidden layers between them):**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\n-   Why do we use 5L as input shape?\n-   Why only one output node and \"linear\" activation layer?\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\n\n</div>\n\n\n5.  **Compile the model**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n```\n:::\n\n\nWhat is the \"mean_squared_error\" loss?\n\n6.  **Fit model:**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n```\n:::\n\n\n\n</div>\n\n\nTip: Only matrices are accepted for $\\boldsymbol{X}$ and $\\boldsymbol{y}$ by Keras. R often drops a one column matrix into a vector (change it back to a matrix!)\n\n7.  **Plot training history.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n</div>\n\n\n8.  **Create predictions.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_keras = predict(model, x)\n```\n:::\n\n\n\n</div>\n\n\n9.  **Compare your Keras model with a linear model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.067499\n```\n:::\n:::\n\n\n## Torch\n\nBefore we start, load and prepare the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n:::\n\n```{.r .cell-code}\nplot(data)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_task_26_torch-1.png){width=672}\n:::\n:::\n\n\n1.  **There are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n```\n:::\n:::\n\n\n\n</div>\n\n\n2.  **Split the data in features ($\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = scale(data[,2:6])\ny = data[,1]\n```\n:::\n\n\n\n</div>\n\n\n3.  **Pass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    ...\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\n-   Why do we use 5L as input shape?\n-   Why only one output node and no activation layer?\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\n\n</div>\n\n\n4.  **Create optimizer**\n\nWe have to pass the network's parameters to the optimizer (how is this different to keras?)\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n```\n:::\n\n\n5.  **Fit model**\n\nIn torch we write the trainings loop on our own. Complete the trainings loop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = ...\nbatch_size = 32\nsteps = ...\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, ...) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = ...\n  Y_batch = ...\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = ...\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n\n</div>\n\n\nTips:\n\n-   Number of training \\$ steps = Number of rows / batchsize \\* Epochs \\$\n-   Search torch::nnf\\_... for the correct loss function (mse...)\n-   Make sure that X_torch and Y_torch have the same data type! (you can set the dtype via torch_tensor(..., dtype = ...)) \\_ Check the dimension of Y_torch, we need a matrix!\n\n6.  **Plot training history.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n</div>\n\n\n7.  **Create predictions.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n```\n:::\n\n\n\n</div>\n\n\n8.  **Compare your Torch model with a linear model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.872959\n```\n:::\n:::\n\n:::\n:::\n\n\n::: {.callout-caution icon=\"false\"}\n#### Task: Titanic dataset\n\nBuild a Keras DNN for the titanic dataset\n\n:::\n\n\n::: {.callout-caution icon=\"false\"}\n#### Bonus Task: More details on the inner working of Keras\n\nThe next task differs for Torch and Keras users. Keras users will learn more about the inner working of training while Torch users will learn how to simplify and generalize the training loop.\n\nGo through the code and try to understand it.\n\n::: panel-tabset\n## Keras\n\nSimilar to Torch, here we will write the training loop ourselves in the following. The training loop consists of several steps:\n\n1.  Sample batches of X and Y data\n2.  Open the gradientTape to create a computational graph (autodiff)\n3.  Make predictions and calculate loss\n4.  Update parameters based on the gradients at the loss (go back to 1. and repeat)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$bX\n        Solar.R        Wind        Temp      Month          Day\n137 -1.76410028  0.26993754 -0.71278225  1.2106304 -0.223487138\n153  0.41905906  0.43858520 -1.02757865  1.2106304  1.614073775\n44  -0.40373969 -0.54519280  0.44147123 -0.8254298 -0.338334695\n93  -1.11683193 -0.85438017  0.33653910  0.5319436 -1.716505380\n131  0.38614711  0.10128988  0.02174269  1.2106304 -0.912572481\n24  -1.01809608  0.57912491 -1.76210359 -1.5041165  0.924988433\n67   1.41738821  0.26993754  0.54640337 -0.1467431 -1.142267595\n105  0.96759156  0.43858520  0.44147123  0.5319436 -0.338334695\n94  -1.76410028  1.08506788  0.33653910  0.5319436 -1.601657823\n64   0.56167751 -0.20789749  0.33653910 -0.1467431 -1.486810266\n21  -1.93963068 -0.06735777 -1.97196786 -1.5041165  0.580445762\n4    1.40641756  0.43858520 -1.65717146 -1.5041165 -1.371962709\n126 -0.01976694 -2.00680582  1.59572471  1.2106304 -1.486810266\n69   0.90176766 -1.02302783  1.49079258 -0.1467431 -0.912572481\n47   0.06799826  1.39425525 -0.08318944 -0.8254298  0.006207976\n76  -1.50080468  1.22560759  0.23160696 -0.1467431 -0.108639581\n38  -0.63412333 -0.06735777  0.44147123 -0.8254298 -1.027420038\n108 -1.24847973  0.10128988 -0.08318944  0.5319436  0.006207976\n118  0.33129386 -0.54519280  0.86119977  0.5319436  1.154683547\n106 -0.30500384 -0.06735777  0.23160696  0.5319436 -0.223487138\n74  -0.10753214  1.39425525  0.33653910 -0.1467431 -0.338334695\n40   1.16506326  1.08506788  1.28092831 -0.8254298 -0.797724924\n104  0.07896891  0.43858520  0.86119977  0.5319436 -0.453182252\n41   1.51612406  0.43858520  0.96613190 -0.8254298 -0.682877366\n148 -1.80798288  1.87209028 -1.55223932  1.2106304  1.039835990\n145 -1.87380678 -0.20789749 -0.71278225  1.2106304  0.695293319\n2   -0.73285918 -0.54519280 -0.60785011 -1.5041165 -1.601657823\n112  0.05702761  0.10128988  0.02174269  0.5319436  0.465598204\n3   -0.39276904  0.74777257 -0.39798584 -1.5041165 -1.486810266\n128 -0.98518413 -0.71384046  0.96613190  1.2106304 -1.257115152\n22   1.48321211  1.87209028 -0.50291798 -1.5041165  0.695293319\n122  0.57264816 -1.02302783  1.91052111  0.5319436  1.614073775\n\n$bY\n [1]  9 20 23 39 23 32 40 28  9 32  1 18 73 97 21  7 29 22 73 65 27 71 44 39 14\n[26] 23 36 44 12 47 11 84\n```\n:::\n\n```{.r .cell-code}\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss:  406.3992 \nLoss:  527.0806 \nLoss:  252.544 \nLoss:  416.4157 \nLoss:  243.0384 \nLoss:  248.1294 \nLoss:  239.0951 \nLoss:  411.0923 \nLoss:  243.4587 \nLoss:  304.1434 \nLoss:  391.0959 \nLoss:  157.4986 \nLoss:  300.0546 \nLoss:  244.0995 \nLoss:  209.3407 \nLoss:  243.3395 \nLoss:  178.5952 \nLoss:  339.505 \nLoss:  175.7782 \nLoss:  160.5273 \n```\n:::\n:::\n\n\n## Torch\n\nKeras and Torch use dataloaders to generate the data batches. Dataloaders are objects that return batches of data infinetly. Keras create the dataloader object automatically in the fit function, in Torch we have to write them ourselves:\n\n1.  Define a dataset object. This object informs the dataloader function about the inputs, outputs, length (nrow), and how to sample from it.\n2.  Create an instance of the dataset object by calling it and passing the actual data to it\n3.  Pass the initiated dataset to the dataloader function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = matrix(data[,1], ncol = 1L)\n\n\ntorch_dataset = torch::dataset(\n    name = \"airquality\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n```\n:::\n\n\nOur dataloader is again an object which has to be initiated. The initiated object returns a list of two elements, batch x and batch y. The initated object stops returning batches when the dataset was completly transversed (no worries, we don't have to all of this ourselves).\n\nOur training loop has changed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = nn_sequential(\n  nn_linear(5L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 1L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 10: 349.544273\nLoss at epoch 20: 289.687613\nLoss at epoch 30: 269.088741\nLoss at epoch 40: 260.377857\nLoss at epoch 50: 213.073002\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n:::\n\nNow change the code from above for the iris data set. Tip: In tf$keras$losses\\$... you can find various loss functions.\n\n::: panel-tabset\n## Keras\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss:  0.002633849 \nLoss:  0.0005500487 \nLoss:  0.001006462 \nLoss:  0.0001315936 \nLoss:  0.0004843124 \nLoss:  0.0004023896 \nLoss:  0.0004356128 \nLoss:  0.000235351 \nLoss:  4.823796e-05 \nLoss:  0.0001512702 \nLoss:  0.0002624761 \nLoss:  0.0001274793 \nLoss:  7.111725e-05 \nLoss:  0.0001509234 \nLoss:  0.0002024032 \nLoss:  0.0001532886 \nLoss:  9.489701e-05 \nLoss:  0.0001040314 \nLoss:  7.334561e-05 \nLoss:  2.743953e-05 \nLoss:  9.655961e-05 \nLoss:  2.361947e-05 \nLoss:  6.918395e-05 \nLoss:  1.603245e-05 \nLoss:  1.772152e-05 \nLoss:  2.512357e-05 \n```\n:::\n:::\n\n\n\n</div>\n\n\n## Torch\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = as.integer(iris$Species)\n\n\ntorch_dataset = torch::dataset(\n    name = \"iris\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(4L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 3L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 10: 15.999315\nLoss at epoch 20: 8.324385\nLoss at epoch 30: 5.632413\nLoss at epoch 40: 4.256267\nLoss at epoch 50: 3.425689\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\nRemarks:\n\n-   Mind the different input and output layer numbers.\n-   The loss function increases randomly, because different subsets of the data were drawn. This is a downside of stochastic gradient descent.\n-   A positive thing about stochastic gradient descent is, that local valleys or hills may be left and global ones can be found instead.\n:::\n\n## Underlying mathematical concepts - optional {#basicMath}\n\nIf are not yet familiar with the underlying concepts of neural networks and want to know more about that, it is suggested to read / view the following videos / sites. Consider the Links and videos with descriptions in parentheses as optional bonus.\n\n***This might be useful to understand the further concepts in more depth.***\n\n-   (<a href=\"https://en.wikipedia.org/wiki/Newton%27s_method#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Newton%27s_method#Description</a> (Especially the animated graphic is interesting).)\n\n-   <a href=\"https://en.wikipedia.org/wiki/Gradient_descent#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Gradient_descent#Description</a>\n\n-   <a href=\"https://mlfromscratch.com/neural-networks-explained/#/\" target=\"_blank\" rel=\"noopener\">Neural networks (Backpropagation, etc.)</a>.\n\n-   <a href=\"https://mlfromscratch.com/activation-functions-explained/#/\" target=\"_blank\" rel=\"noopener\">Activation functions in detail</a> (requires the above as prerequisite).\n\n***Videos about the topic***:\n\n-   **Gradient descent explained**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/sDv4f4s2SB8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   (Stochastic gradient descent explained)\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/vMh0zPT0tLI\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   (Entropy explained)\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/YtebGVx-Fxw\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Short explanation of entropy, cross entropy and Kullback--Leibler divergence**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/ErfnhcEV1O8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Deep Learning (chapter 1)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/aircAruvnKk\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **How neural networks learn - Deep Learning (chapter 2)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/IHZwWFHWa-w\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Backpropagation - Deep Learning (chapter 3)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Another video about backpropagation (extends the previous one) - Deep Learning (chapter 4)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/tIeHLnjs5U8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n### Caveats of neural network optimization\n\nDepending on activation functions, it might occur that the network won't get updated, even with high learning rates (called *vanishing gradient*, especially for \"sigmoid\" functions). Furthermore, updates might overshoot (called *exploding gradients*) or activation functions will result in many zeros (especially for \"relu\", *dying relu*).\n\nIn general, the first layers of a network tend to learn (much) more slowly than subsequent ones.\n",
    "supporting": [
      "C2-DeepNeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}