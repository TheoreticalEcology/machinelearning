{
  "hash": "2d134a7fe9887f72f973f43b9c5baeb8",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Deep Neural Networks (DNN)\n\n\n\n\n\nWe can use TensorFlow directly from R (see @sec-tensorflowintro for an introduction to TensorFlow), and we could use this knowledge to implement a neural network in TensorFlow directly in R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us implement the machine learning models in TensorFlow. The most common of these is Keras.\n\nKeras is a powerful framework for building and training neural networks with just a few lines of code. As of the end of 2018, Keras and TensorFlow are fully interoperable, allowing us to take advantage of the best of both.\n\nThe goal of this lesson is to familiarize you with Keras. If you have TensorFlow installed, you can find Keras inside TensorFlow: tf.keras. However, the RStudio team has built an R package on top of tf.keras that is more convenient to use. To load the Keras package, type\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'keras3'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tensorflow':\n\n    set_random_seed, shape\n```\n:::\n\n```{.r .cell-code}\n# or library(torch)\n```\n:::\n\n\n## Example workflow in Keras / Torch\n\nWe build a small classifier to predict the three species of the iris data set. Load the necessary packages and data sets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras3)\nlibrary(tensorflow)\nlibrary(torch)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'torch'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:keras3':\n\n    as_iterator\n```\n:::\n\n```{.r .cell-code}\ndata(iris)\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n\nFor neural networks, it is beneficial to scale the predictors (scaling = centering and standardization, see ?scale). We also split our data into predictors (X) and response (Y = the three species).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = scale(iris[,1:4])\nY = iris[,5]\n```\n:::\n\n\nAdditionally, Keras/TensorFlow cannot handle factors and we have to create contrasts (one-hot encoding). To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like Python and C++, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Keep this in mind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY = keras3::to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    1    0    0\n[3,]    1    0    0\n[4,]    1    0    0\n[5,]    1    0    0\n[6,]    1    0    0\n```\n:::\n:::\n\n\nAfter having prepared the data, we start now with the typical workflow in keras.\n\n**1. Initialize a sequential model in Keras:**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = keras_model_sequential(shape(4L))\n```\n:::\n\n\n## Torch\n\nTorch users can skip this step.\n:::\n\nA sequential Keras model is a higher order type of model within Keras and consists of one input and one output model.\n\n**2. Add hidden layers to the model (we will learn more about hidden layers during the next days).**\n\nWhen specifying the hidden layers, we also have to specify the shape and a so called *activation function*. You can think of the activation function as decision for what is forwarded to the next neuron (but we will learn more about it later). If you want to know this topic in even more depth, consider watching the videos presented in section \\@ref(basicMath).\n\nThe shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n  layer_dense(units = 20L) |>\n  layer_dense(units = 20L) |>\n  layer_dense(units = 3L, activation = \"softmax\") \n```\n:::\n\n\n## Torch\n\nThe Torch syntax is very similar, we will give a list of layers to the \"nn_sequential\" function. Here, we have to specify the softmax activation function as an extra layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\n```\n:::\n\n:::\n\n-   softmax scales a potential multidimensional vector to the interval $(0, 1]$ for each component. The sum of all components equals 1. This might be very useful for example for handling probabilities. **Ensure ther the labels start at 0! Otherwise the softmax function does not work well!**\n\n**3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).**\n\nWe will learn about other options later, so for now, do not worry about the \"**learning_rate**\" (\"**lr**\" in Torch or earlier in TensorFlow) argument, cross entropy or the optimizer.\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  compile(loss = keras3::loss_categorical_crossentropy,\n          keras3::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ (None, 20)               │           100 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ (None, 20)               │           420 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ (None, 20)               │           420 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ (None, 3)                │            63 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 1,003 (3.92 KB)\n Trainable params: 1,003 (3.92 KB)\n Non-trainable params: 0 (0.00 B)\n```\n:::\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_68-1.png){width=109}\n:::\n:::\n\n\n## Torch\n\nSpecify optimizer and the parameters which will be trained (in our case the parameters of the network):\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n```\n:::\n\n:::\n\n**4. Fit model in 30 iterations (epochs)**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\n\nmodel_history =\n  model |>\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/30\n8/8 - 0s - 51ms/step - loss: 0.8138\nEpoch 2/30\n8/8 - 0s - 2ms/step - loss: 0.6827\nEpoch 3/30\n8/8 - 0s - 2ms/step - loss: 0.6061\nEpoch 4/30\n8/8 - 0s - 2ms/step - loss: 0.5518\nEpoch 5/30\n8/8 - 0s - 2ms/step - loss: 0.5129\nEpoch 6/30\n8/8 - 0s - 2ms/step - loss: 0.4817\nEpoch 7/30\n8/8 - 0s - 2ms/step - loss: 0.4568\nEpoch 8/30\n8/8 - 0s - 2ms/step - loss: 0.4379\nEpoch 9/30\n8/8 - 0s - 2ms/step - loss: 0.4200\nEpoch 10/30\n8/8 - 0s - 2ms/step - loss: 0.4050\nEpoch 11/30\n8/8 - 0s - 2ms/step - loss: 0.3908\nEpoch 12/30\n8/8 - 0s - 2ms/step - loss: 0.3786\nEpoch 13/30\n8/8 - 0s - 2ms/step - loss: 0.3671\nEpoch 14/30\n8/8 - 0s - 2ms/step - loss: 0.3565\nEpoch 15/30\n8/8 - 0s - 2ms/step - loss: 0.3467\nEpoch 16/30\n8/8 - 0s - 2ms/step - loss: 0.3374\nEpoch 17/30\n8/8 - 0s - 2ms/step - loss: 0.3285\nEpoch 18/30\n8/8 - 0s - 2ms/step - loss: 0.3201\nEpoch 19/30\n8/8 - 0s - 2ms/step - loss: 0.3120\nEpoch 20/30\n8/8 - 0s - 2ms/step - loss: 0.3045\nEpoch 21/30\n8/8 - 0s - 2ms/step - loss: 0.2972\nEpoch 22/30\n8/8 - 0s - 2ms/step - loss: 0.2900\nEpoch 23/30\n8/8 - 0s - 2ms/step - loss: 0.2838\nEpoch 24/30\n8/8 - 0s - 2ms/step - loss: 0.2778\nEpoch 25/30\n8/8 - 0s - 2ms/step - loss: 0.2714\nEpoch 26/30\n8/8 - 0s - 2ms/step - loss: 0.2660\nEpoch 27/30\n8/8 - 0s - 2ms/step - loss: 0.2599\nEpoch 28/30\n8/8 - 0s - 2ms/step - loss: 0.2542\nEpoch 29/30\n8/8 - 0s - 2ms/step - loss: 0.2497\nEpoch 30/30\n8/8 - 0s - 2ms/step - loss: 0.2437\n```\n:::\n:::\n\n\n## Torch\n\nIn Torch, we jump directly to the training loop, however, here we have to write our own training loop:\n\n1.  Get a batch of data.\n2.  Predict on batch.\n3.  Ccalculate loss between predictions and true labels.\n4.  Backpropagate error.\n5.  Update weights.\n6.  Go to step 1 and repeat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n:::\n\n**5. Plot training history:**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_72-1.png){width=672}\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_73-1.png){width=672}\n:::\n:::\n\n:::\n\n**6. Create predictions:**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = predict(model, X) # Probabilities for each class.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5/5 - 0s - 8ms/step\n```\n:::\n:::\n\n\nGet probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predictions) # Quasi-probabilities for each species.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]         [,2]         [,3]\n[1,] 0.9971334 0.0022211664 0.0006454307\n[2,] 0.9791400 0.0172321741 0.0036277140\n[3,] 0.9942988 0.0044973581 0.0012038074\n[4,] 0.9903139 0.0081486637 0.0015373421\n[5,] 0.9984665 0.0011704344 0.0003632042\n[6,] 0.9987515 0.0008852119 0.0003634085\n```\n:::\n:::\n\n\nFor each plant, we want to know for which species we got the highest probability:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds = apply(predictions, 1, which.max) \nprint(preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 2 2 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n [75] 2 2 2 3 3 2 2 2 2 3 2 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n```\n:::\n:::\n\n:::\n\n**7. Calculate Accuracy (how often we have been correct):**\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9133333\n```\n:::\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(preds_torch == as.integer(iris$Species))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.96\n```\n:::\n:::\n\n:::\n\n**8. Plot predictions, to see if we have done a good job:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_79-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(oldpar)   # Reset par.\n```\n:::\n\n\nSo you see, building a neural network is very easy with Keras or Torch and you can already do it on your own.\n\n## Exercise\n\n::: {.callout-warning}\n#### Task: Regression with keras\n\nWe now build a regression for the airquality data set with Keras/Torch. We want to predict the variable \"Ozone\" (continuous).\n\nTasks:\n\n-   **Fill in the missing steps**\n\n::: panel-tabset\n## Keras\n\nBefore we start, load and prepare the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\n\ndata = airquality\n\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n:::\n:::\n\n\n1.  **There are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n```\n:::\n:::\n\n\n\n</div>\n\n\n2.  **Split the data in features (**$\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = scale(data[,2:6])\ny = data[,1]\n```\n:::\n\n\n\n</div>\n\n\n3.  **Create a sequential Keras model.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\nmodel = keras_model_sequential(shape(5L))\n```\n:::\n\n\n\n</div>\n\n\n4.  **Add hidden layers (input and output layer are already specified, you have to add hidden layers between them):**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  layer_dense(units = 20L, activation = \"relu\") |>\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\n-   Why do we use 5L as input shape?\n-   Why only one output node and \"linear\" activation layer?\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) |>\n  layer_dense(units = 20L) |>\n  layer_dense(units = 20L) |>\n  layer_dense(units = 1L, activation = \"linear\")\n```\n:::\n\n\n\n</div>\n\n\n5.  **Compile the model**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel |>\n  compile(loss = keras3::loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n```\n:::\n\n\nWhat is the \"mean_squared_error\" loss?\n\n6.  **Fit model:**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_history =\n  model |>\n  fit(x = x, y = as.numeric(y), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/100\n6/6 - 0s - 56ms/step - loss: 2321.9165\nEpoch 2/100\n6/6 - 0s - 2ms/step - loss: 982.9076\nEpoch 3/100\n6/6 - 0s - 2ms/step - loss: 414.8292\nEpoch 4/100\n6/6 - 0s - 2ms/step - loss: 396.0573\nEpoch 5/100\n6/6 - 0s - 2ms/step - loss: 369.4333\nEpoch 6/100\n6/6 - 0s - 3ms/step - loss: 328.9295\nEpoch 7/100\n6/6 - 0s - 2ms/step - loss: 347.4358\nEpoch 8/100\n6/6 - 0s - 2ms/step - loss: 344.1622\nEpoch 9/100\n6/6 - 0s - 2ms/step - loss: 304.5391\nEpoch 10/100\n6/6 - 0s - 2ms/step - loss: 309.4441\nEpoch 11/100\n6/6 - 0s - 2ms/step - loss: 302.7473\nEpoch 12/100\n6/6 - 0s - 2ms/step - loss: 296.1998\nEpoch 13/100\n6/6 - 0s - 2ms/step - loss: 298.8271\nEpoch 14/100\n6/6 - 0s - 2ms/step - loss: 300.6726\nEpoch 15/100\n6/6 - 0s - 2ms/step - loss: 295.4472\nEpoch 16/100\n6/6 - 0s - 2ms/step - loss: 286.6333\nEpoch 17/100\n6/6 - 0s - 2ms/step - loss: 294.5211\nEpoch 18/100\n6/6 - 0s - 2ms/step - loss: 282.2076\nEpoch 19/100\n6/6 - 0s - 2ms/step - loss: 298.1875\nEpoch 20/100\n6/6 - 0s - 2ms/step - loss: 288.9870\nEpoch 21/100\n6/6 - 0s - 2ms/step - loss: 295.2146\nEpoch 22/100\n6/6 - 0s - 2ms/step - loss: 298.9435\nEpoch 23/100\n6/6 - 0s - 2ms/step - loss: 282.4414\nEpoch 24/100\n6/6 - 0s - 2ms/step - loss: 271.2641\nEpoch 25/100\n6/6 - 0s - 2ms/step - loss: 272.8733\nEpoch 26/100\n6/6 - 0s - 2ms/step - loss: 298.0881\nEpoch 27/100\n6/6 - 0s - 2ms/step - loss: 366.6479\nEpoch 28/100\n6/6 - 0s - 2ms/step - loss: 305.4495\nEpoch 29/100\n6/6 - 0s - 2ms/step - loss: 279.9576\nEpoch 30/100\n6/6 - 0s - 2ms/step - loss: 299.0950\nEpoch 31/100\n6/6 - 0s - 2ms/step - loss: 269.6277\nEpoch 32/100\n6/6 - 0s - 2ms/step - loss: 258.9182\nEpoch 33/100\n6/6 - 0s - 2ms/step - loss: 273.8960\nEpoch 34/100\n6/6 - 0s - 2ms/step - loss: 262.1778\nEpoch 35/100\n6/6 - 0s - 2ms/step - loss: 254.0363\nEpoch 36/100\n6/6 - 0s - 3ms/step - loss: 242.2896\nEpoch 37/100\n6/6 - 0s - 2ms/step - loss: 235.7357\nEpoch 38/100\n6/6 - 0s - 2ms/step - loss: 235.8250\nEpoch 39/100\n6/6 - 0s - 2ms/step - loss: 233.0893\nEpoch 40/100\n6/6 - 0s - 2ms/step - loss: 240.2446\nEpoch 41/100\n6/6 - 0s - 2ms/step - loss: 245.0330\nEpoch 42/100\n6/6 - 0s - 2ms/step - loss: 226.4910\nEpoch 43/100\n6/6 - 0s - 2ms/step - loss: 226.6024\nEpoch 44/100\n6/6 - 0s - 2ms/step - loss: 246.4733\nEpoch 45/100\n6/6 - 0s - 2ms/step - loss: 247.8189\nEpoch 46/100\n6/6 - 0s - 2ms/step - loss: 210.9115\nEpoch 47/100\n6/6 - 0s - 2ms/step - loss: 234.4505\nEpoch 48/100\n6/6 - 0s - 2ms/step - loss: 215.9642\nEpoch 49/100\n6/6 - 0s - 2ms/step - loss: 234.2563\nEpoch 50/100\n6/6 - 0s - 2ms/step - loss: 205.7956\nEpoch 51/100\n6/6 - 0s - 2ms/step - loss: 196.3006\nEpoch 52/100\n6/6 - 0s - 2ms/step - loss: 206.1569\nEpoch 53/100\n6/6 - 0s - 2ms/step - loss: 194.4553\nEpoch 54/100\n6/6 - 0s - 2ms/step - loss: 192.1763\nEpoch 55/100\n6/6 - 0s - 2ms/step - loss: 186.6724\nEpoch 56/100\n6/6 - 0s - 2ms/step - loss: 177.2546\nEpoch 57/100\n6/6 - 0s - 2ms/step - loss: 181.2013\nEpoch 58/100\n6/6 - 0s - 2ms/step - loss: 174.2069\nEpoch 59/100\n6/6 - 0s - 2ms/step - loss: 172.3559\nEpoch 60/100\n6/6 - 0s - 2ms/step - loss: 178.4062\nEpoch 61/100\n6/6 - 0s - 2ms/step - loss: 196.5222\nEpoch 62/100\n6/6 - 0s - 4ms/step - loss: 170.1390\nEpoch 63/100\n6/6 - 0s - 4ms/step - loss: 184.2467\nEpoch 64/100\n6/6 - 0s - 2ms/step - loss: 167.0110\nEpoch 65/100\n6/6 - 0s - 2ms/step - loss: 152.8770\nEpoch 66/100\n6/6 - 0s - 2ms/step - loss: 149.1309\nEpoch 67/100\n6/6 - 0s - 2ms/step - loss: 143.6623\nEpoch 68/100\n6/6 - 0s - 2ms/step - loss: 142.8105\nEpoch 69/100\n6/6 - 0s - 2ms/step - loss: 138.8338\nEpoch 70/100\n6/6 - 0s - 2ms/step - loss: 138.4495\nEpoch 71/100\n6/6 - 0s - 2ms/step - loss: 134.9511\nEpoch 72/100\n6/6 - 0s - 2ms/step - loss: 130.0121\nEpoch 73/100\n6/6 - 0s - 2ms/step - loss: 142.0014\nEpoch 74/100\n6/6 - 0s - 2ms/step - loss: 132.7274\nEpoch 75/100\n6/6 - 0s - 2ms/step - loss: 144.1110\nEpoch 76/100\n6/6 - 0s - 2ms/step - loss: 140.9579\nEpoch 77/100\n6/6 - 0s - 2ms/step - loss: 135.8787\nEpoch 78/100\n6/6 - 0s - 2ms/step - loss: 129.4101\nEpoch 79/100\n6/6 - 0s - 2ms/step - loss: 127.1901\nEpoch 80/100\n6/6 - 0s - 2ms/step - loss: 131.2258\nEpoch 81/100\n6/6 - 0s - 2ms/step - loss: 126.4945\nEpoch 82/100\n6/6 - 0s - 2ms/step - loss: 128.9517\nEpoch 83/100\n6/6 - 0s - 2ms/step - loss: 115.9814\nEpoch 84/100\n6/6 - 0s - 2ms/step - loss: 123.2476\nEpoch 85/100\n6/6 - 0s - 2ms/step - loss: 124.0128\nEpoch 86/100\n6/6 - 0s - 2ms/step - loss: 117.8402\nEpoch 87/100\n6/6 - 0s - 2ms/step - loss: 120.1798\nEpoch 88/100\n6/6 - 0s - 2ms/step - loss: 121.4599\nEpoch 89/100\n6/6 - 0s - 2ms/step - loss: 121.0706\nEpoch 90/100\n6/6 - 0s - 2ms/step - loss: 107.0067\nEpoch 91/100\n6/6 - 0s - 2ms/step - loss: 104.8538\nEpoch 92/100\n6/6 - 0s - 2ms/step - loss: 104.8698\nEpoch 93/100\n6/6 - 0s - 2ms/step - loss: 106.9870\nEpoch 94/100\n6/6 - 0s - 2ms/step - loss: 119.5048\nEpoch 95/100\n6/6 - 0s - 2ms/step - loss: 103.5651\nEpoch 96/100\n6/6 - 0s - 2ms/step - loss: 104.8019\nEpoch 97/100\n6/6 - 0s - 2ms/step - loss: 129.2321\nEpoch 98/100\n6/6 - 0s - 2ms/step - loss: 127.1427\nEpoch 99/100\n6/6 - 0s - 2ms/step - loss: 142.9904\nEpoch 100/100\n6/6 - 0s - 2ms/step - loss: 135.1990\n```\n:::\n:::\n\n\n\n</div>\n\n\n7.  **Plot training history.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_history)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n</div>\n\n\n8.  **Create predictions.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_keras = predict(model, x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4/4 - 0s - 9ms/step\n```\n:::\n:::\n\n\n\n</div>\n\n\n9.  **Compare your Keras model with a linear model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_keras)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.864703\n```\n:::\n:::\n\n\n## Torch\n\nBefore we start, load and prepare the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n```\n:::\n\n```{.r .cell-code}\nplot(data)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/chunk_chapter3_task_26_torch-1.png){width=672}\n:::\n:::\n\n\n1.  **There are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don't know how to remove NAs from a data.frame, use Google (e.g. with the query: \"remove-rows-with-all-or-some-nas-missing-values-in-data-frame\").**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n```\n:::\n:::\n\n\n\n</div>\n\n\n2.  **Split the data in features (**$\\boldsymbol{X}$) and response ($\\boldsymbol{y}$, Ozone) and scale the $\\boldsymbol{X}$ matrix.\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = scale(data[,2:6])\ny = data[,1]\n```\n:::\n\n\n\n</div>\n\n\n3.  **Pass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    ...\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\n-   Why do we use 5L as input shape?\n-   Why only one output node and no activation layer?\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n```\n:::\n\n\n\n</div>\n\n\n4.  **Create optimizer**\n\nWe have to pass the network's parameters to the optimizer (how is this different to keras?)\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n```\n:::\n\n\n5.  **Fit model**\n\nIn torch we write the trainings loop on our own. Complete the trainings loop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = ...\nbatch_size = 32\nsteps = ...\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, ...) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = ...\n  Y_batch = ...\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = ...\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate number of training steps.\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n```\n:::\n\n\n\n</div>\n\n\nTips:\n\n-   Number of training \\$ steps = Number of rows / batchsize \\* Epochs \\$\n-   Search torch::nnf\\_... for the correct loss function (mse...)\n-   Make sure that X_torch and Y_torch have the same data type! (you can set the dtype via torch_tensor(..., dtype = ...)) \\_ Check the dimension of Y_torch, we need a matrix!\n\n6.  **Plot training history.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n</div>\n\n\n7.  **Create predictions.**\n\n\n<div class='webex-solution'><button>Solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n```\n:::\n\n\n\n</div>\n\n\n8.  **Compare your Torch model with a linear model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.78897\n```\n:::\n\n```{.r .cell-code}\nprint(rmse_torch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.897064\n```\n:::\n:::\n\n:::\n:::\n\n::: {.callout-warning}\n#### Task: Titanic dataset\n\nBuild a Keras DNN for the titanic dataset\n:::\n\n::: {.callout-warning}\n#### Bonus Task: More details on the inner working of Keras\n\nThe next task differs for Torch and Keras users. Keras users will learn more about the inner working of training while Torch users will learn how to simplify and generalize the training loop.\n\nGo through the code and try to understand it.\n\n::: panel-tabset\n## Keras\n\nSimilar to Torch, here we will write the training loop ourselves in the following. The training loop consists of several steps:\n\n1.  Sample batches of X and Y data\n2.  Open the gradientTape to create a computational graph (autodiff)\n3.  Make predictions and calculate loss\n4.  Update parameters based on the gradients at the loss (go back to 1. and repeat)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$bX\n        Solar.R        Wind        Temp      Month          Day\n66  -0.10753214 -1.50086285  0.54640337 -0.1467431 -1.257115152\n29   0.73720791  1.39425525  0.33653910 -1.5041165  1.499226218\n8   -0.94130153  1.08506788 -1.97196786 -1.5041165 -0.912572481\n122  0.57264816 -1.02302783  1.91052111  0.5319436  1.614073775\n71  -0.10753214 -0.71384046  1.17599617 -0.1467431 -0.682877366\n17   1.34059366  0.57912491 -1.23744292 -1.5041165  0.121055533\n113  0.81400246  1.56290291 -0.08318944  0.5319436  0.580445762\n117  0.58361881 -1.83815816  0.33653910  0.5319436  1.039835990\n139  0.57264816 -0.85438017  0.02174269  1.2106304  0.006207976\n92   0.75914921 -0.20789749  0.33653910 -0.1467431  1.728921332\n79   1.09923936 -1.02302783  0.65133550 -0.1467431  0.235903090\n142  0.58361881  0.10128988 -1.02757865  1.2106304  0.350750647\n143  0.17770476 -0.54519280  0.44147123  1.2106304  0.465598204\n136  0.58361881 -1.02302783 -0.08318944  1.2106304 -0.338334695\n3   -0.39276904  0.74777257 -0.39798584 -1.5041165 -1.486810266\n40   1.16506326  1.08506788  1.28092831 -0.8254298 -0.797724924\n41   1.51612406  0.43858520  0.96613190 -0.8254298 -0.682877366\n140  0.43002971  1.08506788 -1.13251078  1.2106304  0.121055533\n31   1.03341546 -0.71384046 -0.18812157 -1.5041165  1.728921332\n1    0.05702761 -0.71384046 -1.13251078 -1.5041165 -1.716505380\n88  -1.12780258  0.57912491  0.86119977 -0.1467431  1.269531104\n70   0.95662091 -1.19167548  1.49079258 -0.1467431 -0.797724924\n82  -1.95060133 -0.85438017 -0.39798584 -0.1467431  0.580445762\n20  -1.54468728 -0.06735777 -1.65717146 -1.5041165  0.465598204\n149  0.08993956 -0.85438017 -0.81771438  1.2106304  1.154683547\n12   0.78109051 -0.06735777 -0.92264652 -1.5041165 -0.453182252\n116  0.29838191 -0.06735777  0.12667483  0.5319436  0.924988433\n16   1.63680121  0.43858520 -1.44730719 -1.5041165  0.006207976\n81   0.38614711  0.43858520  0.75626764 -0.1467431  0.465598204\n68   1.00050351 -1.36032314  1.07106404 -0.1467431 -1.027420038\n74  -0.10753214  1.39425525  0.33653910 -0.1467431 -0.338334695\n137 -1.76410028  0.26993754 -0.71278225  1.2106304 -0.223487138\n\n$bY\n [1]  64  45  19  84  85  34  21 168  46  59  61  24  16  28  12  71  39  18  37\n[20]  41  52  97  16  11  30  16  45  14  63  77  27   9\n```\n:::\n\n```{.r .cell-code}\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mse(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss:  2087.673 \nLoss:  501.1287 \nLoss:  264.9722 \nLoss:  407.2557 \nLoss:  275.5677 \nLoss:  270.6904 \nLoss:  432.9633 \nLoss:  219.3772 \nLoss:  177.213 \nLoss:  217.6541 \nLoss:  211.9007 \nLoss:  266.5504 \nLoss:  280.5027 \nLoss:  196.8696 \nLoss:  117.5799 \nLoss:  234.4628 \nLoss:  209.5031 \nLoss:  188.2401 \nLoss:  356.6076 \nLoss:  289.6031 \n```\n:::\n:::\n\n\n## Torch\n\nKeras and Torch use dataloaders to generate the data batches. Dataloaders are objects that return batches of data infinetly. Keras create the dataloader object automatically in the fit function, in Torch we have to write them ourselves:\n\n1.  Define a dataset object. This object informs the dataloader function about the inputs, outputs, length (nrow), and how to sample from it.\n2.  Create an instance of the dataset object by calling it and passing the actual data to it\n3.  Pass the initiated dataset to the dataloader function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = matrix(data[,1], ncol = 1L)\n\n\ntorch_dataset = torch::dataset(\n    name = \"airquality\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n```\n:::\n\n\nOur dataloader is again an object which has to be initiated. The initiated object returns a list of two elements, batch x and batch y. The initated object stops returning batches when the dataset was completly transversed (no worries, we don't have to all of this ourselves).\n\nOur training loop has changed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_torch = nn_sequential(\n  nn_linear(5L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 1L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 10: 349.544273\nLoss at epoch 20: 289.687613\nLoss at epoch 30: 269.088741\nLoss at epoch 40: 260.377857\nLoss at epoch 50: 213.073002\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n```\n\n::: {.cell-output-display}\n![](C2-DeepNeuralNetworks_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n:::\n\nNow change the code from above for the iris data set. Tip: In tf$keras$losses\\$... you can find various loss functions.\n\n::: panel-tabset\n## Keras\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras3::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss:  0.001593996 \nLoss:  0.0002652796 \nLoss:  0.0002969306 \nLoss:  0.0002200419 \nLoss:  0.0007639771 \nLoss:  0.0003026673 \nLoss:  0.0001730389 \nLoss:  0.0001458424 \nLoss:  0.0001694351 \nLoss:  0.0001593943 \nLoss:  0.000159012 \nLoss:  8.599473e-05 \nLoss:  0.0001775871 \nLoss:  9.016351e-05 \nLoss:  7.717456e-05 \nLoss:  7.887396e-05 \nLoss:  8.62173e-05 \nLoss:  4.091335e-05 \nLoss:  0.0001012006 \nLoss:  7.022043e-05 \nLoss:  6.433768e-05 \nLoss:  9.785009e-05 \nLoss:  4.884939e-05 \nLoss:  7.735056e-05 \nLoss:  8.833715e-05 \nLoss:  3.353848e-05 \n```\n:::\n:::\n\n\n\n</div>\n\n\n## Torch\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = as.integer(iris$Species)\n\n\ntorch_dataset = torch::dataset(\n    name = \"iris\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(4L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 3L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 10: 15.999315\nLoss at epoch 20: 8.324385\nLoss at epoch 30: 5.632413\nLoss at epoch 40: 4.256267\nLoss at epoch 50: 3.425689\n```\n:::\n:::\n\n\n\n</div>\n\n:::\n\nRemarks:\n\n-   Mind the different input and output layer numbers.\n-   The loss function increases randomly, because different subsets of the data were drawn. This is a downside of stochastic gradient descent.\n-   A positive thing about stochastic gradient descent is, that local valleys or hills may be left and global ones can be found instead.\n:::\n\n## Underlying mathematical concepts - optional {#basicMath}\n\nIf are not yet familiar with the underlying concepts of neural networks and want to know more about that, it is suggested to read / view the following videos / sites. Consider the Links and videos with descriptions in parentheses as optional bonus.\n\n***This might be useful to understand the further concepts in more depth.***\n\n-   (<a href=\"https://en.wikipedia.org/wiki/Newton%27s_method#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Newton%27s_method#Description</a> (Especially the animated graphic is interesting).)\n\n-   <a href=\"https://en.wikipedia.org/wiki/Gradient_descent#Description\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/Gradient_descent#Description</a>\n\n-   <a href=\"https://mlfromscratch.com/neural-networks-explained/#/\" target=\"_blank\" rel=\"noopener\">Neural networks (Backpropagation, etc.)</a>.\n\n-   <a href=\"https://mlfromscratch.com/activation-functions-explained/#/\" target=\"_blank\" rel=\"noopener\">Activation functions in detail</a> (requires the above as prerequisite).\n\n***Videos about the topic***:\n\n-   **Gradient descent explained**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/sDv4f4s2SB8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   (Stochastic gradient descent explained)\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/vMh0zPT0tLI\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   (Entropy explained)\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/YtebGVx-Fxw\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Short explanation of entropy, cross entropy and Kullback--Leibler divergence**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/ErfnhcEV1O8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Deep Learning (chapter 1)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/aircAruvnKk\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **How neural networks learn - Deep Learning (chapter 2)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/IHZwWFHWa-w\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Backpropagation - Deep Learning (chapter 3)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n-   **Another video about backpropagation (extends the previous one) - Deep Learning (chapter 4)**\n\n\n<iframe width=\"560\" height=\"315\"\n  src=\"https://www.youtube.com/embed/tIeHLnjs5U8\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write;\n  encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n### Caveats of neural network optimization\n\nDepending on activation functions, it might occur that the network won't get updated, even with high learning rates (called *vanishing gradient*, especially for \"sigmoid\" functions). Furthermore, updates might overshoot (called *exploding gradients*) or activation functions will result in many zeros (especially for \"relu\", *dying relu*).\n\nIn general, the first layers of a network tend to learn (much) more slowly than subsequent ones.\n",
    "supporting": [
      "C2-DeepNeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}