{
  "hash": "6e604d51ff03c40ccd3f8ff530a2e890",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Artificial Neural Networks\n\nArtificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\n```\n:::\n\n\ncito allows us to fit fully-connected neural networks within one line of code. When we come to other tasks such as image recognition we have to use frameworks with higher flexibility such as keras or torch.\n\nNeural networks are harder to optimize (hey are optimized via backpropagation and gradient descent) and a few hyperparameters that control the optimization should be familiar:\n\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| Hyperparameter | Meaning                                                                                                                                                                                                       | Range                     |\n+================+===============================================================================================================================================================================================================+===========================+\n| learning rate  | the step size of the parameter updating in the iterative optimization routine, if too high, the optimizer will step over good local optima, if too small, the optimizer will be stuck in a bad local optima   | \\[0.00001, 0.5\\]          |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| batch size     | NNs are optimized via stochastic gradient descent, i.e. only a batch of the data is used to update the parameters at a time                                                                                   | Depends on the data:      |\n|                |                                                                                                                                                                                                               |                           |\n|                |                                                                                                                                                                                                               | 10-250                    |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| epoch          | the data is fed into the optimization in batches, once the entire data set has been used in the optimization, the epoch is complete (so e.g. n = 100, batch size = 20, it takes 5 steps to complete an epoch) | 100+ (use early stopping) |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n\nExample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = airquality[complete.cases(airquality),] # DNN cannot handle NAs!\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), # Architecture, number of hidden layers and nodes in each layer\n            activation = c(\"selu\", \"selu\"), # activation functions for the specific hidden layer\n            loss = \"mse\", lr = 0.01, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R     1.326553\n2     Wind     2.200003\n3     Temp     3.259725\n4    Month     1.070405\n5      Day     1.100909\n\nAverage Conditional Effects:\n         Response_1\nSolar.R  0.21097518\nWind    -0.35825575\nTemp     0.53210032\nMonth   -0.07631096\nDay      0.09462875\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.09240233\nWind    0.19776493\nTemp    0.18960611\nMonth   0.06954875\nDay     0.06107787\n```\n:::\n:::\n\n\nThe architecture of the NN can be specified by the `hidden` argument, it is a vector where the length corresponds to the number of hidden layers and value of entry to the number of hidden neurons in each layer (and the same applies for the `activation` argument that specifies the activation functions in the hidden layers). It is hard to make recommendations about the architecture, a kind of general rule is that the width of the hidden layers is more important than the depth of the NN.\n\nThe loss function has to be adjusted to the response type:\n\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| Loss                      | Type                                  | Example                                          |\n+===========================+=======================================+==================================================+\n| mse (mean squared error)  | Regression                            | Numeric values                                   |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| mae (mean absolute error) | Regression                            | Numeric values, often used for skewed data       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| softmax                   | Classification, multi-label           | Species                                          |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| cross-entropy             | Classification, binary or multi-class | Survived/non-survived, Multi-species/communities |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| binomial                  | Classification, binary or multi-class | Binomial likelihood                              |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| poisson                   | Regression                            | Count data                                       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n\n::: callout-caution\n## Importance of the learning rate\n\ncito visualizes the training (see graphic). The reason for this is that the training can easily fail if the learning rate (lr) is poorly chosen. If the lr is too high, the optimizer \"jumps\" over good local optima, while it gets stuck in local optima if the lr is too small:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.4, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIf too high, the training will either directly fail (because the loss jumps to infinity) or the loss will be very wiggly and doesn't decrease over the number of epochs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.0001, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nIf too low, the loss will be very wiggly but doesn't decrease.\n:::\n\n::: callout-note\n## Learning rate scheduler\n\nAdjusting / reducing the learning rate during training is a common approach in neural networks. The idea is to start with a larger learning rate and then steadily decrease it during training (either systematically or based on specific properties):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n:::\n\n## Regularization\n\nWe can use $\\lambda$ and $\\alpha$ to set L1 and L2 regularization on the weights in our NN:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.01,\n            lambda = 0.01, # regularization strength\n            alpha = 0.5,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R     1.187252\n2     Wind     1.810835\n3     Temp     2.885533\n4    Month     1.054191\n5      Day     1.055524\n\nAverage Conditional Effects:\n         Response_1\nSolar.R  0.15909621\nWind    -0.34556210\nTemp     0.53288265\nMonth   -0.05723660\nDay      0.08894293\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.06777266\nWind    0.10204420\nTemp    0.17641610\nMonth   0.07345637\nDay     0.06525291\n```\n:::\n:::\n\n\nBe careful that you don't accidentally set all weights to 0 because of a too high regularization. We check the weights of the first layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfields::image.plot(coef(model)[[1]][[1]]) # weights of the first layer\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Exercise\n\n::: {.callout-warning}\n#### Question: Hyperparameter tuning - Titanic dataset\n\nTune architecture\n\n-   Play around with the architecture and try to improve the AUC on the submission server\n\nBonus:\n\n-   Tune the learning rate and the regularization for the titanic dataset! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\n\nmodel = dnn(survived~., \n          hidden = c(10L, 10L), # change\n          activation = c(\"selu\", \"selu\"), # change\n          loss = \"binomial\", \n          lr = 0.05, #change\n          validation = 0.2,\n          lambda = 0.001, # change\n          alpha = 0.1, # change\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = data_obs, epochs = 40L, verbose = TRUE, plot= TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 1: training: 0.707, validation: 0.659, lr: 0.05000\n```\n:::\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLoss at epoch 2: training: 0.659, validation: 0.671, lr: 0.05000\nLoss at epoch 3: training: 0.630, validation: 0.622, lr: 0.05000\nLoss at epoch 4: training: 0.618, validation: 0.570, lr: 0.05000\nLoss at epoch 5: training: 0.614, validation: 0.526, lr: 0.05000\nLoss at epoch 6: training: 0.615, validation: 0.679, lr: 0.05000\nLoss at epoch 7: training: 0.606, validation: 0.504, lr: 0.05000\nLoss at epoch 8: training: 0.609, validation: 0.500, lr: 0.05000\nLoss at epoch 9: training: 0.574, validation: 0.636, lr: 0.05000\nLoss at epoch 10: training: 0.568, validation: 0.703, lr: 0.05000\nLoss at epoch 11: training: 0.543, validation: 0.767, lr: 0.05000\nLoss at epoch 12: training: 0.541, validation: 0.718, lr: 0.05000\nLoss at epoch 13: training: 0.567, validation: 0.465, lr: 0.05000\nLoss at epoch 14: training: 0.535, validation: 0.446, lr: 0.05000\nLoss at epoch 15: training: 0.558, validation: 0.461, lr: 0.05000\nLoss at epoch 16: training: 0.517, validation: 0.630, lr: 0.05000\nLoss at epoch 17: training: 0.525, validation: 0.466, lr: 0.05000\nLoss at epoch 18: training: 0.556, validation: 0.589, lr: 0.05000\nLoss at epoch 19: training: 0.514, validation: 0.559, lr: 0.05000\nLoss at epoch 20: training: 0.503, validation: 0.497, lr: 0.05000\nLoss at epoch 21: training: 0.486, validation: 0.398, lr: 0.05000\nLoss at epoch 22: training: 0.611, validation: 0.500, lr: 0.05000\nLoss at epoch 23: training: 0.533, validation: 0.413, lr: 0.05000\nLoss at epoch 24: training: 0.517, validation: 0.394, lr: 0.05000\nLoss at epoch 25: training: 0.521, validation: 0.390, lr: 0.05000\nLoss at epoch 26: training: 0.490, validation: 0.391, lr: 0.05000\nLoss at epoch 27: training: 0.494, validation: 0.387, lr: 0.05000\nLoss at epoch 28: training: 0.491, validation: 0.407, lr: 0.05000\nLoss at epoch 29: training: 0.503, validation: 0.385, lr: 0.05000\nLoss at epoch 30: training: 0.506, validation: 0.387, lr: 0.05000\nLoss at epoch 31: training: 0.477, validation: 0.379, lr: 0.05000\nLoss at epoch 32: training: 0.471, validation: 0.377, lr: 0.05000\nLoss at epoch 33: training: 0.493, validation: 0.508, lr: 0.05000\nLoss at epoch 34: training: 0.475, validation: 0.567, lr: 0.05000\nLoss at epoch 35: training: 0.555, validation: 0.518, lr: 0.05000\nLoss at epoch 36: training: 0.466, validation: 0.376, lr: 0.05000\nLoss at epoch 37: training: 0.503, validation: 0.405, lr: 0.05000\nLoss at epoch 38: training: 0.496, validation: 0.404, lr: 0.05000\nLoss at epoch 39: training: 0.505, validation: 0.479, lr: 0.05000\nLoss at epoch 40: training: 0.495, validation: 0.616, lr: 0.05000\n```\n:::\n\n```{.r .cell-code}\n# Predictions:\n\npredictions = predict(model, newdata = data_new, type = \"response\") # change prediction type to response so that cito predicts probabilities\n\nwrite.csv(data.frame(y = predictions[,1]), file = \"Max_titanic_dnn.csv\")\n```\n:::\n\n\n**Hints**\n\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\n- if you pass the function `tune(...)` to a hyperparameter, this hyperparameter will be automatically tuned\n- in the `tuning = config_tuning(...)` argument, you can specify the cross-validation strategy and the number of hyperparameters that shoudl be tested\n- after the tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~., \n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L)\n                  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting hyperparameter tuning...\nFitting final model...\n```\n:::\n\n```{.r .cell-code}\n# tuning results\nmodel_tuned$tuning\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 5\n   steps  test train models  lambda\n   <int> <dbl> <dbl> <lgl>    <dbl>\n 1     1  74.4     0 NA     0.0785 \n 2     2  18.6     0 NA     0.00840\n 3     3 Inf       0 NA     0.137  \n 4     4  77.5     0 NA     0.0894 \n 5     5 Inf       0 NA     0.166  \n 6     6  72.6     0 NA     0.0787 \n 7     7  23.1     0 NA     0.0170 \n 8     8 Inf       0 NA     0.173  \n 9     9 Inf       0 NA     0.142  \n10    10  16.1     0 NA     0.00688\n11    11  81.3     0 NA     0.0990 \n12    12  62.8     0 NA     0.0502 \n13    13  77.0     0 NA     0.0820 \n14    14  34.0     0 NA     0.0223 \n15    15  85.4     0 NA     0.113  \n16    16 Inf       0 NA     0.148  \n17    17 Inf       0 NA     0.186  \n18    18 Inf       0 NA     0.157  \n19    19 Inf       0 NA     0.185  \n20    20  23.2     0 NA     0.0136 \n```\n:::\n\n```{.r .cell-code}\n# model_tuned is now already the best model!\n```\n:::\n\n\n:::\n\n::: {.callout-warning}\n#### Question: Hyperparameter tuning - Plant-pollinator dataset\n\nsee @sec-plantpoll for more information about the dataset.\n\nPrepare the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(missRanger)\nlibrary(dplyr)\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\n\nplant_poll_imputed = plant_poll %>% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction,\n                                           colour, \n                                           nectar,\n                                           feeding,\n                                           season)\n# Remove response variable interaction\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>%\n                                              select(-interaction), verbose = 0)\n\n# scale numeric variables\nplant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)] = scale(plant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)])\n\n# Add response back to the dataset after the imputatiob\nplant_poll_imputed$interaction = plant_poll$interaction\nplant_poll_imputed$colour = as.factor(plant_poll_imputed$colour)\nplant_poll_imputed$nectar = as.factor(plant_poll_imputed$nectar)\nplant_poll_imputed$feeding = as.factor(plant_poll_imputed$feeding)\nplant_poll_imputed$season = as.factor(plant_poll_imputed$season)\n\n\ndata_new = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ] # for which we want to make predictions at the end\ndata_obs = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]# data with known response\ndim(data_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14690     9\n```\n:::\n:::\n\n\nThe dataset is large! More than 10,000 observations. For now, let's switch to a simple holdout strategy for validating our model (e.g. use 80% of the data to train the model and 20% of the data to validate your model.\n\nMoreover:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(data_obs$interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    0     1 \n14095   595 \n```\n:::\n:::\n\n\nThe data is strongly imbalanced, i.e. many 0s but only a few 1. There are different strategies how to deal with that, for example oversampling the 1s or undersampling the 0s.\n\nUndersampling the 0s:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_obs = data_obs[c(sample(which(data_obs$interaction == 0), 1000), which(data_obs$interaction == 1)),]\ntable(data_obs$interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   0    1 \n1000  595 \n```\n:::\n\n```{.r .cell-code}\ndata_obs$interaction = as.integer(data_obs$interaction)-1\n```\n:::\n\n\nYour Tasks:\n\n- Use cito to tune a dnn on the data\n- Submit your predictions\n\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nMinimal example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\nmodel = dnn(interaction~., \n    hidden = c(50, 50), \n    activation = \"selu\", \n    loss = \"binomial\", \n    lr = tune(values = seq(0.0001, 0.03, length.out = 10)),\n    lambda = tune(values = seq(0.0001, 0.1, length.out = 10)),\n    alpha = tune(),\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    data = data_obs, \n    epochs = 100L, \n    tuning = config_tuning(CV = 3, steps = 15, parallel = 5L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting hyperparameter tuning...\nFitting final model...\n```\n:::\n\n```{.r .cell-code}\nprint(model$tuning)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 × 7\n   steps  test train models lambda  alpha      lr\n   <int> <dbl> <dbl> <lgl>   <dbl>  <dbl>   <dbl>\n 1     1  995. 1934. NA     0.0001 0.676  0.0234 \n 2     2 1033. 2048. NA     0.0112 0.750  0.0234 \n 3     3  Inf   701. NA     0.0556 0.959  0.03   \n 4     4  998. 1944. NA     0.0112 0.996  0.03   \n 5     5  Inf   701. NA     0.0112 0.980  0.00342\n 6     6  Inf   703. NA     0.0334 0.566  0.0134 \n 7     7  Inf   701. NA     0.0778 0.916  0.00674\n 8     8  Inf   703. NA     0.1    0.972  0.00674\n 9     9  Inf   701. NA     0.0445 0.963  0.0101 \n10    10  Inf   701. NA     0.0223 0.600  0.0101 \n11    11  Inf  1401. NA     0.0112 0.196  0.00674\n12    12  Inf   713. NA     0.0445 0.0723 0.00674\n13    13  Inf   702. NA     0.0223 0.408  0.00342\n14    14  Inf   701. NA     0.1    0.0130 0.03   \n15    15 1033. 2050. NA     0.0445 0.989  0.0134 \n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = predict(model, newdata = data_new, type = \"response\")[,1]\n\nwrite.csv(data.frame(y = predictions), file = \"Max_plant_poll_ensemble.csv\")\n```\n:::\n\n\n\n</div>\n\n",
    "supporting": [
      "B3-NeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}