{
  "hash": "9081d5faa0c716e61387c936e0a3ca70",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Artificial Neural Networks\n\nArtificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\n```\n:::\n\n\ncito allows us to fit fully-connected neural networks within one line of code. When we come to other tasks such as image recognition we have to use frameworks with higher flexibility such as keras or torch.\n\nNeural networks are harder to optimize (hey are optimized via backpropagation and gradient descent) and a few hyperparameters that control the optimization should be familiar:\n\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| Hyperparameter | Meaning                                                                                                                                                                                                       | Range                     |\n+================+===============================================================================================================================================================================================================+===========================+\n| learning rate  | the step size of the parameter updating in the iterative optimization routine, if too high, the optimizer will step over good local optima, if too small, the optimizer will be stuck in a bad local optima   | \\[0.00001, 0.5\\]          |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| batch size     | NNs are optimized via stochastic gradient descent, i.e. only a batch of the data is used to update the parameters at a time                                                                                   | Depends on the data:      |\n|                |                                                                                                                                                                                                               |                           |\n|                |                                                                                                                                                                                                               | 10-250                    |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| epoch          | the data is fed into the optimization in batches, once the entire data set has been used in the optimization, the epoch is complete (so e.g. n = 100, batch size = 20, it takes 5 steps to complete an epoch) | 100+ (use early stopping) |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n\nExample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = airquality[complete.cases(airquality),]\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), # Architecture, number of hidden layers and nodes in each layer\n            activation = c(\"selu\", \"selu\"), # activation functions for the specific hidden layer\n            loss = \"mse\", lr = 0.01, data = data, epochs = 150L, verbose = FALSE)\nplot(model)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDeep Neural Network Model summary\nModel generated on basis of: \nFeature Importance:\n  variable importance\n1  Solar.R   1.211966\n2     Wind   2.090089\n3     Temp   2.980428\n4    Month   1.094125\n5      Day   1.058092\n```\n:::\n:::\n\n\nThe architecture of the NN can be specified by the `hidden` argument, it is a vector where the length corresponds to the number of hidden layers and value of entry to the number of hidden neurons in each layer (and the same applies for the `activation` argument that specifies the activation functions in the hidden layers). It is hard to make recommendations about the architecture, a kind of general rule is that the width of the hidden layers is more important than the depth of the NN.\n\nThe loss function has to be adjusted to the response type:\n\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| Loss                      | Type                                  | Example                                          |\n+===========================+=======================================+==================================================+\n| mse (mean squared error)  | Regression                            | Numeric values                                   |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| mae (mean absolute error) | Regression                            | Numeric values, often used for skewed data       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| softmax                   | Classification, multi-label           | Species                                          |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| cross-entropy             | Classification, binary or multi-class | Survived/non-survived, Multi-species/communities |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| binomial                  | Classification, binary or multi-class | Binomial likelihood                              |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| poisson                   | Regression                            | Count data                                       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n\n::: callout-caution\n## Importance of the learning rate\n\ncito visualizes the training (see graphic). The reason for this is that the training can easily fail if the learning rate (lr) is poorly chosen. If the lr is too high, the optimizer \"jumps\" over good local optima, while it gets stuck in local optima if the lr is too small:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.4, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIf too high, the training will either directly fail (because the loss jumps to infinity) or the loss will be very wiggly and doesn't decrease over the number of epochs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.0001, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nIf too low, the loss will be very wiggly but doesn't decrease.\n:::\n\n::: callout-note\n## Learning rate scheduler\n\nAdjusting / reducing the learning rate during training is a common approach in neural networks. The idea is to start with a larger learning rate and then steadily decrease it during training (either systematically or based on specific properties):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n:::\n\n\n## Regularization\n\nWe can use $\\lambda$ and $\\alpha$ to set L1 and L2 regularization on the weights in our NN:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.05,\n            lambda = 0.1,\n            alpha = 0.5,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDeep Neural Network Model summary\nModel generated on basis of: \nFeature Importance:\n  variable importance\n1  Solar.R   1.175315\n2     Wind   1.857567\n3     Temp   2.641816\n4    Month   1.014699\n5      Day   1.007481\n```\n:::\n:::\n\n\nBe careful that you don't accidentally set all weights to 0 because of a too high regularization. We check the weights of the first layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfields::image.plot(coef(model)[[1]][[1]]) # weights of the first layer\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Exercise\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Regularization\n\nChange the following code to a pure L1 regularization and try different $\\lambda$ values, what happens to the weights of the first layer?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(40L, 40L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.05,\n            lambda = 0.0,\n            alpha = 0.5,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfields::image.plot(coef(model)[[1]][[1]])\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n$\\alpha = 0.0$ means that only L1 is used: Weak regularization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(40L, 40L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.05,\n            lambda = 0.01,\n            alpha = 0.0,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE, plot = FALSE)\nfields::image.plot(coef(model)[[1]][[1]])\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nStrong regularization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(40L, 40L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.05,\n            lambda = 0.04,\n            alpha = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE, plot= FALSE)\nfields::image.plot(coef(model)[[1]][[1]])\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe weights get sparse, i.e. many of them are zero.\n\n\n</div>\n\n:::\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Hyperparameter tuning - Titanic dataset\n\nCombing back to the titanic dataset from yesterday, we want to optimize $\\lambda$ using nested CV:\n\n-   Use CV to optimize the $\\lambda$ parameter\n-   Train model with the $\\lambda$ parameter with the highest AUC\n-   Make predictions for the new observations and submit them\n\nBonus:\n\n-   Tune the architecture! (depth and width of the NN via the hidden argument)\n\n::: {.callout-tip collapse=\"true\"}\n## Code template\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_lambda), function(k) {\n        model = dnn(survived~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"binomial\", \n            lr = 0.05,\n            lambda = ..., # change this line\n            alpha = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 10, gamma = 0.1),\n            data = train_outer, epochs = 40L, verbose = FALSE, plot= FALSE)\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer )[,2]))\n      })\n  best_lambda = hyper_lambda[which.max(tuning_results)]\n  \n  results[i, 1] = i\n  results[i, 2] = best_lambda\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n```\n:::\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %>% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n```\n:::\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\ncv = 3\nhyper_lambda = runif(5,0.0001, 0.02)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_lambda), function(k) {\n        model = dnn(survived~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"binomial\", \n            lr = 0.05,\n            lambda = hyper_lambda[k],\n            alpha = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 10, gamma = 0.1),\n            data = train_outer, epochs = 40L, verbose = FALSE, plot= FALSE)\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer )[,1]))\n      })\n  best_lambda = hyper_lambda[which.max(tuning_results)]\n  \n  results[i, 1] = i\n  results[i, 2] = best_lambda\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set      lambda       AUC\n1   1 0.012870736 0.8249822\n2   2 0.012870736 0.7867225\n3   3 0.005794177 0.8168278\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n  model = dnn(survived~., \n              hidden = c(10L, 10L), \n              activation = c(\"selu\", \"selu\"), \n              loss = \"binomial\", \n              lr = 0.05,\n              lambda = results$lambda[i],\n              alpha = 0.1,\n              lr_scheduler = config_lr_scheduler(\"step\", step_size = 10, gamma = 0.1),\n              data = data_sub[is.na(data_sub$survived),] , epochs = 40L, verbose = FALSE, plot= FALSE)\n    return(predict(model, data_obs)[,1])\n  })\n\n# Single predictions from the model with the highest AUC:\nwrite.csv(data.frame(y = prediction_ensemble[,which.max(results$AUC)]), file = \"Max_titanic_best_model.csv\")\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n```\n:::\n\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>Click here to see the solution for architecture tuning</button>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\nhyper_lambda = runif(10,0.0001, 0.02)\nhyper_alpha = runif(10,0, 1.0)\nhyper_hidden = sample.int(10, 10)\nhyper_nodes = sample(seq(5, 100), size = 10)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  lambda = rep(NA, cv),\n  alpha = rep(NA, cv),\n  hidden = rep(NA, cv),\n  nodes = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_lambda), function(k) {\n        model = dnn(survived~., \n            hidden = rep(hyper_nodes[k], hyper_hidden[k]), \n            activation = rep(\"selu\", hyper_hidden[k]), \n            loss = \"binomial\", \n            lr = 0.05,\n            lambda = hyper_lambda[k],\n            alpha = hyper_alpha[k],\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 10, gamma = 0.1),\n            data = train_outer, epochs = 40L, verbose = FALSE, plot= FALSE)\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer )[,1]))\n      })\n  results[i, 1] = i\n  results[i, 2] =  hyper_lambda[which.max(tuning_results)]\n  results[i, 3] =  hyper_alpha[which.max(tuning_results)]\n  results[i, 4] =  hyper_hidden[which.max(tuning_results)]\n  results[i, 5] =  hyper_nodes[which.max(tuning_results)]  \n  results[i, 6] = max(tuning_results)\n}\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set     lambda     alpha hidden nodes       AUC\n1   1 0.01043001 0.9400145      5    28 0.8321340\n2   2 0.01287074 0.4622928      4    46 0.8036609\n3   3 0.01287074 0.4622928      4    46 0.8178730\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n  model = dnn(survived~., \n              hidden = rep(results$nodes[i], results$hidden[i]), \n              activation = rep(\"selu\", results$hidden[i]), \n              loss = \"binomial\", \n              lr = 0.05,\n              lambda = results$lambda[i],\n              alpha = results$alpha[i],\n              lr_scheduler = config_lr_scheduler(\"step\", step_size = 10, gamma = 0.1),\n              data = data_sub[is.na(data_sub$survived),] , epochs = 40L, verbose = FALSE, plot= FALSE)\n    return(predict(model, data_obs)[,1])\n  })\n\n# Single predictions from the model with the highest AUC:\nwrite.csv(data.frame(y = prediction_ensemble[,which.max(results$AUC)]), file = \"Max_titanic_best_model.csv\")\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n```\n:::\n\n\n\n</div>\n\n\n:::\n\n\n\n\n::: {.callout-caution icon=\"false\"}\n#### Question: Hyperparameter tuning - Plant-pollinator dataset\n\nsee @sec-plantpoll for more information about the dataset.\n\nPrepare the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(missRanger)\nlibrary(dplyr)\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\n\nplant_poll_imputed = plant_poll %>% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction,\n                                           colour, \n                                           nectar,\n                                           feeding,\n                                           season)\n# Remove response variable interaction\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>%\n                                              select(-interaction), verbose = 0)\n\n# scale numeric variables\nplant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)] = scale(plant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)])\n\n# Add response back to the dataset after the imputatiob\nplant_poll_imputed$interaction = plant_poll$interaction\nplant_poll_imputed$colour = as.factor(plant_poll_imputed$colour)\nplant_poll_imputed$nectar = as.factor(plant_poll_imputed$nectar)\nplant_poll_imputed$feeding = as.factor(plant_poll_imputed$feeding)\nplant_poll_imputed$season = as.factor(plant_poll_imputed$season)\n\n\ndata_new = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ] # for which we want to make predictions at the end\ndata_obs = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]# data with known response\ndim(data_obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14690     9\n```\n:::\n:::\n\n\nThe dataset is large! More than 10,000 observations. For now, let's switch to a simple holdout strategy for validating our model (e.g. use 80% of the data to train the model and 20% of the data to validate your model.\n\nMoreover:\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(data_obs$interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    0     1 \n14095   595 \n```\n:::\n:::\n\nThe data is strongly imbalanced, i.e. many 0s but only a few 1. There are different strategies how to deal with that, for example oversampling the 1s or undersampling the 0s. \n\nUndersampling the 0s:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_obs = data_obs[c(sample(which(data_obs$interaction == 0), 2000), which(data_obs$interaction == 1)),]\ntable(data_obs$interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   0    1 \n2000  595 \n```\n:::\n\n```{.r .cell-code}\ndata_obs$interaction = as.integer(data_obs$interaction)\n```\n:::\n\n\n\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nMinimal example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\ntuning_steps = 2\nhyper_lambda = runif(tuning_steps,0.0001, 0.02)\nhyper_alpha = runif(tuning_steps,0, 1.0)\nhyper_hidden = sample.int(10, tuning_steps)\nhyper_nodes = sample(seq(5, 100), size = tuning_steps)\n\nouter_split = sample(nrow(data_obs), 0.2*nrow(data_obs))\n\nresults = data.frame(\n  set = 1,\n  lambda = rep(NA, 1),\n  alpha = rep(NA, 1),\n  hidden = rep(NA, 1),\n  nodes = rep(NA, 1),\n  AUC = rep(NA, 1)\n)\n\ntrain_outer = data_obs[-outer_split, ]\ntest_outer = data_obs[outer_split, ]\n\ntuning_results = \n    sapply(1:length(hyper_lambda), function(k) {\n      model = dnn(interaction~., \n          hidden = rep(hyper_nodes[k], hyper_hidden[k]), \n          activation = rep(\"selu\", hyper_hidden[k]), \n          loss = \"binomial\", \n          lr = 0.05,\n          lambda = hyper_lambda[k],\n          alpha = hyper_alpha[k],\n          batchsize = 100L, # increasing the batch size will reduce the runtime\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = train_outer, epochs = 50L, verbose = FALSE, plot= FALSE)\n      return(Metrics::auc(test_outer$interaction, predict(model, test_outer )[,1]))\n    })\nresults[1, 1] = 1\nresults[1, 2] =  hyper_lambda[which.max(tuning_results)]\nresults[1, 3] =  hyper_alpha[which.max(tuning_results)]\nresults[1, 4] =  hyper_hidden[which.max(tuning_results)]\nresults[1, 5] =  hyper_nodes[which.max(tuning_results)]  \nresults[1, 6] = max(tuning_results)\n\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  set     lambda     alpha hidden nodes AUC\n1   1 0.01830464 0.2861395     10    22 0.5\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk = 1\nmodel = dnn(interaction~., \n    hidden = rep(results$nodes[k], results$hidden[k]), \n    activation = rep(\"selu\", hyper_hidden[k]), \n    loss = \"binomial\", \n    lr = 0.05,\n    lambda = results$lambda[k],\n    alpha = results$alpha[k],\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n    data = train_outer, epochs = 50L, verbose = FALSE, plot= FALSE)\n\npredictions = predict(model, newdata = data_new)[,1]\n\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_plant_poll_ensemble.csv\")\n```\n:::\n\n\n\n\n</div>\n\n\n:::\n",
    "supporting": [
      "B3-NeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}