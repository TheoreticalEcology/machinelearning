{
  "hash": "17d8d5664a3ac6bca33e80efb14393f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n# Artificial Neural Networks\n\nArtificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\n```\n:::\n\n\n\n\n## Fitting (deep) neural networks with the cito package\n\nDeep neural networks are currently the state of the art in unsupervised learning. Their ability to model different types of data (e.g. graphs, images) is one of the reasons for their rise in recent years. However,  requires extensive (programming) knowledge of the underlying deep learning frameworks (e.g. TensorFlow or PyTorch), which we will teach you in two days. For tabular data, we can use packages like cito, which work similarly to regression functions like lm and allow us to train deep neural networks in one line of code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nnn.fit<- dnn(Species~., data = datasets::iris, loss = \"softmax\", verbose = FALSE, plot = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 methods overwritten by 'reformulas':\n  method       from\n  head.call    cito\n  head.formula cito\n  head.name    cito\n```\n\n\n:::\n:::\n\n\n\n\ncito also supports many of the S3 methods that are available for statistical models, e.g. the summary function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(nn.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n      variable importance_1\n1 Sepal.Length    1.6094284\n2  Sepal.Width    0.2381193\n3 Petal.Length   45.3715753\n4  Petal.Width   10.4426141\n\nAverage Conditional Effects:\n               Response_1  Response_2  Response_3\nSepal.Length  0.001933703  0.08236437 -0.08429806\nSepal.Width   0.005151204  0.07330558 -0.07845679\nPetal.Length -0.009846135 -0.14471930  0.15456542\nPetal.Width  -0.006189441 -0.17347164  0.17966107\n\nStandard Deviation of Conditional Effects:\n              Response_1 Response_2 Response_3\nSepal.Length 0.003113749  0.1736472  0.1730299\nSepal.Width  0.008174312  0.1627127  0.1606906\nPetal.Length 0.014838127  0.3025878  0.2983690\nPetal.Width  0.009500972  0.3541783  0.3519059\n```\n\n\n:::\n:::\n\n\n\n\nVariable importance can also be computed for non-tree algorithms (although it is slightly different, more on that on Thursday). The feature importance reports the importance of the features for distinguishing the three species, the average conditional effects are an approximation of the linear effects, and the standard deviation of the conditional effects is a measure of the non-linearity of these three variables.\n\nWe can also plot the underlying neural network:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(nn.fit)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nThe network starts with the input layer, 4 nodes for our four features, then two hidden layers, each with 50 nodes, and a final layer with 3 output nodes for our three levels of the species variable. The lines between the nodes are the connections between the nodes, and the lines actually represent the weights optimised during training (some connections are almost invisible, meaning they are close to 0).\n\n\n## Loss\n\nTasks such as regression and classification are fundamentally different; the former has continuous responses, while the latter has a discrete response. In ML algorithms, these different tasks can be represented by different loss functions (Classical ML algorithms also use loss functions but often they are automatically inferred, also neural networks are much more versatile, supporting more loss functions). Moreover, the tasks can differ even within regression or classification (e.g., in classification, we have binary classification (0 or 1) or multi-class classification (0, 1, or 2)). As a result, especially in DL, we have different specialized loss functions available for specific response types. The table below shows a list of supported loss functions in cito:\n\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| Loss                      | Type                                  | Example                                          |\n+===========================+=======================================+==================================================+\n| mse (mean squared error)  | Regression                            | Numeric values                                   |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| mae (mean absolute error) | Regression                            | Numeric values, often used for skewed data       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| softmax                   | Classification, multi-label           | Species                                          |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| cross-entropy             | Classification, binary or multi-class | Survived/non-survived, Multi-species/communities |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| binomial                  | Classification, binary or multi-class | Binomial likelihood                              |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| poisson                   | Regression                            | Count data                                       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n\nIn the iris data, we model `Species` which has 3 response levels, so this is was what we call multilabel and it requires a softmax link and a cross-entropy loss function, in cito we specify that by using the `softmax` loss:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nmodel<- dnn(Species~., data = datasets::iris, loss = \"softmax\", verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhead(predict(model, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        setosa  versicolor    virginica\n[1,] 0.9956357 0.004364315 1.738729e-10\n[2,] 0.9927422 0.007257805 5.387814e-10\n[3,] 0.9942655 0.005734487 4.055749e-10\n[4,] 0.9882450 0.011755005 1.823194e-09\n[5,] 0.9955537 0.004446283 1.991508e-10\n[6,] 0.9933258 0.006674126 3.934524e-10\n```\n\n\n:::\n:::\n\n\n\n\n## Validation\n\nA holdout, or validation data, is important for detecting (and preventing) overfitting. In cito, we can directly tell the `dnn` function to automatically use a random subset of the data as validation data, which is validated after each epoch (each iteration of the optimization), allowing us to monitor the training:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = airquality[complete.cases(airquality),] # DNN cannot handle NAs!\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            validation = 0.2,\n            loss = \"mse\",data = data, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nThe validation argument ranges from 0 and 1 is the percent of the data that should be used for validation\n\n### Baseline loss\n\nSince training DNNs can be quite challenging, we provide in cito a baseline loss that is computed from an intercept-only model (e.g., just the mean of the response). And the absolute minimum performance our DNN should achieve is to outperform the baseline model!\n\n## Trainings parameter\n\nIn DL, the optimization (the training of the DNN) is challenging as we have to optimize up to millions of parameters (which are not really identifiable, it is accepted that the optimization does not find a global minimum but just a good local minimum). We have a few important hyperparameters that affect only the optimization:\n\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| Hyperparameter | Meaning                                                                                                                                                                                                       | Range                     |\n+================+===============================================================================================================================================================================================================+===========================+\n| learning rate  | the step size of the parameter updating in the iterative optimization routine, if too high, the optimizer will step over good local optima, if too small, the optimizer will be stuck in a bad local optima   | \\[0.00001, 0.5\\]          |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| batch size     | NNs are optimized via stochastic gradient descent, i.e. only a batch of the data is used to update the parameters at a time                                                                                   | Depends on the data:      |\n|                |                                                                                                                                                                                                               |                           |\n|                |                                                                                                                                                                                                               | 10-250                    |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| epoch          | the data is fed into the optimization in batches, once the entire data set has been used in the optimization, the epoch is complete (so e.g. n = 100, batch size = 20, it takes 5 steps to complete an epoch) | 100+ (use early stopping) |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n\n### Learning rate\n\ncito visualizes the training (see graphic). The reason for this is that the training can easily fail if the learning rate (lr) is poorly chosen. If the lr is too high, the optimizer \"jumps\" over good local optima, while it gets stuck in local optima if the lr is too small:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.4, data = data, epochs = 150L, verbose = FALSE)\n```\n:::\n\n\n\n\nIf too high, the training will either directly fail (because the loss jumps to infinity) or the loss will be very wiggly and doesn't decrease over the number of epochs.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.0001, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nIf too low, the loss will be very wiggly but doesn't decrease.\n\n::: callout-note\n## Learning rate scheduler\n\nAdjusting / reducing the learning rate during training is a common approach in neural networks. The idea is to start with a larger learning rate and then steadily decrease it during training (either systematically or based on specific properties):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n## Architecture\n\nThe architecture of the NN can be specified by the `hidden` argument, it is a vector where the length corresponds to the number of hidden layers and value of entry to the number of hidden neurons in each layer (and the same applies for the `activation` argument that specifies the activation functions in the hidden layers). It is hard to make recommendations about the architecture, a kind of general rule is that the width of the hidden layers is more important than the depth of the NN.\n\nExample:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = airquality[complete.cases(airquality),] # DNN cannot handle NAs!\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), # Architecture, number of hidden layers and nodes in each layer\n            activation = c(\"selu\", \"selu\"), # activation functions for the specific hidden layer\n            loss = \"mse\", lr = 0.01, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R    0.2873814\n2     Wind    1.7571817\n3     Temp    2.4734383\n4    Month    0.1119469\n5      Day    0.1012945\n\nAverage Conditional Effects:\n        Response_1\nSolar.R  0.1625627\nWind    -0.3793555\nTemp     0.4532442\nMonth   -0.1140413\nDay      0.1017338\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.07120892\nWind    0.30708902\nTemp    0.22510422\nMonth   0.06714132\nDay     0.05358164\n```\n\n\n:::\n:::\n\n\n\n\n## Regularization\n\nWe can use $\\lambda$ and $\\alpha$ to set L1 and L2 regularization on the weights in our NN:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.01,\n            lambda = 0.01, # regularization strength\n            alpha = 0.5,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R   0.19268358\n2     Wind   0.84137686\n3     Temp   1.79677616\n4    Month   0.11496549\n5      Day   0.03293281\n\nAverage Conditional Effects:\n        Response_1\nSolar.R  0.1447125\nWind    -0.3445336\nTemp     0.5221033\nMonth   -0.1053451\nDay      0.0620252\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.05401614\nWind    0.08637159\nTemp    0.09301890\nMonth   0.07021153\nDay     0.06142853\n```\n\n\n:::\n:::\n\n\n\n\nBe careful that you don't accidentally set all weights to 0 because of a too high regularization. We check the weights of the first layer:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfields::image.plot(coef(model)[[1]][[1]]) # weights of the first layer\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n## Hyperparameter tuning\n\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\n-   if you pass the function `tune(...)` to a hyperparameter, this hyperparameter will be automatically tuned\n-   in the `tuning = config_tuning(...)` argument, you can specify the cross-validation strategy and the number of hyperparameters that shoudl be tested\n-   after the tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~., \n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L),\n                  verbose = FALSE\n                  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting hyperparameter tuning...\nFitting final model...\n```\n\n\n:::\n\n```{.r .cell-code}\n# tuning results\nmodel_tuned$tuning\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 5\n   steps  test train models  lambda\n   <int> <dbl> <dbl> <lgl>    <dbl>\n 1     1  13.6     0 NA     0.00181\n 2     2  72.2     0 NA     0.0744 \n 3     3  72.9     0 NA     0.0675 \n 4     4  45.7     0 NA     0.0319 \n 5     5 165.      0 NA     0.195  \n 6     6  77.9     0 NA     0.0919 \n 7     7  73.2     0 NA     0.0832 \n 8     8  18.3     0 NA     0.0110 \n 9     9 165.      0 NA     0.170  \n10    10  55.0     0 NA     0.0472 \n11    11  32.0     0 NA     0.0238 \n12    12 165.      0 NA     0.151  \n13    13 139.      0 NA     0.141  \n14    14  21.2     0 NA     0.0136 \n15    15  83.9     0 NA     0.120  \n16    16  49.0     0 NA     0.0373 \n17    17  75.4     0 NA     0.0804 \n18    18 111.      0 NA     0.117  \n19    19 165.      0 NA     0.156  \n20    20 165.      0 NA     0.194  \n```\n\n\n:::\n\n```{.r .cell-code}\n# model_tuned is now already the best model!\n```\n:::\n\n\n\n\n## Exercise\n\n::: callout-warning\n#### Question: Hyperparameter tuning dnn - Titanic dataset\n\nTune architecture\n\n-   Tune training parameters (learning rate, batch size) and regularization\n\n**Hints**\n\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\n-   passing `tune(...)` to a hyperparameter will tell cito to tune this specific hyperparameter\n-   the `tuning = config_tuning(...)` let you specify the cross-validation strategy and the number of hyperparameters that should be tested (steps = number of hyperparameter combinations that should be tried)\n-   after tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~.,\n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L),\n                  burnin = Inf\n                  )\n\n# tuning results\nmodel_tuned$tuning\n\n\n# model_tuned is now already the best model!\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata =\n  data %>% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %>%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\n\nmodel = dnn(survived~.,\n          hidden = c(10L, 10L), # change\n          activation = c(\"selu\", \"selu\"), # change\n          loss = \"binomial\",\n          lr = 0.05, #change\n          validation = 0.2,\n          lambda = 0.001, # change\n          alpha = 0.1, # change\n          burnin = Inf,\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = data_obs, epochs = 40L, verbose = FALSE, plot= TRUE)\n\n# Predictions:\n\npredictions = predict(model, newdata = data_new, type = \"response\") # change prediction type to response so that cito predicts probabilities\n\nwrite.csv(data.frame(y = predictions[,1]), file = \"Max_titanic_dnn.csv\")\n```\n:::\n\n\n\n:::\n\n\n::: callout-warning\n#### Question: Hyperparameter tuning - Plant-pollinator dataset\n\nThe plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13329\" target=\"_blank\" rel=\"noopener\">here</a>.\n\n\nsee @sec-plantpoll for more information about the dataset.\n\nPrepare the data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   crop                       insect          type          \n Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n Daucus_carota       :  256   Andrena_gravida    :   80                     \n Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n (Other)             :18944   (Other)            :20000                     \n    season             diameter        corolla             colour         \n Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n Class :character   1st Qu.:  5.00   Class :character   Class :character  \n Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n                    Mean   : 27.03                                        \n                    3rd Qu.: 25.00                                        \n                    Max.   :150.00                                        \n                    NA's   :9472                                          \n    nectar            b.system         s.pollination      inflorescence     \n Length:20480       Length:20480       Length:20480       Length:20480      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  composite            guild               tongue            body      \n Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n                                       Mean   : 8.104   Mean   :10.66  \n                                       3rd Qu.:10.500   3rd Qu.:13.00  \n                                       Max.   :26.400   Max.   :25.00  \n                                       NA's   :17040    NA's   :6160   \n  sociality           feeding          interaction \n Length:20480       Length:20480       0   :14095  \n Class :character   Class :character   1   :  595  \n Mode  :character   Mode  :character   NA's: 5790  \n                                                   \n                                                   \n                                                   \n                                                   \n```\n\n\n:::\n\n```{.r .cell-code}\n# scale numeric features\nplant_poll[, sapply(plant_poll, is.numeric)] = scale(plant_poll[, sapply(plant_poll, is.numeric)])\n\n# remove NAs\ndf = plant_poll[complete.cases(plant_poll),] # remove NAs\n\n# remove factors with only one level \ndata_obs = df %>% select(-crop, -insect, -season, -colour, -guild, -feeding, -composite)\n\n# change response to integer (because cito wants integer 0/1 for binomial data)\ndata_obs$interaction = as.integer(data_obs$interaction) - 1 \n\n\n\n# prepare the test data\nnewdata = plant_poll[is.na(plantPollinator_df$interaction), ]\nnewdata_imputed = missRanger::missRanger(data = newdata[,-ncol(newdata)], verbose = 0) # fill NAs\nnewdata_imputed$interaction = NA\n```\n:::\n\n\n\n\nMinimal example in cito:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\nmodel = dnn(interaction~., \n    hidden = c(50, 50), \n    activation = \"selu\", \n    loss = \"binomial\", \n    lr = tune(values = seq(0.0001, 0.03, length.out = 10)),\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    data = data_obs, \n    epochs = 200L, \n    burnin = Inf,\n    tuning = config_tuning(CV = 3, steps = 10))\n\n\nprint(model$tuning)\n\n# make final predictions\npredictions = predict(model, newdata_imputed, type = \"response\")[,1]\n\n# prepare submissions\nwrite.csv(data.frame(y = predictions), file = \"my_submission.csv\")\n```\n:::\n\n\n\n\n<!-- ```{r} -->\n\n<!-- library(EcoData) -->\n\n<!-- library(missRanger) -->\n\n<!-- library(dplyr) -->\n\n<!-- data(plantPollinator_df) -->\n\n<!-- plant_poll = plantPollinator_df -->\n\n<!-- plant_poll_imputed = plant_poll %>% select(diameter, -->\n\n<!--                                            corolla, -->\n\n<!--                                            tongue, -->\n\n<!--                                            body, -->\n\n<!--                                            interaction, -->\n\n<!--                                            colour,  -->\n\n<!--                                            nectar, -->\n\n<!--                                            feeding, -->\n\n<!--                                            season) -->\n\n<!-- # Remove response variable interaction -->\n\n<!-- plant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>% -->\n\n<!--                                               select(-interaction), verbose = 0) -->\n\n<!-- # scale numeric variables -->\n\n<!-- plant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)] = scale(plant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)]) -->\n\n<!-- # Add response back to the dataset after the imputation -->\n\n<!-- plant_poll_imputed$interaction = plant_poll$interaction -->\n\n<!-- plant_poll_imputed$colour = as.factor(plant_poll_imputed$colour) -->\n\n<!-- plant_poll_imputed$nectar = as.factor(plant_poll_imputed$nectar) -->\n\n<!-- plant_poll_imputed$feeding = as.factor(plant_poll_imputed$feeding) -->\n\n<!-- plant_poll_imputed$season = as.factor(plant_poll_imputed$season) -->\n\n<!-- data_new = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ] # for which we want to make predictions at the end -->\n\n<!-- data_obs = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]# data with known response -->\n\n<!-- dim(data_obs) -->\n\n<!-- ``` -->\n\n<!-- The dataset is large! More than 10,000 observations. For now, let's switch to a simple holdout strategy for validating our model (e.g. use 80% of the data to train the model and 20% of the data to validate your model. -->\n\n<!-- Moreover: -->\n\n<!-- ```{r} -->\n\n<!-- table(data_obs$interaction) -->\n\n<!-- ``` -->\n\n<!-- The data is strongly imbalanced, i.e. many 0s but only a few 1. There are different strategies how to deal with that, for example oversampling the 1s or undersampling the 0s. -->\n\n<!-- Undersampling the 0s: -->\n\n<!-- ```{r} -->\n\n<!-- data_obs = data_obs[c(sample(which(data_obs$interaction == 0), 1000), which(data_obs$interaction == 1)),] -->\n\n<!-- table(data_obs$interaction) -->\n\n<!-- data_obs$interaction = as.integer(data_obs$interaction)-1 -->\n\n<!-- ``` -->\n\nYour Tasks:\n\n-   Use cito to tune learning parameters and the regularization\n-   Submit your predictions to <http://rhsbio7.uni-regensburg.de:8500/>\n:::\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nMinimal example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\nmodel = dnn(interaction~., \n    hidden = c(50, 50), \n    activation = \"selu\", \n    loss = \"binomial\", \n    lr = tune(values = seq(0.0001, 0.03, length.out = 10)),\n    lambda = tune(values = seq(0.0001, 0.1, length.out = 10)),\n    alpha = tune(),\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    data = data_obs, \n    epochs = 100L, \n    burnin = Inf,\n    tuning = config_tuning(CV = 3, steps = 15))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting hyperparameter tuning...\nFitting final model...\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(model$tuning)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 7\n   steps  test train models lambda  alpha      lr\n   <int> <dbl> <dbl> <lgl>   <dbl>  <dbl>   <dbl>\n 1     1  387.     0 NA     0.0889 0.197  0.00674\n 2     2  355.     0 NA     0.1    0.501  0.0134 \n 3     3  331.     0 NA     0.0334 0.825  0.0200 \n 4     4  334.     0 NA     0.0112 0.0921 0.0134 \n 5     5  338.     0 NA     0.0778 0.470  0.0234 \n 6     6  332.     0 NA     0.0445 0.881  0.0134 \n 7     7  358.     0 NA     0.0445 0.436  0.00674\n 8     8  341.     0 NA     0.0667 0.277  0.0200 \n 9     9  562.     0 NA     0.1    0.0571 0.0001 \n10    10  320.     0 NA     0.0112 0.885  0.0234 \n11    11  332.     0 NA     0.0112 0.148  0.0234 \n12    12  344.     0 NA     0.0667 0.317  0.0167 \n13    13  338.     0 NA     0.0001 0.681  0.00342\n14    14  543.     0 NA     0.1    0.283  0.0001 \n15    15  356.     0 NA     0.0556 0.425  0.0101 \n```\n\n\n:::\n:::\n\n\n\n\nMake predictions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = predict(model, newdata_imputed, type = \"response\")[,1]\n\nwrite.csv(data.frame(y = predictions), file = \"Max_plant_.csv\")\n```\n:::\n\n\n\n\n\n</div>\n\n",
    "supporting": [
      "B3-NeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}