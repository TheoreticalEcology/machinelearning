{
  "hash": "485c64f381092de1eca97a16f7cc5593",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Artificial Neural Networks\n\nArtificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\n```\n:::\n\n\n## Loss\n\nTasks such as regression and classification are fundamentally different; the former has continuous responses, while the latter has a discrete response. In ML algorithms, these different tasks can be represented by different loss functions. Moreover, the tasks can differ even within regression or classification (e.g., in classification, we have binary classification (0 or 1) or multi-class classification (0, 1, or 2)). As a result, especially in DL, we have different specialized loss functions available for specific response types. The table below shows a list of supported loss functions in cito:\n\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| Loss                      | Type                                  | Example                                          |\n+===========================+=======================================+==================================================+\n| mse (mean squared error)  | Regression                            | Numeric values                                   |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| mae (mean absolute error) | Regression                            | Numeric values, often used for skewed data       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| softmax                   | Classification, multi-label           | Species                                          |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| cross-entropy             | Classification, binary or multi-class | Survived/non-survived, Multi-species/communities |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| binomial                  | Classification, binary or multi-class | Binomial likelihood                              |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n| poisson                   | Regression                            | Count data                                       |\n+---------------------------+---------------------------------------+--------------------------------------------------+\n\nIn the iris data, we model `Species` which has 3 response levels, so this is was what we call multilabel and it requires a softmax link and a cross-entropy loss function, in cito we specify that by using the `softmax` loss:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nmodel<- dnn(Species~., data = datasets::iris, loss = \"softmax\", verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhead(predict(model, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        setosa  versicolor    virginica\n[1,] 0.9967312 0.003268858 1.213672e-10\n[2,] 0.9938498 0.006150273 5.073510e-10\n[3,] 0.9953117 0.004688264 3.289294e-10\n[4,] 0.9906046 0.009395462 1.386144e-09\n[5,] 0.9967400 0.003259970 1.265096e-10\n[6,] 0.9953622 0.004637787 2.316394e-10\n```\n:::\n:::\n\n\n## Validation\n\nA holdout, or validation data, is important for detecting (and preventing) overfitting. In cito, we can directly tell the `dnn` function to automatically use a random subset of the data as validation data, which is validated after each epoch (each iteration of the optimization), allowing us to monitor the training:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = airquality[complete.cases(airquality),] # DNN cannot handle NAs!\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            validation = 0.2,\n            loss = \"mse\",data = data, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe validation argument ranges from 0 and 1 is the percent of the data that should be used for validation\n\n### Baseline loss\n\nSince training DNNs can be quite challenging, we provide in cito a baseline loss that is computed from an intercept-only model (e.g., just the mean of the response). And the absolute minimum performance our DNN should achieve is to outperform the baseline model!\n\n## Trainings parameter\n\nIn DL, the optimization (the training of the DNN) is challenging as we have to optimize up to millions of parameters (which are not really identifiable, it is accepted that the optimization does not find a global minimum but just a good local minimum). We have a few important hyperparameters that affect only the optimization:\n\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| Hyperparameter | Meaning                                                                                                                                                                                                       | Range                     |\n+================+===============================================================================================================================================================================================================+===========================+\n| learning rate  | the step size of the parameter updating in the iterative optimization routine, if too high, the optimizer will step over good local optima, if too small, the optimizer will be stuck in a bad local optima   | \\[0.00001, 0.5\\]          |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| batch size     | NNs are optimized via stochastic gradient descent, i.e. only a batch of the data is used to update the parameters at a time                                                                                   | Depends on the data:      |\n|                |                                                                                                                                                                                                               |                           |\n|                |                                                                                                                                                                                                               | 10-250                    |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n| epoch          | the data is fed into the optimization in batches, once the entire data set has been used in the optimization, the epoch is complete (so e.g. n = 100, batch size = 20, it takes 5 steps to complete an epoch) | 100+ (use early stopping) |\n+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+\n\n### Learning rate\n\ncito visualizes the training (see graphic). The reason for this is that the training can easily fail if the learning rate (lr) is poorly chosen. If the lr is too high, the optimizer \"jumps\" over good local optima, while it gets stuck in local optima if the lr is too small:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.4, data = data, epochs = 150L, verbose = FALSE)\n```\n:::\n\n\nIf too high, the training will either directly fail (because the loss jumps to infinity) or the loss will be very wiggly and doesn't decrease over the number of epochs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.0001, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIf too low, the loss will be very wiggly but doesn't decrease.\n\n::: callout-note\n## Learning rate scheduler\n\nAdjusting / reducing the learning rate during training is a common approach in neural networks. The idea is to start with a larger learning rate and then steadily decrease it during training (either systematically or based on specific properties):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\n## Architecture\n\nThe architecture of the NN can be specified by the `hidden` argument, it is a vector where the length corresponds to the number of hidden layers and value of entry to the number of hidden neurons in each layer (and the same applies for the `activation` argument that specifies the activation functions in the hidden layers). It is hard to make recommendations about the architecture, a kind of general rule is that the width of the hidden layers is more important than the depth of the NN.\n\nExample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = airquality[complete.cases(airquality),] # DNN cannot handle NAs!\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), # Architecture, number of hidden layers and nodes in each layer\n            activation = c(\"selu\", \"selu\"), # activation functions for the specific hidden layer\n            loss = \"mse\", lr = 0.01, data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R     1.288589\n2     Wind     2.363074\n3     Temp     3.439806\n4    Month     1.067527\n5      Day     1.097977\n\nAverage Conditional Effects:\n         Response_1\nSolar.R  0.14370963\nWind    -0.32338720\nTemp     0.46808744\nMonth   -0.06165214\nDay      0.09377104\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.08835799\nWind    0.28582694\nTemp    0.28236139\nMonth   0.03840015\nDay     0.03336857\n```\n:::\n:::\n\n\n## Regularization\n\nWe can use $\\lambda$ and $\\alpha$ to set L1 and L2 regularization on the weights in our NN:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.01,\n            lambda = 0.01, # regularization strength\n            alpha = 0.5,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R     1.221437\n2     Wind     1.931930\n3     Temp     3.188223\n4    Month     1.052763\n5      Day     1.038933\n\nAverage Conditional Effects:\n         Response_1\nSolar.R  0.15208087\nWind    -0.34851887\nTemp     0.53599698\nMonth   -0.09532617\nDay      0.06249972\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.08752070\nWind    0.09526082\nTemp    0.16103749\nMonth   0.05305456\nDay     0.04512571\n```\n:::\n:::\n\n\nBe careful that you don't accidentally set all weights to 0 because of a too high regularization. We check the weights of the first layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfields::image.plot(coef(model)[[1]][[1]]) # weights of the first layer\n```\n\n::: {.cell-output-display}\n![](B3-NeuralNetworks_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Hyperparameter tuning\n\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\n-   if you pass the function `tune(...)` to a hyperparameter, this hyperparameter will be automatically tuned\n-   in the `tuning = config_tuning(...)` argument, you can specify the cross-validation strategy and the number of hyperparameters that shoudl be tested\n-   after the tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~., \n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L),\n                  verbose = FALSE\n                  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting hyperparameter tuning...\nFitting final model...\n```\n:::\n\n```{.r .cell-code}\n# tuning results\nmodel_tuned$tuning\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 5\n   steps  test train models lambda\n   <int> <dbl> <dbl> <lgl>   <dbl>\n 1     1  72.3     0 NA     0.0691\n 2     2 Inf       0 NA     0.170 \n 3     3  27.9     0 NA     0.0208\n 4     4  51.7     0 NA     0.0387\n 5     5 Inf       0 NA     0.164 \n 6     6  57.9     0 NA     0.0422\n 7     7 Inf       0 NA     0.134 \n 8     8  32.2     0 NA     0.0217\n 9     9 Inf       0 NA     0.159 \n10    10  52.6     0 NA     0.0428\n11    11  77.9     0 NA     0.0737\n12    12 Inf       0 NA     0.149 \n13    13  82.3     0 NA     0.0962\n14    14  89.9     0 NA     0.114 \n15    15  38.7     0 NA     0.0276\n16    16  44.3     0 NA     0.0317\n17    17 Inf       0 NA     0.145 \n18    18 Inf       0 NA     0.180 \n19    19 Inf       0 NA     0.185 \n20    20 Inf       0 NA     0.140 \n```\n:::\n\n```{.r .cell-code}\n# model_tuned is now already the best model!\n```\n:::\n\n\n## Exercise\n\n::: callout-warning\n#### Question: Hyperparameter tuning - Plant-pollinator dataset\n\nsee @sec-plantpoll for more information about the dataset.\n\nPrepare the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(EcoData)\n\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   crop                       insect          type          \n Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n Daucus_carota       :  256   Andrena_gravida    :   80                     \n Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n (Other)             :18944   (Other)            :20000                     \n    season             diameter        corolla             colour         \n Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n Class :character   1st Qu.:  5.00   Class :character   Class :character  \n Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n                    Mean   : 27.03                                        \n                    3rd Qu.: 25.00                                        \n                    Max.   :150.00                                        \n                    NA's   :9472                                          \n    nectar            b.system         s.pollination      inflorescence     \n Length:20480       Length:20480       Length:20480       Length:20480      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  composite            guild               tongue            body      \n Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n                                       Mean   : 8.104   Mean   :10.66  \n                                       3rd Qu.:10.500   3rd Qu.:13.00  \n                                       Max.   :26.400   Max.   :25.00  \n                                       NA's   :17040    NA's   :6160   \n  sociality           feeding          interaction \n Length:20480       Length:20480       0   :14095  \n Class :character   Class :character   1   :  595  \n Mode  :character   Mode  :character   NA's: 5790  \n                                                   \n                                                   \n                                                   \n                                                   \n```\n:::\n\n```{.r .cell-code}\n# scale numeric features\nplant_poll[, sapply(plant_poll, is.numeric)] = scale(plant_poll[, sapply(plant_poll, is.numeric)])\n\n# remove NAs\ndf = plant_poll[complete.cases(plant_poll),] # remove NAs\n\n# remove factors with only one level \ndf = df[,-12]\n\n# remove first two columns (species names)\ndata_obs = df[,-(1:2)]\n\n# change response to integer (because cito wants integer 0/1 for binomial data)\ndata_obs$interaction = as.integer(data_obs$interaction) - 1 \n\n\n\n# prepare the test data\nnewdata = plantPollinator_df[is.na(plantPollinator_df$interaction), ]\nnewdata_imputed = missRanger::missRanger(data = newdata[,-ncol(newdata)], verbose = 0) # fill NAs\nnewdata_imputed$interaction = NA\n```\n:::\n\n\nMinimal example in cito:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\nmodel = dnn(interaction~., \n    hidden = c(50, 50), \n    activation = \"selu\", \n    loss = \"binomial\", \n    lr = tune(values = seq(0.0001, 0.03, length.out = 10)),\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    data = data_obs, \n    epochs = 2L, \n    tuning = config_tuning(CV = 3, steps = 3))\n\n\nprint(model$tuning)\n\n# make final predictions\npredictions = predict(model, newdata_imputed, type = \"response\")[,1]\n\n# prepare submissions\nwrite.csv(data.frame(y = predictions), file = \"my_submission.csv\")\n```\n:::\n\n\n<!-- ```{r} -->\n\n<!-- library(EcoData) -->\n\n<!-- library(missRanger) -->\n\n<!-- library(dplyr) -->\n\n<!-- data(plantPollinator_df) -->\n\n<!-- plant_poll = plantPollinator_df -->\n\n<!-- plant_poll_imputed = plant_poll %>% select(diameter, -->\n\n<!--                                            corolla, -->\n\n<!--                                            tongue, -->\n\n<!--                                            body, -->\n\n<!--                                            interaction, -->\n\n<!--                                            colour,  -->\n\n<!--                                            nectar, -->\n\n<!--                                            feeding, -->\n\n<!--                                            season) -->\n\n<!-- # Remove response variable interaction -->\n\n<!-- plant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>% -->\n\n<!--                                               select(-interaction), verbose = 0) -->\n\n<!-- # scale numeric variables -->\n\n<!-- plant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)] = scale(plant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)]) -->\n\n<!-- # Add response back to the dataset after the imputation -->\n\n<!-- plant_poll_imputed$interaction = plant_poll$interaction -->\n\n<!-- plant_poll_imputed$colour = as.factor(plant_poll_imputed$colour) -->\n\n<!-- plant_poll_imputed$nectar = as.factor(plant_poll_imputed$nectar) -->\n\n<!-- plant_poll_imputed$feeding = as.factor(plant_poll_imputed$feeding) -->\n\n<!-- plant_poll_imputed$season = as.factor(plant_poll_imputed$season) -->\n\n<!-- data_new = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ] # for which we want to make predictions at the end -->\n\n<!-- data_obs = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]# data with known response -->\n\n<!-- dim(data_obs) -->\n\n<!-- ``` -->\n\n<!-- The dataset is large! More than 10,000 observations. For now, let's switch to a simple holdout strategy for validating our model (e.g. use 80% of the data to train the model and 20% of the data to validate your model. -->\n\n<!-- Moreover: -->\n\n<!-- ```{r} -->\n\n<!-- table(data_obs$interaction) -->\n\n<!-- ``` -->\n\n<!-- The data is strongly imbalanced, i.e. many 0s but only a few 1. There are different strategies how to deal with that, for example oversampling the 1s or undersampling the 0s. -->\n\n<!-- Undersampling the 0s: -->\n\n<!-- ```{r} -->\n\n<!-- data_obs = data_obs[c(sample(which(data_obs$interaction == 0), 1000), which(data_obs$interaction == 1)),] -->\n\n<!-- table(data_obs$interaction) -->\n\n<!-- data_obs$interaction = as.integer(data_obs$interaction)-1 -->\n\n<!-- ``` -->\n\nYour Tasks:\n\n-   Use cito to tune learning parameters and the regularization\n-   Submit your predictions to <http://rhsbio7.uni-regensburg.de:8500/>\n:::\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\nMinimal example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cito)\nset.seed(42)\nmodel = dnn(interaction~., \n    hidden = c(50, 50), \n    activation = \"selu\", \n    loss = \"binomial\", \n    lr = tune(values = seq(0.0001, 0.03, length.out = 10)),\n    lambda = tune(values = seq(0.0001, 0.1, length.out = 10)),\n    alpha = tune(),\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    data = data_obs, \n    epochs = 100L, \n    tuning = config_tuning(CV = 3, steps = 15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting hyperparameter tuning...\nFitting final model...\n```\n:::\n\n```{.r .cell-code}\nprint(model$tuning)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 × 7\n   steps  test train models lambda  alpha      lr\n   <int> <dbl> <dbl> <lgl>   <dbl>  <dbl>   <dbl>\n 1     1  Inf      0 NA     0.0889 0.197  0.00674\n 2     2  Inf      0 NA     0.1    0.501  0.0134 \n 3     3  Inf      0 NA     0.0334 0.825  0.0200 \n 4     4  Inf      0 NA     0.0112 0.0921 0.0134 \n 5     5  Inf      0 NA     0.0778 0.470  0.0234 \n 6     6  Inf      0 NA     0.0445 0.881  0.0134 \n 7     7  Inf      0 NA     0.0445 0.436  0.00674\n 8     8  Inf      0 NA     0.0667 0.277  0.0200 \n 9     9  Inf      0 NA     0.1    0.0571 0.0001 \n10    10  290.     0 NA     0.0112 0.885  0.0234 \n11    11  Inf      0 NA     0.0112 0.148  0.0234 \n12    12  Inf      0 NA     0.0667 0.317  0.0167 \n13    13  Inf      0 NA     0.0001 0.681  0.00342\n14    14  Inf      0 NA     0.1    0.283  0.0001 \n15    15  Inf      0 NA     0.0556 0.425  0.0101 \n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions = predict(model, newdata_imputed, type = \"response\")[,1]\n\nwrite.csv(data.frame(y = predictions), file = \"Max_plant_.csv\")\n```\n:::\n\n\n\n</div>\n\n",
    "supporting": [
      "B3-NeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}