{
  "hash": "08651a8249a5080de72052f34818c721",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n# Managing algorithmic complexity\n\n## Estimating error on the validation data\n\nYou probably remember from statistics that a more complex model always fits the training data better. The decisive question, however, is if it also works better on new (independent) data. Technically, we call this the **out-of-sample error**, as opposed to the i**n-sample error**, which is the error on the training data.\n\nError can be measured in different ways, but usually we calculate some **kind of accuracy** (especially for classification tasks) or how much variance is explained by our model (regression tasks). We also distinguish between the error used to train the model and the error used to validate the model. The error used internally by the ML algorithms to train the model is what we usually call the loss. The smaller the loss, the smaller the error of the model, and vice versa for larger losses.\n\nWhile we can use losses to validate the model, losses are often not interpretable as they are often in the range $[0, \\infty[$ and they cannot be generalized to other datasets because they are often data specific. Therefore, in practice, we usually use **interpretable losses, validation metrics,** during validation that can be also used to compare the models over different datasets (Model A achieves 80% accuarcy on dataset A and 70% accuracy on dataset B), here is an overview of some common validation metrics and their interpretation:\n\nValidation metrics for classification tasks:\n\n| Validation Metric | Range | Classification Types | Explanation |\n|------------------|------------------|------------------|------------------|\n| **A**rea **U**nder the **C**urve (AUC) | $[0, 1]$ | Binary Classification Tasks (e.g. Titanic Dataset, survived or died) | The ability of our models to distinguish between 0 and 1. Requires probability predictions. An AUC of 0.5 means that the algorithm is making random predictions. Lower than 0.5 â€“\\> worse than random |\n| Accuracy | $[0, 1]$ | All types of classifications (including multiclass tasks) | The accuracy of our models, how many of the predicted classes are correct. The baseline accuracy depends on the distributions of the classes (if one class occurs 99% in the data, a random model that will only predict this class, will achieve already a very high accuracy |\n\nValidation metrics for regression tasks:\n\n| Validation Metric | Range | Explanation |\n|------------------------|------------------------|------------------------|\n| $R^2$ | $[0, 1]$ | How much variance is explained by our model. We usually use the sum of squares $R^2$ |\n| Correlation factors (Pearson or Spearman) | $[-1, 1]$ | Measures correlation between predictions and observations. Spearman (rank correlation factor) can be useful for skewed distributed responses (or non-normal distributed responses, such as count data). |\n| **R**oot **m**ean **s**quared **e**rror (RMSE) | $[0, \\infty[$ | RMSE is not a really interpretable but it is still used as a common validation metrics (is also used as a loss to train models). The RMSE reports how much variance is unexplained (so smaller RMSE is better). However, RMSE is not really comparable between different data sets. |\n\n### Splitting off validation data\n\nTo check the out-of-sample error, we usually split out some part of the data for later model validation. Let's look at this at the example of a supervised regression, trying to predict house prices in Boston.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench)\n\ndata(BostonHousing)\ndata = BostonHousing\nset.seed(123)\n```\n:::\n\n\n\n\nCreating a split by deciding randomly for each data point if it is used for training or validation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = nrow(BostonHousing)\ntrain = sample.int(n, size = round(0.7*n))\n```\n:::\n\n\n\n\nFitting two lms, one with a few predictors, one with a lot of predictors (all interaction up to 3-way)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 = lm(medv~., data = data[train,])\nm2 = lm(medv~.^3, data = data[train,])\n```\n:::\n\n\n\n\nTesting predictive ability on training data (in-sample error)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(predict(m1), data[train,]$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8561528\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(predict(m2), data[train,]$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9971297\n```\n\n\n:::\n:::\n\n\n\n\nConclusion: m2 (more complex) is much better on training. As a next step, we are testing the predictive ability on hold-out (aka valikation, out-of-sample error).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(predict(m1, newdata = data[-train,] ), \n    data[-train,]$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8637908\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(predict(m2, newdata = data[-train,] ), \n    data[-train,]$medv)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in predict.lm(m2, newdata = data[-train, ]): prediction from\nrank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.04036532\n```\n\n\n:::\n:::\n\n\n\n\nNow, m2 is much worse!\n\n### Overfitting vs. underfitting\n\nThe phenomenon that the predictive error drops significantly when going from the training to the validation data signals overfitting, i.e. a too complex model!\n\nWhat about m1 - is m1 just complex enough, or is it too simple? Underfitting cannot be directly diagnosed, you just have to try around if making the model more complex can improve results on the validation data. Let's try a random forest\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nm3 = randomForest(medv~., data = data[train,])\ncor(predict(m3), data[train,]$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9345165\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(predict(m3, newdata = data[-train,] ), \n    data[-train,]$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9380828\n```\n\n\n:::\n:::\n\n\n\n\nNo drop on validation data (i.e. no overfitting), but error on training and validation is much better than for m1 - so this seems to be a better model, and m1 was probably underfitting, i.e. it was not complex enough to get good performance!\n\n### Validation vs. cross-validation {#sec-cv}\n\nA problem with the validation split is that we test only on a certain fraction of the data (say: 20% in a 80/20 split).\n\nIf computationally possible, a better method to estimate error is cross-validation. The idea of cross-validation is to perform the train/validation split again and again until all data was used for the validation, and then average the validation error over this data.\n\nHere an example of a k-fold cross-validation, which is akin to 5x an 80/20 split.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk = 5 # folds\nsplit = sample.int(k, n, replace = T)\npred = rep(NA, n)\n\nfor(i in 1:k){\n  m1 = randomForest(medv~., data = data[split != i,])\n  pred[split == i] = predict(m1, newdata = data[split == i,])\n}\n\ncor(pred, data$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9368875\n```\n\n\n:::\n:::\n\n\n\n\n## Optimizing the bias-variance trade-off\n\n### The bias-variance trade-off\n\nWhat we have just seen in the previous chapter is an example of the bias-variance trade-off. The idea is that we look at the error of the model on new test data. The total error comes from 2 contributions:\n\n-   Bias = **systematic error** that comes from the fact that the model is not flexible enough, related to underfitting\n\n-   Variance = **statistical error** that comes from that fact that estimates of the model parameters get more uncertain when we add complexity\n\nOptimizing the bias-variance trade-off means adjusting the complexity of the model which can be achieved by:\n\n-   Feature selection (more features increases the flexibility of the model)\n\n-   Regularization\n\n::: {.webex-check .webex-box}\n![](images/biasVarianceTradeoff.png)\n\n\n\n\nWhich of the following statements about the bias-variance trade-off is correct? (see figure above) <div class='webex-radiogroup' id='radio_KLBUKOIPTJ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_KLBUKOIPTJ\" value=\"answer\"></input> <span>The goal of considering the bias-variance trade-off is to realize that increasing complexity typically leads to more flexibility (allowing you to reduce bias) but at the cost of uncertainty (variance) in the estimated parameters.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_KLBUKOIPTJ\" value=\"\"></input> <span>The goal of considering the bias-variance trade-off is to get the bias of the model as small as possible.</span></label></div>\n\n\n\n:::\n\n### Feature selection\n\nAdding features increases the flexibility of the model and the goodness of fit:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:randomForest':\n\n    combine\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(BostonHousing)\ndata = BostonHousing\n\nsummary(lm(medv~rm, data = data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ rm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -34.671      2.650  -13.08   <2e-16 ***\nrm             9.102      0.419   21.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,\tAdjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(lm(medv~rm+dis, data = data))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4955246\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(lm(medv~., data = data))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7406427\n```\n\n\n:::\n\n```{.r .cell-code}\n# Main effects + all potential interactions:\nsummary(lm(medv~.^2, data = data))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9211876\n```\n\n\n:::\n:::\n\n\n\n\nThe model with all features and their potential interactions has the highest $R^2$, but it also has the highest uncertainty because there are on average only 5 observations for each parameter (92 parameters and 506 observations). So how do we decide which level of complexity is appropriate for our task? For the data we use to train the model, $R^2$ will always get better with higher model complexity, so it is a poor decision criterion. We will show this in the @sec-cv section. In short, the idea is that we need to split the data so that we have an evaluation (test) dataset that wasn't used to train the model, which we can then use in turn to see if our model generalizes well to new data.\n\n### Regularization\n\nRegularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a machine learning model. The most important distinction is between *shrinkage estimators* and estimators based on *model averaging*.\n\n**Shrinkage estimators** are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are *\"shrunk\"* to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also *Lasso* or *LASSO*, where the penalty is proportional to the sum of absolute deviations ($L1$ penalty), and the *Tikhonov regularization* aka *Ridge regression*, where the penalty is proportional to the sum of squared distances from the reference ($L2$ penalty). Thus, the loss function that we optimize is given by\n\n$$\nloss = fit - \\lambda \\cdot d\n$$\n\nwhere fit refers to the standard loss function, $\\lambda$ is the strength of the regularization, and $d$ is the chosen metric, e.g. $L1$ or$L2$:\n\n$$\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n$$\n\n$$\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n$$\n\n$\\lambda$ and possibly d are typically optimized under cross-validation. $L1$ and $L2$ can be also combined what is then called *elastic net* (see @zou2005).\n\n**Model averaging** refers to an entire set of techniques, including *boosting*, *bagging* and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it is often more efficient having many simpler models and average them, than one \"super model\". The reasons are complicated, and explained in more detail in @dormann2018.\n\nA particular important application of averaging is *boosting*, where the idea is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is *bootstrap aggregating*, also called *bagging*. Idea here is to *boostrap* (use random sampling with replacement ) the data, and average the bootstrapped predictions.\n\nTo see how these techniques work in practice, let's first focus on LASSO and Ridge regularization for weights in neural networks. We can imagine that the LASSO and Ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunken) to zero.\n\nLASSO $\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)$ and Ridge $\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)$ have slightly different properties. They are best understood if we express those as the effective prior preference they create on the parameters:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/chunk_chapter4_10-1.png){width=672}\n:::\n:::\n\n\n\n\nAs you can see, the LASSO creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the Ridge. For this reason, LASSO is often more interpreted as a model selection method.\n\nThe Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.\n\n#### Ridge - Example\n\nWe can use the `glmnet` package for Ridge, LASSO, and elastic-net regressions.\n\nWe want to predict the house prices of Boston (see help of the dataset):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %>% select(-medv, -chas) %>% scale()\n\nhist(cor(X))\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 = glmnet(y = Y, x = X, alpha = 0)\n```\n:::\n\n\n\n\nThe `glmnet` function automatically tests different values for lambda:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(m1, s = 0.001), coef(m1, s = 100.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1          s1\n(Intercept) 22.53280632 22.53280632\ncrim        -0.79174957 -0.21113427\nzn           0.76313031  0.18846808\nindus       -0.17037817 -0.25120998\nnox         -1.32794787 -0.21314250\nrm           2.85780876  0.46463202\nage         -0.05389395 -0.18279762\ndis         -2.38716188  0.07906631\nrad          1.42772476 -0.17967948\ntax         -1.09026758 -0.24233282\nptratio     -1.93105019 -0.31587466\nb            0.86718037  0.18764060\nlstat       -3.43236617 -0.46055837\n```\n\n\n:::\n:::\n\n\n\n\n#### LASSO - Example\n\nBy changing $alpha$ to 1.0 we use a LASSO instead of a Ridge regression:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 = glmnet(y = Y, x = X, alpha = 1.0)\ncbind(coef(m2, s = 0.001), coef(m2, s = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1           s1\n(Intercept) 22.53280632 22.532806324\ncrim        -0.95543108 -0.135047323\nzn           1.06718108  .          \nindus        0.21519500  .          \nnox         -1.95945910 -0.000537715\nrm           2.71666891  2.998520195\nage          0.05184895  .          \ndis         -3.10566908 -0.244045205\nrad          2.73963771  .          \ntax         -2.20279273  .          \nptratio     -2.13052857 -1.644234575\nb            0.88420283  0.561686909\nlstat       -3.80177809 -3.682148016\n```\n\n\n:::\n:::\n\n\n\n\n#### Elastic-net - Example\n\nBy setting $alpha$ to a value between 0 and 1.0, we use a combination of LASSO and Rdige:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm3 = glmnet(y = Y, x = X, alpha = 0.5)\ncbind(coef(m3, s = 0.001), coef(m3, s = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1         s1\n(Intercept) 22.53280632 22.5328063\ncrim        -0.95716118 -0.3488473\nzn           1.06836343  0.1995842\nindus        0.21825187  .        \nnox         -1.96211736 -0.7613698\nrm           2.71859592  3.0137090\nage          0.05299551  .        \ndis         -3.10330132 -1.3011740\nrad          2.73321635  .        \ntax         -2.19638611  .        \nptratio     -2.13041090 -1.8051547\nb            0.88458269  0.6897165\nlstat       -3.79836182 -3.6136853\n```\n\n\n:::\n:::\n\n\n\n\n## Hyperparameter tuning\n\n### What is a hyperparameter?\n\nGenerally, parameters such as $\\lambda$ and $\\alpha$ that, for example, control the complexity of the model or other model features such as learning or the optimization are called hyperparameters.\n\nHyperparameter tuning describes the process of finding the optimal set of hyperparameters for a certain task. They are usually data specific, so they have to tuned for each dataset.\n\nLet's have a look at this using our glmnet example - we can plot the effect of $\\lambda$ on the effect estimates:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\nSo which lambda should we choose now? If we calculate the model fit for different lambdas (e.g. using the RMSE):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambdas = seq(0.001, 1.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = X, s = l)\n    RMSE = Metrics::rmse(Y, prediction)\n    return(RMSE)\n    })\nplot(lambdas, RMSEs)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\nWe see that the lowest lambda achieved the highest RMSE - which is not surprising because the unconstrained model, the most complex model, has the highest fit, so no bias but probably high variance (with respect to the bias-variance tradeoff).\n\n### Tuning with a train / test split\n\nWe want a model that generalizes well to new data, which we need to \"simulate\" here by splitting of a holdout before the training and using the holdout then for testing our model. This split is often called the train / test split.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(glmnet)\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %>% select(-medv, -chas) %>% scale()\n\n# Split data\nindices = sample.int(nrow(X), 0.2*nrow(X))\ntrain_X = X[indices,]\ntest_X = X[-indices,]\ntrain_Y = Y[indices]\ntest_Y = Y[-indices]\n\n# Train model on train data\nm1 = glmnet(y = train_Y, x = train_X, alpha = 0.5)\n\n# Test model on test data\npred = predict(m1, newx = test_X, s = 0.01)\n\n# Calculate performance on test data\nMetrics::rmse(test_Y, pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.063774\n```\n\n\n:::\n:::\n\n\n\n\nLet's do it again for different values of lambdas:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambdas = seq(0.0000001, 0.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = test_X, s = l)\n    return(Metrics::rmse(test_Y, prediction))\n    })\nplot(lambdas, RMSEs, xlab = \"Lambda\", ylab = \"RMSE\", type = \"l\", las = 2)\nabline(v = lambdas[which.min(RMSEs)], col = \"red\", lwd = 1.5)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\nAlternatively, you automatically run a CV to determine the hyperparameters for glmnet, using the `cv.glmnet` function which does per default a 5xCV (so 5 splits) and in each split different values for $\\lambda$ are tested\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 = glmnet::cv.glmnet(x = X, y = Y, alpha = 0.5, nfolds = 5)\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glmnet::cv.glmnet(x = X, y = Y, nfolds = 5, alpha = 0.5) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure    SE Nonzero\nmin 0.0105    78   23.80 3.247      12\n1se 0.6905    33   26.88 4.014       8\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\nm1$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01049538\n```\n\n\n:::\n:::\n\n\n\n\nSo low values of $\\lambda$ seem to achieve the lowest error, thus the highest predictive performance.\n\n### Nested (cross)-validation\n\nIn the previous example, we have used the train/test split to find the best model. However, we have not done a validation split yet to see how the finally selected model would do on new data. This is absolutely necessary, because else you will overfit with your model selection to the test data.\n\nIf we have several nested splits, we talk about a nested validation / cross-validation. For each level, you can in principle switch between validation and cross-validation. Here, and example of tuning with a inner cross-validation and an outer validation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# outer split\nvalidation = sample.int(n, round(0.2*n))\ndat = data[-validation,]\n\n# inner split\nnI = nrow(dat)\nhyperparameter = data.frame(mtry = c(3,5))\nm = nrow(hyperparameter)\nk = 5 # folds\nsplit = sample.int(k, nI, replace = T)\n\n\n# making predictions for all hyperparameters / splits\npred = matrix(NA, nI, m)\nfor(l in 1:m){\n  for(i in 1:k){\n    m1 = randomForest(medv~., data = dat[split != i,], mtry = hyperparameter$mtry[l])\n    pred[split == i,l] = predict(m1, newdata = dat[split == i,])\n  }\n}\n\n# getting best hyperparameter option on test\ninnerLoss = function(x) cor(x, dat$medv)\nres = apply(pred, 2, innerLoss)\nchoice = which.max(res) \n\n# fitting model again with best hyperparameters \n# and all test / validation data \nmFinal = randomForest(medv~., data = dat, mtry = hyperparameter$mtry[choice])\n\n# testing final prediction on validation data \nfinalPred = predict(mFinal, newdata = data[validation,])\n\ncor(finalPred, \n    data[validation,]$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.93926\n```\n\n\n:::\n:::\n\n\n\n\n## Exercise - Predicting survival rate in the titanic dataset\n\n<!-- The plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13329\" target=\"_blank\" rel=\"noopener\">here</a>. -->\n\nThe titanic dataset is a collection of data about the titanic passengers and their survival status. The goal is to train a model that can predict whether a passenger survives or not based on their features (e.g. their passenger class)\n\nYou can also find a small explanation of he dataset in the Appendix of the book @sec-plantpoll .\n\n::: callout-warning\n### Task: Tune random orest model for the Titanic dataset\n\nTune the **nodesize** hyperparameter, from the randomForest help:\n\nnodesize = Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).\n\n**Nodesize determines the complexity of the individual trees (we will talk about the exact working tomorrow)**\n\n**1. Prepare data**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest) # alternative faster random forest implementation\nlibrary(EcoData)\n\ndata(titanic_ml)\ntitanic_df = titanic_ml\nsummary(titanic_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      body      \n Min.   :  0.000                  :1014    :  2    Min.   :  1.0  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    1st Qu.: 72.0  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    Median :155.0  \n Mean   : 33.295   G6             :   5   S:914    Mean   :160.8  \n 3rd Qu.: 31.275   B96 B98        :   4            3rd Qu.:256.0  \n Max.   :512.329   C22 C26        :   4            Max.   :328.0  \n NA's   :1         (Other)        : 271            NA's   :1188   \n                home.dest  \n                     :564  \n New York, NY        : 64  \n London              : 14  \n Montreal, PQ        : 10  \n Cornwall / Akron, OH:  9  \n Paris, France       :  9  \n (Other)             :639  \n```\n\n\n:::\n\n```{.r .cell-code}\n# fill NA\ntitanic_df[,-2] = missRanger::missRanger(titanic_df[,-2], verbose = 0)\n\n# remove name column, too many levels\ntitanic_df = subset(titanic_df, select = c(-name, -home.dest, -ticket, -cabin))\n\n# change response to factor\ntitanic_df$survived = as.factor(titanic_df$survived)\n\n# remove NAs\ndf = titanic_df[complete.cases(titanic_df),] # remove NAs\n\n\n# Example:\nrf = randomForest(survived~., \n                  data = df[1:300,], \n                  min.node.size = 20) # we want our model to predict probabilities!\n\n# the predict function of the ranger will return an object, the actual predictions\n# are inside a matrix with the name predictions\npred = predict(rf, newdata = df[-(1:300),], type = \"prob\")[,2]\nMetrics::auc(as.integer(df[-(1:300),]$survived)-1, pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8349267\n```\n\n\n:::\n:::\n\n\n\n\n**2. Create an outer split**\n\n**3. Tune min.node.size under nested Cross-Validation on the training split from step 2**\n\n**4. Create submissions**\n\nWe separated data from the original dataset. Observations with NA in the survived column are held back by us to simulate a real-world scenario where you have training data to train your model, and then use the model in production on new data where you now have information about the response variable. After tuning your model on the training data (again, where the response variable is not NA) and you are happy with your model, you can make predictions for the observations where the response is unknown and upload the predictions to our server (http://rhsbio7.uni-regensburg.de:8500/, ignore the unsecure warning and UR VPN is required). The server will report your final performance and compare it with other predictions):\n\nHow to create your submission file:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata = titanic_df[is.na(titanic_df$survived), ]\npredictions = predict(rf, newdata = newdata, type = \"prob\")[,2]\n\nwrite.csv(data.frame(y = predictions), file = \"rf_max.csv\")\n```\n:::\n\n\n\n:::\n\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn = nrow(df)\n# outer split\nvalidation = sample.int(n, round(0.2*n))\ndat = df[-validation,]\n\n# inner split\nnI = nrow(dat)\nhyperparameter = data.frame(nodesize = seq(10, 500, by = 25))\nm = nrow(hyperparameter)\nk = 5 # folds\nsplit = sample.int(k, nI, replace = T)\n\n\n# making predictions for all hyperparameters / splits\npred = matrix(NA, nI, m)\nfor(l in 1:m){\n  # loop over the hyperparameters and do CV for each hyperparameter\n  for(i in 1:k){\n    m1 = randomForest(survived~., data = dat[split != i,], nodesize = hyperparameter$nodesize[l])\n    pred[split == i,l] = predict(m1, newdata = dat[split == i,], type = \"prob\")[,2]\n  }\n}\n\n# getting best hyperparameter option on test\ninnerLoss = function(x) Metrics::auc(dat$survived, x)\nres = apply(pred, 2, innerLoss)\nchoice = which.max(res) \n\n# fitting model again with best hyperparameters \n# and all test / validation data \nmFinal = randomForest(survived~., data = dat, nodesize = hyperparameter$nodesize[choice])\n\n# testing final prediction on validation data \nfinalPred = predict(mFinal, newdata = df[validation,], type = \"prob\")[,2]\n\nMetrics::auc(df[validation,]$survived, finalPred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8265476\n```\n\n\n:::\n:::\n\n\n\n\nCreate submissions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata = titanic_df[is.na(titanic_df$survived), ]\npredictions = predict(rf, newdata = newdata, type = \"prob\")[,2]\n\nwrite.csv(data.frame(y = predictions), file = \"rf_max.csv\")\n```\n:::\n\n\n\n\nAnd upload the csv file\n\n**Important**: The predictions must be probabilities and have the same number of observations, this is why we impute the NA in the features, otherwise these observations would be dropped. \n\n\n</div>\n\n\n## References {.unnumbered}\n",
    "supporting": [
      "A3-BiasVarianceTradeOff_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}