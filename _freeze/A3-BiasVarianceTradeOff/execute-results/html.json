{
  "hash": "10de85e52b958a849a1a07e1d32485ea",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Bias-variance trade-off\n\n## Understanding the bias-variance trade-off\n\n::: {.webex-check .webex-box}\n![](images/biasVarianceTradeoff.png)\n\n\nWhich of the following statements about the bias-variance trade-off is correct? (see figure above) <div class='webex-radiogroup' id='radio_UZPKFYNFJV'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_UZPKFYNFJV\" value=\"answer\"></input> <span>The goal of considering the bias-variance trade-off is to realize that increasing complexity typically leads to more flexibility (allowing you to reduce bias) but at the cost of uncertainty (variance) in the estimated parameters.</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_UZPKFYNFJV\" value=\"\"></input> <span>The goal of considering the bias-variance trade-off is to get the bias of the model as small as possible.</span></label></div>\n\n:::\n\n## Optimizing the bias-variance trade-off\n\n### Feature selection\n\n### Regularization\n\nRegularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a machine learning model. The most important distinction is between *shrinkage estimators* and estimators based on *model averaging*.\n\n**Shrikage estimators** are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are *\"shrunk\"* to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also *Lasso* or *LASSO*, where the penalty is proportional to the sum of absolute deviations ($L1$ penalty), and the *Tikhonov regularization* aka *Ridge regression*, where the penalty is proportional to the sum of squared distances from the reference ($L2$ penalty). Thus, the loss function that we optimize is given by\n\n$$\nloss = fit - \\lambda \\cdot d\n$$\n\nwhere fit refers to the standard loss function, $\\lambda$ is the strength of the regularization, and $d$ is the chosen metric, e.g. $L1$ or$L2$:\n\n$$\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n$$\n\n$$\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n$$\n\n$\\lambda$ and possibly d are typically optimized under cross-validation. $L1$ and $L2$ can be also combined what is then called *elastic net* (see @zou2005).\n\n**Model averaging** refers to an entire set of techniques, including *boosting*, *bagging* and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it is often more efficient having many simpler models and average them, than one \"super model\". The reasons are complicated, and explained in more detail in @dormann2018.\n\nA particular important application of averaging is *boosting*, where the idea is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is *bootstrap aggregating*, also called *bagging*. Idea here is to *boostrap* (use random sampling with replacement ) the data, and average the bootstrapped predictions.\n\nTo see how these techniques work in practice, let's first focus on LASSO and Ridge regularization for weights in neural networks. We can imagine that the LASSO and Ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunken) to zero.\n\nLASSO $\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)$ and Ridge $\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)$ have slightly different properties. They are best understood if we express those as the effective prior preference they create on the parameters:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/chunk_chapter4_10-1.png){width=672}\n:::\n:::\n\n\nAs you can see, the LASSO creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the Ridge. For this reason, LASSO is often more interpreted as a model selection method.\n\nThe Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.\n\n#### Ridge - Example\n\nWe can use the `glmnet` package for Ridge, LASSO, and elastic-net regressions.\n\nWe want to predict the occurrence of the African Elephant (Classification Task), 19 bioclim (environmental variables, see help of the dataset) are our predictors and they are highly collinear:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-7\n```\n:::\n\n```{.r .cell-code}\nlibrary(EcoData)\ndata = EcoData::elephant$occurenceData\nhist(cor(data))\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 = glmnet(y = data$Presence, x = scale(data[,-1]), family = \"binomial\", alpha = 0)\n```\n:::\n\n\nThe `glmnet` function automatically tries different values for lambda:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(m1, s = 0.001), coef(m1, s = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n20 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1           s1\n(Intercept) -0.99488324 -0.780014853\nbio1         0.11271724 -0.055980600\nbio2         0.13314858  0.002595185\nbio3        -0.30527292 -0.026252048\nbio4        -0.39768514  0.024350194\nbio5        -0.18331069 -0.073192300\nbio6        -0.02860151 -0.044787626\nbio7        -0.08476021 -0.001723074\nbio8         0.25325588  0.010139565\nbio9        -0.32415880 -0.085263374\nbio10       -0.25068839 -0.054433012\nbio11        0.13073390 -0.057325676\nbio12       -0.78787155 -0.134876767\nbio13       -0.16752784 -0.146520805\nbio14        0.23564006  0.043352576\nbio15       -0.38701027 -0.094950092\nbio16       -0.37231958 -0.162089670\nbio17        0.04547655  0.026451709\nbio18        0.21734755 -0.003571031\nbio19       -0.30681718 -0.075885175\n```\n:::\n:::\n\n\n#### LASSO - Example\n\nBy changing $alpha$ to 1.0 we use a LASSO instead of a Ridge regression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 = glmnet(y = data$Presence, x = scale(data[,-1]), family = \"binomial\", alpha = 1.0)\ncbind(coef(m2, s = 0.001), coef(m2, s = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n20 x 2 sparse Matrix of class \"dgCMatrix\"\n                       s1         s1\n(Intercept) -1.1790698999 -0.7021879\nbio1         5.2201447390  .        \nbio2         0.4825871907  .        \nbio3        -1.2446394503  .        \nbio4        -0.8831104598  .        \nbio5        -0.0009278259  .        \nbio6         .             .        \nbio7         .             .        \nbio8         .             .        \nbio9        -2.1368721806  .        \nbio10       -3.4058093768  .        \nbio11        .             .        \nbio12       -4.3452607245  .        \nbio13        1.1177210101  .        \nbio14        0.8468788216  .        \nbio15       -1.7662333813  .        \nbio16        0.4975102865  .        \nbio17        .             .        \nbio18        0.5713151479  .        \nbio19        0.0041414946  .        \n```\n:::\n:::\n\n\n#### Elastic-net - Example\n\nBy setting $alpha$ to a value between 0 and 1.0, we use a combination of LASSO and Rdige:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm3 = glmnet(y = data$Presence, x = scale(data[,-1]), family = \"binomial\", alpha = 0.5)\ncbind(coef(m3, s = 0.001), coef(m3, s = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n20 x 2 sparse Matrix of class \"dgCMatrix\"\n                    s1         s1\n(Intercept) -1.1283328 -0.7021879\nbio1         3.0270008  .        \nbio2         0.6036299  .        \nbio3        -1.2874038  .        \nbio4        -1.5366454  .        \nbio5        -0.2581393  .        \nbio6         .          .        \nbio7         .          .        \nbio8         0.2640327  .        \nbio9        -1.6896702  .        \nbio10       -1.7723493  .        \nbio11        .          .        \nbio12       -3.6918225  .        \nbio13        0.9367339  .        \nbio14        0.6333207  .        \nbio15       -1.4308360  .        \nbio16        0.2560483  .        \nbio17        0.1654974  .        \nbio18        0.5438264  .        \nbio19        .          .        \n```\n:::\n:::\n\n\n### Hyperparameters\n\nGenerally, parameters such as $\\lambda$ and $\\alpha$ that, for example, control the complexity or other parameters that control their learning or the optimization are called hyperparameters. Comming back to our glmnet example:\n\nWe can plot the effect of $\\lambda$ on the effect estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nSo which lambda should we choose now? If we calculate the model fit for different lambdas (e.g. using the AUC):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambdas = seq(0.001, 0.5, length.out = 100)\nAUCs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = scale(data[,-1]), s = l)\n    AUC = Metrics::auc(data[,1], prediction)\n    return(AUC)\n    })\nplot(lambdas, AUCs)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nWe see that the lowest lambda achieved the higehst AUC - which is not surpsiring because the unconstrained model, the most complex model, has the highest fit, so no bias but probably high variance (with respect to the bias-variance tradeoff).\n\nWe want a model that generalizes well to new data, which we need to \"simulate\" here by splitting of a holdout before the training and using the holdout then for testing our model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\nlibrary(EcoData)\ndata = EcoData::elephant$occurenceData\n\ndata[,-1] = scale(data[,-1])\n\n# Split data\nindices = sample.int(nrow(data), 0.8*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\n# Train model on train data\nm1 = glmnet(y = train$Presence, x = train[,-1], family = \"binomial\", alpha = 0.0)\n\n# Test model on test data\npred = predict(m1, newx = as.matrix(test[,-1]), s = 0.01)\n\n# Calculate performance on test data\nMetrics::auc(test$Presence, pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8001035\n```\n:::\n:::\n\n\nLet's do it again for different values of lambdas:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambdas = seq(0.00001, 0.5, length.out = 100)\nAUCs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = as.matrix(test[,-1]), s = l)\n    AUC = Metrics::auc(test[,1], prediction)\n    return(AUC)\n    })\nplot(lambdas, AUCs)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nHyperparameter tuning describes the process of finding the optimal set of hyperparameters for a certain task. They are usually data specific, so they have to tuned for each dataset.\n\nIf we do only one split it could happen that we only find a set of hyperparameters that are best suited for this specific split and thus we usally do several splits so that each observation is once an observation in the test dataset, cross-validation\n\n### Cross-validation\n\nThe `cv.glmnet` function does per default a 10xCV (so 10 splits) and in each split different values for $\\lambda$ are tested (based on the deviance (-2xlogLik)):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 = glmnet::cv.glmnet(x = scale(data[,-1]), y = data$Presence, family = \"binomial\")\nm1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glmnet::cv.glmnet(x = scale(data[, -1]), y = data$Presence, family = \"binomial\") \n\nMeasure: Binomial Deviance \n\n       Lambda Index Measure       SE Nonzero\nmin 0.0001448    79  0.8217 0.009043      18\n1se 0.0005845    64  0.8307 0.008226      15\n```\n:::\n\n```{.r .cell-code}\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nSo low values of $\\lambda$ seem to achieve the highest (or lowest Binomial Deviance) predictive performance.\n\nIf we want to tune $\\alpha$ and $\\lambda$ simoustanously, we need the `glmnetUtils` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnetUtils)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'glmnetUtils'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:glmnet':\n\n    cv.glmnet, glmnet\n```\n:::\n\n```{.r .cell-code}\nm2 = cva.glmnet(x = scale(data[,-1]), y = data$Presence, family = \"binomial\", alpha = seq(0, 1.0, length.out = 10), lambdas = seq(0.0001, 0.2, length.out = 10), nfolds = 3)\n\nplot(m2)\n```\n\n::: {.cell-output-display}\n![](A3-BiasVarianceTradeOff_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n### Error metrics\n",
    "supporting": [
      "A3-BiasVarianceTradeOff_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}