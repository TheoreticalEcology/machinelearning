{
  "hash": "58841c3c52b866f05aba76868fba05b2",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Generative Adversarial Networks (GANs) {#sec-GAN}\n\n\n\n\n\nThe idea of a generative adversarial network (GAN) is that two neural networks contest against each other in a \"game\". One network is creating data and is trying to \"trick\" the other network into deciding the generated data is real. The *generator* (similar to the decoder in autoencoders) creates new images from noise. The *discriminator* is getting a mix of true (from the data set) and artificially generated images from the generator. Thereby, the loss of the generator rises when fakes are identified as fakes by the discriminator (simple binary cross entropy loss, 0/1...). The loss of the discriminator rises when fakes are identified as real images (class 0) or real images as fakes (class 1), again with binary cross entropy.\n\n**Binary cross entropy:** Entropy or *Shannon entropy* (named after Claude Shannon) $\\mathbf{H}$ (uppercase \"eta\") in context of information theory is the expected value of information content or the mean/average information content of an \"event\" compared to all possible outcomes. Encountering an event with low probability holds more information than encountering an event with high probability.\n\n*Binary cross entropy* is a measure to determine the similarity of two (discrete) probability distributions $A~(\\mathrm{true~distribution}), B~(\\mathrm{predicted~distribution})$ according to the inherent information.\n\nIt is not (!) symmetric, in general: $\\textbf{H}_{A}(B) \\neq \\textbf{H}_{B}(A)$. The minimum value depends on the distribution of $A$ and is the entropy of $A$: $$\\mathrm{min}~\\textbf{H}_{A}(B) = \\underset{B}{\\mathrm{min}}~\\textbf{H}_{A}(B) = \\textbf{H}_{A}(B = A) = \\textbf{H}_{A}(A) = \\textbf{H}(A)$$\n\nThe setup:\n\n-   Outcomes $y_{i} \\in \\{0, 1\\}$ (labels).\n-   Predictions $\\hat{y}_{i} \\in[0, 1]$ (probabilities).\n\nThe binary cross entropy or log loss of a system of outcomes/predictions is then defined as follows: $$\n  \\textbf{H}_{A}(B) =\n  -\\frac{1}{N} \\sum_{i = 1}^{N} y_{i} \\cdot \\mathrm{log} \\left( p(y_{i}) \\right) + (1 -y_{i}) \\cdot \\mathrm{log} \\left( 1-p(y_{i}) \\right) =\\\\\n  = -\\frac{1}{N} \\sum_{i = 1}^{N} y_{i} \\cdot \\mathrm{log} (\\hat{y}_{i}) + (1 -y_{i}) \\cdot \\mathrm{log} \\left( 1- \\hat{y}_{i} \\right)\n$$ High predicted probabilities of having the label for originally labeled data (1) yield a low loss as well as predicting a low probability of having the label for originally unlabeled data (0). Mind the properties of probabilities and the logarithm.\n\nA possible application of generative adversarial networks is to create pictures that look like real photographs e.g. <a href=\"https://thispersondoesnotexist.com/\" target=\"_blank\" rel=\"noopener\">https://thispersondoesnotexist.com/</a>. Visit that site (several times)!. However, the application of generative adversarial networks today is much wider than just the creation of data. For example, generative adversarial networks can also be used to \"augment\" data, i.e. to create new data and thereby improve the fitted model.\n\n## MNIST - Generative Adversarial Networks Based on Deep Neural Networks\n\nWe will now explore this on the MNIST data set.\n\n``` r\nlibrary(keras)\nlibrary(tensorflow)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nrotate = function(x){ t(apply(x, 2, rev)) }\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE,\n        main = paste0(\"Label: \", as.character(title)))\n}\n```\n\nWe don't need the test set here.\n\n``` r\ndata = dataset_mnist()\ntrain = data$train\ntrain_x = array((train$x-127.5)/127.5, c(dim(train$x)[1], 784L))\n```\n\nWe need a function to sample images for the discriminator.\n\n``` r\nbatch_size = 32L\ndataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, \"float32\"))\ndataset$batch(batch_size)\n```\n\nCreate function that returns the generator model:\n\n``` r\nget_generator = function(){\n  generator = keras_model_sequential()\n  generator %>% \n  layer_dense(units = 200L, input_shape = c(100L)) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 200L) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 784L, activation = \"tanh\")\n  \n  return(generator)\n}\n```\n\nTest the generator:\n\n``` r\ngenerator = get_generator()\nsample = tf$random$normal(c(1L, 100L))\nimgPlot(array(generator(sample)$numpy(), c(28L, 28L)))\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_24-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\nIn the discriminator, noise (random vector with 100 values) is passed through the network such that the output corresponds to the number of pixels of one MNIST image (784). We therefore define the discriminator function now.\n\n``` r\nget_discriminator = function(){\n  discriminator = keras_model_sequential()\n  discriminator %>% \n  layer_dense(units = 200L, input_shape = c(784L)) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 100L) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 1L, activation = \"sigmoid\")\n  \n  return(discriminator)\n}\n```\n\nAnd we also test the discriminator function.\n\n``` r\ndiscriminator = get_discriminator()\ndiscriminator(generator(tf$random$normal(c(1L, 100L))))\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\ntf.Tensor([[0.5089391]], shape=(1, 1), dtype=float32)\n    \n    \n```\n:::\n:::\n\n\nWe also have to define the loss functions for both networks.We use the already known binary cross entropy. However, we have to encode the real and predicted values for the two networks individually.\n\nThe discriminator will get two losses - one for identifying fake images as fake, and one for identifying real MNIST images as real images.\n\nThe generator will just get one loss - was it able to deceive the discriminator?\n\n``` r\nce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE)\n\nloss_discriminator = function(real, fake){\n  real_loss = ce(tf$ones_like(real), real)\n  fake_loss = ce(tf$zeros_like(fake), fake)\n  return(real_loss + fake_loss)\n}\n\nloss_generator = function(fake){\n  return(ce(tf$ones_like(fake), fake))\n}\n```\n\nEach network will get its own optimizer (in a GAN the networks are treated independently):\n\n``` r\ngen_opt = tf$keras$optimizers$RMSprop(1e-4)\ndisc_opt = tf$keras$optimizers$RMSprop(1e-4)\n```\n\nWe have to write our own training loop here (we cannot use the fit function). In each iteration (for each batch) we will do the following (the GradientTape records computations to do automatic differentiation):\n\n1.  Sample noise.\n2.  Generator creates images from the noise.\n3.  Discriminator makes predictions for fake images and real images (response is a probability between \\[0,1\\]).\n4.  Calculate loss for generator.\n5.  Calculate loss for discriminator.\n6.  Calculate gradients for weights and the loss.\n7.  Update weights of generator.\n8.  Update weights of discriminator.\n9.  Return losses.\n\n``` r\ngenerator = get_generator()\ndiscriminator = get_discriminator()\n\ntrain_step = function(images){\n  noise = tf$random$normal(c(128L, 100L))\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      gen_images = generator(noise)\n      fake_output = discriminator(gen_images)\n      real_output = discriminator(images)\n      gen_loss = loss_generator(fake_output)\n      disc_loss = loss_discriminator(real_output, fake_output)\n    }\n  )\n  \n  gen_grads = tape$gradient(gen_loss, generator$weights)\n  disc_grads = tape$gradient(disc_loss, discriminator$weights)\n  rm(tape)\n  gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights)))\n  disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights)))\n  \n  return(c(gen_loss, disc_loss))\n}\n\ntrain_step = tf$`function`(reticulate::py_func(train_step))\n```\n\nNow we can finally train our networks in a training loop:\n\n1.  Create networks.\n2.  Get batch of images.\n3.  Run train_step function.\n4.  Print losses.\n5.  Repeat step 2-4 for number of epochs.\n\n``` r\nbatch_size = 128L\nepochs = 20L\nsteps = as.integer(nrow(train_x)/batch_size)\ncounter = 1\ngen_loss = c()\ndisc_loss = c()\n\ndataset2 = dataset$prefetch(tf$data$AUTOTUNE)\n\nfor(e in 1:epochs){\n  dat = reticulate::as_iterator(dataset2$batch(batch_size))\n  \n  coro::loop(\n    for(images in dat){\n      losses = train_step(images)\n      gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy())\n      disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy())\n    }\n  )\n   \n  if(e %% 5 == 0){ #Print output every 5 steps.\n    cat(\"Gen: \", mean(gen_loss), \" Disc: \", mean(disc_loss), \" \\n\")\n  }\n  noise = tf$random$normal(c(1L, 100L))\n  if(e %% 10 == 0){  #Plot image every 10 steps.\n    imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), \"Gen\")\n  }\n}\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nGen:  0.8095555  Disc:  1.10533  \nGen:  0.8928918  Disc:  1.287504    \n    \n```\n:::\n:::\n\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_30-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nGen:  0.9071119  Disc:  1.314586  \nGen:  0.9514963  Disc:  1.31548 \n    \n```\n:::\n:::\n\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_30-2.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n## Flower - GAN\n\nWe can now also do the same for the flower data set. We will write this completely on our own following the steps also done for the MNIST data set.\n\n``` r\nlibrary(keras)\nlibrary(tidyverse)\nlibrary(tensorflow)\nlibrary(EcoData)\n\ndata = EcoData::dataset_flower()\ntrain = (data$train-127.5)/127.5\ntest = (data$test-127.5)/127.5\ntrain_x = abind::abind(list(train, test), along = 1L)\ndataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, \"float32\"))\n```\n\nDefine the generator model and test it:\n\n``` r\nget_generator = function(){\n  generator = keras_model_sequential()\n  generator %>% \n    layer_dense(units = 20L*20L*128L, input_shape = c(100L),\n                use_bias = FALSE) %>% \n    layer_activation_leaky_relu() %>% \n    layer_reshape(c(20L, 20L, 128L)) %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 256L, kernel_size = c(3L, 3L),\n                            padding = \"same\", strides = c(1L, 1L),\n                            use_bias = FALSE) %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 128L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(1L, 1L),\n                            use_bias = FALSE) %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 64L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(2L, 2L),\n                            use_bias = FALSE) %>%\n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 3L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(2L, 2L),\n                            activation = \"tanh\", use_bias = FALSE)\n  return(generator)\n}\n\ngenerator = get_generator()\nimage = generator(tf$random$normal(c(1L,100L)))$numpy()[1,,,]\nimage = scales::rescale(image, to = c(0, 255))\nimage %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_32-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\nDefine the discriminator and test it:\n\n``` r\nget_discriminator = function(){\n  discriminator = keras_model_sequential()\n  discriminator %>% \n    layer_conv_2d(filters = 64L, kernel_size = c(5L, 5L),\n                  strides = c(2L, 2L), padding = \"same\",\n                  input_shape = c(80L, 80L, 3L)) %>%\n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d(filters = 128L, kernel_size = c(5L, 5L),\n                  strides = c(2L, 2L), padding = \"same\") %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d(filters = 256L, kernel_size = c(3L, 3L),\n                  strides = c(2L, 2L), padding = \"same\") %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_flatten() %>% \n    layer_dense(units = 1L, activation = \"sigmoid\")\n  return(discriminator)\n}\n\ndiscriminator = get_discriminator()\ndiscriminator\ndiscriminator(generator(tf$random$normal(c(1L, 100L))))\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat('\nModel: \"sequential_13\"\n__________________________________________________________________________________________\n Layer (type)                           Output Shape                        Param #       \n==========================================================================================\n conv2d_21 (Conv2D)                     (None, 40, 40, 64)                  4864          \n leaky_re_lu_14 (LeakyReLU)             (None, 40, 40, 64)                  0             \n dropout_6 (Dropout)                    (None, 40, 40, 64)                  0             \n conv2d_20 (Conv2D)                     (None, 20, 20, 128)                 204928        \n leaky_re_lu_13 (LeakyReLU)             (None, 20, 20, 128)                 0             \n dropout_5 (Dropout)                    (None, 20, 20, 128)                 0             \n conv2d_19 (Conv2D)                     (None, 10, 10, 256)                 295168        \n leaky_re_lu_12 (LeakyReLU)             (None, 10, 10, 256)                 0             \n dropout_4 (Dropout)                    (None, 10, 10, 256)                 0             \n flatten_3 (Flatten)                    (None, 25600)                       0             \n dense_25 (Dense)                       (None, 1)                           25601         \n==========================================================================================\nTotal params: 530,561\nTrainable params: 530,561\nNon-trainable params: 0\n__________________________________________________________________________________________\n\ntf.Tensor([[0.49996078]], shape=(1, 1), dtype=float32)    \n    ')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel: \"sequential_13\"\n__________________________________________________________________________________________\n Layer (type)                           Output Shape                        Param #       \n==========================================================================================\n conv2d_21 (Conv2D)                     (None, 40, 40, 64)                  4864          \n leaky_re_lu_14 (LeakyReLU)             (None, 40, 40, 64)                  0             \n dropout_6 (Dropout)                    (None, 40, 40, 64)                  0             \n conv2d_20 (Conv2D)                     (None, 20, 20, 128)                 204928        \n leaky_re_lu_13 (LeakyReLU)             (None, 20, 20, 128)                 0             \n dropout_5 (Dropout)                    (None, 20, 20, 128)                 0             \n conv2d_19 (Conv2D)                     (None, 10, 10, 256)                 295168        \n leaky_re_lu_12 (LeakyReLU)             (None, 10, 10, 256)                 0             \n dropout_4 (Dropout)                    (None, 10, 10, 256)                 0             \n flatten_3 (Flatten)                    (None, 25600)                       0             \n dense_25 (Dense)                       (None, 1)                           25601         \n==========================================================================================\nTotal params: 530,561\nTrainable params: 530,561\nNon-trainable params: 0\n__________________________________________________________________________________________\n\ntf.Tensor([[0.49996078]], shape=(1, 1), dtype=float32)    \n    \n```\n:::\n:::\n\n\nLoss functions:\n\n``` r\nce = tf$keras$losses$BinaryCrossentropy(from_logits = FALSE,\n                                        label_smoothing = 0.1)\n\nloss_discriminator = function(real, fake){\n  real_loss = ce(tf$ones_like(real), real)\n  fake_loss = ce(tf$zeros_like(fake), fake)\n  return(real_loss+fake_loss)\n}\n\nloss_generator = function(fake){\n  return(ce(tf$ones_like(fake), fake))\n}\n```\n\nOptimizers (two, each for one network):\n\n``` r\ngen_opt = tf$keras$optimizers$RMSprop(1e-4)\ndisc_opt = tf$keras$optimizers$RMSprop(1e-4)\n```\n\nDefine functions for the generator and discriminator:\n\n``` r\ngenerator = get_generator()\ndiscriminator = get_discriminator()\n\ntrain_step = function(images){\n  noise = tf$random$normal(c(32L, 100L))\n  \n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      gen_images = generator(noise)\n      \n      real_output = discriminator(images)\n      fake_output = discriminator(gen_images)\n      \n      gen_loss = loss_generator(fake_output)\n      disc_loss = loss_discriminator(real_output, fake_output)\n    }\n  )\n  \n  gen_grads = tape$gradient(gen_loss, generator$weights)\n  disc_grads = tape$gradient(disc_loss, discriminator$weights)\n  rm(tape)\n  \n  gen_opt$apply_gradients(purrr::transpose(list(gen_grads,\n                                                generator$weights)))\n  disc_opt$apply_gradients(purrr::transpose(list(disc_grads,\n                                                 discriminator$weights)))\n  \n  return(c(gen_loss, disc_loss))\n}\n\ntrain_step = tf$`function`(reticulate::py_func(train_step))\n```\n\nTraining:\n\n``` r\nbatch_size = 32L\nepochs = 30L\nsteps = as.integer(dim(train_x)[1]/batch_size)\ncounter = 1\ngen_loss = c()\ndisc_loss = c()\n\ndataset = dataset$prefetch(tf$data$AUTOTUNE)\n\nfor(e in 1:epochs){\n  dat = reticulate::as_iterator(dataset$batch(batch_size))\n  \n  coro::loop(\n    for(images in dat){\n      losses = train_step(images)\n      gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy())\n      disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy())\n    }\n  )\n   \n  noise = tf$random$normal(c(1L, 100L))\n  image = generator(noise)$numpy()[1,,,]\n  image = scales::rescale(image, to = c(0, 255))\n  if(e %% 15 == 0){\n    image %>% \n      image_to_array() %>%\n        `/`(., 255) %>%\n        as.raster() %>%\n        plot()\n  }\n   \n  if(e %% 10 == 0) cat(\"Gen: \", mean(gen_loss), \" Disc: \", mean(disc_loss), \" \\n\")\n}\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nGen:  1.651127  Disc:  0.8720699    \n    \n```\n:::\n:::\n\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_37-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nGen:  1.303061  Disc:  1.037192 \n    \n```\n:::\n:::\n\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_37-2.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nGen:  1.168868  Disc:  1.100166\n    \n```\n:::\n:::\n\n\n``` r\nnoise = tf$random$normal(c(1L, 100L))\nimage = generator(noise)$numpy()[1,,,]\nimage = scales::rescale(image, to = c(0, 255))\nimage %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n```\n\n<img src=\"09-GAN_files/figure-html/chunk_chapter7_38-1.png\" width=\"100%\" style=\"display: block; margin: auto;\"/>\n\nMore images:\n\n<img src=\"images/flower2.png\" width=\"150%\" height=\"150%\" style=\"display: block; margin: auto;\"/><img src=\"images/flower3.png\" width=\"150%\" height=\"150%\" style=\"display: block; margin: auto;\"/><img src=\"images/flower4.png\" width=\"150%\" height=\"150%\" style=\"display: block; margin: auto;\"/><img src=\"images/flower5.png\" width=\"150%\" height=\"150%\" style=\"display: block; margin: auto;\"/>\n\n## Exercise\n\n::: {.callout-caution icon=\"false\"}\n#### Question\n\nGo through the R examples on generative adversarial networks (@sec-GAN) and compare the flower example with the MNIST example - where are the differences - and why?\n\nThe MNIST example uses a \"simple\" deep neural network which is sufficient for a classification that easy. The flower example uses a much more expensive convolutional neural network to classify the images.\n:::\n",
    "supporting": [
      "E2-GAN_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}