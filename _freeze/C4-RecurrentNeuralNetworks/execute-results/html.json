{
  "hash": "e039598f8f10dd0a16fe7db6b9d8851b",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Recurrent Neural Networks (RNN)\n\n\n\n\n\nRecurrent neural networks are used to model sequential data, i.e. a temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic:\n\n\n<iframe width=\"560\" height=\"315\" \n  src=\"https://www.youtube.com/embed/SEnXr6v2ifU\"\n  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media;\n  gyroscope; picture-in-picture\" allowfullscreen>\n  </iframe>\n\n\n## Case Study: Predicting drought\n\nWe will use a subset of the data explained in [this github repository](https://github.com/Epistoteles/predicting-drought)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nutils::download.file(\"https://www.dropbox.com/s/radyscnl5zcf57b/weather_soil.RDS?raw=1\", destfile = \"weather_soil.RDS\")\ndata = readRDS(\"weather_soil.RDS\")\nX = data$train # Features of the last 180 days\ndim(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 999 180  21\n```\n:::\n\n```{.r .cell-code}\n# 999 batches of 180 days with 21 features each\nY = data$target\ndim(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 999   6\n```\n:::\n\n```{.r .cell-code}\n# 999 batches of 6 week drought predictions\n\n# let's visualize drought over 24 months:\n# -> We have to take 16 batches (16*6 = 96 weaks ( = 24 months) )\nplot(as.vector(Y[1:16,]), type = \"l\", xlab = \"week\", ylab = \"Drought\")\n```\n\n::: {.cell-output-display}\n![](C4-RecurrentNeuralNetworks_files/figure-html/chunk_chapter5_0_Rnn-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras3)\n\nholdout = 700:999\nX_train = X[-holdout,,]\nX_test = X[holdout,,]\n\nY_train = Y[-holdout,]\nY_test = Y[holdout,]\n\nmodel = keras_model_sequential()\nmodel %>% \n  layer_lstm(units = 60L,input_shape = dim(X)[2:3]) %>% \n  layer_dense(units = 6L)\n\nmodel %>% compile(loss = keras3::loss_mean_squared_error, optimizer = optimizer_adamax(learning_rate = 0.01))\n  \nmodel %>% fit(x = X_train, y = Y_train, epochs = 30L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/30\n22/22 - 1s - 60ms/step - loss: 0.7506\nEpoch 2/30\n22/22 - 1s - 40ms/step - loss: 0.4153\nEpoch 3/30\n22/22 - 1s - 29ms/step - loss: 0.3555\nEpoch 4/30\n22/22 - 1s - 26ms/step - loss: 0.3522\nEpoch 5/30\n22/22 - 1s - 27ms/step - loss: 0.3493\nEpoch 6/30\n22/22 - 1s - 26ms/step - loss: 0.3175\nEpoch 7/30\n22/22 - 1s - 26ms/step - loss: 0.3007\nEpoch 8/30\n22/22 - 1s - 26ms/step - loss: 0.3136\nEpoch 9/30\n22/22 - 1s - 26ms/step - loss: 0.3007\nEpoch 10/30\n22/22 - 1s - 26ms/step - loss: 0.2950\nEpoch 11/30\n22/22 - 1s - 26ms/step - loss: 0.2871\nEpoch 12/30\n22/22 - 1s - 27ms/step - loss: 0.2743\nEpoch 13/30\n22/22 - 1s - 27ms/step - loss: 0.2745\nEpoch 14/30\n22/22 - 1s - 29ms/step - loss: 0.2906\nEpoch 15/30\n22/22 - 1s - 27ms/step - loss: 0.2722\nEpoch 16/30\n22/22 - 1s - 26ms/step - loss: 0.2571\nEpoch 17/30\n22/22 - 1s - 26ms/step - loss: 0.2449\nEpoch 18/30\n22/22 - 1s - 26ms/step - loss: 0.2502\nEpoch 19/30\n22/22 - 1s - 28ms/step - loss: 0.2393\nEpoch 20/30\n22/22 - 1s - 31ms/step - loss: 0.2437\nEpoch 21/30\n22/22 - 1s - 31ms/step - loss: 0.2560\nEpoch 22/30\n22/22 - 1s - 51ms/step - loss: 0.2310\nEpoch 23/30\n22/22 - 1s - 28ms/step - loss: 0.2176\nEpoch 24/30\n22/22 - 1s - 26ms/step - loss: 0.2204\nEpoch 25/30\n22/22 - 1s - 43ms/step - loss: 0.2055\nEpoch 26/30\n22/22 - 1s - 26ms/step - loss: 0.1906\nEpoch 27/30\n22/22 - 1s - 27ms/step - loss: 0.1895\nEpoch 28/30\n22/22 - 1s - 27ms/step - loss: 0.1866\nEpoch 29/30\n22/22 - 1s - 26ms/step - loss: 0.1923\nEpoch 30/30\n22/22 - 1s - 26ms/step - loss: 0.1837\n```\n:::\n\n```{.r .cell-code}\npreds = \n  model %>% predict(X_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10/10 - 0s - 19ms/step\n```\n:::\n\n```{.r .cell-code}\nmatplot(cbind(as.vector(preds[1:48,]),  \n              as.vector(Y_test[1:48,])), \n        col = c(\"darkblue\", \"darkred\"),\n        type = \"o\", \n        pch = c(15, 16),\n        xlab = \"week\", ylab = \"Drought\")\nlegend(\"topright\", bty = \"n\", \n       col = c(\"darkblue\", \"darkred\"),\n      pch = c(15, 16), \n      legend = c(\"Prediction\", \"True Values\"))\n```\n\n::: {.cell-output-display}\n![](C4-RecurrentNeuralNetworks_files/figure-html/chunk_chapter5_1_Rnn-1.png){width=672}\n:::\n:::\n\n\nThe following code snippet shows you many (technical) things you need for building more complex network structures, even with LSTM cells (the following example doesn't have any functionality, it is just an example for how to process two different inputs in different ways within one network):\n\n::: panel-tabset\n## Keras\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras3)\n\n\ninputDimension1 = 50L\ninputDimension2 = 10L\n\ninput1 = layer_input(shape = inputDimension1)\ninput2 = layer_input(shape = inputDimension2)\n\nmodelInput2 = input2 %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = inputDimension2, activation = \"gelu\")\n\nmodelMemory = input1 %>%\n  layer_embedding(input_dim = inputDimension1, output_dim = 64L) %>%\n  layer_lstm(units = 64L) %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 2L, activation = \"sigmoid\")\n\nmodelDeep = input1 %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dense(units = 5L, activation = \"sigmoid\")\n\nmodelMain = layer_concatenate(c(modelMemory, modelDeep, modelInput2)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dense(units = 2L, activation = \"sigmoid\")\n\nmodel = keras_model(\n  inputs = c(input1, input2),\n  outputs = c(modelMain)  # Use the whole modelMain (resp. its output) as output.\n)\n\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"functional_3\"\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1         │ (None, 50)        │           0 │ -                  │\n│ (InputLayer)          │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_2 (Dropout)   │ (None, 50)        │           0 │ input_layer_1[0][… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_3 (Dense)       │ (None, 64)        │       3,264 │ dropout_2[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ embedding (Embedding) │ (None, 50, 64)    │       3,200 │ input_layer_1[0][… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_3 (Dropout)   │ (None, 64)        │           0 │ dense_3[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ lstm_1 (LSTM)         │ (None, 64)        │      33,024 │ embedding[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_4 (Dense)       │ (None, 64)        │       4,160 │ dropout_3[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ input_layer_2         │ (None, 10)        │           0 │ -                  │\n│ (InputLayer)          │                   │             │                    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_1 (Dropout)   │ (None, 64)        │           0 │ lstm_1[0][0]       │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_5 (Dense)       │ (None, 64)        │       4,160 │ dense_4[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout (Dropout)     │ (None, 10)        │           0 │ input_layer_2[0][… │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_2 (Dense)       │ (None, 2)         │         130 │ dropout_1[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_6 (Dense)       │ (None, 5)         │         325 │ dense_5[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_1 (Dense)       │ (None, 10)        │         110 │ dropout[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ concatenate           │ (None, 17)        │           0 │ dense_2[0][0],     │\n│ (Concatenate)         │                   │             │ dense_6[0][0],     │\n│                       │                   │             │ dense_1[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_4 (Dropout)   │ (None, 17)        │           0 │ concatenate[0][0]  │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_7 (Dense)       │ (None, 64)        │       1,152 │ dropout_4[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dropout_5 (Dropout)   │ (None, 64)        │           0 │ dense_7[0][0]      │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_8 (Dense)       │ (None, 64)        │       4,160 │ dropout_5[0][0]    │\n├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n│ dense_9 (Dense)       │ (None, 2)         │         130 │ dense_8[0][0]      │\n└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n Total params: 53,815 (210.21 KB)\n Trainable params: 53,815 (210.21 KB)\n Non-trainable params: 0 (0.00 B)\n```\n:::\n\n```{.r .cell-code}\n# model %>% plot_model()\n```\n:::\n\n\n## Torch\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\nmodel_torch = nn_module(\n  initialize = function(type, inputDimension1 = 50L, inputDimension2 = 10L) {\n    self$dim1 = inputDimension1\n    self$dim2 = inputDimension2\n    self$modelInput2 = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(in_features = self$dim2, out_features = self$dim2),\n      nn_selu()\n    )\n    self$modelMemory = nn_sequential(\n      nn_embedding(self$dim1, 64),\n      nn_lstm(64, 64)\n    )\n    self$modelMemoryOutput = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(64L, 2L),\n      nn_sigmoid()\n    )\n    \n    self$modelDeep = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(self$dim1, 64L),\n      nn_relu(),\n      nn_dropout(0.3),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_linear(64, 5),\n      nn_sigmoid()\n    )\n    \n    self$modelMain = nn_sequential(\n      nn_linear(7+self$dim2, 64),\n      nn_relu(),\n      nn_dropout(0.5),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_dropout(),\n      nn_linear(64, 2),\n      nn_sigmoid()\n    )\n  },\n  \n  forward = function(x) {\n    input1 = x[[1]]\n    input2 = x[[2]]\n    out2 = self$modelInput2(input2)\n    out1 = self$modelMemoryOutput( self$modelMemory(input1)$view(list(dim(input1)[1], -1)) )\n    out3 = self$modelDeep(input1)\n    out = self$modelMain(torch_cat(list(out1, out2, out3), 2))\n    return(out)\n  }\n  \n)\n\n(model_torch())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAn `nn_module` containing 54,071 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• modelInput2: <nn_sequential> #110 parameters\n• modelMemory: <nn_sequential> #36,480 parameters\n• modelMemoryOutput: <nn_sequential> #130 parameters\n• modelDeep: <nn_sequential> #11,909 parameters\n• modelMain: <nn_sequential> #5,442 parameters\n```\n:::\n:::\n\n:::\n",
    "supporting": [
      "C4-RecurrentNeuralNetworks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}