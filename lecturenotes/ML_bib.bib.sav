% Encoding: UTF-8

@Article{Breiman2001,
  author        = {Breiman, Leo},
  title         = {Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)},
  journal       = {Statist. Sci.},
  year          = {2001},
  volume        = {16},
  number        = {3},
  pages         = {199--231},
  month         = aug,
  issn          = {0883-4237},
  __markedentry = {[LocalAdmin:1]},
  abstract      = {There are two cultures in the use of statistical modeling to reach       conclusions from data. One assumes that the data are generated by a       given stochastic data model. The other uses algorithmic models and       treats the data mechanism as unknown. The statistical community has       been committed to the almost exclusive use of data models. This       commitment has led to irrelevant theory, questionable conclusions, and      has kept statisticians from working on a large range of interesting       current problems. Algorithmic modeling, both in theory and practice,       has developed rapidly in fields outside statistics. It can be used       both on large complex data sets and as a more accurate and informative      alternative to data modeling on smaller data sets. If our goal as a       field is to use data to solve problems, then we need to move away from      exclusive dependence on data models and adopt a more diverse set of       tools.},
  groups        = {Link_Prediction, machine learning, Verwendet},
  language      = {en},
  publisher     = {The Institute of Mathematical Statistics},
  url           = {https://projecteuclid.org:443/euclid.ss/1009213726},
}

@Article{Breiman2001a,
  author        = {Breiman, Leo},
  title         = {Random Forests},
  journal       = {Machine Learning},
  year          = {2001},
  volume        = {45},
  number        = {1},
  pages         = {5--32},
  month         = oct,
  issn          = {1573-0565},
  __markedentry = {[LocalAdmin:1]},
  abstract      = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  groups        = {ANN, machine learning, Verwendet},
  refid         = {Breiman2001},
  url           = {https://doi.org/10.1023/A:1010933404324},
}

@Article{Friedman2001,
  author        = {Friedman, Jerome H.},
  title         = {Greedy Function Approximation: A Gradient Boosting Machine},
  journal       = {The Annals of Statistics},
  year          = {2001},
  volume        = {29},
  number        = {5},
  pages         = {1189--1232},
  issn          = {00905364},
  __markedentry = {[LocalAdmin:1]},
  abstract      = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  groups        = {Link_Prediction, ML_Interpret, Verwendet},
  publisher     = {Institute of Mathematical Statistics},
  url           = {http://www.jstor.org/stable/2699986},
}

@Article{Friedman2008,
  author        = {Friedman, Jerome H. and Popescu, Bogdan E.},
  title         = {Predictive learning via rule ensembles},
  journal       = {Ann. Appl. Stat.},
  year          = {2008},
  volume        = {2},
  number        = {3},
  pages         = {916--954},
  month         = sep,
  issn          = {1932-6157},
  __markedentry = {[LocalAdmin:1]},
  abstract      = {General regression and classification models are constructed as linear      combinations of simple rules derived from the data. Each rule consists      of a conjunction of a small number of simple statements concerning the      values of individual input variables. These rule ensembles are shown       to produce predictive accuracy comparable to the best methods.       However, their principal advantage lies in interpretation. Because of       its simple form, each rule is easy to understand, as is its influence       on individual predictions, selected subsets of predictions, or       globally over the entire space of joint input variable values.       Similarly, the degree of relevance of the respective input variables       can be assessed globally, locally in different regions of the input       space, or at individual prediction points. Techniques are presented       for automatically identifying those variables that are involved in       interactions with other variables, the strength and degree of those       interactions, as well as the identities of the other variables with       which they interact. Graphical representations are used to visualize       both main and interaction effects.},
  file          = {:Friedman2008.pdf:PDF},
  groups        = {Link_Prediction, Verwendet},
  keywords      = {Regression, classification, learning ensembles, rules, interaction effects, variable importance, machine learning, data mining},
  language      = {en},
  publisher     = {The Institute of Mathematical Statistics},
  url           = {https://projecteuclid.org:443/euclid.aoas/1223908046},
}

@Comment{jabref-meta: databaseType:bibtex;}
