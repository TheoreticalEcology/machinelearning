\documentclass[a4paper,twoside]{tufte-book} %style file is in the same folder.

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{german}

\usepackage{color}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{listings}

\usepackage{graphicx}

\usepackage{multicol}              
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{natbib}

\usepackage[innerrightmargin = 0.7cm, innerleftmargin = 0.3cm]{mdframed}
\usepackage{mdwlist}

\usepackage[]{hyperref}
\definecolor{darkblue}{rgb}{0,0,.5}
\hypersetup{colorlinks=true, breaklinks=true, linkcolor=darkblue, menucolor=darkblue, urlcolor=blue, citecolor=darkblue}

\usepackage[toc,page]{appendix}




\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}

\lstset{ % settings for listings needs to be be changed to R sytanx 
language=R,
breaklines = true,
columns=fullflexible,
breakautoindent = false,
%basicstyle=\listingsfont, 
basicstyle=\ttfamily \scriptsize,
keywordstyle=\color{black},                          
identifierstyle=\color{black},
commentstyle=\color{gray},
xleftmargin=3.4pt,
xrightmargin=3.4pt,
numbers=none,
literate={*}{{\char42}}1
         {-}{{\char45}}1
         {\ }{{\copyablespace}}1
}
% http://www.monperrus.net/martin/copy-pastable-listings-in-pdf-from-latex
\usepackage[space=true]{accsupp}
% requires the latest version of package accsupp
\newcommand{\copyablespace}{
    \BeginAccSupp{method=hex,unicode,ActualText=00A0}
\ %
    \EndAccSupp{}
}



<<setup, cache=FALSE, include=FALSE>>=
library(knitr)
opts_knit$set(tidy = T, fig=TRUE, fig.height = 4, fig.width=4, fig.align='center')
render_listings()
@

<<echo=FALSE, cache=TRUE, results='hide', message=FALSE, warning=FALSE>>=
set.seed(123)
@



\title{Machine Learning}
\author{Maximilian Pichler, Florian Hartig}


\begin{document}
%\SweaveOpts{concordance=TRUE} % don't activate this for knitr

\let\cleardoublepage\clearpage % No empty pages between chapters
\maketitle


\thispagestyle{empty}
\null


\href{Prof. Dr. Florian Hartig}{http://www.uni-regensburg.de/biologie-vorklinische-medizin/theoretische-oekologie/mitarbeiter/hartig/index.html}\\
University of Regensburg\\
Germany\\[0.5cm]

\begin{fullwidth}
Vorlesungsunterlagen für Studierende der

\begin{itemize*}
 % \item BSc Biostatistik
  \item MSc Machine Learning and AI in TensorFlow and R
\end{itemize*}

\vspace{0.5cm}

Fehler oder Verbesserungsvorschläge bitte über den \href{https://github.com/TheoreticalEcology/machinelearning/issues}{issue tracker} melden. 

\end{fullwidth}


\vfill
\begin{fullwidth}
Machine Learning and AI in TensorFlow and R Version 0.0.1, compiled \today.
%
% Translated into German 2017, based on lecture notes "Essential Statistics", created 2014-2016. 
%
This work is licensed under a \href{https://creativecommons.org/licenses/by-nc-nd/4.0/}{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License}. 
\end{fullwidth}


\newpage
\tableofcontents

\chapter{Data science, machine learning, big data and artificial intelligence}
Since a few years machine learning is experiencing a revival. In particular, the resurrection of neural networks might have triggered the current success and boom of machine learning. Neural networks have a long history but were long considered unpracticable because they were extremely computationally expensive. However, the imagenet competition (a machine learning competition) 2012 was the game changer. They trained their convolutional neural network on gpus, thus reducing training time from weeks/months to hours/days. Detached from computational limits, in the following artificial neural networks beat humans in strategic games such as GO and proved their superiority in visualization tasks.

With the resurrection of neural network

the demand of jobs in this field is on the rise


\chapter{Day 1} % Use chapters instead of sections

\section{Goals of data analysis}

\subsection{Explanatory modelling}

\subsection{Predictive modelling}


\section{Machine learning - a crash course}

\subsection{ML problems / language}

% classification, regression, overfitting, 


\subsection{ML models}

% nur grob erklären, genau später


\subsection{ML workflow}

% cleanen, fitten, prediction, CV

\subsection{The Keras / Tensorflow framework}

\href{https://www.tensorflow.org/}{TensorFlow} is a open source linear algebra library with a focus on neural networks. 
%
Google published opfficialy TensorFlow (TF) in 2015. 
%
TF supports several interesting features:
%
\begin{itemize}
\item Automatic differentation (analytic)
\item Several gradient optimiziers
\item CPU and GPU parallelization
\end{itemize}
%
All operations in TF are written in C++ and are highly optimized. 
%
But dont worry, we don't have to use C++ to use TF because there are several bindings for other languages.
%
TensorFlow officialy supports a Python API, but meanwhile there are several community carried APIs for other languages:
%
\begin{itemize}
\item R
\item Go
\item Rust
\item Swift
\item JavaScript
\end{itemize}
%
In this course we will use TF with the \href{R}{https://tensorflow.rstudio.com/} binding, that was developed and published 2017 by the RStudio Team.
%
They developed first a R package (\href{https://rstudio.github.io/reticulate/}{reticulate}) to call python in R. 
%
Actually, we are using in R the python TF module (more about this later). \\
%
TF offers different levels of API.
%
We could implement a neural network completly by ourselves, or we could use Keras which is provided by TF as a submodule.
%
Keras is a powerful module for building and training neural networks.
%
It allows us to build and train neural networks in a few lines of codes.
%
Since the end of 2018, Keras and TF are completly interoperable, allowing us to utilize the best of both.
%
In this course, we will show how we can use Keras for neural networks but also how we can use the TF's automatic differenation for using complex objective functions. 
%

\subsection{TensorFlow installation}
For the TF-R binding there are several ways to install TensorFlow. 
%
The easiest way is to the follow the instructions on the RStudio-TF homepage \href{https://tensorflow.rstudio.com/tensorflow/articles/installation.html}{RStudio-TF homepage}.
%
\textbf{However}, in this course we will use TensorFlow 2.0 API, which is the new, reworked API, and yet not stable or supported by the TF-R installer.   
%
To install TF-2.0, follow the OS specific instructions below:
%
\begin{itemize}
\item \hyperref[Mac]{MacOS and Linux}
\item \hyperref[Windows]{Windows}
\end{itemize}

\paragraph{MacOS and Linux}\label{Mac} 
On Linux and and MacOS, python 2 or python 3 is already pre-installed.
%
First, we have to check whether pip (python2 package installer) or pip3 (python3 package installer) is installed. 
%
Open the terminal and hit:
<<label=install_1, eval=FALSE>>=
which pip 
which pip3
@
If one is successfull, skip the next step, otherwise we have to install first pip now:
%
\textbf{MacOS: }
<<label=install_2,eval=FALSE>>=
sudo easy_install pip
@
%

\textbf{Linux (Debian, Ubuntu here): }
<<label=install_3,eval=FALSE>>=
sudo apt-get install python-pip
@
%
Now open R or execute the command in the shell for installing TF:
<<label=install_4,eval=FALSE>>=
system("pip install --user tensorflow==2.0.0-beta0")
@



\paragraph{Windows}\label{Windows} 
On Windows, python is not pre-installed. 
%
First, download and install python2 or python3 from the python website: \href{https://www.python.org/downloads/windows/}{https://www.python.org/downloads/windows/}. 
%
\textbf{Warning:} The installer will ask you if it should set the paths for python, set the checkmark there!
%
Download the \href{https://bootstrap.pypa.io/get-pip.py}{get-pip.py} script and run in the powershell (as admin):
%
<<label=install_5,eval=FALSE>>=
python get-pip.py
@
%
Afterwards install TF in the powershell with:
<<label=install_6,eval=FALSE>>=
pip install --user tensorflow==2.0.0-beta0
@
%
or in R with:
<<label=install_7,eval=FALSE>>=
system("pip install --user tensorflow==2.0.0-beta0")
@


After we have installed the TF-python package, we have to setup R for TF:
<<label=install_8,eval=FALSE>>=
install.packages(c("reticulate", "keras"))
@
Try it out:
<<label=install_9,eval=TRUE>>=
library(tensorflow)
# or: tf <- reticulate::import('tensorflow')
print(tf$version)
@


\section{The ML workflow in practice }

\subsection{Reading and cleaning data}

\subsection{Pre-processing and feature selection}

\subsection{Baseline, model fitting and optimization}

\subsection{Prediction and validation}


\chapter{Day 2 - a closer view at the black box}

In day 2, we look at the black box, i.e. the ML models and principles for fitting them

% stress regression and classification differences

% stress supervised, unsupervised, 

\section{Structures and optimization}

\subsection{Structures - trees, networks, etc}


\subsection{Loss function and optimization}


\section{Regularization}

Regularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a ML model. The most important distiction is between shrinkage estimators and estimators based on average 

\subsection{Shrinkage estimators} 

Shrikage estimators are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are shrunk to the given value.

The most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penality is proportional to the absolute deviation (L1 penalty), and the Tikhonov regularization aka ridge regression, where the penalty is proportional to the squared distance from the reference (L2 penalty). 

The loss function is thus given by 

\begin{equation}
loss = fit - \lambda \cdot d
\end{equation}

where fit refers to the standard loss function, $\lambda$ is the strength of the regularization, and $d$ is the chosen metrics, e.g. L1 or L2.






\subsection{Boosting, bagging and averaging}

Model averaging means that predictions of many models are combined. 

A particular important application of averaging is boosting, where the principle is that many weak learners are combined to a model average, resulting in a strong learner. 

Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap the data, and average the bootstrapped predictions. $\lambda$ and possibly d are typically optimized under cross-validation



\section{LM and regularization}

\subsection{Linear model}

\subsection{Generalized linear model}

\subsection{Why und what is regularization?}


\subsection{l1 and l2 regularization}


\section{CARTs, BRTs and RF}

\subsection{CART}
Given $Y \sim X_1 + X_2$, the CART tries to partition the feature space into rectangulars.

\subsection{Random Forest}
\citep[BRT,][]{Friedman2001}

\subsection{Boosted Regression Trees}
\citep[RF,][]{Breiman2001a}

%
Random Forest \citep[RF,][]{Breiman2001a} and Boosted Regression Trees \citep[BRT,][]{Friedman2001} are both based on the Classification And Regression Tree (CART) algorithm.
%

\section{SVMs}

\section{distance-based}

% knn 

\section{NNs}


\chapter{Day 3 - deep learning and AI}

\section{DNNs}

\section{CNNs}

\section{RNNs}

\chapter{Day 4 - Advanced}

\section{Autoencoder}

\section{Tuning and validation}


\chapter{Day 5}





\chapter{Resources}


\bibliographystyle{chicago}
\bibliography{ML_bib}
\end{document}
