---
title: "Day 1 - ML course "
author: "Maximilian Pichler, Florian Hartig"
date: "6/21/2019"
output: 
  html_document:
     toc: true
     theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
set.seed(42)
```


## TensorFlow ecosystem

At the moment the [TensorFlow](https://www.tensorflow.org/) is rapdily changing. With TF1.x, [Keras](https://keras.io/) is a fully integrated sub-module of TF. Keras is high level neural-network API on Top of TF. Keras simplifies building, training, evaluation and predicting of all kinds of neural networks. 

The RStudio team developed R bindings for the TF ecosystem with the TensorFlow and Keras package. For documentation and help see:
+ [R-TensorFlow](https://tensorflow.rstudio.com/)
+ [R-Keras](https://keras.rstudio.com/)
+ [TensorFlow (Python API)](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf). The python API documentation has a more detailed documentation with examples

TensorFlow works by setting up a operation graph and only when executed the graph is constructed. With TF 2.0, eager execution, meaning that aber each addition of an operation the graph is built, is the default and simplifies the useage. In this course, we will use solely the TF 2.0 API and eager execution.

### TensorFlow Core

Loading the R-tensorflow package will link the python tensorflow API to the 'tf' object. The syntax will seem, at first, weird but is derived from python (OOP Style).

```{r}
library(tensorflow) # will load and link the python api to the "tf" in the enviornment. The API can be called by tf$...
tf$enable_eager_execution()
tf$print("TensorFlow: Hello World!")
print(tf$version) # Base R methods fully support TF objects and Operations. 
```

Before we can exploit TF operations, we have to export or create explicitly tf tensors:
```{r}
# We create two scalar values
a = tf$constant(10.0) # R style: a = 10.0
b = tf$constant(20.0) # R style: b = 20.0

c1 = tf$add(a, b) 
print(c1)
c2 = a + b # Default methods are available for TF operations ->  a+b returns a tf tensor
print(c2)
c3 = tf$add(10.0, 20.0) # tf tensor as return type
print(c3)
c4 = 10.0 + 20.0
print(c4) # Base R operation, return type is a R numeric
```

Before we continue, we have to talk about data types. The above printed tf tensors have the dtype "float32". TF is a highly optimized library written in C++. For speed up and safety, we have to set the dtypes of our operations. For more detailed information on dtypes read [Wikipedia](https://en.wikipedia.org/wiki/Data_type). Basically, there are four important dtypes:

+ Integers: e.g. 10 (in R we can write 10L to get a explicit integer) -> count data
+ Floating point: e.g. 10 -> real values
+ Characters: e.g. "Hallo"
+ Boolean: e.g. TRUE/FALSE or binary 0/1

In addition to the base dtypes, we can set how many memory the type should take up. For instance, TF supports 16, 32, and 64 Bits for floats. (This results in different precisions. But why should we use 16 or 32 bits for floats when today's systems support 64 bits and we have more than enough RAM? -> GPU computation! Many GPU support only 32 bit floats and RAM is also limiting GPU computation).

**WARNING** In my experience, most of the errors occur because the R-Python interpreter (reticulate package) which transform our R numerics to the corresponding Python numerics changes the type if not explicitly set:

```{r, eval=FALSE}
a = tf$constant(1:10) # float or integer?
try(a/2) 

# Better:
a = tf$constant(1:10, dtype = "float64") # set type directly in TF
try(a/2) # error again, our R-64bit numeric is cast to 32-float!  

# We could solve that by using float32
a = tf$constant(1:10, dtype = "float32") 
try(a/2)

# Best practise - Cast everything first to a TF tensor and set the type explicitly!
dtype = "float64"
a = tf$constant(1:10, dtype = dtype)
b = tf$constant(2, dtype = dtype)
a/b

# Also set the arguments explicitly to Integer:
try(
  tf$reshape(a, shape = list(2, 5)) # reshape vector into 2x5 Matrix
)
# ERROR because the argument types were cast wrongly!! Use always the explicit R integer type:

try(
  tf$reshape(a, shape = list(2L, 5L)) # reshape vector into 2x5 Matrix
)
```

Side note: I learned this lesssion the hard way after hours/days of debugging errors caused by wrong R-Py-TF casts.

You may have noticed that TF always return a Tensor of TF operations. If we want to work the the TF result in R we have to cast it back to a R type:
```{r}
a = tf$constant(1:10)
try(sum(a)) # no sum method implemented for tf tensors
# Cast back to R by:
a_R = a$numpy()
print(sum(a_R))

```

### Exercise - 1: Simple RSSE optimization
```{r}
set.seed(42)
n_predictors = 1L
n_observations = 500L
X = matrix(runif(n_predictors * n_observations, -1, 1), n_observations, n_predictors)
W = 4
Y = X %*% W + rnorm(n_observations, 0, 0.5)

plot(Y~X[,1], xlab = "Predictor 1", ylab = "Y")

# R - Style:

RSSE = function(par){
  y_hat = X %*% par
  return(sum(y_hat-Y)^2)
}

(opt = optim(par = -1, lower = -15, upper = 15, fn = RSSE, method = "Brent")$par)

```
Export the operations in the RSSE to TF operations (Remember to return the tensor as tensor$numpy())

```{r,eval=FALSE}
# help:
# the tf$math module provides all required operations

RSSE_tf = function(par){
  # cast par vector to tensor:
  
  # calculate y_hat
  
  # calculate rsse
  
  return(rsse$numpy())
}

(opt = optim(par = -1, lower = -15, upper = 15, fn = RSSE_tf, method = "Brent")$par)
```


### Exercise - 1: Solution with Keras
Keras is a high level neural network API in the TF ecosyste and simplifies not only the model building but in particular the training. The Keras API is available by:
```{r}
library(keras)
print(keras::backend())
```
Or by the RStudio keras package which is basically just a wrapper around the submodule. Fitting a model with keras involves three steps:

1. Build the model:
  + Input shape
  + Hidden layer definition
  + Output shape
2. Compile the model:
  + Define loss function
  + Define Optimizer
3. Fitting:
  + Train the model in n epochs
  
We will explain on Day 3 what or how neural networks work. For now, we will fit without a hidden layer, thus our model is just a linear model:
```{r, eval=FALSE}
# 1. Build the model:
model = keras_model_sequential()
model %>% 
  layer_dense(units = 1L, input_shape = c(1L))
summary(model)


# 2. Compile model:
model %>% 
  compile(loss = "mean_squared_error", optimizer = optimizer_adamax(0.01))

# 3. Training:
fit_history =
  model %>% 
    fit(x = X, y = matrix(Y, ncol = 1L), epochs = 200L, verbose = 0L)
print(sapply(model$weights, function(w) w$numpy()))
plot(fit_history)

```

## Machine Learning workflow 
### Step 1: load and explore data
Boston housing data:
target_variable: medv
type: Regression

Var | Description
----| -----------
crim |	per capita crime rate by town
zn |	proportion of residential land zoned for lots over 25,000 sq.ft
indus |	proportion of non-retail business acres per town
chas |	Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
nox |	nitric oxides concentration (parts per 10 million)
rm |	average number of rooms per dwelling
age |	proportion of owner-occupied units built prior to 1940
dis |	weighted distances to five Boston employment centres
rad |	index of accessibility to radial highways
tax |	full-value property-tax rate per USD 10,000
ptratio |	pupil-teacher ratio by town
b |	1000(B - 0.63)^2 where B is the proportion of blacks by town
lstat |	percentage of lower status of the population
medv |	median value of owner-occupied homes in USD 1000's 


```{r}
housing = read.csv("data/BostonHousing.csv")
housing = housing[,-1]

summary(housing)

par(mfrow = c(4,4))
for(i in 1:ncol(housing)){
  hist(housing[,i], main = colnames(housing)[i])
  abline(v = mean(housing[,i]) , col = "red")
}


```

### Step 2: Cleaning, prepocssing and splitting
```{r}
correlations = cor(housing)
correlations[upper.tri(correlations)] = NA
image(t(abs(correlations)),xaxt = "n", yaxt = "n",useRaster = TRUE)
axis(1, at=seq(0,1,length.out=ncol(housing)), labels=colnames(housing), las= 1)
axis(2, at=seq(0,1,length.out=ncol(housing)), labels=colnames(housing), las= 2)

# scale predictors
X = housing[,-which(colnames(housing) == "medv", arr.ind = TRUE)]
Y = housing[,which(colnames(housing) == "medv", arr.ind = TRUE)]

X = scale(X)

# split into training and testing datasets
indices = sample.int(nrow(X), 0.8*nrow(X),replace = FALSE)
train = list(X = X[indices, ], Y = Y[indices])
test = list(X = X[-indices, ], Y = Y[-indices])

```



### Step 3: Model fitting

```{r}
# 1. Build model
model = keras_model_sequential()
model %>% 
  layer_dense(units = 20L,input_shape = c(ncol(X)), activation = "relu") %>% 
  layer_dense(units = 20L, activation = "relu") %>% 
  layer_dense(units = 1L)
summary(model)

# 2. specify loss and optimizer
model %>% 
  compile(loss = loss_mean_squared_error, optimizer = optimizer_adamax(0.01))

# 3. fit model on training data
fit_history = 
  model %>% 
    fit(x = train$X, y = matrix(train$Y, ncol = 1L), epochs = 100L, validation_data = list(test$X,matrix(test$Y, ncol = 1L)), verbose = 0L)

# lm:
data = data.frame(train$X, medv = train$Y)
lm = lm(medv~., data)

```


### Step 4: Evaluation
```{r}
plot(fit_history)

model %>% 
  evaluate(x = test$X, y = matrix(test$Y, ncol = 1L))

pred_keras = predict(model, x = test$X)
pred_lm = predict(lm, newdata = data.frame(test$X))

Metrics::rmse(test$Y, pred_keras)
Metrics::rmse(test$Y, matrix(pred_lm, ncol = 1L))

#print(sapply(model$weights, function(w) w$numpy()))
print(coef(lm))

```



### Exercise - 2: Titanic Dataset, Classification
target variable: Survived
type: Classification

+ Load the data set
+ Explore the data
+ Preprocess (which columns to include/exclude, transform categorical into dummy variables)
+ Fit keras model (is given)
+ Fit glm (why glm here?)
+ Evaluate on Titanic_test

```{r, include=FALSE,eval=TRUE}
set.seed(42)
data = read.csv2("data/Titanic.csv")
data = data[,-c(1,2)]
train_indices = sample.int(nrow(data), 0.5*nrow(data))
write.csv2(data[train_indices, ],"data/Titanic_train.csv")
write.csv2(data[-train_indices, ],"data/Titanic_test.csv")

```

```{r,eval=FALSE}
train = read.csv("data/Titanic_train.csv")
test = read.csv("data/Titanic_test.csv")
```


```{r, eval=FALSE}
model = keras_model_sequential()
model %>% 
  layer_dense(units = 20L, activation = "relu", input_shape = ncol(train_x)) %>% 
  layer_dense(units = 20L, activation = "relu") %>% 
  layer_dense(units = 1L, activation = "sigmoid")

model %>% 
  compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax())

fit_history =
  model %>% 
    fit(x = train_x, y = train_y, epochs = 100L)

```

