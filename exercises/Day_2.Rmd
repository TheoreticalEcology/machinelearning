---
title: "Day 2 - ML course "
author: "Maximilian Pichler, Florian Hartig"
date: "`r Sys.Date()`"
output: 
  html_document:
     toc: true
     theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
set.seed(42)
```


## LM and regularization
### LM

$$ y_i \sim b_0 + X_i * b_1 + \epsilon _i,   e_i \sim N(0, \sigma) $$
```{r}
data = EcoData::boston
lm = lm(medv~., data = data)
summary(lm)
```

```{r}
library(keras)
library(tensorflow)
try(tf$enable_eager_execution())

model = keras_model_sequential()
model %>% 
  layer_dense(units = 1L, activation = "linear", input_shape = 13L)
model %>% 
  compile(loss = loss_mean_squared_error, optimizer = optimizer_adam(lr = 0.01))
model_history = 
  model %>% 
    fit(x = as.matrix(data[,-which(colnames(data) == "medv", arr.ind = TRUE)]),
        y = matrix(data[,which(colnames(data) == "medv", arr.ind = TRUE)], ncol = 1L), 
        verbose = 0L,
        epochs = 400L)

cbind(coef(lm),c(model$weights[[2]]$numpy(), model$weights[[1]]$numpy()))

```


### GLM

Poisson: $$ log(y_i) \sim b_0 + X_i * b_1 $$
Binomial: $$ logit(y_i) \sim b_0 + X_i * b_1 $$

```{r}
X = scale(as.matrix(data[,-which(colnames(data) == "ABUND", arr.ind = TRUE)]))
data = EcoData::birdabundance
glm = glm(data$ABUND~X)
summary(glm)
```

```{r}

model = keras_model_sequential()
model %>% 
  layer_dense(units = 1L, activation = "linear", input_shape = 6L) %>% 
  layer_lambda(function(t) tf$exp(t))

model %>% 
  compile(loss = loss_poisson, optimizer = optimizer_adam(lr = 0.004))

model_history = 
  model %>% 
    fit(x = X,
        y = matrix( as.integer(data$ABUND), ncol = 1L), 
        verbose = 0L,
        epochs = 400L)

cbind(coef(glm),c(model$get_weights()[[2]], model$get_weights()[[1]]))

```



### Regularization
L1 = Lasso (Least Absolute Shrinkage and Selection Operator)
$$\lambda * \sum_{i = 1}^{p} |W_i| $$
```{r}
plot(density(LaplacesDemon::rlaplace(1e5)), main = "l1")
```



L2 = Ridge 
$$\lambda * \sum_{i = 1}^{p} W_i^2 $$
```{r}
plot(density(rnorm(rnorm(1e5,0,1))), main = "l2")
```


### Exercise 1:
```{r}
library(EcoData)
data = EcoData::boston
```
```{r}
library(tensorflow)
tf$enable_eager_execution()
```




```{r}
W = tf$Variable(
  tf$constant(
    matrix(rnorm(13,0.0, sd = 0.5), nrow = 13L, ncol = 1L),
    dtype = "float32"
    )
  )

X = tf$constant(
  as.matrix(scale(data[,-which(colnames(data) == "medv", arr.ind = TRUE)])),
  dtype = "float32"
  )
Y = tf$constant(
  matrix(scale(data[,which(colnames(data) == "medv", arr.ind = TRUE)]), ncol = 1L),
  dtype = "float32"
)

optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)

loss = tf$losses$mean_squared_error(Y, tf$matmul(X, W))

for(i in 1:200){
  with(tf$GradientTape() %as% tape, {
    loss = tf$losses$mean_squared_error(Y, tf$matmul(X, W))
  })
  
  if(i %% 20 == 0) cat("Loss: ", loss$numpy(), "\n")
  
  gradients = tape$gradient(loss, W)
  optimizer$apply_gradients(list(c(gradients, W)))
}
cbind(W$numpy(), coef(summary(lm(Y$numpy()~X$numpy())))[-1,1])
```

Implement l1 and l2 regularization on the weights:
(Hint: use tf$reduce_sum/mean to reduce tensors)
```{r}
W = tf$Variable(
  tf$constant(
    matrix(rnorm(13,0.0, sd = 0.5), nrow = 13L, ncol = 1L),
    dtype = "float32"
    )
  )

X = tf$constant(
  as.matrix(scale(data[,-which(colnames(data) == "medv", arr.ind = TRUE)])),
  dtype = "float32"
  )
Y = tf$constant(
  matrix(scale(data[,which(colnames(data) == "medv", arr.ind = TRUE)]), ncol = 1L),
  dtype = "float32"
)

optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)

loss = tf$losses$mean_squared_error(Y, tf$matmul(X, W))

for(i in 1:200){
  with(tf$GradientTape() %as% tape, {
    loss = tf$losses$mean_squared_error(Y, tf$matmul(X, W)) # Add L1 / L2
  })
  
  if(i %% 20 == 0) cat("Loss: ", loss$numpy(), "\n")
  
  gradients = tape$gradient(loss, W)
  optimizer$apply_gradients(list(c(gradients, W)))
}
cbind(W$numpy(), coef(summary(lm(Y$numpy()~X$numpy())))[-1,1])
```

## Tree based methods

Before we continue, we will introduce an additional TensorFlow API. Besides Keras, which is a high level neural network API, there is the [Estimator](https://www.tensorflow.org/guide/estimators) API. The Estimator API is a more general Machine Learning API which supports not only neural networks but also other community driven algorithms such as trees. Keras models can also transformed into Estimators.
In this chapter we will introduce Trees which are included in the Estimator API. 

### CART

<!--  https://www.tensorflow.org/guide/premade_estimators -->

#### Regression
The Estimator API is useful because we can handle discrete features easier than with the Core API or keras (where we have to introduce dummy variables for our discrete features by ourselves)

```{r}
library(tfestimators)
data = carData::Salaries

plot(data)

input = input_fn(data, 
                 features = c("rank", "discipline", "yrs.since.phd", "yrs.service", "sex"),
                 response = c("salary"),
                 batch_size = nrow(data),
                 num_epochs = 1L
                )

cat_wrapper = function(key, vocabulary_list){
  tf$feature_column$indicator_column(
    tf$feature_column$categorical_column_with_vocabulary_list(key = key, vocabulary_list = vocabulary_list, dtype = tf$string)
  )
}

cols = c(
  cat_wrapper(
    "rank", vocabulary_list = as.character(unique(data$rank))
    ),
  cat_wrapper(
    "discipline", 
    vocabulary_list = as.character(unique(data$discipline))
    ),
  tf$feature_column$numeric_column("yrs.since.phd"),
  tf$feature_column$numeric_column("yrs.service"),
  cat_wrapper(
    "sex", vocabulary_list = as.character(unique(data$sex))
    )
)

i <- sapply(data, is.factor)
data[i] <- lapply(data[i], as.character)

i <- sapply(data, is.integer)
data[i] <- lapply(data[i], as.double)

input = tf$estimator$inputs$pandas_input_fn(data[,-6], shuffle = FALSE,target_column = "salary" )

RT = tf$estimator$BoostedTreesRegressor(cols, n_batches_per_layer = 1L, n_trees = 1L)

RT$train(input, steps = 1L)

```



#### Classification
```{r}
data = EcoData::regrowth

predictors = c(
  tf$feature_column$numeric_column("Root"),
  tf$feature_column$numeric_column("Fruit")
  )

input_function = function(){
  X = data[,1:2]
  Y = as.integer(as.numeric(data[,3])) -1L
  return(reticulate::tuple(reticulate::dict(X), Y))
}

CART = 
  tf$estimator$BoostedTreesClassifier(feature_columns = predictors, n_batches_per_layer = 1L,
                                     n_trees = 1L, n_classes = 2L,pruning_mode = "none")


CART$train(input_fn = input_function,steps = 1L)

results = CART$evaluate(input_fn = input_function, steps = 1L)

print(results$accuracy)


```



### Random Forest (RF)


### Boosted Regression Trees (BRT)
<!-- Nice blog: https://medium.com/tensorflow/how-to-train-boosted-trees-models-in-tensorflow-ca8466a53127 -->


## SVM
### Linear SVM

### Kernel SVM

## Distance based
### k-Nearest-Neighbor (kNN)

### kernel kNN

## Neural Networks
### Deep Neural Networks

### Convolutional Neural Networks

### RNN?
