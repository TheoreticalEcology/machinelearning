# Deep Learning Architectures{#deep}

```{=html}
<!-- Put this here (right after the first markdown headline) and only here for each document! -->
<script src="./scripts/multipleChoice.js"></script>
```

```{r, include=FALSE}
# example R options set globally
options(width = 90)

# example chunk options set globally
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  fig.align = 'center',
  cache=TRUE,
  out.width="100%"
)

xaringanExtra::use_panelset()
```


In this section, we will discuss both, different (deep) network architectures and different means to regularize and improve those deep architectures. 



## Deep Neural Networks (DNNs)

Deep neural networks are basically the same as simple artificial neural networks, only that they have more hidden layers.



### Dropout and Early Stopping in DNNs

Regularization in deep neural networks is very important because of the problem of overfitting. Standard regularization from statistics like $L1$ and $L2$ regularization are often fuzzy and require a lot of tuning. There are more stable and robust methods:

* Early stopping: Early stopping allows us to stop the training when for instance the test loss does not decrease anymore or the validation loss starts increasing.
* Dropout: The Dropout layer randomly sets input units to 0 with a frequency of a given rate at each step during training time, which helps prevent overfitting. Dropout is more robust than $L1$ and $L2$, and tuning of the dropout rate can be beneficial but a rate between $0.2-0.5$ often works quite well.

**Data preparation**

See \@ref(mlr) for explanation about the preprocessing pipeline. 

```{r chunk_chapter5_3, message=FALSE}
library(EcoData)
library(tidyverse)
library(mlr3)
library(mlr3pipelines)
data(nasa)
str(nasa)

data = nasa %>% select(-Orbit.Determination.Date,
                       -Close.Approach.Date, -Name, -Neo.Reference.ID)
data$Hazardous = as.factor(data$Hazardous)
task = TaskClassif$new(id = "nasa", backend = data,
                       target = "Hazardous", positive = "1")
preprocessing = po("imputeoor") %>>% po("scale") %>>% po("encode") 
data = preprocessing$train(task)[[1]]$data()

train = data[!is.na(data$Hazardous),]
submit = data[is.na(data$Hazardous),]

X = scale(train %>% select(-Hazardous))
Y = train %>% select(Hazardous)
Y = model.matrix(~0+as.factor(Hazardous), data = Y)
```

**Early stopping**

::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}

```{r chunk_chapter5_4}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X)) %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dense(units = ncol(Y), activation = "softmax") 

model %>%
  keras::compile(loss = loss_categorical_crossentropy,
                 keras::optimizer_adamax(learning_rate = 0.001))
summary(model)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 50L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4)
plot(model_history)
```


:::

::: {.panel}
[Torch]{.panel-name}

```{r chunk_chapter5_4_torch}
library(torch)
torch_manual_seed(321L)
set.seed(123)

model_torch = nn_sequential(
  nn_linear(ncol(X), 50L),
  nn_relu(),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_linear(50L, 2L)
)

YT = apply(Y, 1, which.max)

dataset_nasa = dataset(
  name = "nasa",
  initialize = function(nasa){
    self$X = nasa$X
    self$Y = nasa$Y
  },
  .getitem = function(i){
    X = self$X[i,,drop = FALSE] %>% torch_tensor()
    Y = self$Y[i] %>% torch_tensor()
    list(X, Y)
  },
  .length = function(){
    nrow(self$X)
  })

train_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), 
                      batch_size = 32, shuffle = TRUE)
test_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), 
                      batch_size = 32)

model_torch$train()

opt = optim_adam(model_torch$parameters, 0.01)

train_losses = c()
test_losses = c()
for(epoch in 1:50){
  train_loss = c()
  test_loss = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      loss$backward()
      opt$step()
      train_loss = c(train_loss, loss$item())
    }
  )
  
  coro::loop(
    for(batch in test_dl){
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      test_loss = c(test_loss, loss$item())
    }
  )

  train_losses = c(train_losses, mean(train_loss))
  test_losses = c(test_losses, mean(test_loss))
  if(!epoch%%10) cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(train_loss)))
}

matplot(cbind(train_losses, test_losses), type = "o", pch = c(15, 16),
        col = c("darkblue", "darkred"), lty = 1, xlab = "Epoch",
        ylab = "Loss", las = 1)
legend("topright", bty = "n", col = c("darkblue", "darkred"),
       lty = 1, pch = c(15, 16), legend = c("Train loss", "Val loss") )
```


:::

::: {.panel}
[Cito]{.panel-name}

```{r chunk_chapter5_4_cito}
library(cito)
data_train = data.frame(target = apply(Y, 1, which.max)-1, X)
model_cito = dnn(
  target~., 
  data = data_train,
  loss = "binomial",
  validation = 0.4,
  hidden = rep(50L, 3L),
  activation = rep("relu", 3),
)
```

:::

:::::

The validation loss first decreases but then starts increasing again, can you explain this behavior?
$\rightarrow$ Overfitting!

Let's try an $L1+L2$ regularization:


::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}

```{r chunk_chapter5_5}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X),
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = ncol(Y), activation = "softmax",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) 

model %>%
  keras::compile(loss = loss_categorical_crossentropy,
                 keras::optimizer_adamax(learning_rate = 0.001))
summary(model)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4)
plot(model_history)
```

:::

::: {.panel}
[Torch]{.panel-name}

```{r chunk_chapter5_5_torch}
library(torch)
torch_manual_seed(321L)
set.seed(123)

model_torch = nn_sequential(
  nn_linear(ncol(X), 50L),
  nn_relu(),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_linear(50L, 2L)
)

YT = apply(Y, 1, which.max)

dataset_nasa = dataset(
  name = "nasa",
  initialize = function(nasa){
    self$X = nasa$X
    self$Y = nasa$Y
  },
  .getitem = function(i){
    X = self$X[i,,drop = FALSE] %>% torch_tensor()
    Y = self$Y[i] %>% torch_tensor()
    list(X, Y)
  },
  .length = function(){
    nrow(self$X)
  })

train_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), 
                      batch_size = 32, shuffle = TRUE)
test_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), 
                      batch_size = 32)

model_torch$train()

opt = optim_adam(model_torch$parameters, 0.01)

train_losses = c()
test_losses = c()
for(epoch in 1:50){
  train_loss = c()
  test_loss = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      l1 = 0
      for(p in model_torch$parameters) l1 = l1 + torch_norm(p, 1)
      l2 = 0
      for(p in model_torch$parameters) l2 = l2 + torch_norm(p, 2)
      loss = loss + 0.001*l1 + 0.001*l2
      loss$backward()
      opt$step()
      train_loss = c(train_loss, loss$item())
    }
  )
  
  coro::loop(
    for(batch in test_dl){
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      test_loss = c(test_loss, loss$item())
    }
  )

  train_losses = c(train_losses, mean(train_loss))
  test_losses = c(test_losses, mean(test_loss))
  if(!epoch%%10) cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(train_loss)))
}

matplot(cbind(train_losses, test_losses), type = "o", pch = c(15, 16),
        col = c("darkblue", "darkred"), lty = 1, xlab = "Epoch",
        ylab = "Loss", las = 1)
legend("topright", bty = "n", col = c("darkblue", "darkred"),
       lty = 1, pch = c(15, 16), legend = c("Train loss", "Val loss") )
```


:::

::: {.panel}
[Cito]{.panel-name}

```{r chunk_chapter5_5_cito}
library(cito)
data_train = data.frame(target = apply(Y, 1, which.max)-1, X)
model_cito = dnn(
  target~., 
  data = data_train,
  loss = "binomial",
  validation = 0.4,
  hidden = rep(50L, 3L),
  activation = rep("relu", 3),
  lambda = 0.001,
  alpha = 0.5
)
```


:::

:::::

Better, but the validation loss still starts increasing after 40 epochs. We can use early stopping to end the training before the validation loss starts increasing again!

::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}


```{r chunk_chapter5_6}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X),
              kernel_regularizer = regularizer_l1_l2( 0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = ncol(Y), activation = "softmax",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) 

model %>%
  keras::compile(loss = loss_categorical_crossentropy,
                 keras::optimizer_adamax(learning_rate = 0.001))
summary(model)

early = keras::callback_early_stopping(patience = 5L)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4, callbacks = c(early))

plot(model_history)
```



:::

::: {.panel}
[Torch]{.panel-name}

```{r chunk_chapter5_6_torch}
library(torch)
torch_manual_seed(321L)
set.seed(123)

model_torch = nn_sequential(
  nn_linear(ncol(X), 50L),
  nn_relu(),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_linear(50L, 2L)
)

YT = apply(Y, 1, which.max)

dataset_nasa = dataset(
  name = "nasa",
  initialize = function(nasa){
    self$X = nasa$X
    self$Y = nasa$Y
  },
  .getitem = function(i){
    X = self$X[i,,drop = FALSE] %>% torch_tensor()
    Y = self$Y[i] %>% torch_tensor()
    list(X, Y)
  },
  .length = function(){
    nrow(self$X)
  })

train_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), 
                      batch_size = 32, shuffle = TRUE)
test_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), 
                      batch_size = 32)

model_torch$train()

opt = optim_adam(model_torch$parameters, 0.01)

train_losses = c()
test_losses = c()
early_epoch = 0
min_loss = Inf
patience = 5
for(epoch in 1:50){
  if(early_epoch >= patience){ break }
  train_loss = c()
  test_loss = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      l1 = 0
      for(p in model_torch$parameters) l1 = l1 + torch_norm(p, 1)
      l2 = 0
      for(p in model_torch$parameters) l2 = l2 + torch_norm(p, 2)
      loss = loss + 0.001*l1 + 0.001*l2
      loss$backward()
      opt$step()
      train_loss = c(train_loss, loss$item())
    }
  )
  
  coro::loop(
    for(batch in test_dl){
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      test_loss = c(test_loss, loss$item())
    }
  )
  
    ### Early stopping ###
  if(mean(test_loss) < min_loss){
    min_loss = mean(test_loss)
    early_epoch = 0
  } else {
    early_epoch = early_epoch + 1
  }
  ###

  train_losses = c(train_losses, mean(train_loss))
  test_losses = c(test_losses, mean(test_loss))
  if(!epoch%%5) cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(train_loss)))
}

matplot(cbind(train_losses, test_losses), type = "o", pch = c(15, 16),
        col = c("darkblue", "darkred"), lty = 1, xlab = "Epoch",
        ylab = "Loss", las = 1)
legend("topright", bty = "n", col = c("darkblue", "darkred"),
       lty = 1, pch = c(15, 16), legend = c("Train loss", "Val loss") )
```


:::

::: {.panel}
[Cito]{.panel-name}

```{r chunk_chapter5_6_cito}
library(cito)
data_train = data.frame(target = apply(Y, 1, which.max)-1, X)
model_cito = dnn(
  target~., 
  data = data_train,
  loss = "binomial",
  validation = 0.4,
  hidden = rep(50L, 3L),
  activation = rep("relu", 3),
  lambda = 0.001,
  alpha = 0.5,
  early_stopping = 5.
)
```


:::

:::::



Patience is the number of epochs to wait before aborting the training. 

**Dropout - another type of regularization**

@dropout suggests a dropout rate of 50% for internal hidden layers and 20% for the input layer. One advantage of dropout is that the training is more independent of the number of epochs i.e. the validation loss usually doesn't start to increase after several epochs. 


::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}


```{r chunk_chapter5_7}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X)) %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dropout(0.5) %>%
  layer_dense(units = ncol(Y), activation = "softmax") 

model %>%
  keras::compile(loss = loss_categorical_crossentropy,
                 keras::optimizer_adamax(learning_rate = 0.001))

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4)

plot(model_history)
```

Of course, you can still combine early stopping and dropout, which is normally a good idea since it improves training efficiency (e.g. you could start with 1000 epochs and you know training will be aborted if it doesn't improve anymore).

:::

::: {.panel}
[Torch]{.panel-name}

```{r chunk_chapter5_8}
library(torch)
torch_manual_seed(321L)
set.seed(123)

model_torch = nn_sequential(
  nn_dropout(0.2),
  nn_linear(ncol(X), 50L),
  nn_relu(),
  nn_dropout(0.5),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_dropout(0.5),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_dropout(0.5),
  nn_linear(50L, 2L)
)

YT = apply(Y, 1, which.max)

dataset_nasa = dataset(
  name = "nasa",
  initialize = function(nasa){
    self$X = nasa$X
    self$Y = nasa$Y
  },
  .getitem = function(i){
    X = self$X[i,,drop = FALSE] %>% torch_tensor()
    Y = self$Y[i] %>% torch_tensor()
    list(X, Y)
  },
  .length = function(){
    nrow(self$X)
  })

train_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), 
                      batch_size = 32, shuffle = TRUE)
test_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), 
                      batch_size = 32)

model_torch$train()

opt = optim_adam(model_torch$parameters, 0.01)

train_losses = c()
test_losses = c()
early_epoch = 0
min_loss = Inf
patience = 5
for(epoch in 1:50){
  if(early_epoch >= patience){ break }
  
  train_loss = c()
  test_loss = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      loss$backward()
      opt$step()
      train_loss = c(train_loss, loss$item())
    }
  )
  
  coro::loop(
    for(batch in test_dl){
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      test_loss = c(test_loss, loss$item())
    }
  )
  
  ### Early stopping ###
  if(mean(test_loss) < min_loss){
    min_loss = mean(test_loss)
    early_epoch = 0
  } else {
    early_epoch = early_epoch + 1
  }
  ###
  
  train_losses = c(train_losses, mean(train_loss))
  test_losses = c(test_losses, mean(test_loss))
  cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(train_loss)))
}

model_torch$eval() # to turn off dropout

matplot(cbind(train_losses, test_losses), type = "o", pch = c(15, 16),
        col = c("darkblue", "darkred"), lty = 1, xlab = "Epoch",
        ylab = "Loss", las = 1)
legend("topright", bty = "n", col = c("darkblue", "darkred"),
       lty = 1, pch = c(15, 16), legend = c("Train loss", "Val loss") )
```

:::

::: {.panel}
[Cito]{.panel-name}

```{r chunk_chapter5_7_cito}
library(cito)
data_train = data.frame(target = apply(Y, 1, which.max)-1, X)
model_cito = dnn(
  target~., 
  data = data_train,
  loss = "binomial",
  validation = 0.4,
  hidden = rep(50L, 3L),
  activation = rep("relu", 3),
  dropout = 0.5,
  early_stopping = 5.
)
```


:::

:::::


### Exercises
```{=html}
  <hr/>
  <strong><span style="color: #0011AA; font-size:18px;">1. Task</span></strong><br/>
```

In this section, we will fit a DNN on the NASA data set from kaggle (available via EcoData, see data description via help) by using a 10-CV. You can start immediately, or get inspiration by reading section above ("Case study: dropout and early stopping in a deep neural network").

A basic network for this problem follows:

Data preparation:

```{r chunk_chapter5_task_0, message=FALSE, warning=FALSE}
library(tidyverse)
library(missRanger)
library(Metrics)
library(EcoData)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

data("nasa")
data = nasa

data$subset = ifelse(is.na(data$Hazardous), "test", "train")

## Explore and clean data.
str(data)
summary(data)

# Remove Equinox and other features.
data = data %>% select(-Equinox, -Orbiting.Body,
                       -Orbit.Determination.Date, -Close.Approach.Date)

# Impute missing values using a random forest.
imputed = data %>%
  select(-Hazardous) %>%
    missRanger::missRanger(maxiter = 5L, num.trees = 20L)

# See the usual function call:
# data_impute = data %>% select(-Hazardous)
# imputed = missRanger::missRanger(data_impute, maxiter = 5L, num.trees = 20L)

# Scale data.
data = cbind(
  data %>% select(Hazardous),
  scale(imputed %>% select(-subset)),
  data.frame("subset" = data$subset)
)

## Outer split.
train = data[data$subset == "train", ]
test = data[data$subset == "test", ]

train = train %>% select(-subset)
test = test %>% select(-subset)

## 10-fold cross-validation:
len = nrow(train)
ord = sample.int(len)
k = 10
cv_indices = lapply(0:9, function(i) sort(ord[(i*len/k + 1):((i+1)*len/k)]))

result = matrix(NA, 10L, 2L)
colnames(result) = c("train_auc", "test_auc")

```


::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}

```{r chunk_chapter5_task_0b, message=FALSE, warning=FALSE}

for(i in 1:10) {
  indices = cv_indices[[i]]
  sub_train = train[-indices,] # Leave one "bucket" out.
  sub_test = train[indices,]
  
  # "Deep" neural networks and regularization:
  model = keras_model_sequential()
  model %>%
  layer_dense(units = 100L, activation = "relu",
             input_shape = ncol(sub_train) - 1L) %>%
  layer_dense(units = 100L, activation = "relu") %>%
  layer_dense(units = 1L, activation = "sigmoid")
  
  model %>%
  keras::compile(loss = loss_binary_crossentropy, 
                 optimizer = optimizer_adamax(0.01))
  
  model_history = 
   model %>%
     fit(
       x = as.matrix(sub_train %>% select(-Hazardous)),
       y = as.matrix(sub_train %>% select(Hazardous)),
       validation_split = 0.2,
       epochs = 35L, batch = 50L, shuffle = TRUE
      )
  
  plot(model_history)
  
  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))
  pred_test = predict(model, as.matrix(sub_test %>% select(-Hazardous)))
  
  # AUC: Area under the (ROC) curve, this is a performance measure [0, 1].
  # 0.5 is worst for a binary classifier.
  # 1 perfectly classifies all samples, 0 perfectly misclassifies all samples.
  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)
  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)
}
print(result)
(colMeans(result))

# The model setup seems to be fine.

## Train and predict for outer validation split (on the complete training data):
model = keras_model_sequential()
model %>%
  layer_dense(units = 100L, activation = "relu",
              input_shape = ncol(sub_train) - 1L) %>%
  layer_dense(units = 100L, activation = "relu") %>%
  layer_dense(units = 1L, activation = "sigmoid")

model %>%
  keras::compile(loss = loss_binary_crossentropy, 
                 optimizer = optimizer_adamax(0.01))

model_history = 
  model %>%
    fit(
      x = as.matrix(train %>% select(-Hazardous)),
      y = as.matrix(train %>% select(Hazardous)),
      validation_split = 0.2,
      epochs = 35L, batch = 50L, shuffle = TRUE
    )

pred = round(predict(model, as.matrix(test[,-1])))

write.csv(data.frame(y = pred), file = "submission_DNN.csv")
```

:::

::: {.panel}
[Torch]{.panel-name}

```{r chunk_chapter5_task_0b_torch, message=FALSE, warning=FALSE}

library(torch)
torch_dataset = torch::dataset(
    name = "data",
    initialize = function(X,Y) {
      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())
      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())
    },
    .getitem = function(index) {
      x = self$X[index,]
      y = self$Y[index,]
      list(x, y)
    },
    .length = function() {
      self$Y$size()[[1]]
    }
  )



for(i in 1:10) {
  indices = cv_indices[[i]]
  sub_train = train[-indices,] # Leave one "bucket" out.
  sub_test = train[indices,]
  
  # "Deep" neural networks and regularization:
  
  model_torch = nn_sequential(
    nn_linear(in_features = ncol(sub_train)-1, out_features = 50L,  bias = TRUE),
    nn_relu(),
    nn_linear(in_features = 50L, out_features = 50L,  bias = TRUE),
    nn_relu(),
    nn_linear(in_features = 50L, out_features = 1L,  bias = TRUE),
    nn_sigmoid()
  )
  
  opt = optim_adam(params = model_torch$parameters, lr = 0.1)
  
  indices = sample.int(nrow(sub_train), 0.8*nrow(sub_train))
  dataset = torch_dataset((as.matrix(sub_train %>% select(-Hazardous)))[indices, ],
                          (as.matrix(sub_train %>% select(Hazardous)))[indices, ,drop=FALSE])
  dataset_val = torch_dataset((as.matrix(sub_train %>% select(-Hazardous)))[-indices, ],
                              (as.matrix(sub_train %>% select(Hazardous)))[-indices, ,drop=FALSE])
  dataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)
  dataloader_val = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)
  
  epochs = 50L
  train_losses = c()
  val_losses = c()
  lambda = torch_tensor(0.01)
  alpha = torch_tensor(0.2)
  for(epoch in 1:epochs){
    train_loss = c()
    val_loss = c()
    coro::loop(
      for(batch in dataloader) { 
        opt$zero_grad()
        pred = model_torch(batch[[1]])
        loss = nnf_binary_cross_entropy( pred , batch[[2]])
        for(p in model_torch$parameters) {
          if(length(dim(p)) > 1) loss = loss + lambda*((1-alpha)*torch_norm(p, 1L) + alpha*torch_norm(p, 2L))
        }
        loss$backward()
        opt$step()
        train_loss = c(train_loss, loss$item())
      }
    )
    ## Calculate validation loss ##
    coro::loop(
      for(batch in dataloader_val) { 
        pred = model_torch(batch[[1]])
        loss = nnf_binary_cross_entropy(pred, batch[[2]])
        val_loss = c(val_loss, loss$item())
      }
    )
    
    train_losses = c(train_losses, mean(train_loss))
    val_losses = c(val_losses, mean(val_loss))
    if(!epoch%%10) cat(sprintf("Loss at epoch %d: %3f  Val loss: %3f\n", epoch, mean(train_loss), mean(val_loss)))
  }
  
  matplot(cbind(train_losses, val_losses), type = "o", pch = c(15, 16),
          col = c("darkblue", "darkred"), lty = 1, xlab = "Epoch",
          ylab = "Loss", las = 1)
  legend("topright", bty = "n", 
         legend = c("Train loss", "Val loss"), 
         pch = c(15, 16), col = c("darkblue", "darkred"))
  
  
  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))
  pred_train = model_torch(torch_tensor(as.matrix(sub_train %>% select(-Hazardous)))) %>% as.numeric
  pred_test = model_torch(torch_tensor(as.matrix(sub_test %>% select(-Hazardous)))) %>% as.numeric
  
  # AUC: Area under the (ROC) curve, this is a performance measure [0, 1].
  # 0.5 is worst for a binary classifier.
  # 1 perfectly classifies all samples, 0 perfectly misclassifies all samples.
  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)
  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)
}
print(result)
(colMeans(result))

# The model setup seems to be fine.

## Train and predict for outer validation split (on the complete training data):
model_torch = nn_sequential(
  nn_linear(in_features = ncol(sub_train)-1, out_features = 50L,  bias = TRUE),
  nn_relu(),
  nn_linear(in_features = 50L, out_features = 50L,  bias = TRUE),
  nn_relu(),
  nn_linear(in_features = 50L, out_features = 1L,  bias = TRUE),
  nn_sigmoid()
)
  
opt = optim_adam(params = model_torch$parameters, lr = 0.1)
dataset = torch_dataset((as.matrix(train %>% select(-Hazardous)))[indices, ],
                        (as.matrix(train %>% select(Hazardous)))[indices, ,drop=FALSE])
dataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)

epochs = 50L
train_losses = c()
lambda = torch_tensor(0.01)
alpha = torch_tensor(0.2)
for(epoch in 1:epochs){
  train_loss = c()
  coro::loop(
    for(batch in dataloader) { 
      opt$zero_grad()
      pred = model_torch(batch[[1]])
      loss = nnf_binary_cross_entropy( pred , batch[[2]])
      for(p in model_torch$parameters) {
        if(length(dim(p)) > 1) loss = loss + lambda*((1-alpha)*torch_norm(p, 1L) + alpha*torch_norm(p, 2L))
      }
      loss$backward()
      opt$step()
      train_loss = c(train_loss, loss$item())
    }
  )
  
  train_losses = c(train_losses, mean(train_loss))
  if(!epoch%%10) cat(sprintf("Loss at epoch %d: %3f \n", epoch, mean(train_loss)))
}

plot(train_losses, type = "o", pch = 15,
        col = c("darkblue"), lty = 1, xlab = "Epoch",
        ylab = "Loss", las = 1)
  


pred = round( model_torch(torch_tensor(as.matrix(test[,-1]))) %>% as.numeric )

write.csv(data.frame(y = pred), file = "submission_DNN.csv")
```


:::

::: {.panel}
[Cito]{.panel-name}

```{r chunk_chapter5_task_0cito, message=FALSE, warning=FALSE}
library(cito)

for(i in 1:10) {
  indices = cv_indices[[i]]
  sub_train = train[-indices,] # Leave one "bucket" out.
  sub_test = train[indices,]
  
  model_cito = dnn(Hazardous~., data = sub_train,validation = 0.2, hidden = rep(100L, 2L), activation = rep("relu", 2), loss = "binomial")

  pred_train = predict(model_cito, sub_train)
  pred_test = predict(model_cito, sub_test)
  
  # AUC: Area under the (ROC) curve, this is a performance measure [0, 1].
  # 0.5 is worst for a binary classifier.
  # 1 perfectly classifies all samples, 0 perfectly misclassifies all samples.
  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)
  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)
}
print(result)
(colMeans(result))

# The model setup seems to be fine.

## Train and predict for outer validation split (on the complete training data):
model_cito = dnn(Hazardous~., data = train, hidden = rep(100L, 2L), activation = rep("relu", 2), loss = "binomial")
test$Hazardous = 1 # rows with NA wills be removed
pred = round(predict(model_cito, newdata= test))

write.csv(data.frame(y = pred), file = "submission_DNN.csv")
```


:::

:::::


Go trough the code line by line and try to understand it. Especially have a focus on the general machine learning workflow (remember the general steps), the new type of imputation and the hand coded 10-fold cross-validation (the for loop).

```{=html}
  <hr/>
  <strong><span style="color: #0011AA; font-size:18px;">2. Task</span></strong><br/>
```

Use early stopping and dropout (i.e. use these options, we explained in the book now) within the same algorithm from above and compare them.
Try to tune the network (play around with the number of layers, the width of the layers, dropout layers, early stopping and other regularization and so on) to make better predicitons (monitor the the training and validation loss).

In the end, submit predictions to the <a href="http://rhsbio7.uni-regensburg.de:8500/" target="_blank" rel="noopener">submission server</a> (if you have time you can also transfer your new knowledge to the titanic data set)!

```{=html}
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:18px;">Solution</span></strong>
    </summary>
    <p>
```

```{r chunk_chapter5_task_1, message=FALSE, warning=FALSE}
library(tensorflow)
library(keras)
library(tidyverse)
library(missRanger)
library(Metrics)
library(EcoData)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

data("nasa")
data = nasa

data$subset = ifelse(is.na(data$Hazardous), "test", "train")
data = data %>% select(-Equinox, -Orbiting.Body,
                       -Orbit.Determination.Date, -Close.Approach.Date)
imputed = data %>%
  select(-Hazardous) %>%
  missRanger::missRanger(maxiter = 5L, num.trees = 20L)

data = cbind(
  data %>% select(Hazardous),
  scale(imputed %>% select(-subset)),
  data.frame("subset" = data$subset)
)

train = data[data$subset == "train", ]
test = data[data$subset == "test", ]

train = train %>% select(-subset)
test = test %>% select(-subset)

len = nrow(train)
ord = sample.int(len)
k = 10
cv_indices = lapply(0:9, function(i) sort(ord[(i*len/k + 1):((i+1)*len/k)]))

result = matrix(NA, 10L, 2L)
colnames(result) = c("train_auc", "test_auc")

for(i in 1:10) {
  indices = cv_indices[[i]]
  sub_train = train[-indices,]
  sub_test = train[indices,]
  
  # "Deep" neural networks and regularization:
  model = keras_model_sequential()
  model %>%
  layer_dense(units = 100L, activation = "relu",
             input_shape = ncol(sub_train) - 1L) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 100L, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1L, activation = "sigmoid")
  
  early_stopping = callback_early_stopping(monitor = "val_loss", patience = 5L)
  # You need a validation split for this!
  
  model %>%
  keras::compile(loss = loss_binary_crossentropy, 
                 optimizer = optimizer_adamax(0.01))
  
  model_history = 
   model %>%
     fit(
       x = as.matrix(sub_train %>% select(-Hazardous)),
       y = as.matrix(sub_train %>% select(Hazardous)),
       callbacks = c(early_stopping),
       validation_split = 0.2,
       epochs = 35L, batch = 50L, shuffle = TRUE
      )
  
  plot(model_history)
  
  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))
  pred_test = predict(model, as.matrix(sub_test %>% select(-Hazardous)))
  
  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)
  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)
}
print(result)
(colMeans(result))

# The model setup seems to be fine.

## Train and predict for outer validation split (on the complete training data):
model = keras_model_sequential()
model %>%
  layer_dense(units = 100L, activation = "relu",
              input_shape = ncol(sub_train) - 1L) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 100L, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1L, activation = "sigmoid")

early_stopping = callback_early_stopping(monitor = "val_loss", patience = 5L)
# You need a validation split for this!

model %>%
  keras::compile(loss = loss_binary_crossentropy, 
                 optimizer = optimizer_adamax(0.01))

model_history = 
  model %>%
    fit(
      x = as.matrix(train %>% select(-Hazardous)),
      y = as.matrix(train %>% select(Hazardous)),
      callbacks = c(early_stopping),
      validation_split = 0.2,
      epochs = 35L, batch = 50L, shuffle = TRUE
    )

pred = round(predict(model, as.matrix(test[,-1])))

write.csv(data.frame(y = pred), file = "submission_DNN_dropout_early.csv")
```

```{=html}
    </p>
  </details>
  <br/><hr/>
```

Better predictions:

```{r chunk_chapter5_task_2, message=FALSE, warning=FALSE}
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

result = matrix(NA, 10L, 2L)
colnames(result) = c("train_auc", "test_auc")

for(i in 1:10) {
  indices = cv_indices[[i]]
  sub_train = train[-indices,]
  sub_test = train[indices,]
  
  # "Deep" neural networks and regularization:
  model = keras_model_sequential()
  model %>%
  layer_dense(units = 100L, activation = "relu",
             input_shape = ncol(sub_train) - 1L) %>%
  layer_dropout(rate = 0.45) %>%
  layer_dense(units = 50L, activation = "gelu",
              kernel_regularizer = regularizer_l1(.00125),
              bias_regularizer = regularizer_l1_l2(.25)
              ) %>%
  layer_dropout(rate = 0.35) %>%
  layer_dense(units = 30L, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1L, activation = "sigmoid")
  
  early_stopping = callback_early_stopping(monitor = "val_loss", patience = 8L)
  # You need a validation split for this!
  
  model %>%
  keras::compile(loss = loss_binary_crossentropy, 
                 optimizer = optimizer_adamax(0.0072))
  
  model_history = 
   model %>%
     fit(
       x = as.matrix(sub_train %>% select(-Hazardous)),
       y = as.matrix(sub_train %>% select(Hazardous)), 
       callbacks = c(early_stopping),
       validation_split = 0.2,
       epochs = 50L, batch = 50L, shuffle = TRUE
      )
  
  plot(model_history)
  
  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))
  pred_test = predict(model, as.matrix(sub_test %>% select(-Hazardous)))
  
  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)
  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)
}
print(result)
(colMeans(result))


model = keras_model_sequential()
  model %>%
  layer_dense(units = 100L, activation = "relu",
             input_shape = ncol(sub_train) - 1L) %>%
  layer_dropout(rate = 0.45) %>%
  layer_dense(units = 50L, activation = "gelu",
              kernel_regularizer = regularizer_l1(.00125),
              bias_regularizer = regularizer_l1_l2(.25)
              ) %>%
  layer_dropout(rate = 0.35) %>%
  layer_dense(units = 30L, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1L, activation = "sigmoid")
  
  early_stopping = callback_early_stopping(monitor = "val_loss", patience = 8L)
  # You need a validation split for this!
  
  model %>%
  keras::compile(loss = loss_binary_crossentropy, 
                 optimizer = optimizer_adamax(0.0072))
  
  model_history = 
   model %>%
     fit(
        x = as.matrix(train %>% select(-Hazardous)),
        y = as.matrix(train %>% select(Hazardous)),
        callbacks = c(early_stopping),
        validation_split = 0.2,
        epochs = 50L, batch = 50L, shuffle = TRUE
      )

pred = round(predict(model, as.matrix(test[,-1])))

write.csv(data.frame(y = pred), file = "submission_DNN_optimal.csv")
```



## Convolutional Neural Networks (CNNs)

The main purpose of convolutional neural networks is image recognition. (Sound can be understood as an image as well!)
In a convolutional neural network, we have at least one convolution layer, additional to the normal, fully connected deep neural network layers. 

Neurons in a convolution layer are connected only to a small spatially contiguous area of the input layer (_receptive field_). We use this structure (_feature map_) to scan the **entire** features / neurons (e.g. picture). Think of the feature map as a _kernel_ or _filter_ (or imagine a sliding window with weighted pixels) that is used to scan the image. As the name is already indicating, this operation is a convolution in mathematics.
The kernel weights are optimized, but we use the same weights across the entire input neurons (_shared weights_).

The resulting (hidden) convolutional layer after training is called a _feature map_. You can think of the feature map as a map that shows you where the “shapes” expressed by the kernel appear in the input. One kernel / feature map will not be enough, we typically have many shapes that we want to recognize. Thus, the input layer is typically connected to several feature maps, which can be aggregated and followed by a second layer of feature maps, and so on.

You get one convolution map/layer for each kernel of one convolutional layer.



### Example MNIST

We will show the use of convolutional neural networks with the MNIST data set. This data set is maybe one of the most famous image data sets. It consists of 60,000 handwritten digits from 0-9.

To do so, we define a few helper functions:

```{r chunk_chapter5_9}
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

rotate = function(x){ t(apply(x, 2, rev)) }

imgPlot = function(img, title = ""){
  col = grey.colors(255)
  image(rotate(img), col = col, xlab = "", ylab = "", axes = FALSE,
     main = paste0("Label: ", as.character(title)))
}
```

The MNIST data set is so famous that there is an automatic download function in Keras:

```{r chunk_chapter5_10}
data = dataset_mnist()
train = data$train
test = data$test
```

Let's visualize a few digits:

```{r chunk_chapter5_11}
oldpar = par(mfrow = c(1, 3))
.n = sapply(1:3, function(x) imgPlot(train$x[x,,], train$y[x]))
par(oldpar)
```

Similar to the normal machine learning workflow, we have to scale the pixels (from 0-255) to the range of $[0, 1]$ and one hot encode the response. For scaling the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2D array/tensor is typically called a matrix.

```{r chunk_chapter5_12}
train_x = array(train$x/255, c(dim(train$x), 1))
test_x = array(test$x/255, c(dim(test$x), 1))
train_y = to_categorical(train$y, 10)
test_y = to_categorical(test$y, 10)
```

The last dimension denotes the number of channels in the image. In our case we have only one channel because the images are black and white.

Most times, we would have at least 3 color channels, for example RGB (red, green, blue) or HSV (hue, saturation, value), sometimes with several additional dimensions like transparency.

To build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size $2\times2$. These are 2D kernels because our images are 2D. For movies for example, one would use 3D kernels (the third dimension would correspond to time and not to the color channels).

::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}


```{r chunk_chapter5_13}
model = keras_model_sequential()
model %>%
 layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,
               kernel_size = c(2L, 2L), activation = "relu") %>%
 layer_max_pooling_2d() %>%
 layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>%
 layer_max_pooling_2d() %>%
 layer_flatten() %>%
 layer_dense(100L, activation = "relu") %>%
 layer_dense(10L, activation = "softmax")
summary(model)
```

:::

::: {.panel}
[Torch]{.panel-name}

  
    Prepare/download data:

```{r chunk_chapter5_14}
library(torch)
library(torchvision)
torch_manual_seed(321L)
set.seed(123)

train_ds = mnist_dataset(
  ".",
  download = TRUE,
  train = TRUE,
  transform = transform_to_tensor
)

test_ds = mnist_dataset(
  ".",
  download = TRUE,
  train = FALSE,
  transform = transform_to_tensor
)
```

Build dataloader:

```{r chunk_chapter5_15}
train_dl = dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 32)
first_batch = train_dl$.iter()
df = first_batch$.next()

df$x$size()
```

Build convolutional neural network:
We have here to calculate the shapes of our layers on our own:

**We start with our input of shape (batch_size, 1, 28, 28)**

```{r chunk_chapter5_16}
sample = df$x
sample$size()
```

**First convolutional layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2)**

```{r chunk_chapter5_17}
conv1 = nn_conv2d(1, 16L, 2L, stride = 1L)
(sample %>% conv1)$size()
```

Output: batch_size = 32,  number of feature maps = 16, dimensions of each feature map = $(27 , 27)$
Wit a kernel size of two and stride = 1 we will lose one pixel in each dimension...
Questions: 

* What happens if we increase the stride?
* What happens if we increase the kernel size?

**Pooling layer summarizes each feature map**

```{r chunk_chapter5_18}
(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()
```

kernel_size = 2L and stride = 2L halfs the pixel dimensions of our image.

**Fully connected layer**

Now we have to flatten our final output of the convolutional neural network model to use a normal fully connected layer, but to do so we have to calculate the number of inputs for the fully connected layer:

```{r chunk_chapter5_19}
dims = (sample %>% conv1 %>%
          nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()
# Without the batch size of course.
final = prod(dims[-1]) 
print(final)
fc = nn_linear(final, 10L)
(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L)
  %>% torch_flatten(start_dim = 2L) %>% fc)$size()
```

Build the network:

```{r chunk_chapter5_20, eval=FALSE}
net = nn_module(
  "mnist",
  initialize = function(){
    self$conv1 = nn_conv2d(1, 16L, 2L)
    self$conv2 = nn_conv2d(16L, 16L, 3L)
    self$fc1 = nn_linear(400L, 100L)
    self$fc2 = nn_linear(100L, 10L)
  },
  forward = function(x){
    x %>%
      self$conv1() %>%
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$conv2() %>%
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      torch_flatten(start_dim = 2) %>%
      self$fc1() %>%
      nnf_relu() %>%
      self$fc2()
  }
)
```


:::

:::::

We additionally used a pooling layer for downsizing the resulting feature maps.
Without further specification, a $2\times2$ pooling layer is taken automatically.
Pooling layers take the input feature map and divide it into (in our case) parts of $2\times2$ size. Then the respective pooling operation is executed.
For every input map/layer, you get one (downsized) output map/layer.

As we are using the max pooling layer (there are sever other methods like the mean pooling), only the maximum value of these 4 parts is taken and forwarded further.
Example input:

```{}
1   2   |   5   8   |   3   6
6   5   |   2   4   |   8   1
------------------------------
9   4   |   3   7   |   2   5
0   3   |   2   7   |   4   9
```

We use max pooling for every field:

```{}
max(1, 2, 6, 5)   |   max(5, 8, 2, 4)   |   max(3, 6, 8, 1)
-----------------------------------------------------------
max(9, 4, 0, 3)   |   max(3, 7, 2, 7)   |   max(2, 5, 4, 9)
```

So the resulting pooled information is:

```{}
6   |   8   |   8
------------------
9   |   7   |   9
```

In this example, a $4\times6$ layer was transformed to a $2\times3$ layer and thus downsized.
This is similar to the biological process called _lateral inhibition_ where active neurons inhibit the activity of neighboring neurons.
It's a loss of information but often very useful for aggregating information and prevent overfitting.

After another convolutional and pooling layer we flatten the output. That means the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all weights from the last feature maps). You can imagine that like reshaping a matrix (2D) to a simple 1D vector. Then the full vector is used.
Having flattened the layer, we can simply use our typical output layer.


::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}

The rest is as usual: First we compile the model.

```{r chunk_chapter5_21}
model %>%
  keras::compile(
      optimizer = keras::optimizer_adamax(0.01),
      loss = loss_categorical_crossentropy
  )
summary(model)
```

Then, we train the model:

```{r chunk_chapter5_22, eval=FALSE}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

epochs = 5L
batch_size = 32L
model %>%
  fit(
    x = train_x, 
    y = train_y,
    epochs = epochs,
    batch_size = batch_size,
    shuffle = TRUE,
    validation_split = 0.2
  )
```

:::

::: {.panel}
[Torch]{.panel-name}

Train model:

```{r chunk_chapter5_23, eval=FALSE}
library(torch)
torch_manual_seed(321L)
set.seed(123)

model_torch = net()
opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:3){
  losses = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]])
      loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
      loss$backward()
      opt$step()
      losses = c(losses, loss$item())
    }
  )
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}
```

Evaluation:

```{r chunk_chapter5_24, eval=FALSE}
model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(
  for(batch in test_dl){
    output = model_torch(batch[[1]])
    labels = batch[[2]]
    loss = nnf_cross_entropy(output, labels)
    test_losses = c(test_losses, loss$item())
    predicted = torch_max(output$data(), dim = 2)[[2]]
    total = total + labels$size(1)
    correct = correct + (predicted == labels)$sum()$item()
  }
)

mean(test_losses)
test_accuracy =  correct/total
test_accuracy
```

:::

:::::

### Example CIFAR

CIFAR10 is another famous dataset. It consists of ten classes with colored images (see https://www.cs.toronto.edu/~kriz/cifar.html).

```{r chunk_chapter5_24_cifar, eval=FALSE}
library(keras)
data = keras::dataset_cifar10()
train = data$train
test = data$test
image = train$x[1,,,]
image %>% 
 image_to_array() %>%
 `/`(., 255) %>%
 as.raster() %>%
 plot()
## normalize pixel to 0-1
train_x = array(train$x/255, c(dim(train$x)))
test_x = array(test$x/255, c(dim(test$x)))
train_y = to_categorical(train$y, 10)
test_y = to_categorical(test$y, 10)
model = keras_model_sequential()
model %>% 
 layer_conv_2d(input_shape = c(32L, 32L,3L),filters = 16L, kernel_size = c(2L,2L), activation = "relu") %>% 
 layer_max_pooling_2d() %>% 
 layer_dropout(0.3) %>% 
 layer_conv_2d(filters = 16L, kernel_size = c(3L,3L), activation = "relu") %>% 
 layer_max_pooling_2d() %>% 
 layer_flatten() %>% 
 layer_dense(10, activation = "softmax")
summary(model)
model %>% 
 compile(
 optimizer = optimizer_adamax(),
 loss = loss_categorical_crossentropy
 )
early = callback_early_stopping(patience = 5L)
epochs = 1L
batch_size =20L
model %>% fit(
 x = train_x, 
 y = train_y,
 epochs = epochs,
 batch_size = batch_size,
 shuffle = TRUE,
 validation_split = 0.2,
 callbacks = c(early)
)
```



### Exercise

```{=html}
  <hr/>
  <strong><span style="color: #0011AA; font-size:18px;">Task</span></strong><br/>
```

The next exercise is on the flower data set in the Ecodata package.

Follow the steps, we did above and build your own convolutional neural network.

In the end, submit your predictions to the submission server. If you have extra time, have a look at kaggle and find the flower data set challenge for specific architectures tailored for this data set.

```{=html}
  <details>
    <summary>
      <strong><span style="color: #0011AA; font-size:18px;">Solution</span></strong>
    </summary>
    <p>
```

The following code shows different behavior in the context of data augmentation and model complexity.

The topic of overfitting can be seen cleary: Compare the simple model and its performance on the training and the test data. Then compare the more complex or even the regularized models and their performance on training and test data.

You see that the very simple models tend to overfit.


```{r message=FALSE, warning=FALSE, include=TRUE}
  # Prepare training and test sets:

  ## Outer split -> dataset$train and dataset$test, dataset$labels are the labels for dataset$train
  dataset = EcoData::dataset_flower()

  ## Inner split:
  train = dataset$train/255
  indicesTrain = sample.int(nrow(train), 0.8 * nrow(train))
  test = train[-indicesTrain,,,]
  train = train[indicesTrain,,,]
  
  labelsTrain = dataset$labels
  labelsTest = labelsTrain[-indicesTrain]
  labelsTrain = labelsTrain[indicesTrain]
```



```{r chunk_chapter5_task_3, message=FALSE, warning=FALSE, include=FALSE}
library(tensorflow)
library(keras)

flowerCNN = function(
  networkSize = c("small", "medium", "big")[3],
  useGenerator = c(1, 2, NA)[3],
  batch_size = 25L,
  epochs = 10L,
  learning_rate = 0.01,
  percentageTrain = 0.9
){
  gc(FALSE) # Clean up system (use garbage collection).
  set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.
  
  ###############
  # Prepare training and test sets:
  
  train = EcoData::dataset_flower()$train/255
  indicesTrain = sample.int(nrow(train), percentageTrain * nrow(train))
  test = train[-indicesTrain,,,]
  train = train[indicesTrain,,,]
  
  labelsTrain = EcoData::dataset_flower()$labels
  labelsTest = labelsTrain[-indicesTrain]
  labelsTrain = labelsTrain[indicesTrain]
  
  
  ###############
  # Models:
  
  model = keras_model_sequential()
  
  if(networkSize == "small"){
    modelString = "small model"
    
    model %>% 
      layer_conv_2d(filters = 4L, kernel_size = 2L,
                    input_shape = list(80L, 80L, 3L)) %>% 
      layer_max_pooling_2d() %>% 
      layer_flatten() %>% 
      layer_dense(units = 5L, activation = "softmax")
    
  }else if(networkSize == "medium"){
    modelString = "medium model"
    
    model %>%
      layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L),
                    input_shape = c(80L, 80L, 3L), activation = "relu") %>%
      layer_max_pooling_2d() %>%
      layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L),
                    activation = "relu") %>%
      layer_max_pooling_2d() %>%
      layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),
                    strides = c(2L, 2L), activation = "relu") %>%
      layer_max_pooling_2d() %>%
      layer_flatten() %>%
      layer_dropout(0.5) %>%
      layer_dense(units = 5L, activation = "softmax")
    
  } else if(networkSize == "big"){
    modelString = "big model"
    
    model %>%
      layer_conv_2d(filter = 48L, kernel_size = c(5L, 5L),
                    input_shape = c(80L, 80L, 3L), activation = "leaky_relu") %>%
      layer_max_pooling_2d() %>%
      layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L),
                    activation = "leaky_relu") %>%
      layer_max_pooling_2d() %>%
      layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),
                    strides = c(2L, 2L), activation = "leaky_relu") %>%
      layer_max_pooling_2d() %>%
      layer_flatten() %>%
      layer_dropout(0.35) %>%
      layer_dense(units = 256L, activation = "relu",
                  bias_regularizer = regularizer_l2(.25)
      ) %>%
      layer_dropout(0.4) %>%
      layer_dense(units = 128L, activation = "leaky_relu") %>%
      layer_dropout(0.4) %>%
      layer_dense(units = 64L, activation = "leaky_relu") %>%
      layer_dropout(0.4) %>%
      layer_dense(units = 5L, activation = "softmax")
  }


  ###############
  # Generators for augmentation:
  
  if(!is.na(useGenerator)){
    if(useGenerator == 1){
      generatorString = "generator 1"
      
      generator = keras::flow_images_from_data(
        x = train,
        y = k_one_hot(labelsTrain, num_classes = 5L),
        generator = keras::image_data_generator(
          rotation_range = 180,
          zoom_range = c(0.3),
          horizontal_flip = TRUE,
          vertical_flip = TRUE,
          samplewise_center = TRUE,
          samplewise_std_normalization = TRUE),
        batch_size = batch_size,
        shuffle = TRUE
      )
      
    }else if(useGenerator == 2){
      generatorString = "generator 2"
      
      generator = keras::flow_images_from_data(
        x = train,
        y = keras::k_one_hot(labelsTrain, 5L),
        batch_size = batch_size
      )
    }
  }else{ generatorString = "no generator" }
  
  
  ###############
  # Model selection and compilation:

  model %>%
    keras::compile(loss = loss_categorical_crossentropy,
                   optimizer = keras::optimizer_adamax(learning_rate = learning_rate))
  
  if(is.na(useGenerator)){ # Use no generator.
    model %>%
      fit(x = train, y = to_categorical(matrix(labelsTrain, ncol = 1L), 5L),
          epochs = epochs, batch_size = batch_size, shuffle = TRUE)
  }else{
    model %>%
      fit(generator, epochs = epochs, batch_size = batch_size, shuffle = TRUE)
  }
  
  
  ###############
  # Predictions:
  
  cat(paste0("\nModalities: ", modelString, ", ", generatorString, "\n"))
  
  # Predictions on the training set:
  predTrain = predict(model, train) %>% apply(1, which.max) - 1
  cat(paste0("\nAccuracy on training set: ",
               round(Metrics::accuracy(predTrain, labelsTrain), 2), "\n"))
  print(round(table(predTrain) / nrow(train), 2))
  
  # Predictions on the test set:
  predTest = predict(model, test) %>% apply(1, which.max) - 1
  cat(paste0("\nAccuracy on test set: ",
               round(Metrics::accuracy(predTest, labelsTest), 2), "\n"))
  print(round(table(predTest) / nrow(test), 2))
  
  # Predictions on the holdout for submission:
  predHoldout = predict(model, EcoData::dataset_flower()$test/255) %>%
    apply(1, which.max) - 1
  
  return(predHoldout)
}


for(networkSize in c("small", "medium", "big")){
  for(useGenerator in c(1, 2, NA)){
    pred = flowerCNN(networkSize = networkSize, useGenerator = useGenerator)
  }
}
```

<!-- **Even more complex model:** -->

<!-- The following snippet offers a solution for data generation with oversampling and undersampling, because the distribution of classes is not equal in the flower data set. -->

```{r chunk_chapter5_task_4, message=FALSE, warning=FALSE, include=FALSE}
getData = function(oversample = TRUE, undersample = FALSE){
  # "undersample" has priority over "oversample".
  
  # As the whole task is very compute-intensive and needs much memory,
  # pack data acquisition in a function.
  # The used local memory is cleaned automatically at the end of the scope.
  
  data = EcoData::dataset_flower()
  # <<-: Global scope.
  trainLocal = data$train/255
  labelsLocal = data$labels
  
  
  print(table(labelsLocal)) # The classes are not equally distributed.
  # Many models tend to predict class 1 overproportionally often.
  
  if(undersample){
    n = min(table(labelsLocal))
    
    # Minimal size of classes times number of classes.
    total = n * length(levels(as.factor(labelsLocal)))
    newIndices = rep(FALSE, total)
    
    for(i in 1:length(levels(as.factor(labelsLocal)))){
      newIndices[sample(which(labelsLocal == i - 1), n, replace = FALSE)] = TRUE
    }
    
    newIndices = which(newIndices)
    
    trainingSet <- trainLocal[newIndices,,,]
    flowerLabels <- labelsLocal[newIndices]
    
    print(table(flowerLabels))
    return(list(trainingSet, flowerLabels, trainLocal))
  }
  
  if(!oversample){
    trainingSet <- trainLocal
    flowerLabels <- labelsLocal
    return(list(trainingSet, flowerLabels, trainLocal))
  }
  
  n = round(max(table(labelsLocal)) + 14) # Number of samples to extend each class to.
  
  ## Sample new data (with replacement):
  for(i in 1:length(levels(as.factor(labelsLocal)))){
    missing = n - table(labelsLocal)[i]  # Number of elements missing compared to n.
    indices = which(labelsLocal == i - 1)  # Indices of all elements of class i.
    newIndices = sample(indices, missing, replace = TRUE)
    
    trainLocal <- abind::abind(trainLocal, trainLocal[newIndices,,,], along = 1)
    # As only new indices are added, there is no confusion with using the old ones.
    
    labelsLocal = c(labelsLocal, rep(as.integer(i - 1), missing))
  }
  
  trainingSet <- trainLocal
  flowerLabels <- labelsLocal
  
  print(table(flowerLabels))
  
  return(list(trainingSet, flowerLabels, trainLocal))
}
```

<!-- Read in for example the following way: -->

```{r chunk_chapter5_task_5, message=FALSE, warning=FALSE, eval=FALSE, purl=FALSE, include=FALSE}
if(!exists("done")){  # Do not calculate this more often than 1 time.
  trainingSet = c()
  flowerLabels = c()
  data = getData(oversample = FALSE, undersample = FALSE)
  trainingSet = data[[1]]
  flowerLabels = data[[2]]
  trainLocal = data[[3]]
  done = 1
}
```

<!-- <h2>Be careful, this exercise uses A LOT OF MEMORY!! If you have a SSD, you might want to turn pagefile / swap off. If you don't have at least 8 GB RAM, don't try to run this exercise, it won't work.</h2> -->

<!-- To avoid crashes of your system, you might want to do some memory management, like: -->

<!-- * After model training, unload the training set and load the test set. -->
<!-- * Generally remove data that is used no longer. -->
<!-- * Lazy load new images (not shown here) out of a generator (later shown in section \@ref(gan), but with manually written training loop). -->

```{r chunk_chapter5_task_6, message=FALSE, warning=FALSE, include=FALSE}
library(keras)
library(tensorflow)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

if(!exists("doneWithTest")){  # Do not calculate this more often than 1 time.
  trainingSet = c()
  flowerLabels = c()
  data = getData(oversample = FALSE, undersample = FALSE)
  trainingSet = data[[1]]
  flowerLabels = data[[2]]
  trainLocal = data[[3]]
  doneWithTest = 1
}

model = keras_model_sequential()
model %>%
  layer_conv_2d(filter = 48L, kernel_size = c(4L, 4L),
                input_shape = c(80L, 80L, 3L), activation = "relu") %>% 
  layer_max_pooling_2d() %>%
  layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L), activation = "elu") %>% 
  layer_max_pooling_2d() %>%
  layer_conv_2d(filter = 64L, kernel_size = c(2L, 2L), activation = "gelu") %>% 
  layer_max_pooling_2d() %>%
  layer_flatten() %>%
  layer_dropout(0.33) %>%
  layer_dense(units = 750L, activation = "gelu",
              bias_regularizer = regularizer_l1(.75),
              kernel_regularizer = regularizer_l2(.0055)
  ) %>%
  layer_dropout(0.4) %>%
  layer_dense(units = 175L, activation = "gelu") %>%
  layer_dropout(0.35) %>%
  layer_dense(units = 75L, activation = "relu") %>%
  layer_dense(units = 50L, activation = "gelu",
              bias_regularizer = regularizer_l2(.75),
              kernel_regularizer = regularizer_l1(.0055)
  ) %>%
  layer_dropout(0.3) %>% 
  layer_dense(units = 5L, activation = "softmax")

model %>%
  keras::compile(loss = loss_categorical_crossentropy,
                 optimizer = optimizer_adamax(learning_rate = 0.011))

model %>%
  fit(x = trainingSet, y = to_categorical(matrix(flowerLabels, ncol = 1L), 5L),
      epochs = 50L, batch_size = 100L, shuffle = TRUE, validation_split = 0.2)

pred = model %>% predict(trainingSet)
pred_classes = apply(pred, 1, which.max)
print(table(pred_classes))

Metrics::accuracy(pred_classes - 1L, flowerLabels)

#pred_classes = model %>% predict(EcoData::dataset_flower()$test/255) %>%
#  apply(1, which.max) - 1L  # Do not forget "/255" and  "- 1L"!!

#write.csv(pred_classes, file = "flower_CNN.csv")
```

<!-- As you can see, the network works in principle (76% accuracy for training data). Mind, that this is not a binary classification problem and we are expecting roughly 20% accuracy by chance. -->

<!-- Just a little hint: More complex networks are not always better. This won't be shown explicitly (as it is very computing-intensive). You can try for example 64 filter kernels per layer or copy one of the convolutional layers (including pooling layer) to see what happens. -->

<!-- Now, we are training without holdouts to get the most power. -->

```{r chunk_chapter5_task_7, message=FALSE, warning=FALSE, include=FALSE}
library(keras)
library(tensorflow)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

if(!exists("doneWithoutTest")){  # Do not calculate this more often than 1 time.
  trainingSet = c()
  flowerLabels = c()
  data = getData(oversample = FALSE, undersample = FALSE)
  trainingSet = data[[1]]
  flowerLabels = data[[2]]
  trainLocal = data[[3]]
  doneWithoutTest = 1
}

model = keras_model_sequential()
model %>% 
  layer_conv_2d(filter = 48L, kernel_size = c(4L, 4L),
                input_shape = c(80L, 80L, 3L), activation = "relu") %>% 
  layer_max_pooling_2d() %>%
  layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L), activation = "elu") %>% 
  layer_max_pooling_2d() %>%
  layer_conv_2d(filter = 64L, kernel_size = c(2L, 2L), activation = "gelu") %>% 
  layer_max_pooling_2d() %>%
  layer_flatten() %>%
  layer_dropout(0.33) %>%
  layer_dense(units = 750L, activation = "gelu",
              bias_regularizer = regularizer_l1(.75),
              kernel_regularizer = regularizer_l2(.0055)
  ) %>%
  layer_dropout(0.4) %>%
  layer_dense(units = 175L, activation = "gelu") %>%
  layer_dropout(0.35) %>%
  layer_dense(units = 75L, activation = "relu") %>%
  layer_dense(units = 50L, activation = "gelu",
              bias_regularizer = regularizer_l2(.75),
              kernel_regularizer = regularizer_l1(.0055)
  ) %>%
  layer_dropout(0.3) %>% 
  layer_dense(units = 5L, activation = "softmax")

model %>%
  keras::compile(loss = loss_categorical_crossentropy,
                 optimizer = optimizer_adamax(learning_rate = 0.011))

model %>%
  fit(x = trainingSet, y = to_categorical(matrix(flowerLabels, ncol = 1L), 5L),
      epochs = 50L, batch_size = 100L, shuffle = TRUE)

pred_classes = apply(model %>% predict(trainingSet), 1, which.max)
print(table(pred_classes))

Metrics::accuracy(pred_classes - 1L, flowerLabels)

pred_classes = model %>% predict(EcoData::dataset_flower()$test/255) %>%
  apply(1, which.max) - 1L  # Do not forget "/255" and  "- 1L"!!

write.csv(pred_classes, file = "flower_CNN.csv")
```

<!-- Maybe you can find (much?) better networks, that fit already better than 84% on the training data. -->
<!-- Mind, that there are 5 classes. If your model predicts only 3 or 4 of them, is is not surprising, that the accuracy is low. -->

```{=html}
    </p>
  </details>
  <br/><hr/>
```





## Recurrent Neural Networks (RNNs)

Recurrent neural networks are used to model sequential data, i.e. a temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic:

```{r chunk_chapter5_0, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
  '<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/SEnXr6v2ifU"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```


### Case Study: Predicting drought
We will use a subset of the data explained in [this github repository](https://github.com/Epistoteles/predicting-drought)
```{r chunk_chapter5_0_Rnn, message=FALSE, warning=FALSE}
utils::download.file("https://www.dropbox.com/s/bqooarazbvg1g7u/weather_soil.RDS?raw=1", destfile = "weather_soil.RDS")
data = readRDS("weather_soil.RDS")
X = data$train # Features of the last 180 days
dim(X)
# 999 batches of 180 days with 21 features each
Y = data$target
dim(Y)
# 999 batches of 6 week drought predictions

# let's visualize drought over 24 months:
# -> We have to take 16 batches (16*6 = 96 weaks ( = 24 months) )
plot(as.vector(Y[1:16,]), type = "l", xlab = "week", ylab = "Drought")
```



```{r chunk_chapter5_1_Rnn, message=FALSE, warning=FALSE}
library(keras)

holdout = 700:999
X_train = X[-holdout,,]
X_test = X[holdout,,]

Y_train = Y[-holdout,]
Y_test = Y[holdout,]

model = keras_model_sequential()
model %>% 
  layer_rnn(cell = layer_lstm_cell(units = 60L),input_shape = dim(X)[2:3]) %>% 
  layer_dense(units = 6L)

model %>% compile(loss = loss_mean_squared_error, optimizer = optimizer_adamax(learning_rate = 0.01))
  
model %>% fit(x = X_train, y = Y_train, epochs = 30L)

preds = 
  model %>% predict(X_test)


matplot(cbind(as.vector(preds[1:48,]),  
              as.vector(Y_test[1:48,])), 
        col = c("darkblue", "darkred"),
        type = "o", 
        pch = c(15, 16),
        xlab = "week", ylab = "Drought")
legend("topright", bty = "n", 
       col = c("darkblue", "darkred"),
      pch = c(15, 16), 
      legend = c("Prediction", "True Values"))

```





The following code snippet shows you many (technical) things you need for building more complex network structures, even with LSTM cells (the following example doesn't have any functionality, it is just an example for how to process two different inputs in different ways within one network):


::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}


```{r chunk_chapter5_1, message=FALSE, warning=FALSE}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

tf$keras$backend$clear_session()  # Resets especially layer counter.

inputDimension1 = 50L
inputDimension2 = 10L

input1 = layer_input(shape = inputDimension1)
input2 = layer_input(shape = inputDimension2)

modelInput2 = input2 %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = inputDimension2, activation = "gelu")

modelMemory = input1 %>%
  layer_embedding(input_dim = inputDimension1, output_dim = 64L) %>%
  layer_lstm(units = 64L) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 2L, activation = "sigmoid")

modelDeep = input1 %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 64L, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64L, activation = "relu") %>%
  layer_dense(units = 64L, activation = "relu") %>%
  layer_dense(units = 5L, activation = "sigmoid")

modelMain = layer_concatenate(c(modelMemory, modelDeep, modelInput2)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 64L, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64L, activation = "relu") %>%
  layer_dense(units = 2L, activation = "sigmoid")

model = keras_model(
  inputs = c(input1, input2),
  outputs = c(modelMain)  # Use the whole modelMain (resp. its output) as output.
)

summary(model)
# model %>% plot_model()
```

:::

::: {.panel}
[Torch]{.panel-name}

```{r chunk_chapter5_1_torch, message=FALSE, warning=FALSE}
library(torch)

model_torch = nn_module(
  initialize = function(type, inputDimension1 = 50L, inputDimension2 = 10L) {
    self$dim1 = inputDimension1
    self$dim2 = inputDimension2
    self$modelInput2 = nn_sequential(
      nn_dropout(0.5),
      nn_linear(in_features = self$dim2, out_features = self$dim2),
      nn_selu()
    )
    self$modelMemory = nn_sequential(
      nn_embedding(self$dim1, 64),
      nn_lstm(64, 64)
    )
    self$modelMemoryOutput = nn_sequential(
      nn_dropout(0.5),
      nn_linear(64L, 2L),
      nn_sigmoid()
    )
    
    self$modelDeep = nn_sequential(
      nn_dropout(0.5),
      nn_linear(self$dim1, 64L),
      nn_relu(),
      nn_dropout(0.3),
      nn_linear(64, 64),
      nn_relu(),
      nn_linear(64, 64),
      nn_relu(),
      nn_linear(64, 5),
      nn_sigmoid()
    )
    
    self$modelMain = nn_sequential(
      nn_linear(7+self$dim2, 64),
      nn_relu(),
      nn_dropout(0.5),
      nn_linear(64, 64),
      nn_relu(),
      nn_dropout(),
      nn_linear(64, 2),
      nn_sigmoid()
    )
  },
  
  forward = function(x) {
    input1 = x[[1]]
    input2 = x[[2]]
    out2 = self$modelInput2(input2)
    out1 = self$modelMemoryOutput( self$modelMemory(input1)$view(list(dim(input1)[1], -1)) )
    out3 = self$modelDeep(input1)
    out = self$modelMain(torch_cat(list(out1, out2, out3), 2))
    return(out)
  }
  
)

(model_torch())

```


:::

:::::

<!-- ### Natural Language Processing (NLP) -->

<!-- Natural language processing is actually more of a task than a network structure, but in the area of deep learning for natural language processing, particular network structures are used. The following video should give you an idea about what NLP is about. -->

<!-- ```{r chunk_chapter5_2, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F} -->
<!-- cat( -->
<!--   '<iframe width="560" height="315"  -->
<!--   src="https://www.youtube.com/embed/UFtXy0KRxVI" -->
<!--   frameborder="0" allow="accelerometer; autoplay; encrypted-media; -->
<!--   gyroscope; picture-in-picture" allowfullscreen> -->
<!--   </iframe>' -->
<!-- ) -->
<!-- ``` -->

<!-- See also the blog post linked with the Youtube video with accompanying code. Moreover, here is an <a href="https://nlpforhackers.io/keras-intro/" target="_blank" rel="noopener">article</a> that shows how natural language processing works with Keras, however, written in Python. As a challenge, you can take the code and implement it in R. -->




## Advanced Training Techniques 


### Data Augmentation

Having to train a convolutional neural network using very little data is a common problem. Data augmentation helps to artificially increase the number of images.

The idea is that a convolutional neural network learns specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures.

Luckily, it is very easy to use data augmentation in Keras.

To show this, we will use our flower data set. We have to define a generator object (a specific object which infinitely draws samples from our data set). In the generator we can turn on the data augmentation.


::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}

```{r chunk_chapter5_25, eval=FALSE}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

data = EcoData::dataset_flower()
train = data$train/255
labels = data$labels

model = keras_model_sequential()
model %>%
  layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L),
                input_shape = c(80L, 80L, 3L), activation = "relu") %>%
  layer_max_pooling_2d() %>%
  layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L),
                activation = "relu") %>%
  layer_max_pooling_2d() %>%
  layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),
                strides = c(2L, 2L), activation = "relu") %>%
  layer_max_pooling_2d() %>%
  layer_flatten() %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 5L, activation = "softmax")

  
# Data augmentation.
aug = image_data_generator(rotation_range = 90, 
                           zoom_range = c(0.3), 
                           horizontal_flip = TRUE, 
                           vertical_flip = TRUE)

# Data preparation / splitting.
indices = sample.int(nrow(train), 0.1 * nrow(train))
generator = flow_images_from_data(train[-indices,,,],
                                  k_one_hot(labels[-indices], num_classes = 5L),
                                  generator = aug,
                                  batch_size = 25L,
                                  shuffle = TRUE)

test = train[indices,,,]

## Training loop with early stopping:

# As we use an iterator (the generator), validation loss is not applicable.
# An available metric is the normal loss.
early = keras::callback_early_stopping(patience = 2L, monitor = "loss")

model %>%
	keras::compile(loss = loss_categorical_crossentropy,
	               optimizer = keras::optimizer_adamax(learning_rate = 0.01))

model %>%
	fit(generator, epochs = 20L, batch_size = 25L,
	    shuffle = TRUE, callbacks = c(early))

# Predictions on the training set:
pred = predict(model, data$train[-indices,,,]) %>% apply(1, which.max) - 1
Metrics::accuracy(pred, labels[-indices])
table(pred)

# Predictions on the holdout / test set:
pred = predict(model, test) %>% apply(1, which.max) - 1
Metrics::accuracy(pred, labels[indices])
table(pred)

# If you want to predict on the holdout for submission, use:
pred = predict(model, EcoData::dataset_flower()$test/255) %>%
  apply(1, which.max) - 1
table(pred)
```

Using data augmentation we can artificially increase the number of images.

:::

::: {.panel}
[Torch]{.panel-name}

In Torch, we have to change the transform function (but only for the train dataloader):

```{r chunk_chapter5_26, eval=FALSE}
library(torch)
torch_manual_seed(321L)
set.seed(123)

train_transforms = function(img){
  img %>%
    transform_to_tensor() %>%
    transform_random_horizontal_flip(p = 0.3) %>%
    transform_random_resized_crop(size = c(28L, 28L)) %>%
    transform_random_vertical_flip(0.3)
}

train_ds = mnist_dataset(".", download = TRUE, train = TRUE,
                         transform = train_transforms)
test_ds = mnist_dataset(".", download = TRUE, train = FALSE,
                        transform = transform_to_tensor)

train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 100L)

model_torch = net()
opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:1){
  losses = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]])
      loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
      loss$backward()
      opt$step()
      losses = c(losses, loss$item())
    }
  )
  
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}

model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(
  for(batch in test_dl){
    output = model_torch(batch[[1]])
    labels = batch[[2]]
    loss = nnf_cross_entropy(output, labels)
    test_losses = c(test_losses, loss$item())
    predicted = torch_max(output$data(), dim = 2)[[2]]
    total = total + labels$size(1)
    correct = correct + (predicted == labels)$sum()$item()
  }
)

test_accuracy =  correct/total
print(test_accuracy)
```

:::

:::::

### Transfer Learning {#transfer}

Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning.

The main idea of transfer learning is that all the convolutional layers have mainly one task - learning to identify highly correlated neighboring features. This knowledge is then used for new tasks.
The convolutional layers learn structures such as edges in images and only the top layer, the dense layer is the actual classifier of the convolutional neural network for a specific task.
Thus, one could think that we could only train the top layer as classifier. To do so, it will be confronted by sets of different edges/structures and has to decide the label based on these.

Again, this sounds very complicated but it is again quite easy with Keras and Torch.

::::: {.panelset}

::: {.panel}
[Keras]{.panel-name}

We will do this now with the CIFAR10 data set, so we have to prepare the data:

```{r chunk_chapter5_27, eval=TRUE}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

data = keras::dataset_cifar10()
train = data$train
test = data$test

rm(data)

image = train$x[5,,,]
image %>%
  image_to_array() %>%
  `/`(., 255) %>%
  as.raster() %>%
  plot()

train_x = array(train$x/255, c(dim(train$x)))
test_x = array(test$x/255, c(dim(test$x)))
train_y = to_categorical(train$y, 10)
test_y = to_categorical(test$y, 10)

rm(train, test)
```

Keras provides download functions for all famous architectures/convolutional neural network models which are already trained on the imagenet data set (another famous data set). These trained networks come already without their top layer, so we have to set include_top to false and change the input shape.

```{r chunk_chapter5_28, eval=TRUE}
densenet = application_densenet201(include_top = FALSE,
                                   input_shape  = c(32L, 32L, 3L))
```

Now, we will not use a sequential model but just a "keras_model" where we can specify the inputs and outputs. Thereby, the output is our own top layer, but the inputs are the densenet inputs, as these are already pre-trained.

```{r chunk_chapter5_29, eval=TRUE}
model = keras::keras_model(
  inputs = densenet$input,
  outputs = layer_flatten(
    layer_dense(densenet$output, units = 10L, activation = "softmax")
  )
)

# Notice that this snippet just creates one (!) new layer.
# The densenet's inputs are connected with the model's inputs.
# The densenet's outputs are connected with our own layer (with 10 nodes).
# This layer is also the output layer of the model.
```

In the next step we want to freeze all layers except for our own last layer. Freezing means that these are not trained: We do not want to train the complete model, we only want to train the last layer. You can check the number of trainable weights via summary(model).

```{r chunk_chapter5_30, eval=TRUE}
model %>% freeze_weights(to = length(model$layers) - 1)
summary(model)
```

And then the usual training:

```{r chunk_chapter5_31, eval=FALSE}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

model %>%
  keras::compile(loss = loss_categorical_crossentropy, 
                 optimizer = optimizer_adamax())

model %>%
  fit(
    x = train_x, 
    y = train_y,
    epochs = 1L,
    batch_size = 32L,
    shuffle = TRUE,
    validation_split = 0.2
  )
```

We have seen, that transfer learning can easily be done using Keras.

:::

::: {.panel}
[Torch]{.panel-name}

```{r chunk_chapter5_32, eval=FALSE}
library(torchvision)
library(torch)
torch_manual_seed(321L)
set.seed(123)

train_ds = cifar10_dataset(".", download = TRUE, train = TRUE,
                           transform = transform_to_tensor)
test_ds = cifar10_dataset(".", download = TRUE, train = FALSE,
                          transform = transform_to_tensor)

train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 100L)

model_torch = model_resnet18(pretrained = TRUE)

# We will set all model parameters to constant values:
model_torch$parameters %>%
  purrr::walk(function(param) param$requires_grad_(FALSE))

# Let's replace the last layer (last layer is named 'fc') with our own layer:
inFeat = model_torch$fc$in_features
model_torch$fc = nn_linear(inFeat, out_features = 10L)

opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:1){
  losses = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]])
      loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
      loss$backward()
      opt$step()
      losses = c(losses, loss$item())
    }
  )
  
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}

model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(
  for(batch in test_dl){
    output = model_torch(batch[[1]])
    labels = batch[[2]]
    loss = nnf_cross_entropy(output, labels)
    test_losses = c(test_losses, loss$item())
    predicted = torch_max(output$data(), dim = 2)[[2]]
    total = total + labels$size(1)
    correct = correct + (predicted == labels)$sum()$item()
  }
)

test_accuracy =  correct/total
print(test_accuracy)
```

:::

:::::


**Flower data set**

Let's do that with our flower data set:

```{r chunk_chapter5_33, eval=FALSE}
library(keras)
library(tensorflow)
set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed.

data = EcoData::dataset_flower()
train = data$train
test = data$test
labels = data$labels

densenet = keras::application_densenet201(include_top = FALSE,
                                          input_shape = list(80L, 80L, 3L))

keras::freeze_weights(densenet)

model = keras_model(inputs = densenet$input, 
                    outputs = densenet$output %>%
                      layer_flatten() %>%
                      layer_dropout(0.2) %>%
                      layer_dense(units = 200L) %>%
                      layer_dropout(0.2) %>%
                      layer_dense(units = 5L, activation = "softmax"))

# Data augmentation.
aug = image_data_generator(rotation_range = 180, zoom_range = 0.4,
                           width_shift_range = 0.2, height_shift_range = 0.2,
                           vertical_flip = TRUE, horizontal_flip = TRUE,
                           preprocessing_function = imagenet_preprocess_input)

# Data preparation / splitting.
indices = sample.int(nrow(train), 0.1 * nrow(train))
generator = flow_images_from_data(train[-indices,,,]/255,
                                  k_one_hot(labels[-indices], num_classes = 5L), 
                                  batch_size = 25L, shuffle = TRUE,
                                  generator = aug)

test = imagenet_preprocess_input(train[indices,,,])
test_labels = k_one_hot(labels[indices], num_classes = 5L)

## Training loop with early stopping:

# As we use an iterator (the generator), validation loss is not applicable.
# An available metric is the normal loss.
early = keras::callback_early_stopping(patience = 2L, monitor = "loss")

model %>%
	keras::compile(loss = loss_categorical_crossentropy,
	               optimizer = keras::optimizer_rmsprop(learning_rate = 0.0005))

model %>%
	fit(generator, epochs = 8L, batch_size = 45L,
	    shuffle = TRUE, callbacks = c(early))

pred = predict(model, imagenet_preprocess_input(data$test))
pred = apply(pred, 1, which.max) - 1
```


<!-- ### Batch Size and Learning Rate -->

<!-- In this chapter, the influence of batch size and learning rate is explored using the MNIST data set. -->
<!-- If you are more interested in this topic (you should be), read this  -->
<!-- <a href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e" target="_blank" rel="noopener">article</a>. -->


<!-- #### Batch Size -->

<!-- Different batch sizes may massively influence the outcome of a training step. Finding a suitable batch size is a task itself. -->

<!-- As a general rule of thumb: -->

<!-- * The lower the batch size, the longer the calculations take, the less (!) memory is needed and the more accurate the training -->
<!-- * The higher the batch size, the wider the training steps (like with a higher learning rate). -->
<!-- * Changing batch sizes and learning rates is always possible and this might "heal" previous mistakes. Maybe you have to "push" the system out of its local neighborhood. -->
<!-- * It also depends on the respective problem. -->

<!-- ```{r chunk_chapter5_34, eval=TRUE} -->
<!-- library(tensorflow) -->
<!-- library(keras) -->
<!-- set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed. -->

<!-- data = dataset_mnist() -->
<!-- train = data$train -->
<!-- test = data$test -->

<!-- train_x = array(train$x/255, c(dim(train$x), 1)) -->
<!-- test_x = array(test$x/255, c(dim(test$x), 1)) -->
<!-- train_y = to_categorical(train$y, 10) -->

<!-- model = keras_model_sequential() -->
<!-- model %>% -->
<!--   layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, -->
<!--                kernel_size = c(2L, 2L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_flatten() %>% -->
<!--   layer_dense(100L, activation = "relu") %>% -->
<!--   layer_dense(10L, activation = "softmax") -->

<!-- model %>% -->
<!--   keras::compile( -->
<!--       optimizer = keras::optimizer_adamax(learning_rate = 0.01), -->
<!--       loss = loss_categorical_crossentropy -->
<!--   ) -->

<!-- epochs = 5L -->
<!-- batch_size = 32L -->
<!-- model %>% -->
<!--   fit( -->
<!--     x = train_x,  -->
<!--     y = train_y, -->
<!--     epochs = epochs, -->
<!--     batch_size = batch_size, -->
<!--     shuffle = TRUE, -->
<!--     validation_split = 0.2 -->
<!--   ) -->

<!-- pred = model %>% predict(test_x) %>% apply(1, which.max) - 1 -->
<!-- Metrics::accuracy(pred, test$y) # 0.9884 -->
<!-- ``` -->

<!-- **Higher batch size:** -->

<!-- ```{r chunk_chapter5_35, eval=TRUE} -->
<!-- library(tensorflow) -->
<!-- library(keras) -->
<!-- set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed. -->

<!-- data = dataset_mnist() -->
<!-- train = data$train -->
<!-- test = data$test -->

<!-- train_x = array(train$x/255, c(dim(train$x), 1)) -->
<!-- test_x = array(test$x/255, c(dim(test$x), 1)) -->
<!-- train_y = to_categorical(train$y, 10) -->

<!-- model = keras_model_sequential() -->
<!-- model %>% -->
<!--   layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, -->
<!--                kernel_size = c(2L, 2L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_flatten() %>% -->
<!--   layer_dense(100L, activation = "relu") %>% -->
<!--   layer_dense(10L, activation = "softmax") -->

<!-- model %>% -->
<!--   keras::compile( -->
<!--     optimizer = keras::optimizer_adamax(learning_rate = 0.01), -->
<!--     loss = loss_categorical_crossentropy -->
<!--   ) -->

<!-- epochs = 5L -->
<!-- batch_size = 100L -->
<!-- model %>% -->
<!--   fit( -->
<!--     x = train_x,  -->
<!--     y = train_y, -->
<!--     epochs = epochs, -->
<!--     batch_size = batch_size, -->
<!--     shuffle = TRUE, -->
<!--     validation_split = 0.2 -->
<!--   ) -->

<!-- pred = model %>% predict(test_x) %>% apply(1, which.max) - 1 -->
<!-- Metrics::accuracy(pred, test$y) # 0.9864 -->
<!-- ``` -->

<!-- **Lower batch size:** -->

<!-- ```{r chunk_chapter5_36, eval=TRUE} -->
<!-- library(tensorflow) -->
<!-- library(keras) -->
<!-- set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed. -->

<!-- data = dataset_mnist() -->
<!-- train = data$train -->
<!-- test = data$test -->

<!-- train_x = array(train$x/255, c(dim(train$x), 1)) -->
<!-- test_x = array(test$x/255, c(dim(test$x), 1)) -->
<!-- train_y = to_categorical(train$y, 10) -->

<!-- model = keras_model_sequential() -->
<!-- model %>% -->
<!--   layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, -->
<!--                kernel_size = c(2L, 2L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_flatten() %>% -->
<!--   layer_dense(100L, activation = "relu") %>% -->
<!--   layer_dense(10L, activation = "softmax") -->

<!-- model %>% -->
<!--   keras::compile( -->
<!--     optimizer = keras::optimizer_adamax(learning_rate = 0.01), -->
<!--     loss = loss_categorical_crossentropy -->
<!--   ) -->

<!-- epochs = 1L -->
<!-- batch_size = 10L -->
<!-- model %>% -->
<!--   fit( -->
<!--     x = train_x,  -->
<!--     y = train_y, -->
<!--     epochs = epochs, -->
<!--     batch_size = batch_size, -->
<!--     shuffle = TRUE, -->
<!--     validation_split = 0.2 -->
<!--   ) -->

<!-- pred = model %>% predict(test_x) %>% apply(1, which.max) - 1 -->
<!-- Metrics::accuracy(pred, test$y) # 0.9869 -->
<!-- ``` -->

<!-- **Lowest (1) batch size:** -->

<!-- ```{r chunk_chapter5_37, eval=TRUE} -->
<!-- library(tensorflow) -->
<!-- library(keras) -->
<!-- set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed. -->

<!-- data = dataset_mnist() -->
<!-- train = data$train -->
<!-- test = data$test -->

<!-- train_x = array(train$x/255, c(dim(train$x), 1)) -->
<!-- test_x = array(test$x/255, c(dim(test$x), 1)) -->
<!-- train_y = to_categorical(train$y, 10) -->

<!-- model = keras_model_sequential() -->
<!-- model %>% -->
<!--   layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, -->
<!--                kernel_size = c(2L, 2L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_flatten() %>% -->
<!--   layer_dense(100L, activation = "relu") %>% -->
<!--   layer_dense(10L, activation = "softmax") -->

<!-- model %>% -->
<!--   keras::compile( -->
<!--     optimizer = keras::optimizer_adamax(learning_rate = 0.01), -->
<!--     loss = loss_categorical_crossentropy -->
<!--   ) -->

<!-- epochs = 1L -->
<!-- batch_size = 1L -->
<!-- model %>% -->
<!--   fit( -->
<!--     x = train_x,  -->
<!--     y = train_y, -->
<!--     epochs = epochs, -->
<!--     batch_size = batch_size, -->
<!--     shuffle = TRUE, -->
<!--     validation_split = 0.2 -->
<!--   ) -->

<!-- pred = model %>% predict(test_x) %>% apply(1, which.max) - 1 -->
<!-- Metrics::accuracy(pred, test$y) # 0.982 -->
<!-- ``` -->

<!-- **Highest (complete) batch size:** -->

<!-- This might not run, because too much memory is needed. -->

<!-- ```{r chunk_chapter5_38, eval=FALSE, purl=FALSE} -->
<!-- library(tensorflow) -->
<!-- library(keras) -->
<!-- set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed. -->

<!-- data = dataset_mnist() -->
<!-- train = data$train -->
<!-- test = data$test -->

<!-- train_x = array(train$x/255, c(dim(train$x), 1)) -->
<!-- test_x = array(test$x/255, c(dim(test$x), 1)) -->
<!-- train_y = to_categorical(train$y, 10) -->

<!-- model = keras_model_sequential() -->
<!-- model %>% -->
<!--   layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, -->
<!--                kernel_size = c(2L, 2L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_flatten() %>% -->
<!--   layer_dense(100L, activation = "relu") %>% -->
<!--   layer_dense(10L, activation = "softmax") -->

<!-- model %>% -->
<!--   keras::compile( -->
<!--     optimizer = keras::optimizer_adamax(learning_rate = 0.01), -->
<!--     loss = loss_categorical_crossentropy -->
<!--   ) -->

<!-- epochs = 1L -->
<!-- batch_size = nrow(test_x) -->
<!-- model %>% -->
<!--   fit( -->
<!--     x = train_x,  -->
<!--     y = train_y, -->
<!--     epochs = epochs, -->
<!--     batch_size = batch_size, -->
<!--     shuffle = TRUE, -->
<!--     validation_split = 0.2 -->
<!--   ) -->

<!-- pred = model %>% predict(test_x) %>% apply(1, which.max) - 1 -->
<!-- Metrics::accuracy(pred, test$y) # ??? -->
<!-- ``` -->


<!-- #### Learning Rate -->

<!-- Choosing a high learning rate at the beginning may yield acceptable results relatively fast. It would take much more time to get there with a small learning rate. But keeping the learning rate this high may result in jumping over the desired optimal values. So decreasing the learning rate with time might help. -->

<!-- TensorFlow / Keras can manage a changing learning rate. This may also be a periodic function or an increase one. You are not limited to a decreasing learning rate. Defining own learning rates is a bit more complicated than just using the inbuilt Keras functions. If you need a self-made learning rate, you can find some recipe <a href="http://thecooldata.com/2018/12/changing-learning-rate-during-training-on-each-batch-iteration-using-callbacks-in-r-keras/" target="_blank" rel="noopener">here</a>. -->

<!-- **An example of the inbuilt functions for managing learning rates in Keras:** -->

<!-- The function declaration of the adamax optimizer is as follows: -->

<!-- ```{r chunk_chapter5_39, eval=FALSE, purl=FALSE} -->
<!-- optimizer_adamax( -->
<!--   learning_rate = 0.002, -->
<!--   beta_1 = 0.9, -->
<!--   beta_2 = 0.999, -->
<!--   epsilon = NULL, -->
<!--   decay = 0, -->
<!--   clipnorm = NULL, -->
<!--   clipvalue = NULL, -->
<!--   ... -->
<!-- ) -->
<!-- # learning_rate:  float >= 0. Learning rate. -->
<!-- # beta_1:         The exponential decay rate for the 1st moment estimates. -->
<!--                   # float, 0 < beta < 1. Generally close to 1. -->
<!-- # beta_2: 	      The exponential decay rate for the 2nd moment estimates. -->
<!--                   # float, 0 < beta < 1. Generally close to 1. -->
<!-- # epsilon:        float >= 0. Fuzz factor. If NULL, defaults to k_epsilon(). -->
<!-- # decay:	        float >= 0. Learning rate decay over each update. -->
<!-- # clipnorm:       Gradients will be clipped when their L2 norm exceeds this value. -->
<!-- # clipvalue:      Gradients will be clipped when their absolute value exceeds this value. -->
<!-- ``` -->

<!-- You can easily specify a decay this way. Mind interval boundaries and suitable parameter values! -->

<!-- ```{r chunk_chapter5_40, eval=TRUE} -->
<!-- library(tensorflow) -->
<!-- library(keras) -->
<!-- set_random_seed(321L, disable_gpu = FALSE)	# Already sets R's random seed. -->

<!-- data = dataset_mnist() -->
<!-- train = data$train -->
<!-- test = data$test -->

<!-- train_x = array(train$x/255, c(dim(train$x), 1)) -->
<!-- test_x = array(test$x/255, c(dim(test$x), 1)) -->
<!-- train_y = to_categorical(train$y, 10) -->

<!-- model = keras_model_sequential() -->
<!-- model %>% -->
<!--   layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, -->
<!--                kernel_size = c(2L, 2L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>% -->
<!--   layer_max_pooling_2d() %>% -->
<!--   layer_flatten() %>% -->
<!--   layer_dense(100L, activation = "relu") %>% -->
<!--   layer_dense(10L, activation = "softmax") -->

<!-- model %>% -->
<!--   keras::compile( -->
<!--     optimizer = keras::optimizer_adamax(learning_rate = 0.02, decay = 0.002), -->
<!--     loss = loss_categorical_crossentropy -->
<!--   ) -->

<!-- epochs = 1L -->
<!-- batch_size = 32L -->
<!-- model %>% -->
<!--   fit( -->
<!--     x = train_x,  -->
<!--     y = train_y, -->
<!--     epochs = epochs, -->
<!--     batch_size = batch_size, -->
<!--     shuffle = TRUE, -->
<!--     validation_split = 0.2 -->
<!--   ) -->

<!-- pred = model %>% predict(test_x) %>% apply(1, which.max) - 1 -->
<!-- Metrics::accuracy(pred, test$y) # 0.9882 -->
<!-- ``` -->

<!-- Except for the decay in learning rate, this example is identical to the first one concerning the batch size (0.9884). -->


<!-- #### Conclusion -->

<!-- There is a (very) complex interplay of batch size, learning rate and optimization algorithm. This topic is to deep for this course. -->

<!-- Most times, doing a longer training or increasing the batch size rather than decreasing the learning rate is recommended. -->
<!-- But this is a matter of further research and also personal attitude. Of course, it also depends on the respective problem. -->


<!-- ### Caveat About Learning Rates and Activation Functions (already mentioned in the intro) -->

<!-- Depending on activation functions, it might occur that the network won't get updated, even with high learning rates (called *vanishing gradient*, especially for "sigmoid" functions). -->
<!-- Furthermore, updates might overshoot (called *exploding gradients*) or activation functions will result in many zeros (especially for "relu", *dying relu*). -->

