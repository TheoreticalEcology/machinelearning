# Datasets
You can download the datasets we use in the course [here](http://rhsbio7.uni-regensburg.de:8500) (ignore browser warnings) or by installing the EcoData package:

```{r,eval=FALSE}
devtools::install_github(repo = "florianhartig/EcoData", subdir = "EcoData", 
dependencies = TRUE, build_vignettes = FALSE)
```


## Titanic 
The dataset is a collection of titanic passengers with information about their age, class, sex, and their survival status. The competition here is simple: train a ML model and predict the survival probability.

The titanic dataset is very well explored and serves as a stepping stone in many ML careers. For inspiration and data exploration notebooks, check out this kaggle competition: [](https://www.kaggle.com/c/titanic/data)

**Response variable:** 'survived'

A minimal working example:

1. Load dataset
```{r}
load("datasets.RData")
# library(EcoData)
# data(titanic_ml)
# titanic = titanic_ml
summary(titanic)
```

2. Impute missing values (not our response variable!)
```{r}
library(missRanger)
library(dplyr)
titanic_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)
titanic_imputed = missRanger::missRanger(data = titanic_imputed %>% select(-survived))
titanic_imputed$survived = titanic$survived
```

3. Split into training and testing
```{r}
train = titanic_imputed[!is.na(titanic$survived), ]
test = titanic_imputed[is.na(titanic$survived), ]
```

4. Train model
```{r}
model = glm(survived~., data=train, family = binomial())
```

5. Predictions
```{r}
preds = predict(model, data = test, type = "response")
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "glm.csv")
```

And submit the csv on [](http://rhsbio7.uni-regensburg.de:8500)

## Plant-pollinator database
The plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower).
We explored the advantage of ML algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look ![here](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13329).

```{r,echo=FALSE}
knitr::include_graphics(c("images/TM.png"))
```


**Response variable:** 'interaction'

A minimal working example:

1. Load dataset
```{r}
load("datasets.RData")
# library(EcoData)
# data(plantPollinator_df)
# plant_poll = plantPollinator_df
summary(plant_poll)
```

2. Impute missing values (not our response variable!)
We will select only a few predictors here (you can work with all predictors ofc).
```{r}
library(missRanger)
library(dplyr)
plant_poll_imputed = plant_poll %>% select(diameter, corolla, tongue, body, interaction)
plant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>% select(-interaction))
plant_poll_imputed$interaction = plant_poll$interaction
```

3. Split into training and testing
```{r}
train = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]
test = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ]
```

4. Train model
```{r}
model = glm(interaction~., data=train, family = binomial())
```

5. Predictions
```{r}
preds = predict(model, data = test, type = "response")
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "glm.csv")
```

## Wine
The dataset is a collection of wines of different quality. The aim is to predict the quality of the wine based on physochemical predictors. 

For inspiration and data exploration notebooks, check out this kaggle competition: [](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
For instance, check out this very nice [notebook](https://www.kaggle.com/aditimulye/red-wine-quality-assesment-starter-pack) which removes a few problems from the data. 

**Response variable:** 'quality'

We could use theoretically a regression model for this task but we will stick with a classification model

A minimal working example:

1. Load dataset
```{r}
load("datasets.RData")
# library(EcoData)
# data(wine)
summary(wine)
```

2. Impute missing values (not our response variable!)
```{r}
library(missRanger)
library(dplyr)
#wine_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)
wine_imputed = missRanger::missRanger(data = wine %>% select(-quality))
wine_imputed$quality = wine$quality
```

3. Split into training and testing
```{r}
train = wine_imputed[!is.na(wine$quality), ]
test = wine_imputed[is.na(wine$quality), ]
```

4. Train model
```{r}
library(ranger)
rf = ranger(quality~., data = train, classification = TRUE)
```

5. Predictions
```{r}
preds = predict(rf, data = test)$predictions
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "rf.csv")
```

## Nasa
A collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. 
For inspiration and data exploration notebooks, check out the kaggle competition: [](https://www.kaggle.com/shrutimehta/nasa-asteroids-classification)

**Response variable:** 'Hazardous'
1. Load dataset
```{r}
load("datasets.RData")
# library(EcoData)
# data(nasa)
summary(nasa)
```

2. Impute missing values (not our response variable!)
```{r}
library(missRanger)
library(dplyr)
#wine_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)
nasa_imputed = missRanger::missRanger(data = nasa %>% select(-Hazardous), maxiter = 1, num.trees=5L)
nasa_imputed$Hazardous = nasa$Hazardous
```

3. Split into training and testing
```{r}
train = nasa_imputed[!is.na(nasa$Hazardous), ]
test = nasa_imputed[is.na(nasa$Hazardous), ]
```

4. Train model
```{r}
library(ranger)
rf = ranger(Hazardous~., data = train, classification = TRUE, probability = TRUE)
```

5. Predictions
```{r}
preds = predict(rf, data = test)$predictions[,2]
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "rf.csv")
```


## Flower
A collection of over 4000 flower images of 5 plant species. The dataset is from [kaggle](https://www.kaggle.com/alxmamaev/flowers-recognition) but we downsampled the images from $320*240$ to $80*80$ pixels. 
You can download the dataset [here](http://rhsbio7.uni-regensburg.de:8500).

**Notes:**

- check out CNN notebooks on kaggle (they are often written in python but you can still copy the CNN architectures), e.g. [this one](https://www.kaggle.com/alirazaaliqadri/flower-recognition-tensorflow-keras-sequential)
- Last year's winner have used a transfer learning approach (they achieved around 70% accuracy), check out this [notebook](https://www.kaggle.com/stpeteishii/flower-name-classify-densenet201), see also the section about transfer learning \@ref(transfer)


**Response variable:** Plant species

1. Load dataset
The dataset requires pre-processing (we have to concatenate the train and test images):
```{r}
library(keras)
library(stringr)
data_files = list.files("flower/", full.names = TRUE)
train = data_files[str_detect(data_files, "train")]
test = readRDS(file = "flower/test.RDS")
train = lapply(train, readRDS)
train_classes = lapply(train, function(d) dim(d)[1])
train = abind::abind(train, along = 1L)
labels_train = rep(0:4, unlist(train_classes))

# flower = EcoData::dataset_flower()
# train = flower$train
# test = flower$test
# train_classes = flower$labels
```

Let's visualize a flower:
```{r}
train[100,,,] %>% 
 image_to_array() %>%
 `/`(., 255) %>%
 as.raster() %>%
 plot()
```

2. Build & train model:
```{r,eval=FALSE}
model = keras_model_sequential()
model %>% 
  layer_conv_2d(filters = 4L, kernel_size = 2L, input_shape = c(80L, 80L, 3L)) %>% 
  layer_max_pooling_2d() %>% 
  layer_flatten() %>% 
  layer_dense(units = 5L, activation = "softmax")

model %>% 
  compile(optimizer = keras::optimizer_adamax(0.01), loss = keras::loss_categorical_crossentropy)

model %>% 
  fit(epochs = 2L, x = train/255, y = keras::k_one_hot(labels_train, 5L))
```

3. Predictions
```{r,eval=FALSE}
preds = model %>% predict(test/255)
preds = apply(preds, 1, which.max)-1
head(preds)
```

4. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "cnn.csv")
```



