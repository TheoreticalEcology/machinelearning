--- 
title: "Machine Learning and AI in TensorFlow and R"
author: "Maximilian Pichler and Florian Hartig"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    highlight: kate
documentclass: book
bibliography: ["packages.bib", "literature.bib"]
biblio-style: "apalike"
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "Machine Learning and AI in TensorFlow and R"
---

# Prerequisites

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')

knitr::opts_chunk$set(cache=TRUE)
``` 

**R system**

Make sure you have a recent version of R (>=3.6, ideally >=4.0) and RStudio on your computers. 

**Keras and tensorflow**

If you want to run the code on your own laptops, you also will need to install TensorFlow / Keras for R. For this, the following should work for most people:

Run in R: 
```{r, eval=FALSE}
install.packages("keras", dependencies = T)
keras::install_keras()
```

This should work on most computers, in particular of all software is recent. Sometimes, however, things don't work well, in particular the python distribution often makes problems. If the install does not work for you, we can look at it on Monday together. Also, we will provide some virtual machines in case your computers / laptops are too old or you don't manage to install tensorflow.

**Torch for R**

We may also use Torch for R. This is an R frontend for the popular PyTorch framework. To install torch, type in R:

```{r,eval=FALSE}
install.packages("torch")
library(torch)
```

**EcoData**

Finally, we may sometimes use datasets from the EcoData package. To install the package, run:
```{r,eval=FALSE}
devtools::install_github(repo = "florianhartig/EcoData", subdir = "EcoData", 
dependencies = TRUE, build_vignettes = TRUE)
```

<!--chapter:end:index.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---


# Introduction to Machine Learning {#introduction}

There are three basic ML tasks

* Unsupervised learning
* Supervised learning
* Reinforcement learning

**Unsupervised learning** is a technique, where one does not need to supervise the model. Instead, you allow the model to work on its own to discover information.

In **supervised learning**, you train an algorithm using labeled data, which means that you already know the correct answer for a part of the data (the so called tracings data). 

**Reinforcement learning** is a technique that emulates a game-like situation. The algorithm comes up with a solution by try and error and gets for the actions ether rewards or penalties. As in games, the goal is to maximize the rewards. We will talk on the last day more about this technique.

For the moment, we will focus on the first two tasks, supervised and unsupervised learning. To do so, we will first start with a small example, but before you start with the code, here a video to remind you of what we talked about in the class:

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/1AVrWvRvfxs"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```


## Unsupervised learning

In unsupervised learning, we  want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider our iris dataset. Here, we have 150 observations of 4 floral traits

```{r, fig.width=10, fig.height=4}
colors = hcl.colors(3)
traits = as.matrix(iris[,1:4]) 
species = iris$Species
image(y = 1:4, x = 1:length(species) , z = traits, 
      ylab = "Floral trait", xlab = "Individual")
```

The observations are from 3 species, and indeed those species tend to have different traits, meaning that the observations form 3 clusters. 

```{r}
pairs(traits, pch = as.integer(species), col = colors[as.integer(species)])
```

However, imagine we didn't know what species are, which is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does.

### Hierarchical clustering

Build up a hierarchy (tree) between data points

* Agglomerative: start with each data point in their own cluster, merge them up hierarchically
* Divisive: start with all data in one cluster, and split hierarchically

Merges / splits are done according to linkage criterion, which measures distance between (potential) clusters. Cut the tree at a certain height to get clusters. 

Here an example

```{r}
set.seed(123)

d = dist(traits)
hc <- hclust(d, method = "complete")

plot(hc)
rect.hclust(hc, k = 3)
```

Same plot, but with colors for true species identity

```{r}
library(ape)
plot(as.phylo(hc), 
     tip.color = colors[as.integer(species)], 
     direction = "downwards")

hcRes3 <- cutree(hc, k = 3)
```

Calculate confusion matrix - note we switching labels here so that it fits to the species

```{r}
tmp <- hcRes3
tmp[hcRes3 == 2] = 3
tmp[hcRes3 == 3] = 2
hcRes3 <- tmp
table(hcRes3, species)
```

Note that results might change if you choose a different agglomeration method, distance metric, or whether you scale your variables. Compare, e.g. to this example

```{r}
hc <- hclust(d, method = "ward.D2")

plot(as.phylo(hc), 
     tip.color = colors[as.integer(species)], 
     direction = "downwards")

hcRes3 <- cutree(hc, k = 3)
table(hcRes3, species)
```

Which method is best? 

```{r}
library(dendextend)
methods <- c("ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2")
out <- dendlist()
for(i in seq_along(methods)) {
  res <- hclust(d, method = methods[i])   
  out <- dendlist(out, as.dendrogram(res))
}
names(out) <- methods
out

get_ordered_3_clusters <- function(dend) {
  cutree(dend, k = 3)[order.dendrogram(dend)]
}
dend_3_clusters <- lapply(out, get_ordered_3_clusters)
compare_clusters_to_iris <- function(clus) {FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)}
clusters_performance <- sapply(dend_3_clusters, compare_clusters_to_iris)
dotchart(sort(clusters_performance), xlim = c(0.3,1),
         xlab = "Fowlkes-Mallows index",
         main = "Performance of linkage methods \n in detecting the 3 species",
         pch = 19)
```


We might conclude here that ward.D2 works best. However, as we will learn later, optimizing the method without a hold-out for testing means that we may be overfitting. We should check this using cross-validation. 

### k-means clustering

Another example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms.

A cluster refers to a collection of data points aggregated together because of certain similarities. In our example from above this similarities could be similar flowers aggregated together to a plant. 

To start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the imaginary or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere and then adds each new data point to the cluster which minimizes the overall in-cluster sum of squares. After the algorithm has assigned a new data point to a cluster the centroid gets updated. By iterating this procedure for all data points and then starting again, the algorithm can find the optimum centroids and the data-points belonging to this cluster.

The k in K-means refers to the number of clusters and the ‘means’ refers to averaging of the data-points to find the centroids.

A typical pipeline for using kmeans clustering looks the same as for the other algortihms. After having visualized the data, we fit the model, visualize the results and have a look at the performance by use of the confusion matrix.

```{r}
set.seed(123)

kc <- kmeans(traits, 3)
kc
```

Visualizing the results. Color codes true species identity, symbol shows cluster result

```{r}
plot(iris[c("Sepal.Length", "Sepal.Width")], col =  colors[as.integer(species)], pch = kc$cluster)
points(kc$centers[, c("Sepal.Length", "Sepal.Width")], col = colors, pch = 1:3, cex = 3)
```

We see that there are are some discrepancies. Confusion matrix:
```{r}
table(iris$Species, kc$cluster)
```

If you want to animate the clustering process, you could run 

```{r, eval = F}
library(animation)
saveGIF(kmeans.ani(x = traits[,1:2], col = colors), interval = 1, ani.width = 800, ani.height = 800)
```

Ellbow technique to determine the number of clusters

```{r}
getSumSq <- function(k){kmeans(traits, k, nstart=25)$tot.withinss}
iris.kmeans1to10 <- sapply(1:10, getSumSq)
plot(1:10, iris.kmeans1to10, type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```


### Density-based clustering


```{r}
set.seed(123)

library(dbscan)
kNNdistplot(traits, k =  4)
abline(h = 0.4, lty = 2)

# fpc package
dc <- dbscan(traits, eps = 0.4, minPts = 6)
dc

library(factoextra)
fviz_cluster(dc, traits, geom = "point", ggtheme = theme_light())
```

### Model-based clustering

The last class of methods for unsupervised clustering are so-called model-based clustering methods. 

```{r}
library(mclust)
mb = Mclust(traits)
```

Mclust automatically compares a number of candidate models (#clusters, shape) according to BIC. We can look at the selected model via

```{r}
mb$G # two clusters
mb$modelName # > ellipsoidal, equal shape
```

We see that the algorithm prefers to have 2 clusters. For better comparability to the other 2 methods, we will overrule this by setting:

```{r}
mb3 = Mclust(traits, 3)
```

Result in terms of the predicted densities for the 3 clusters

```{r}
plot(mb3, "density")
```

Predicted clusters


```{r}
plot(mb3, what=c("classification"), add = T)
```

Confusion matrix

```{r}
table(iris$Species, mb3$classification)
```

### Ordination 

Note the relationship between clustering and ordination. Here a PCA ordination on on the 

```{r}
pcTraits <- prcomp(traits, center = TRUE,scale. = TRUE)
biplot(pcTraits, xlim = c(-0.25,0.25), ylim = c(-0.25,0.25))
```

You can cluster the results of this ordination, ordinate before clustering, or superimpose one on the other. 


## Supervised learning: regression and classification
The two most prominent branches of supervised learning are regression and classification. Fundamentally, classification is about predicting a label and regression is about predicting a quantity. The following video explains that in more depth:

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/i04Pfrb71vk"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```


### Supervised regression using Random Forest

The random forest (RF) algorithm is possibly the most widely used ML algorithm and can be used for regression and classification. We will talk more about the algorithm on Day 2. 

For the moment, we want to go through typical workflow for a supervised regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. We will again use the iris dataset that we used before. The goal is now to predict Sepal.Length based on the infomration about the other variables (including species). 

Fitting the model
```{r}
library(randomForest)
m1 <- randomForest(Sepal.Length ~ ., data = iris)
# str(m1)
# m1$type
# predict(m1)
print(m1)
```

Visualization of the results
```{r}
par(mfrow = c(1,2))
plot(predict(m1), iris$Sepal.Length, xlab = "predicted", ylab = "observed")
abline(0,1)
varImpPlot(m1)
```
To understand, the structure of a RF in more detail, we can use a package from GitHub

```{r}
# devtools::install_github('araastat/reprtree')
reprtree:::plot.getTree(m1, iris)
```

### Supervised classification using Random Forest

With the RF, we can also do classification. The steps are the same as for regression tasks, but we can additionally, see how well it performed by looking at the so called confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represent the instances in an actual class. Thus the diagonals are the correctly predicted classes and the off-diagnoal elements are the falsly classified elements.

Fitting the model:
```{r}
set.seed(123)
m1 <- randomForest(Species ~ ., data = iris)
# str(m1)
# m1$type
# predict(m1)
print(m1)
```
Visualizing the fitted model:

```{r}
par(mfrow = c(1,2))
reprtree:::plot.getTree(m1, iris)
```

Visualizing results ecologically:
```{r}
oldpar <- par(mfrow = c(1,2))
plot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = "observed")
plot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = "predicted")
```

```{r,echo=FALSE}
par(oldpar)
```


Confusion matrix:
```{r}
table(predict(m1),iris$Species)
```


## Introduction to Tensorflow

All operations in TF are written in C++ and are highly optimized. But dont worry, we don’t have to use C++ to use TF because there are several bindings for other languages. TensorFlow officialy supports a Python API, but meanwhile there are several community carried APIs for other languages:

* R
* Go
* Rust
* Swift
* JavaScript

In this course we will use TF with the https://tensorflow.rstudio.com/ binding, that was developed and published 2017 by the RStudio
Team. They developed first a R package (reticulate) to call python in R. Actually, we are using in R the python TF module (more about this later).
TF offers different levels of API. We could implement a neural network completly by ourselves, or we could use Keras which is provided by TF as a submodule. Keras is a powerful module for building and training neural networks. It allows us to build and train neural networks in a few lines of codes. Since the end of 2018, Keras and TF are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras
for neural networks but also how we can use the TF’s automatic differenation for using complex objective functions.

One of the most commonly used frameworks for machine learning is TensorFlow. TensorFlow is a open source linear algebra library with a focus on neural networks, published by Google in 2015. TF supports several interesting features, im particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization. 

These advantages are nicely explained in the following video: 

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/MotG3XI2qSs"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```

To sum the most important points of the video up: 

* TF is a math library which is highly optimized for neural networks
* If a GPU is available, computations can be easily run on the GPU but even on a CPU is TF still very fast
* The "backend" (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for the GPU)
* The interface (the part of TF that we use) is written in python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend. 

All operations in TF are written in C++ and are highly optimized. But dont worry, we don’t have to use C++ to use TF, because there are several bindings for other languages. Officially, TensorFlow only supports a Python API, but meanwhile there are several community carried APIs for other languages, including R, Go, Rust, Swift or JavaScript. In this book, we will use TF with the https://tensorflow.rstudio.com/ binding that was developed and published 2017 by the RStudio Team. They developed first a R package (reticulate) to call python in R. Actually, we are using in R the python TF module (more about this later).

Useful links:

* [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf) (which is for the python API, but just replace the '.' with '$')
* [Rstudio tensorflow website](https://tensorflow.rstudio.com/)


### Tensorflow data containers
TF has two data containers (structures):

* constant (tf$constant) :creates a constant (immutable) value in the computation graph
* variable (tf$Variable): creates a mutable value in the computation graph (used as parameter/weight in models)

To get started with tensorflow, we have to load the library and check if the installation worked. 

```{r}
library(tensorflow)
# Don't worry about weird messages. TF supports additional optimizations
exists("tf")
```

Don't worry about weird messages (they will only appear once at the start of the session).

We now can define the variables and do some math with them:

```{r}
a = tf$constant(5)
b = tf$constant(10)
print(a)
print(b)
c = tf$add(a, b)
print(c)
tf$print(c)
```

Normal R methods such as print() are provided by the R package "tensorflow". 

The tensorflow library (created by the RStudio team) built R methods for all common operations:

```{r}
`+.tensorflow.tensor` = function(a, b) return(tf$add(a,b))
tf$print(a+b)
```

Their operators also transfrom automatically R numbers into constant tensors when attempting to add a tensor to a R number:

```{r}
d = c + 5  # 5 is automatically converted to a tensor
print(d)
```

TF container are objects, which means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object)
For instance, there is a method to transform the tensor object back to a R object:

```{r}
class(d)
class(d$numpy())
```

### Tensorflow data types - good practise with R-TF
R uses dynamic typing, which means you can assign to a variable a number, character, function or whatever, and the the type is automatically infered.
In other languages you have to state explicitly the type, e.g. in C: int a = 5; float a = 5.0; char a = "a";
While TF tries to infer dynamically the type, often you must state it explicitly.
Common important types: 
- float32 (floating point number with 32bits, "single precision")
- float64 (floating point number with 64bits, "double precision")
- int8 (integer with 8bits)
The reason why TF is so explicit about the types is that many GPUs (e.g. the NVIDIA geforces) can handle only up to 32bit numbers! (you do not need high precision in graphical modeling)

But let us see in practice, what we have to do with these types and how to specifcy them:
```{r,eval=FALSE}
r_matrix = matrix(runif(10*10), 10,10)
m = tf$constant(r_matrix, dtype = "float32") 
b = tf$constant(2.0, dtype = "float64")
c = m / b # doesn't work! we try to divide float32/float64
```

So what went wrong here: we tried to divide a float32 to a float64 number, but, we can only divide numbers of the same type! 
```{r, eval=TRUE}
r_matrix = matrix(runif(10*10), 10,10)
m = tf$constant(r_matrix, dtype = "float64")
b = tf$constant(2.0, dtype = "float64")
c = m / b # now it works
```

We can also specify the type of the object by providing an object e.g. tf$float64.

```{r}
r_matrix = matrix(runif(10*10), 10,10)
m = tf$constant(r_matrix, dtype = tf$float64)
```


Tensorflow arguments often require exact/explicit data types:
TF often expects for arguments integers. In R however an integer is normally saved as float. 
Thus, we have to use a "L" after an integer to tell the R interpreter that it should be treated as an integer:

```{r,eval=FALSE}
is.integer(5)
is.integer(5L)
matrix(t(r_matrix), 5, 20, byrow = TRUE)
tf$reshape(r_matrix, shape = c(5, 20))$numpy()
tf$reshape(r_matrix, shape = c(5L, 20L))$numpy()
```

Skipping the "L" is one of the most common errors when using R-TF!

## Introduction to PyTorch
PyTorch is another famous library for deep learning. As for tensorflow, torch itself is written in c++ but the API in python. Last year, the RStudio team released R-torch, and while r-tensorflow calls the python API in the background, the r-torch API is built directly on the c++ torch library! 

Useful links:

* [PyTorch documentation](https://pytorch.org/docs/stable/index.html) (which is for the python API, bust just replace the '.' with '$')
* [R-torch website](https://torch.mlverse.org/)


### PyTorch data containers
TF has two data containers (structures):

* constant (tf_tensor(...)) :creates a constant (immutable) value in the computation graph
* variable (tf_$Variable_tensor(..., requires_grad=TRUE)): creates a mutable value in the computation graph (used as parameter/weight in models)

To get started with torch, we have to load the library and check if the installation worked. 

```{r}
library(torch)
```

Don't worry about weird messages (they will only appear once at the start of the session).

We now can define the variables and do some math with them:

```{r}
a = torch_tensor(5.)
b = torch_tensor(10.)
print(a)
print(b)
c = a$add( b )
print(c)
```

The r-torch package provides all common methods (an advantage over tensorflow)

```{r}
a = torch_tensor(5.)
b = torch_tensor(10.)
print(a+b)
print(a/b)
print(a*b)
```


Their operators also transfrom automatically R numbers into tensors when attempting to add a tensor to a R number:

```{r}
d = a + 5  # 5 is automatically converted to a tensor
print(d)
```

As for tensorflow, we have to explicitly transform the tensors back to R:

```{r}
class(d)
class(as.numeric(d))
```

### Torch data types - good practise with R-TF
Similar to tensorflow:

```{r,eval=FALSE}
r_matrix = matrix(runif(10*10), 10,10)
m = torch_tensor(r_matrix, dtype = torch_float32()) 
b = torch_tensor(2.0, dtype = torch_float64())
c = m / b 
```
But here's a difference! With tensorfow we would get an error, but with r-torch, m is automatically casted to a double (float64). However, this is still bad practise!

During the course we will try to provide for all keras/tensorflow examples the corresponding pytorch code snippets.


## First steps with the keras framework

We have seen that we can use TF directly from R, and we could use this knowledge to implement a neural network in TF directly from R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us with implementing the machine learning models in TF. The most common of those is Keras.

Keras is a powerful framework for building and training neural networks with a few lines of codes. Since the end of 2018, Keras and TF are completely interoperable, allowing us to utilize the best of both. 

The objective of this lesson is to familiarize yourself with keras. If you have TF installed, Keras can be found within TF: tf.keras. However, the RStudio team has built an R package on top of tf.keras, and it is more convenient to use this. To load the keras package, type

```{r}
library(keras)
```

### Example workflow in keras

To show how keras works, we will now build a small classifier in keras to predict the three species of the iris dataset. Load the necessary packages and datasets:
```{r}
library(keras)
library(tensorflow)
data(iris)
head(iris)
```

It is beneficial for neural networks to scale the predictors (scaling = centering and standardization, see ?scale)
We also split our data into the predictors (X) and the response (Y = the three species).
```{r, cache=TRUE}
X = scale(iris[,1:4])
Y = iris[,5]
```

Additionally, keras/tf cannot handle factors and we have to create contrasts (one-hot encoding):
To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like python and C++ on which TF is built, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Therefore, we have to substract it. 
```{r, cache=TRUE}
Y = to_categorical(as.integer(Y)-1L, 3)
head(Y) # 3 colums, one for each level in the response
```
After having prepared the data, we will now see a typical workflow to specify a model in keras. 

**1. Initiliaze a sequential model in keras:**
```{r, cache=TRUE}
model = keras_model_sequential()
```
A sequential keras model is a higher order type of model within keras and consists of one input and one output model. 


**2. Add hidden layers to the model (we will learn more about hidden layers during the next days).**
When specifiying the hidden layers, we also have to specify a so called activation function and their shape. 
You can think of the activation function as decisive for what is forwarded to the next neuron (but we will learn more about it later). The shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).
```{r, cache=TRUE}
model %>%
  layer_dense(units = 20L, activation = "relu", input_shape = list(4L)) %>%
  layer_dense(units = 20L) %>%
  layer_dense(units = 20L) %>%
  layer_dense(units = 3L, activation = "softmax") 
```
- softmax scales a potential multidimensional vector to the interval (0,1]

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
The torch syntax is very similar, we will give a list of layers to 'nn_sequential' function. Here, we have to specify the softmax activation function as an extra layer:
```{r}
model_torch = 
  nn_sequential(
    nn_linear(4L, 20L),
    nn_linear(20L, 20L),
    nn_linear(20L, 20L),
    nn_linear(20L, 3L),
    nn_softmax(2)
  )
```
</details>
<br/>


**3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).** 

We will leaern about other options later, so for now, do not worry about the "lr" argument, crossentropy or the optimizer.
```{r, cache=TRUE}
model %>%
  compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(lr = 0.001))
summary(model)
```

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
Specify optimizer and the parameters which will be trained (in our case the parameters of the network)
```{r}
optimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.01)
```
</details>
<br/>


**4. Fit model in 30 iterations(epochs)**

```{r, cache=TRUE}
model_history =
  model %>%
    fit(x = X, y = apply(Y,2,as.integer), epochs = 30L, batch_size = 20L, shuffle = TRUE)
```

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
In torch, we jump directly to the training loop, however, here we have to write our own training loop:

1. get a batch of data
2. predict on batch
3. calculate loss between predictions and true labels
4. backpropagate error
5. update weights
6. go to step 1 and repeat
```{r}
# Calculate number of training steps
epochs = 30
batch_size = 20
steps = round(nrow(X)/batch_size*30)

X_torch = torch_tensor(X)
Y_torch = torch_tensor(apply(Y, 1, which.max)) 

# set model into training status
model_torch$train()

log_losses = NULL

# training loop
for(i in 1:steps) {
  # get batch
  indices = sample.int( nrow(X), batch_size)
  
  # reset backpropagation
  optimizer_torch$zero_grad()
  
  # predict and calculate loss
  pred = model_torch(X_torch[indices, ])
  loss = nnf_cross_entropy(pred, Y_torch[indices])
  
  # backprop and weight update
  loss$backward()
  optimizer_torch$step()
  
  log_losses[i] = as.numeric(loss)
}
```
</details>
<br/>

**5. Plot training history:**
```{r, cache=TRUE}
plot(model_history)
```

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
```{r}
plot(log_losses, xlab = "steps", ylab = "loss", las = 1)
```
</details>
<br/>

**6. Create predictions:**
```{r, cache=TRUE}
predictions = predict(model, X) # probabilities for each class
```

We will get probabilites:
```{r, cache=TRUE}
head(predictions) # quasi-probabilities for each species
```

For each plant, we want to know for which species we got the highest probability:
```{r, cache=TRUE}
preds = apply(predictions, 1, which.max) 
print(preds)
```

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
The torch syntax is very similar, we will give a list of layers to 'nn_sequential' function. Here, we have to specify the softmax activation function as an extra layer:
```{r}
model_torch$eval()
preds_torch = model_torch(torch_tensor(X))
preds_torch = apply(preds_torch, 1, which.max) 
print(preds_torch)
mean(preds_torch == as.integer(iris$Species))
```
</details>
<br/>

**7. Calculate Accuracy (how often we have been correct):**

```{r, cache=TRUE}
mean(preds == as.integer(iris$Species))
```

**8. Plot predictions, to see if we have done a good job:**
```{r, cache=TRUE}
oldpar = par()
par(mfrow = c(1,2))
plot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species, main = "Observed")
plot(iris$Sepal.Length, iris$Petal.Length, col = preds, main = "Predicted")
```

So you see, building a neural network is with keras very easy and you can already do it on your own.

<!--chapter:end:01-intro.Rmd-->

# Fundamental principles and techniques {#fund}

## Machine learning principles

### Optimization
from wikipedia: " an optimization problem is the problem of finding the best solution from all feasible solutions"

Why do we need this "optimization"?

We need to somehow tell the algorithm what it should learn. To do so we have the so called loss-function, which expresses what our goal is. But we also need to somewhow find the configurations for which the loss function is 
minimized. This is the job of the optimizer. Thus, an optimization consists of:

- A loss function (e.g. we tell in each training step the algorithm how many observations were miss-classified) guides the training of ML algorithms

- The optimizer, which tries to update the weights of the ML algorithms in a way that the loss function is minimized

Calculating analytically the global optima is a non-trivial problem and thus a bunch of diverse optimization algorithms evolved

Some optimization algorithms are inspired by biological systems e.g. Ants, Bee, or even slime algorithms. These optimizers are explained int the following video, have a look:

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/X-iSQQgOd1A"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```

#### Small optimization example
As an easy example for optimization we can think of a quadratic function:
```{r}
func = function(x) return(x^2)
```

This function is so easy, we can randomly prob it and identify the optimum by plotting

```{r}
a = rnorm(100)
plot(a, func(a))
```

The smallest value is at x = 0 (to be honest, we can calculate this for this simple case analytically)

We can also use an optimizer with the optim-function (the first argument is the starting value)

```{r}
opt = optim(1.0, func)
print(opt$par)
```

opt$par will return the best values found by the optimizer, which is really close to zeor :)


#### Advanced optimization example

Optimization is also done when fitting a linear regression model. Thereby, we optimize the weights (intercept and slope). But using lm (y~x) is too simple, we would like to do this by hand to also better understand what optimization is and how it works.

As an example we take the airquality data set. First, we have to be sure to have no NAs in there. Then we split into response (Ozone) and predictors (Month, Day, Solar.R, Wind, Temp).Additionally it is beneficial for the optimizer, when the different predictors have the same support, and thus we scale them. 


```{r}
data = airquality[complete.cases(airquality$Ozone) & complete.cases(airquality$Solar.R),]
X = scale(data[,-1])
Y = data$Ozone
```

The model we want to optimize: $ozone = Solar.R*X1 + Wind*X2 + Temp*X3 + Month*X4 + Day*X5 + X6$

As the we assume that the residuals are normally distributed, our loss function is the mean squared errors: mean(predicted ozone - true ozone)^2) 

Our task is now to find the parameters X1-X6 for which this loss function is the smallest. Therefore, we implement a function, that takes parameters and returns the loss.

```{r}
linear_regression = function(w) {
  pred = w[1]*X[,1] + # Solar.R
         w[2]*X[,2] + # Wind
         w[3]*X[,3] + # Temp
         w[4]*X[,4] + # Month
         w[5]*X[,5] +
         w[6]         # or X %*% w[1:5] + w[6]
  # loss  = MSE, we want to find the optimal weights 
  # to minimize the sum of squared residuals
  loss = mean((pred - Y)^2)
  return(loss)
}
```

For example we can sample some weights and see what the loss with this weights is.
```{r}
linear_regression(runif(6))
```

We can try to find the optimum bruteforce (which means we will use a random set of weights and see for which the loss function is smallest):

```{r}
random_search = matrix(runif(6*5000,-10,10), 5000, 6)
losses = apply(random_search, 1, linear_regression)
plot(losses, type = "l")
random_search[which.min(losses),]
```

Bruteforce isn't a good approach, it might work well with only a few parameters, but with increasing complexity and more parameters it will take a long time.

In R the optim function helps to get faster to the optimum.
```{r}
opt = optim(runif(6, -1, 1), linear_regression)
opt$par
```

By comparing the weights from the optimizer to the estimated weights of the lm() function, we see that our self-written code obtains the same weights as the lm.

```{r}
coef(lm(Y~X))
```

### Regularization

Regularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a ML model. The most important distinction is between shrinkage estimators and estimators based on model averaging. 

**Shrikage estimators** are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are *"shrunk"* to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penality is proportional to the absolute deviation (L1 penalty), and the Tikhonov regularization aka ridge regression, where the penalty is proportional to the squared distance from the reference (L2 penalty). Thus, the loss function that we optimize is thus given by

$$
loss = fit - \lambda \cdot d
$$
where fit refers to the standard loss function, $\lambda$ is the strength of the regularization, and $d$ is the chosen metrics, e.g. L1 or L2:
$$
loss_{L1} = fit - \lambda \cdot \Vert weights \Vert_1
$$
$$
loss_{L2} = fit - \lambda \cdot \Vert weights \Vert_2
$$
$\lambda$ and possibly d are typically optimized under cross-validation. L1 and L2 can be also combined which is then called elastic net (see @zou2005)

**Model averaging** refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it often more efficient to have many simpler models and average them, than to have one "super model". The reasons are complicated, and explained in more detail in @dormann2018.

A particular important application of averaging is boosting, where the principle is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap the data, and average the boot-strapped predictions.

To see how these techniques work in practice, let's first focus on lasso and ridge regularization for weights in neural networks. We can imagine that the lasso and ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunk) to zero.

Lasso (penalty ~ abs(sum(Weights))) and ridge (penalty ~ (sum(Weights))^2) have slightly different properties, which are best understood if we express those as the effective prior preference that they create on the parameters:

```{r, echo = F}
par(mfrow = c(1,2))
curve(dexp(abs(x)), -5, 5, main = "Lasso prior")
curve(dnorm(abs(x)), -5, 5, main = "Ridge prior")
```

As you can see, the Lasso creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the ridge. For this reason, Lasso is often interpreted more as a model selection method. 

The Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero. 

We can implement the linear regression also in keras, when we do not specify any hidden layers

```{r, cache=TRUE}
library(keras)
data = airquality[complete.cases(airquality),]
X = scale(data[,-1])
Y = data$Ozone
# l1/l2 on linear model
model = keras_model_sequential()
model %>%
 layer_dense(units = 1L, activation = "linear", input_shape = list(dim(X)[2]))
summary(model)
model %>%
 compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5))
model_history =
 model %>%
 fit(x = X, y = Y, epochs = 50L, batch_size = 20L, shuffle = TRUE)
unconstrained = model$get_weights()
summary(lm(Y~X))
coef(lm(Y~X))
```

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
```{r}
library(torch)
model_torch = nn_sequential(
  nn_linear(in_features = dim(X)[2], out_features = 1L)
)
opt = optim_adam(params = model_torch$parameters, lr = 0.5)

X_torch = torch_tensor(X)
Y_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())
for(i in 1:500) {
  indices = sample.int(nrow(X), 20L)
  opt$zero_grad()
  pred = model_torch(X_torch[indices, ])
  loss = nnf_mse_loss(pred, Y_torch[indices,,drop=FALSE])
  loss$sum()$backward()
  opt$step()
}
coef(lm(Y~X))
model_torch$parameters
```
</details>
<br/>


But keras also allows use to use lasso and ridge on the weights. 
Lets see what happens when we put a l1 (lasso) regularization on the weights:
```{r, cache=TRUE}
model = keras_model_sequential()
model %>%
  layer_dense(units = 1L, activation = "linear", input_shape = list(dim(X)[2]), 
              kernel_regularizer = regularizer_l1(10), bias_regularizer = regularizer_l1(10))
summary(model)
model %>%
  compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5), metrics = c(metric_mean_squared_error))
model_history =
  model %>%
  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)
l1 = model$get_weights()
summary(lm(Y~X))
coef(lm(Y~X))
cbind(unlist(l1), unlist(unconstrained))
```

One can clearly see that parameters are pulled towards zero because of the regularization.

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
In torch, we have to specify the regularization on our own when calculating the loss.
```{r}
model_torch = nn_sequential(
  nn_linear(in_features = dim(X)[2], out_features = 1L)
)
opt = optim_adam(params = model_torch$parameters, lr = 0.5)

X_torch = torch_tensor(X)
Y_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())
for(i in 1:500) {
  indices = sample.int(nrow(X), 20L)
  opt$zero_grad()
  pred = model_torch(X_torch[indices, ])
  loss = nnf_mse_loss(pred, Y_torch[indices,,drop=FALSE])
  
  ## Add l1:
  for(i in 1:length(model_torch$parameters)) loss = loss + model_torch$parameters[[i]]$abs()$sum()*10.0
  
  loss$sum()$backward()
  opt$step()
}
coef(lm(Y~X))
model_torch$parameters
```
</details>
<br/>


## Tree-based ML algorithms
Famous ML algorithms such as random Forest and gradient boosted trees are based on classification- and regression trees.

### Classification and Regression Trees
Tree-based models in general use a series of if-then rules to generate predictions from one or more decision trees.
In this lecture, we will explore regression and classifaction trees at the example of the airquality data set. There is one important hyper-parameter for regression trees: minsplit

- it controls the depth of tree (see the help of rpart for a description)
- it controls the complexity of the tree and thus also be seen as a regularization parameter

We first prepare and visualize the data and afterwards fit a decision tree. 

```{r}
library(rpart)
library(rpart.plot)
data=airquality[complete.cases(airquality),]
```

Fit and visualize a regression tree:

```{r}
rt = rpart(Ozone~., data = data,control = rpart.control(minsplit = 10))
rpart.plot(rt)
```

Visualize the predictions:

```{r}
pred = predict(rt, data)
plot(data$Temp, data$Ozone)
lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = "red")
```

The angular form of the prediction line is typical for regression trees and is a weakness of it.



### Random Forest
To overcome this weakness, a random forest uses an ensemble of regression/classification trees. Thus, the random forest is in principle nothing else than a normal regression/classification tree, but it uses the idea of the "wisdom of the crowd": By asking many people (regression/classification trees) one can make a more informed decision (prediction/classification). When you buy a new phone for example you would also no directly go into the shop, but search in the internet and ask your friends and family.  

There are two randomization steps with the RF that are responsible for the success of RF:

- bootstrap sample for each tree (we will sample observations with replacement from the dataset, for the phone this is like that not everyone has experience about each phone)
- at each split, we will sample a subset of predictors which are then considered as potential splitting criterion (for the phone this is like that not everyone has the same decision criteria). 

Applying the random forest follows the same principle as for the methods before: we visualize the data (we have already done this so often for the airquality data set, thus we skip it here), fit the algorithm and then plot the outcomes.

Fit a RF and visualize the predictions:

```{r}
library(randomForest)
rf = randomForest(Ozone~., data = data)
pred = predict(rf, data)
plot(Ozone~Temp, data = data)
lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = "red")
```

One advantage of RF is that we will get a variable importance. At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Thus the variable importance shows us how important a variable is averaged over all trees.

```{r}
rf$importance
```

There are several important hyperparameters in a random forest, that we can tune to get better results:

- Similar to the minsplit parameter in regression and classification trees, the hyper parameter nodesize controls for complexity -> Minimum size of terminal nodes in the tree. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).
- mtry - 	Number of features randomly sampled as candidates at each split.


### Boosted regression trees
RF fits hundreds of trees independent of each other. Here, the idea of a boosted regression tree comes in. Maybe we could learn from the errors the previous weak learners make and thus enhance the performance of the algorithm. 

Thus, a boosted regression tree (BRT) starts with a simple regression tree (weak learner) and then fits sequentially additional trees to improve the results.
There are two different strategies to do so:

- AdaBoost, wrong classified observations (by the previous tree) will get a higher weight and therefore the next trees will focus on difficult/missclassified observations.
- Gradient boosting (state of the art), each sequential model will be fit on the residual errors of the previous model.

We can fit a BRT using xgboost, but before we have to transform the data into a xgb.Dmatrix.
```{r BRT1,cache=TRUE}
library(xgboost)
data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)
brt = xgboost(data_xg, nrounds = 16L, nthreads = 4L)
```

The nrounds controls how many sequantial trees we fit, in our example this was 16. When we predict to new data, we can limit the number of trees used to prevent overfitting (remeber: each new tree tries to improve the predictions of the previous trees). 

Let us visualize the predictions for different number of trees:
```{r BRT2,cache=TRUE}
par(mfrow = c(2,2))
for(i in 1:4){
  pred = predict(brt, newdata = data_xg, ntreelimit = i)
  plot(data$Temp, data$Ozone, main = i)
  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = "red")
}
```
There are also other ways to control for complexity of the BRT algorithm:

- max_depth, depth of each tree
- shrinkage (each tree will get a weight and the weight will decrease with the number of trees)

When having specified the final model, we can as for random forests get a variable importance:

```{r BRT3,cache=TRUE}
xgboost::xgb.importance(model = brt)
sqrt(mean((data$Ozone - pred)^2)) # RMSE
data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)
```

One important strength of xgboost is that we can directly do a cross-validation (which is indepdent on the BRT itself!) and specify its properties with nfold (the original dataset is randomly partitioned intonfoldequal size subsamples and each time one of these data sets is used for predictions to judge the performance):

```{r BRT4,cache=TRUE}
brt = xgboost(data_xg, nrounds = 5L)
brt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L, nrounds = 3L, nthreads = 4L)
print(brt_cv)
```

If we do three-folded CV, we actually fit three different BRT models (xgboost models)

This now tells us how well the model performed.


## Distance-based algorithms
In this chapter, we introduce support-vector machines (SVMs) and other distance-based methods.

### k-nearest-neighbor
K Nearest Neighbour (kNN) is a simple algorithm that stores all the available cases and classifies the new data based on a similarity measure. It is mostly used to classifies a data point based on how its k nearest neighbours are classified.

Let us first see an example:
```{r}
X = scale(iris[,1:4])
Y = iris[,5]
plot(X[-100,1], X[-100,3], col = Y)
points(X[100,1], X[100,3], col = "blue", pch = 18, cex = 1.3)
```

Which class would you decide for the blue point? What are the classes of the nearest points? Well this procedure is used by the kNN and thus there is actually no "real" learning in a kNN.

For applying a kNN, we first have to scale teh data set, because we deal with distances and a priori want the same influence of all predictors (image one variable has values from -10.000 to 10.000 and one from -1 to 1, then the influence of the first variable on the distance to the other points is stronger than the second variable). As in the iris-data set there are no real test, we also have to split the data into train and test. Then we will follow the usual pipeline. 

```{r}
data = iris
data[,1:4] = apply(data[,1:4],2, scale)
indices = sample.int(nrow(data), 0.7*nrow(data))
train = data[indices,]
test = data[-indices,]
```

Fit model and create predictions:

```{r}
library(kknn)
knn = kknn(Species~., train = train, test = test)
summary(knn)
table(test$Species, fitted(knn))
```

 

### Support Vector Machines (SVM)
Support vectors machines have a different approach. They try to divide the predictor space into spaces sectors for each class. To do so a SVM fits the parameters of a hyperplane (a n-1 dimensional subspace in a n-dimensional space) in the predictor space by optimizing the distance between the hyperlane and the nearest point from each class. 

Fitting a SVM:

```{r}
library(e1071)
data = iris
data[,1:4] = apply(data[,1:4],2, scale)
indices = sample.int(nrow(data), 0.7*nrow(data))
train = data[indices,]
test = data[-indices,]

sm = svm(Species~., data = train, kernel = "linear")
pred = predict(sm, newdata = test)
```

```{r}
oldpar = par()
par(mfrow = c(1,2))
plot(test$Sepal.Length, test$Petal.Length, col =  pred, main = "predicted")
plot(test$Sepal.Length, test$Petal.Length, col =  test$Species, main = "observed")
par(oldpar)

mean(pred==test$Species) # accuracy
```


SVM can only work on linear separable problems (A problem is called linearly separable if there exists at least one line in the plane with all of the points of one class on one side of the hyperplane and all the points of the others classes on the other side).

If this is not possible, we however, can use the so called kernel trick, which maps the predictor space into a (higher dimensional) space in which the problem is linear separable. After having identified the boundaries in the higher-dimensional space, we can project them back into the original dimensions. 

```{r, eval=FALSE}
set.seed(42)
x1 = seq(-3, 3, length.out = 100)
x2 = seq(-3, 3, length.out = 100)
X = expand.grid(x1, x2)
y = apply(X, 1, function(x) exp(-x[1]^2 - x[2]^2))
y = ifelse(1/(1+exp(-y)) < 0.62, 0, 1)
image(matrix(y, 100, 100))
animation::saveGIF({
  for (i in c("truth","linear", "radial", "sigmoid")) {
    if(i == "truth"){
      image(matrix(y, 100,100),main = "Ground truth",axes = FALSE, las = 2)
    }else{
      sv = e1071::svm(x = X, y = factor(y), kernel = i)
      image(matrix(as.numeric(as.character(predict(sv, X))), 100,100),main = paste0("Kernel: ", i),axes = FALSE, las = 2)
      axis(1, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1))
      axis(2, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1), las = 2)
    }
  }
},movie.name = "svm.gif", autobrowse = FALSE)
```


```{r,echo=FALSE}
knitr::include_graphics("images/svm.gif")
```

As you have seen this does not work with each kernel. Thus, the problem is to find the actual correct kernel, which is again an optimization procedure and can thus be approximated.

## Artificial neural networks
Now, we will come to artificial neural networks (ANNs), for which the topic of regularization is also important. We can specify the regularization in each layer via the kernel_regularization argument. 

```{r}
library(keras)
data = airquality
summary(data)
data = data[complete.cases(data),] # remove NAs
summary(data)
X = scale(data[,2:6])
Y = data[,1]
model = keras_model_sequential()
penalty = 0.1
model %>%
 layer_dense(units = 100L, activation = "relu", input_shape = list(5L), kernel_regularizer = regularizer_l1(penalty)) %>%
 layer_dense(units = 100L, activation = "relu", kernel_regularizer = regularizer_l1(penalty) ) %>%
 layer_dense(units = 100L, activation = "relu", kernel_regularizer = regularizer_l1(penalty)) %>%
 layer_dense(units = 1L, activation = "linear", kernel_regularizer = regularizer_l1(penalty)) # one output dimension with a linear activation function
summary(model)
model %>%
 compile(loss = loss_mean_squared_error, keras::optimizer_adamax(0.1))
model_history =
 model %>%
 fit(x = X, y = matrix(Y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2)
plot(model_history)
weights = lapply(model$weights, function(w) w$numpy() )
fields::image.plot(weights[[1]])
```

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
Again, we have to do the regularization on our own:
```{r}
model_torch = nn_sequential(
  nn_linear(in_features = dim(X)[2], out_features = 100L),
  nn_relu(),
  nn_linear(100L, 100L),
  nn_relu(),
  nn_linear(100L, 100L),
  nn_relu(),
  nn_linear(100L, 1L),
)
opt = optim_adam(params = model_torch$parameters, lr = 0.1)

X_torch = torch_tensor(X)
Y_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())
for(i in 1:500) {
  indices = sample.int(nrow(X), 20L)
  opt$zero_grad()
  pred = model_torch(X_torch[indices, ])
  loss = nnf_mse_loss(pred, Y_torch[indices,,drop=FALSE])
  
  ## Add l1 (only on the 'kernel weights'):
  for(i in seq(1, 8, by = 2)) loss = loss + model_torch$parameters[[i]]$abs()$sum()*0.1
  
  loss$sum()$backward()
  opt$step()
}
```

Let's visualize the first (input layer):
```{r}
fields::image.plot(as.matrix(model_torch$parameters$`0.weight`))
```

</details>
<br/>


Additionally to the usual l1 and l2 regularisation there is an additional regularisation: the so called dropout-layer (we will learn about this in more detail later).

Before we specialise on any tuning it is important to understand that ML always consists of a pipeline of actions. 

## The standard ML pipeline at the example of the titanic dataset
The typical ML workflow consist of:

- Data cleaning and exploration (EDA=explorative data analysis) with tidyverse
- Pre-processing and feature selection
- Splitting dataset into train and test set for evaluation
- Model fitting
- Model evaluation
- New predictions
Here is an (optional) video that explains the entire pipeline from a slightly different perspective


```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/nKW8Ndu7Mjw"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```

In the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics can be found here 

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/nRtp7wSEtJA"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```

Another good reference is R for data science by Hadley [](https://r4ds.had.co.nz/)

For this lecture you need the titanic dataset provided by us. You can find it in GRIPS (datasets.RData in the dataset and submission section) or at [](http://rhsbio6.uni-regensburg.de:8500).

We have split the dataset already into training and testing datasets (the test split has one column less than the train split, as the result is not known a priori for the test)

### Data cleaning
Load necessary libraries:
```{r, message=FALSE}
library(keras)
library(tensorflow)
library(tidyverse)
```

Load dataset:
```{r}
load("datasets.RData")
# library(EcoData)
# data(titanic_ml)
# titanic = titanic_ml
data = titanic
```

Standard summaries:
```{r}
str(data)
summary(data)
head(data)
```

The name variable consists of 1309 unique factors (there are 1309 observations...):

```{r}
length(unique(data$name))
```

However, there is a title in each name. Let's extract the titles:

1. we will extract all names and split each name after each comma ","
2. we will split the second split of the name after a point "." and extract the titles
```{r}
first_split = sapply(data$name, function(x) stringr::str_split(x, pattern = ",")[[1]][2])
titles = sapply(first_split, function(x) strsplit(x, ".",fixed = TRUE)[[1]][1])
```

We get 18 unique titles:
```{r}
table(titles)
```


A few titles have a very low occurrence rate:
```{r}
titles = stringr::str_trim((titles))
titles %>%
 fct_count()
```

We will collapse titles with low occurrences into one title, which we can easily do with the forcats package.

```{r}
titles2 =
  forcats::fct_collapse(titles,
                        officer = c("Capt", "Col", "Major", "Dr", "Rev"),
                        royal = c("Jonkheer", "Don", "Sir", "the Countess", "Dona", "Lady"),
                        miss = c("Miss", "Mlle"),
                        mrs = c("Mrs", "Mme", "Ms")
                        )
```

We can count titles again to see the new number of titles

```{r}
titles2 %>%  
   fct_count()
```

Add new title variable to dataset:

```{r}
data =
  data %>%
    mutate(title = titles2)
```

As a second example, we will explore and clean the numeric "age" variable:

Explore the variable:
```{r}
summary(data)
sum(is.na(data$age))/nrow(data)
```

20% NAs!
Either we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups.
In tidyverse we can easily "group" the data, i.e. we will nest the observations (here: group_by after sex, pclass and title).
After grouping, all operations (such as our median(age....)) will be done within the specified groups.
 
```{r}
data =
  data %>%
    group_by(sex, pclass, title) %>%
    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %>%
    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %>%
    ungroup()
```
 

### Pre-processing and feature selection
We want to you keras in our example, but it cannot handle factors and requires scaled the data.

Normally, one would do this for all predictors, but as we here only showe the pipeline, we have sub-selected a bunch of predictors and do this only for them.

We first scale the numeric predictors abd change the factors with only two groups/levels into integer (this can be handled from keras)
```{r}
data_sub =
  data %>%
    select(survived, sex, age2, fare2, title, pclass) %>%
    mutate(age2 = scales::rescale(age2, c(0,1)), fare2 = scales::rescale(fare2, c(0,1))) %>%
    mutate(sex = as.integer(sex) - 1L, title = as.integer(title) - 1L, pclass = as.integer(pclass - 1L))
```

Factors with more than two levels, should be one hot encoded:
```{r}
one_title = k_one_hot(data_sub$title, length(unique(data$title)))$numpy()
colnames(one_title) = levels(data$title)

one_sex = k_one_hot(data_sub$sex, length(unique(data$sex)))$numpy()
colnames(one_sex) = levels(data$sex)

one_pclass = k_one_hot(data_sub$pclass,  length(unique(data$pclass)))$numpy()
colnames(one_pclass) = paste0(1:length(unique(data$pclass)), "pclass")
```
And we have to add the dummy encoded variables to the dataset:

```{r}
data_sub = cbind(data.frame(survived= data_sub$survived), one_title, one_sex, age = data_sub$age2, fare = data_sub$fare2, one_pclass)
head(data_sub)
```

### Split data for training and testing
The splitting consists of two splits:

- an outer split (the original split, remember we got a train and test split without the response "survived")
- an inner split (we will split further the train dataset into another train and test split with known response)
The inner split is important because to assess the model's performance and potential overfitting

Outer split:

```{r}
train = data_sub[!is.na(data_sub$survived),]
test = data_sub[is.na(data_sub$survived),]
```
Inner split:

```{r}
indices = sample.int(nrow(train), 0.7*nrow(train))
sub_train = train[indices,]
sub_test = train[-indices,]
```
What is the difference between the two splits? (Tip: have a look at the variable survived)

### Model fitting
In the next step we will fit a keras model on the train data of the inner split:
```{r}
model = keras_model_sequential()
model %>%
  layer_dense(units = 20L, input_shape = ncol(sub_train) - 1L, activation = "relu") %>%
  layer_dense(units = 20L, activation = "relu") %>%
  layer_dense(units = 20L, activation = "relu") %>%
  layer_dense(units = 2L, activation = "softmax")
summary(model)
model_history =
model %>%
  compile(loss = loss_categorical_crossentropy, optimizer = keras::optimizer_adamax(0.01))
model_history =
  model %>%
    fit(x = as.matrix(sub_train[,-1]), y = to_categorical(sub_train[,1],num_classes = 2L), epochs = 100L, batch_size = 32L, validation_split = 0.2, shuffle = TRUE)

plot(model_history)
```

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
```{r}
model_torch = nn_sequential(
  nn_linear(in_features = dim(sub_train[,-1])[2], out_features = 20L),
  nn_relu(),
  nn_linear(20L, 20L),
  nn_relu(),
  nn_linear(20L, 2L)
)
opt = optim_adam(params = model_torch$parameters, lr = 0.01)

X_torch = torch_tensor( as.matrix(sub_train[,-1])) 
Y_torch = torch_tensor(sub_train[,1]+1, dtype= torch_long())
for(i in 1:500) {
  indices = sample.int(nrow(sub_train), 20L)
  opt$zero_grad()
  pred = model_torch(X_torch[indices, ])
  loss = nnf_cross_entropy(pred, Y_torch[indices], reduction = "mean")
  print(loss)
  loss$backward()
  opt$step()
}
```
Note: the 'nnf_cross_entropy' expects predictions on the scale of the linear predictors (the loss function itself will apply the softmax!)
</details>
<br/>



### Model evaluation
We will predict survived for the test data of the inner split and calculate the accuracy:

```{r}
preds =
  model %>%
    predict(x = as.matrix(sub_test[,-1]))
predicted = ifelse(preds[,2] < 0.5, 0, 1)
observed = sub_test[,1]
(accuracy = mean(predicted == observed))
```


<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
```{r}
model_torch$eval()
preds_torch = nnf_softmax( model_torch(torch_tensor(as.matrix(sub_test[,-1]))) , dim = 2L)
preds_torch = as.matrix(preds_torch)
preds_torch = apply(preds_torch, 1, which.max)
(accuracy = mean(preds_torch-1 == observed))
```
Now we have to use the softmax function.
</details>
<br/>


### Predictions and submission
When we are satisfied with the performance of our model in the inner split, we will create predictions for the test data of the outer split:

To do so, we select all observations that belong to the outer test split (use the filter function) and remove the survived (NAs) columns
```{r}
submit = 
  test %>% 
      select(-survived)
```

We cannot assess the performance on the test split because the true survived ratio is unknown, however, we can now submit our predictions to the submission server at 
http://rhsbio7.uni-regensburg.de:8500
To do so, we have to transform our survived probabilities into actual 0/1 predictions (probabilities are not allowed) and create a csv:

```{r}
pred = model %>% 
  predict(as.matrix(submit))

```

All values > 0.5 will be set to 1 and values < 0.5 to zero.
For the submission it is critical to change the predictions into a data.frame, select the second column (the probablity to survive), and save it with the write.csv function:
```{r,eval=FALSE}
write.csv(data.frame(y=pred[,2]), file = "Max_1.csv")
```

The file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself. 

## Bonus - ML pipelines with mlr3 {#mlr}

As we have seen today, many of the ML algorithms are distributed over several packages but the general ML pipeline is very similar for all models: feature engineering, feature selection?, hyper-parameter tuning and cross validation. 

The idea of the mlr3 framework is now to provide a general ML interface which you can use to build reproducible and automatic ML pipelines. The key features of mlr3 are:

* All common ML packages are integrated into mlr3, you can easily switch between different ML algorithms
* A common 'language'/workflow to specify ML pipelines
* Support for different CV strategies
* Hyper-parameter tuning for all supported ML algorithms
* Ensemble models

Useful links:

* [mlr3-book](https://mlr3book.mlr-org.com/) (still in work)
* [mlr3 website](https://mlr3.mlr-org.com/)
* [mlr3 cheatsheet](https://cheatsheets.mlr-org.com/mlr3.pdf)

### mlr3 - the basic workflow
The mlr3 actually consists of several packages for different tasks (e.g. mlr3tuning for hyper-parameter tuning, mlr3pipelines for data preparation pipes).
But let's start with the basic workflow.
```{r,message=FALSE}
library(EcoData)
library(tidyverse)
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(mlr3tuning)
library(mlr3measures)
data(nasa)
str(nasa)
```

Let's drop time, name, ID variable, and create a classification task:
```{r}
data = nasa %>% select(-Orbit.Determination.Date, -Close.Approach.Date, -Name, -Neo.Reference.ID)
data$Hazardous = as.factor(data$Hazardous)


# create a classification task
task = TaskClassif$new(id = "nasa", backend = data, target = "Hazardous", positive = "1")
```

Create a generic pipeline of data transformation (imputation -> scaling -> encoding of categorical variables):
```{r}
# let's create the preprossing graph
preprocessing = po("imputeoor") %>>% po("scale") %>>% po("encode") 

# run the task trhough it
transformed_task = preprocessing$train(task)[[1]]

transformed_task$missings()

```

We can even visualize the pre-processing graph:
```{r}
preprocessing$plot()
```

Now, to test our model (randomForest) by 10-CV, we will:

* specify the missing target rows as validation so that they will be ignores
* specify the CV, the learner (the ML model we want to use), and the measurement (AUC)
* run (benchmark) our model

```{r mlr1}
transformed_task$data()
transformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)], "validation")

cv10 = mlr3::rsmp("cv", folds = 10L)
rf = lrn("classif.ranger", predict_type = "prob")
measurement =  msr("classif.auc")

```


```{r,eval=FALSE}
result = mlr3::resample(transformed_task, rf, resampling = cv10, store_models = TRUE)

# calclate the average AUC of the holdouts
result$aggregate( measurement )
```

Very cool! Pre-processing + CV10 model evaluation in a few lines of code!

Let's create the final predictions:
```{r mlr2,eval=FALSE}
preds = 
  sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task, 
                                                        row_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[,"1",drop=FALSE])
dim(preds)
predictions = apply(preds, 1, mean)
```
You could now submit the predictions [here](http://rhsbio7.uni-regensburg.de:8500)

But we are still not happy, let's do some hyper-parameter tuning!

### mlr3 - hyper-parameter tuning
ML algorithms have a varying number of hyper-parameters which can (!!!) have a high impact on the predictive performance. To list a few hyper parameters:

**Random Forest**

* mtry
* min node size

**kNN**

* kernel
* number of neighbors
* distance metric

**BRT**

* nrounds
* max depth
* alpha
* booster
* eta
* gamma
* lambda

With mlr3, we can easily extend the above example to do hyper-parameter tuning within nested cross-validation (the tuning has its own inner CV)

Print the hyper-parameter space of our RF learner:
```{r}
rf$param_set
```


Define the hyper-parameter space of RF:

```{r mlr3}
library(paradox)
rf_pars = 
    paradox::ParamSet$new(
      list(paradox::ParamInt$new("min.node.size", lower = 1, upper = 30L),
           paradox::ParamInt$new("mtry", lower = 1, upper = 30L),
           paradox::ParamLgl$new("regularization.usedepth", default = TRUE)))
print(rf_pars)
```

To setup the tuning pipeline we need:

* inner CV resample object
* tuning criterion (e.g. AUC)
* tuning method (e.g. random or block search)
* tuning terminator (when should we stop tune? E.g. after n iterations)


```{r mlr4}
inner3 = mlr3::rsmp("cv", folds = 3L)
measurement =  msr("classif.auc")
tuner =  mlr3tuning::tnr("random_search") 
terminator = mlr3tuning::trm("evals", n_evals = 5L)
rf = lrn("classif.ranger", predict_type = "prob")

learner_tuner = AutoTuner$new(learner = rf, 
                              measure = measurement, 
                              tuner = tuner, 
                              terminator = terminator,
                              search_space = rf_pars,
                              resampling = inner3)
print(learner_tuner)
```

Now we can wrap it normally into the 10-CV setup as previously:
```{r,eval=FALSE}
outer3 = mlr3::rsmp("cv", folds = 3L)
result = mlr3::resample(transformed_task, learner_tuner, resampling = outer3, store_models = TRUE)

# calclate the average AUC of the holdouts
result$aggregate( measurement )
```
Yeah, we were able to improve the performance!

Let's create the final predictions:
```{r,eval=FALSE}
preds = 
  sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task, 
                                                        row_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[,"1",drop=FALSE])
dim(preds)
predictions = apply(preds, 1, mean)
```


### mlr3 - hyper-parameter tuning with oversampling
Let's go one step back, maybe you have noticed that our classes are unbalanced:
```{r}
table(data$Hazardous)
```
Many ML algorithms have problems with unbalanced data because if the imbalance is too strong it is cheaper for the algorithm to focus on only one class (e.g. by predicting only 0s oder 1s). You need to keep in mind that ML algorithms are greedy and their main focus is to minimize the loss function.

There are few techniques to correct for imbalance:

* oversampling (oversample the undersampled class)
* undersampling (undersample the oversampled class)
* SMOTE synthetic minority over-sampling technique (in short, we will use a kNN to create new samples around our undersampled class)

Here, we will use oversampling which we can do by extending our rf learner:
```{r}
rf_over = po("classbalancing", id = "over", adjust = "minor")  %>>%  rf

# However rf_over is now a "graph", but we can easily transform it back into a learner:
rf_over_learner = GraphLearner$new(rf_over)
print(rf_over_learner)
```
The learner has now a new feature space:

```{r}
rf_over_learner$param_set
```
We can also tune the oversampling rate!
```{r}
rf_pars_over = 
    paradox::ParamSet$new(
      list(paradox::ParamInt$new("over.ratio", lower = 1, upper = 7L),
           paradox::ParamInt$new("classif.ranger.min.node.size", lower = 1, upper = 30L),
           paradox::ParamInt$new("classif.ranger.mtry", lower = 1, upper = 30L),
           paradox::ParamLgl$new("classif.ranger.regularization.usedepth", default = TRUE)))

inner3 = mlr3::rsmp("cv", folds = 3L)
measurement =  msr("classif.auc")
tuner =  mlr3tuning::tnr("random_search") 
terminator = mlr3tuning::trm("evals", n_evals = 5L)

learner_tuner_over = AutoTuner$new(learner = rf_over_learner, 
                                   measure = measurement, 
                                   tuner = tuner, 
                                   terminator = terminator,
                                   search_space = rf_pars_over,
                                   resampling = inner3)
print(learner_tuner)
```

```{r,eval=FALSE}
outer3 = mlr3::rsmp("cv", folds = 3L)
result = mlr3::resample(transformed_task, learner_tuner_over, resampling = outer3, store_models = TRUE)

# calclate the average AUC of the holdouts
result$aggregate( measurement )
```


5 iterations in the hyper-space is not very much...

Let's create the final predictions:
```{r,eval=FALSE}
preds = 
  sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task, 
                                                        row_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[,"1",drop=FALSE])
dim(preds)
predictions = apply(preds, 1, mean)
```


<!--chapter:end:02-fundamental.Rmd-->

# Deep learning {#Deep}

In this section, we will discuss both different (deep) network architectures and different means to regularize and improve those deep architectures. 

## Network architectures

### Deep neural networks (DNNs)

Deep neural networks are basically the same as simple ANN, only that they have more hidden layers.


### Convolutional neural networks (DNNs)

The main purpose of CNNs is image recognition. In a CNN, we have at least one convolution layer, additional to the normal, fully connected DNN layers. 

Neurons in a convolution layer are connected only to a small spatially contiguous area of the input layer (receptive field). We use this structure (feature map) to scan the entire picture. The weights are optimized, but the same for all nodes of the hidden layer (shared weights). Think of the feature map as a kernel or filter that is used to scan the image. 

We use this kernel to scan the input features / neurons (e.g. picture). The kernel weights are optimized, but we use the same weights across the entire input neurons (shared weights). The resulting hidden layer is called a feature map. You can think of the feature maps as a map that shows you where the “shapes” expressed by the kernel appear in the input. One kernel / feature map will not be enough, we typically have many shapes that we want to recognize. Thus, the input layer is typically connected to several feature maps, which can be aggregated and followed by a second layer of feature maps, and so on. 

### Recurrent neural networks (RNNs)

Recurrent Neural Networks are used to model sequential data, i.e. temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic:

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/SEnXr6v2ifU"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```


### Natural language processing (NLP)

NLP is actually more of a task than a network structure, but in the area of deep learning for NLP, particular network structures are used. This video should get you an idea about what NLP is about

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
'<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/UFtXy0KRxVI"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```

See also the blog post linked with the youtube video with accompanying code to the video. Moreover, here is an article that shows now NLP works with keras, however, written in Python. As a challenge, you can take the code and implement it in R https://nlpforhackers.io/keras-intro/


## Case study: dropout and early stopping in a deep neural network 

Regularization in deep neural networks is very important because the problem of overfitting. Standard regularization from statistics like l1 and l2 regularization are often feasy and require a lot of tuning. There are more stable and robust methods:

* Early stopping: Early stopping allows us to stop the training when for instance the test loss does not increase anymore
* Dropout: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Dropout is more robust than l1 and l2, and tuning of the dropout rate can be beneficial but a rate between 0.2-0.5 works often quite well

**Data preparation**

See \ref(mlr) for explanation about the pre-processing pipeline. 

```{r,message=FALSE}
library(EcoData)
library(tidyverse)
library(mlr3)
library(mlr3pipelines)
data(nasa)
str(nasa)
data = nasa %>% select(-Orbit.Determination.Date, -Close.Approach.Date, -Name, -Neo.Reference.ID)
data$Hazardous = as.factor(data$Hazardous)
task = TaskClassif$new(id = "nasa", backend = data, target = "Hazardous", positive = "1")
preprocessing = po("imputeoor") %>>% po("scale") %>>% po("encode") 
data = preprocessing$train(task)[[1]]$data()

train = data[!is.na(data$Hazardous),]
submit = data[is.na(data$Hazardous),]

X = scale(train %>% select(-Hazardous))
Y = train %>% select(Hazardous)
Y = to_categorical(as.matrix(Y), 2)
```


**Early stopping**

```{r}
library(keras)

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X)) %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dense(units = ncol(Y), activation = "softmax") 

model %>%
  compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(lr = 0.001))
summary(model)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 50L, batch_size = 20L, 
        shuffle = TRUE, validation_split=0.4)
plot(model_history)
```

The validation loss first decreases but then starts to increase again, can you explain this behavior?
-> Overfitting!

Let's try a l1+l2 regularization:

```{r}
library(keras)

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X), kernel_regularizer = regularizer_l1_l2( 0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu", kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu", kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = ncol(Y), activation = "softmax", kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) 

model %>%
  compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(lr = 0.001))
summary(model)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split=0.4)
plot(model_history)

```
Better, but the validation loss still starts to increase after 40 epochs. But we can use early stopping to end the training before the val loss starts to increase again!

```{r}
library(keras)

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X), kernel_regularizer = regularizer_l1_l2( 0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu", kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu", kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = ncol(Y), activation = "softmax", kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) 

model %>%
  compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(lr = 0.001))
summary(model)

early = keras::callback_early_stopping(patience = 5L)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split=0.4, callbacks=c(early))
plot(model_history)

```
Patience is the number of epochs to wait before aborting the training. 

**Dropout - another type of regularization**

@dropout suggests a dropout rate of 50% for internal hidden layers and 20% for the input layer. One advantage of dropout is that the training is more independent of the number of epochs i.e. the val loss usually doesn't start to increase after several epochs. 

```{r}
model = keras_model_sequential()
model %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X)) %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = ncol(Y), activation = "softmax") 

model %>%
  compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(lr = 0.001))

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split=0.4)
plot(model_history)

```
Ofc, you can still combine early stopping and dropout, which is normally a good idea since it improves training efficiency (e.g. you could start with 1000 epochs and you know training will be aborted if it doesn't improve anymore).


<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
Dropout and early stopping with torch:
```{r}
model_torch = nn_sequential(
  nn_dropout(0.2),
  nn_linear(ncol(X), 50L),
  nn_relu(),
  nn_dropout(0.5),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_dropout(0.5),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_dropout(0.5),
  nn_linear(50L, 2L)
)

YT = apply(Y, 1,which.max)

dataset_nasa = dataset(
  name = "nasa",
  initialize = function(nasa) {
    self$X = nasa$X
    self$Y = nasa$Y
  },
  .getitem = function(i) {
    X = self$X[i,,drop=FALSE] %>% torch_tensor()
    Y = self$Y[i] %>% torch_tensor()
    list(X, Y)
  },
  .length = function() {
    nrow(self$X)
  })

train_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), 
                      batch_size = 32, shuffle = TRUE)
test_dl = dataloader( dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), 
                      batch_size = 32)

model_torch$train()

opt = optim_adam(model_torch$parameters, 0.01)

train_losses = c()
test_losses = c()
early_epoch = 0
min_loss = Inf
patience = 5
for(epoch in 1:50) {
  
  if(early_epoch >= patience) break
  
  train_loss = c()
  test_loss = c()
  coro::loop(for (batch in train_dl) {
    opt$zero_grad()
    pred = model_torch(batch[[1]]$squeeze())
    loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(),reduction = "mean")
    loss$backward()
    opt$step()
    train_loss = c(train_loss, loss$item())
  })
  
  coro::loop(for (batch in test_dl) {
    pred = model_torch(batch[[1]]$squeeze())
    loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(),reduction = "mean")
    test_loss = c(test_loss, loss$item())
  })
  
  ### early stopping ###
  if(mean(test_loss) < min_loss) {
    min_loss = mean(test_loss)
    early_epoch = 0
  } else {
    early_epoch = early_epoch + 1
  }
  ###
  
  train_losses = c(train_losses, mean(train_loss))
  test_losses = c(test_losses, mean(test_loss))
  cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(train_loss)))
}


matplot(cbind(train_losses, test_losses), type = "o", pch = c(15, 16), col = c("darkblue", "darkred"), lty = 1, xlab = "Epoch", ylab = "Loss", las = 1)
legend("topright", bty = "n", col = c("darkblue", "darkred"), lty = 1, pch = c(15, 16), legend = c("Train loss", "Val loss") )

```
</details>
<br/>

## Case study - fitting a Convolutional Neural Networks on MNIST
We will show the use of convolutinal neural networks with the MNIST dataset.The MNIST dataset is maybe one of the most famous image datasets. It is a dataset of 60,000 handwritten digits from 0-9.

To do so, we define a few helper functions:

```{r}
library(keras)
rotate = function(x) t(apply(x, 2, rev))
imgPlot = function(img, title = ""){
 col=grey.colors(255)
 image(rotate(img), col = col, xlab = "", ylab = "", axes=FALSE, main = paste0("Label: ", as.character(title)))
}
```

The dataset is so famous that there is an automatic download function in keras:

```{r}
data = dataset_mnist()
train = data$train
test = data$test
```

Let's visualize a few digits:

```{r}
par(mfrow = c(1,3))
.n = sapply(1:3, function(x) imgPlot(train$x[x,,], train$y[x]))
```

Similar to the normal ML workflow, we have to scale the pixels (from 0-255) to the range of [0,1] and one hot encode the response. To scale the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2d array/tensor is typically called a matrix.

```{r}
train_x = array(train$x/255, c(dim(train$x), 1))
test_x = array(test$x/255, c(dim(test$x), 1))
train_y = to_categorical(train$y, 10)
test_y = to_categorical(test$y, 10)
```

The last dimension stands for the number of channels in the image. In our case we have only one channel because the images are white-black.

Normally we would have three channels - colors are encoded by the combination of three base colors (usually red,green,blue).

To build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size 2x2. These are 2D kernels because our images are 2D. For movies for example, one would use a 3D kernel (the third dimension would correspond to time and not to the color channels).

```{r}
model = keras_model_sequential()
model %>% 
 layer_conv_2d(input_shape = c(28L, 28L,1L),filters = 16L, kernel_size = c(2L,2L), activation = "relu") %>% 
 layer_max_pooling_2d() %>% 
 layer_conv_2d(filters = 16L, kernel_size = c(3L,3L), activation = "relu") %>% 
 layer_max_pooling_2d() %>% 
 layer_flatten() %>% 
 layer_dense(100L, activation = "relu") %>% 
 layer_dense(10L, activation = "softmax")
summary(model)
```

We additionally used a pooling layer to downsize the resulting feature maps. After another convolutional and pooling layer we flatten the output, i.e. the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all weights from the last feature maps).Having flattened the layer, we can simply use our typical output layer.


<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>
Prepare/download data:
```{r}
library(torch)
library(torchvision)

train_ds = mnist_dataset(
  ".",
  download = TRUE,
  train = TRUE,
  transform = transform_to_tensor
)

test_ds = mnist_dataset(
  ".",
  download = TRUE,
  train = FALSE,
  transform = transform_to_tensor
)
```

Build dataloader:
```{r}
train_dl = dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 32)
first_batch = train_dl$.iter()
df = first_batch$.next()

df$x$size()
```

Build CNN:
We have here to calculate the shapes of our layers on our own:

**We start with our input of shape (batch_size, 1, 28, 28)**

```{r}
sample = df$x
sample$size()
```


**first conv layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2)**

```{r}
conv1 = nn_conv2d(1, 16L, 2L, stride = 1L)
(sample %>% conv1)$size()
```
Output: batch_size = 32,  number of feature maps = 16, dimensions of each feature map = ( 27 , 27 )
Wit a kernel size of two and stride =1 we wil lose one pixel in each dimension...
Questions: 

* what does happen if we increase the stride?
* what does happen if we increase the kernel size?

**pooling layer summarizes each feature map**

```{r}
(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L,stride = 2L))$size()
```
kernel_size = 2L and stride = 2L halfs the pixel dimensions of our image

**fully connected layer**

Now we have to flatten our final output of the CNN model to use a normal fully connected layer, but to do so we have to calulate the number of inputs for the fully connected layer:
```{r}
dims = (sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L,stride = 2L))$size()
# without the batch size ofc
final = prod(dims[-1]) 
print(final)
fc = nn_linear(final, 10L)
(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L,stride = 2L) %>% torch_flatten(start_dim = 2L) %>% fc)$size()
```

Build the network:

```{r}
net <- nn_module(
  "mnist",
  initialize = function() {
    self$conv1 <- nn_conv2d(1, 16L, 2L)
    self$conv2 <- nn_conv2d(16L, 16L, 3)
    self$fc1 <- nn_linear(400L, 100L)
    self$fc2 <- nn_linear(100L, 10L)
  },
  forward = function(x) {
    x %>% 
      self$conv1() %>%
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%         
      self$conv2() %>%
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      torch_flatten(start_dim = 2) %>%
      self$fc1() %>%
      nnf_relu() %>%
      self$fc2()
  }
)
```

</details>
<br/>



The rest is as usual: First we compile the model.

```{r}
model %>% 
 compile(
 optimizer = keras::optimizer_adamax(0.01),
 loss = loss_categorical_crossentropy
 )
summary(model)
```

Then, we train the model:

```{r}
epochs = 5L
batch_size = 32L
model %>% 
 fit(
 x = train_x, 
 y = train_y,
 epochs = epochs,
 batch_size = batch_size,
 shuffle = TRUE,
 validation_split = 0.2
)
```


<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>

Train model:
```{r}
model_torch = net()
opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:3) {
  losses = c()
  coro::loop(for (batch in train_dl) {
    opt$zero_grad()
    pred = model_torch(batch[[1]])
    loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
    loss$backward()
    opt$step()
    losses = c(losses, loss$item())
  })
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}
```

Evaluation:
```{r}
model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(for (b in test_dl) {
  output = model_torch(b[[1]])
  labels = b[[2]]
  loss = nnf_cross_entropy(output, labels)
  test_losses = c(test_losses, loss$item())
  predicted = torch_max(output$data(), dim = 2)[[2]]
  total = total + labels$size(1)
  correct = correct + (predicted == labels)$sum()$item()
})

mean(test_losses)
test_accuracy <-  correct/total
test_accuracy
```

</details>
<br/>

## Advanced training techniques 
### Data Augmentation
Having to train a CNN using very little data is a common problem. Data augmentation helps to artificially increase the number of images.

The idea is that a CNN learns specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures.

Luckily, it is very easy to use data augmentation in keras.

To show this, we will use again the MNIST dataset. We have to define a generator object (it is a specific object which infinitly draws samples from our dataset). In the generator we can turn on the data augementation. However, now we have to set the step size (steps_per_epoch) because the model does not know the first dimension of the image.

```{r,eval=FALSE}
data = EcoData::dataset_flower()
train = data$train
test = data$test
labels = data$labels

model = keras_model_sequential()
model %>% 
  layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L), input_shape = c(80L, 80L, 3L), activation = "relu") %>% 
  layer_max_pooling_2d() %>% 
  layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L), activation = "relu") %>% 
  layer_max_pooling_2d() %>% 
  layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L), strides = c(2L, 2L), activation = "relu") %>% 
  layer_max_pooling_2d() %>% 
  layer_flatten() %>% 
  layer_dropout(0.5) %>% 
  layer_dense(units = 5L, activation = "softmax")

  
# Data augmentation
aug = image_data_generator(rotation_range = 90, 
                           zoom_range = c(0.3), 
                           horizontal_flip = TRUE, 
                           vertical_flip = TRUE)

# Data preparation / splitting
indices = sample.int(nrow(train), 0.1*nrow(train))
generator = flow_images_from_data(train[-indices,,,]/255, k_one_hot(labels[-indices], num_classes = 5L),generator = aug, batch_size = 25L, shuffle = TRUE)

test = train[indices,,,]/255
test_labels = k_one_hot(labels[indices], num_classes = 5L)


# Our own training loop with early stopping:
epochs = 50L
batch_size = 25L
steps = floor(dim(train)[1]/batch_size)
optim = keras::optimizer_rmsprop()
max_patience = 5L
patience = 1L
min_val_loss = Inf
val_losses = c()
epoch_losses = c()
for(e in 1:epochs) {
  epoch_loss = c()
  for(s in 1:steps) {
    batch = reticulate::iter_next(generator)
    with(tf$GradientTape() %as% tape, {
        pred = model(batch[[1]], training = TRUE)
        loss = keras::loss_categorical_crossentropy(batch[[2]], pred)
        loss = tf$reduce_mean(loss)
      })
    gradients = tape$gradient(target = loss, sources = model$trainable_variables)
    optim$apply_gradients(purrr::transpose(list(gradients, model$trainable_variables)))
    epoch_loss = c(epoch_loss, loss$numpy())
  }
  epoch_losses = c(epoch_losses, epoch_loss)
  ## test loss ##
  preds = model %>% predict(test)
  val_losses = c(val_losses, tf$reduce_mean( keras::loss_categorical_crossentropy(test_labels, preds) )$numpy())
  
  cat("Epoch: ", e, " Train Loss: ", mean(epoch_losses)," Val Loss: ", val_losses[e],  " \n")
  
  if(val_losses[e] < min_val_loss) {
    min_val_loss = val_losses[e]
    patience = 1
  } else { patience = patience+1 }
  if(patience == max_patience) break
}

preds = predict(model, data$test/255)
preds = apply(preds, 1, which.max)-1
```

So using data augmentation we can artificially increase the number of images.

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>

In torch, we have to change the transform function (but only for the train dataloader):
```{r}
train_transforms <- function(img) {
  img %>%
    transform_to_tensor() %>%
    transform_random_horizontal_flip(p = 0.3) %>% 
    transform_random_resized_crop(size = c(28L, 28L)) %>%
    transform_random_vertical_flip(0.3)
}

train_ds = mnist_dataset(".", download = TRUE, train = TRUE, transform = train_transforms)
test_ds = mnist_dataset(".", download = TRUE, train = FALSE,transform = transform_to_tensor)

train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 100L)

model_torch = net()
opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:1) {
  losses = c()
  coro::loop(for (batch in train_dl) {
    opt$zero_grad()
    pred = model_torch(batch[[1]])
    loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
    loss$backward()
    opt$step()
    losses = c(losses, loss$item())
  })
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}

model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(for (b in test_dl) {
  output = model_torch(b[[1]])
  labels = b[[2]]
  loss = nnf_cross_entropy(output, labels)
  test_losses = c(test_losses, loss$item())
  predicted = torch_max(output$data(), dim = 2)[[2]]
  total = total + labels$size(1)
  correct = correct + (predicted == labels)$sum()$item()
})

test_accuracy <-  correct/total
print(test_accuracy)
```

</details>
<br/>


### Transfer learning {#transfer}

Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning.

The main idea of transfer learning is that all the convolutional layers have mainly one task - learning to identify highly correlated neighbored features and therefore these learn structures such as edges in the image and only the top layer, the dense layer is the actual classifier of the CNN. Thus, one could think that we could only train the top layer as classifier. To do so, it will be confronted by sets of different edges/structures and has to decide the label based on these.

Again, this sounds very complicating but is again quite easy with keras:

We will do this now with the CIFAR10 data set, so we have to prepare the data:
```{r}
data = keras::dataset_cifar10()
train = data$train
test = data$test
image = train$x[5,,,]
image %>% 
 image_to_array() %>%
 `/`(., 255) %>%
 as.raster() %>%
 plot()
train_x = array(train$x/255, c(dim(train$x)))
test_x = array(test$x/255, c(dim(test$x)))
train_y = to_categorical(train$y, 10)
test_y = to_categorical(test$y, 10)
```

Keras provides download functions for all famous architectures/CNN models which are already trained on the imagenet dataset (another famous dataset). These trained networks come already without their top layer, so we have to set include_top to false and change the input shape.

```{r,eval=FALSE}
densenet = application_densenet201(include_top = FALSE, input_shape  = c(32L, 32L, 3L))
```

Now, we will use not a sequential model but just a "keras_model" where we can specify the inputs and outputs. Thereby, the outputs are our own top layer, but the inputs are the densenet inputs, as these are already pre-trained.
```{r,eval=FALSE}
model = keras::keras_model(inputs = densenet$input, outputs = 
 layer_flatten(layer_dense(densenet$output, units = 10L, activation = "softmax"))
 )
```


In the next step we want to freeze all layers except for our own last layer (with freezing I mean that these are not trained: we do not want to train the complete model, we only want to train the last layer). You can check the number of trainable weights via summary(model)

```{r,eval=FALSE}
model %>% freeze_weights(to = length(model$layers)-1)
summary(model)
```

And then the usual training:
```{r,eval=FALSE}
model %>% 
 compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax())
model %>% 
 fit(
 x = train_x, 
 y = train_y,
 epochs = 1L,
 batch_size = 32L,
 shuffle = T,
 validation_split = 0.2,
)
```

We have seen, that transfer-learning can easily be done using keras.

<details>
<summary>
**<span style="color: #CC2FAA">torch</span>**
</summary>
<p>

In torch, we have to change the transform function (but only for the train dataloader):
```{r}
library(torchvision)
train_ds = cifar10_dataset(".", download = TRUE, train = TRUE, transform = transform_to_tensor)
test_ds = cifar10_dataset(".", download = TRUE, train = FALSE,transform = transform_to_tensor)

train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 100L)

model_torch = model_resnet18(pretrained = TRUE)

# we will set all model parameters to constant values:
model_torch$parameters %>% purrr::walk(function(param) param$requires_grad_(FALSE))

# let's replace the last layer (last layer is named 'fc') with our own layer:
inFeat = model_torch$fc$in_features
model_torch$fc = nn_linear(inFeat, out_features = 10L)

opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:1) {
  losses = c()
  coro::loop(for (batch in train_dl) {
    opt$zero_grad()
    pred = model_torch(batch[[1]])
    loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
    loss$backward()
    opt$step()
    losses = c(losses, loss$item())
  })
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}

model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(for (b in test_dl) {
  output = model_torch(b[[1]])
  labels = b[[2]]
  loss = nnf_cross_entropy(output, labels)
  test_losses = c(test_losses, loss$item())
  predicted = torch_max(output$data(), dim = 2)[[2]]
  total = total + labels$size(1)
  correct = correct + (predicted == labels)$sum()$item()
})

test_accuracy <-  correct/total
print(test_accuracy)
```
</details>
<br/>

**Flower dataset**

Let's do it with our flower dataset:

```{r,eval=FALSE}
data = EcoData::dataset_flower()
train = data$train
test = data$test
labels = data$labels
library(keras)

densenet = keras::application_densenet201(include_top = FALSE, input_shape = list(80L, 80L, 3L))

keras::freeze_weights(inception)

model = keras_model(inputs = densenet$input, 
                    outputs = densenet$output %>% 
                      layer_flatten() %>% 
                      layer_dropout(0.2) %>% 
                      layer_dense(units = 200L) %>% 
                      layer_dropout(0.2) %>% 
                      layer_dense(units = 5L, activation="softmax"))


# Data augmentation
aug = image_data_generator(rotation_range = 180,zoom_range = 0.4,width_shift_range = 0.2, height_shift_range = 0.2, vertical_flip = TRUE, horizontal_flip = TRUE,preprocessing_function = imagenet_preprocess_input)

# Data preparation / splitting
indices = sample.int(nrow(train), 0.1*nrow(train))
generator = flow_images_from_data(train[-indices,,,], k_one_hot(labels[-indices], num_classes = 5L), batch_size = 25L, shuffle = TRUE)

test = imagenet_preprocess_input(train[indices,,,])
test_labels = k_one_hot(labels[indices], num_classes = 5L)

# Our own training loop with early stopping:
epochs = 1L
batch_size = 45L
steps = floor(dim(train)[1]/batch_size)
optim = keras::optimizer_rmsprop(lr = 0.0005)
max_patience = 10L
patience = 1L
min_val_loss = Inf
val_losses = c()
epoch_losses = c()
for(e in 1:epochs) {
  epoch_loss = c()
  for(s in 1:steps) {
    batch = reticulate::iter_next(generator)
    with(tf$GradientTape() %as% tape, {
        pred = model(batch[[1]], training = TRUE)
        loss = keras::loss_categorical_crossentropy(batch[[2]], pred)
        loss = tf$reduce_mean(loss)
      })
    gradients = tape$gradient(target = loss, sources = model$trainable_variables)
    optim$apply_gradients(purrr::transpose(list(gradients, model$trainable_variables)))
    epoch_loss = c(epoch_loss, loss$numpy())
  }
  epoch_losses = c(epoch_losses, epoch_loss)
  ## test loss ##
  preds = model %>% predict(test)
  val_losses = c(val_losses, tf$reduce_mean( keras::loss_categorical_crossentropy(test_labels, preds) )$numpy())
  
  cat("Epoch: ", e, " Train Loss: ", mean(epoch_losses)," Val Loss: ", val_losses[e],  " \n")
  
  if(val_losses[e] < min_val_loss) {
    min_val_loss = val_losses[e]
    patience = 1
  } else { patience = patience+1 }
  if(patience == max_patience) break
}

preds = predict(model, imagenet_preprocess_input(data$test))

```



<!--chapter:end:03-Deep.Rmd-->

# Interpretation and causality with machine learning

## Explainable AI

The goal of explainable AI (xAI, aka interpretable machine learning) is to explain WHY a fitted ML models makes certain predictions. A typical example is to understand  how important different variables are for predictions. There incentives to do so range from a better technical understanding of the models over understanding which data is important to improve predictions to questions of fairness and discrimination (e.g. to understand if an algorithm uses skin color to make a decision).

### A practical example

In this lecture we will work with another famous dataset, the Boston housing dataset:

We will fit a random forest and use the iml pkg for xAI, see ![](https://christophm.github.io/interpretable-ml-book/)

```{r}
set.seed(123)
library("iml")
library("randomForest")
data("Boston", package = "MASS")
rf = randomForest(medv ~ ., data = Boston, ntree = 50)
```

xAI packages are written generic, i.e. they can handle almost all ML models.
When we want to use them, we first have to create a Predictor object, that holds the model and the data. The iml package uses R6 classes, that means new objects can be created by calling Predictor$new(). (do not worry if you do not know what R6 classes are, just use the command)

```{r}
X = Boston[which(names(Boston) != "medv")]
predictor = Predictor$new(rf, data = X, y = Boston$medv)
```

### Feature Importance
Feature importance, should not be mistaken with the RF variable importance. It tells us how important the individual variables are for predictions and can be calculated for all ML models and is based on a permutation approach (have a look at the book):

```{r}
imp = FeatureImp$new(predictor, loss = "mae")
plot(imp)
```

### Partial dependencies

Partial dependencies are similar to allEffects plots for normal regressions, the idea is to visualize "marginal effects" of predictors (with the feature argument we specify the variable we want to visualize):

```{r}
eff = FeatureEffect$new(predictor, feature = "rm", method = "pdp", grid.size = 30)
plot(eff)
```

Partial dependencies can be also plotted for single observations:

```{r}
eff = FeatureEffect$new(predictor, feature = "rm", method = "pdp+ice", grid.size = 30)
plot(eff)
```

One disadvantage of partial dependencies is that they are sensitive to correlated predictors. Accumulated local effects can be used to account for correlation for predictors

### Accumulated local effects

Accumulated local effects (ALE) are basically partial dependencies plots but try to correct for correlations between predictors
```{r}
ale = FeatureEffect$new(predictor, feature = "rm", method = "ale")
ale$plot()
```

If there is no colinearity, you shouldn't see much difference between partial dependencies and ALE plots.

### Friedmans H-statistic

The H-statistic can be used to find interactions between predictors. However, again, keep in mind that the H-statistic is sensible to correlation between predictors:

```{r}
interact = Interaction$new(predictor, "lstat")
plot(interact)
```

### Global explainer - Simplifying the ML model

Another idea is to simplify the ML model with another simpler model such as a decision tree. We create predictions with the ML model for a lot of different input values and then we fit on these predictions a decision tree, which we can then interpret.

```{r}
library(partykit)
tree = TreeSurrogate$new(predictor, maxdepth = 2)
plot(tree)
```

### Local explainer - LIME explaining single instances (observations)

The global approach is to simplify the entire ML-black-box model via a simpler model, which is then interpretable.

However, sometimes we are only interested in understanding how single observations/predictions are generated. The lime approach explores the feature space around one observations and based on this local spare fits then a simpler model (e.g. a linear model):

```{r}
library(glmnet)
lime.explain = LocalModel$new(predictor, x.interest = X[1,])
lime.explain$results
plot(lime.explain)
```


### Local explainer - Shapley

The Shapley method computes the so called Shapley value, feature contributions for single predictions, and is based on an approach from cooperative game theory. The idea is that each feature value of the instance is a "player" in a game, where the prediction is the reward. The Shapley value tells us how to fairly distribute the award among the feature.

```{r}
shapley = Shapley$new(predictor, x.interest = X[1,])
shapley$plot()
```

## Causal inference and machine learning

xAI aims at explaining how predictions are being made. In general, xAI != causality. xAI methods measure which variables are used by the algorithm for predictions, or how much variables improve predictions. The important point to note here: if a variable causes something, we could also expect that it helps to predict the very thing. The opposite, however, is not generally true - it is very often possible that a variable that doesn't cause something can predict something.

In statistical courses (in particular course: advanced biostatistics), we discuss the issue of causality at length. Here, we don't want to go into the details, but again, you should in general resist to interpret indicators of importance in xAI as causal effects. They tell you something about what's going on in the algorithm, not about what's going on in reality.

### Causal inference on static data

Methods for causal inference depend on whether we have dynamic or static data. The latter is the more common case. With static data, the problem is confounding - if you have several predictors that are correlated, you can get spurious correlations between a given predictor and the response.

A multiple regression, and a few other methods are able to correct for other predictors, and thus isolate the causal effect. The same is not necessarily true for ML algorithms and xAI methods. This is not a bug, but a feature - for making good predictions, it is often no problem, but rather an advantage to also use non-causal predictors.

Here an example for the variable importance indicators in the RF algorithm. The purpose of this script is to show that RF variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are separable and would be correctly separated by an lm / ANOVA

We first simulate a dataset with 2 predictors that are strongly correlated, but only one of them has an effect on the response.
```{r}
# simulation parameters
n = 1000
col = 0.7
# create collinear predictors
x1 = runif(n)
x2 = col * x1 + (1-col) * runif(n)
# response is only influenced by x1
y = x1 + rnorm(n)
```
lm / anova correctly identify x1 as causal variable
```{r}
anova(lm(y ~ x1 + x2))
```

Fit RF and show variable importance
```{r}
fit <- randomForest(y ~ x1 + x2, importance=TRUE)
varImpPlot(fit)
```
Variable importance is now split nearly evenly.

Task: understand why this is - remember:

* How the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built
* Remember that as x1 ~ x2, we can use x2 as a replacement for x1
* Remember that the variable importance measures the average contributions of the different variables in the trees of the forest

### Structural equation models

If causal relationships get more complicated, it will not be possible to adjust correctly with a simple lm. In this case, in statistics, we will usually use structural equation models (SEMs). SEMs are are designed to estimate entire causal diagrams. There are two main SEM packages in R: for anything that is non-normal, you will currently have to estimate the DAG piece-wise with CRAN package piecewiseSEM. Example for a vegetation dataset:

```{r,eval=FALSE}
library(piecewiseSEM)
mod = psem(
 lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, data = keeley),
 lm(firesev ~ elev + age + cover, data = keeley),
 lm(cover ~ age + elev + hetero + abiotic, data = keeley)
)
summary(mod)
plot(mod)
```

For linear SEMs, we can estimate the entire DAG in one go. This also allows to have unobserved variables in the DAG. One of the most popular packages for this is lavaan

```{r,eval=FALSE}
library(lavaan)
mod <- "
 rich ~ distance + elev + abiotic + age + hetero + firesev + cover
 firesev ~ elev + age + cover
 cover ~ age + elev + abiotic
"
fit<-sem(mod,data=keeley)
summary(fit)
```


Plot options ... not so nice as before

```{r,eval=FALSE}
library(lavaanPlot)
lavaanPlot(model = fit)
```
Another plotting option

```{r,eval=FALSE}
library(semPlot)
semPaths(fit)
```


### Automatic causal discovery

But how to we get the causal graph? In statistics, it common to "guess" it and afterwards do residual checks, in the same way as we guess the structure of a regression. For more complicated problems, however, this is unsatisfying. Some groups therefore work on so-called causal discovery algorithsm, i.e. algorithms that automatically generate causal graphs from data. One of the most classic algorithms of this sort is the PC algorithm. Here an example using the pcalg package:

```{r,eval=FALSE}
# Bioconductor dependencies have to installed by hand, e.g. 
# BiocManager::install(c("Rgraphviz", "graph", "RBGL")
library(pcalg)
```

Loading the data

```{r,eval=FALSE}
data("gmG", package = "pcalg") ## loads data sets gmG and gmG8
suffStat <- list(C = cor(gmG8$x), n = nrow(gmG8$x))
varNames <- gmG8$g@nodes
```

First, the kkeleton algorithm creates a basic graph without connections

```{r,eval=FALSE}
skel.gmG8 <- skeleton(suffStat, indepTest = gaussCItest,
labels = varNames, alpha = 0.01)
Rgraphviz::plot(skel.gmG8)
```

What is missing here is the direction of the errors. The PC algorith now makes tests for conditional independence, which allows fixing a part (but typically not all) of the directions of the causal arrows.

```{r,eval=FALSE}
pc.gmG8 <- pc(suffStat, indepTest = gaussCItest,
labels = varNames, alpha = 0.01)
Rgraphviz::plot(pc.gmG8 )
```

### Causal inference on dynamic data

When working with dynamic data, we can use an additional piece of information - the effect usually preceeds the cause, which means that we can test for a time-lag between cause and effect to determine the direction of causality. This way of testing for causality is known as Granger causality, or Granger methods. Here an example:

```{r}
library(lmtest)
## Which came first: the chicken or the egg?
data(ChickEgg)
grangertest(egg ~ chicken, order = 3, data = ChickEgg)
grangertest(chicken ~ egg, order = 3, data = ChickEgg)
```

### Outlook for machine learning

As we have seen, there are already a few methods / algorithms to discover causality from large data, but the systematic transfer of these concepts to machine learning, in particular deep learning, is still at its infancy. At the moment, this field is actively researched and changes extremely fast, so we recommend to use google to see what is currently going on. Particular, in business and industry, there is a large interest in learning about causal effect from large datasets. In our opinion, a great topic for young scientists to specialize on.

<!--chapter:end:04-xAI.Rmd-->

# GANs, VAEs, and Reinforcement learning

## Generative adversarial network (GANs)
The idea of generative adversarial network (GAN) is that two neural networks contest with each other in a game. On network is creating data and is trying to "trick" the other into thinking that this data is real. A possible application is to create pictures that look like real photographs. However, the application of GANs today is much wider than just the creation of data. For example, GANs can also be used to "augment" data, i.e. to create new data and thereby improve the fitted model. 

### MNIST - GAN based on DNNs
GANs - two networks are playing against each other. The generator (similar to the decoder in AEs) creates new images from noise and tries to convince the discriminator that this is a real image.

The discriminator is getting a mix of true images (from the dataset) and of artificially generated images from the generator. 

Loss of the generator - when fakes are identified as fakes by the discriminator (simple binary_crossentropy loss, 0/1...)

Loss of the discriminator - when fakes are identified as fakes (class 1) and true images as true images (class 0), again simple binary crossentropy.

MNIST example:

```{r}
library(keras)
library(tensorflow)
rotate = function(x) t(apply(x, 2, rev))
imgPlot = function(img, title = ""){
 col=grey.colors(255)
 image(rotate(img), col = col, xlab = "", ylab = "", axes=FALSE, main = paste0("Label: ", as.character(title)))
}
```

We don't need the test set:

```{r}
data = dataset_mnist()
train = data$train
train_x = array((train$x-127.5)/127.5, c(dim(train$x)[1], 784L))
```

```{r}
batch_size = 32L
get_batch = function(){  # Helper function to get batches of images
 indices = sample.int(nrow(train_x), batch_size)
 return(tf$constant(train_x[indices,], "float32"))
}

dataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, "float32"))
dataset$batch(batch_size)
```


Define and test generator model:

```{r}
get_generator = function(){
 generator = keras_model_sequential()
 generator %>% 
 layer_dense(units = 200L ,input_shape = c(100L)) %>% 
 layer_activation_leaky_relu() %>% 
 layer_dense(units = 200L) %>% 
 layer_activation_leaky_relu() %>% 
 layer_dense(units = 784L, activation = "tanh")
 return(generator)
}
```

```{r}
generator = get_generator()
sample = tf$random$normal(c(1L, 100L))
imgPlot(array(generator(sample)$numpy(), c(28L, 28L)))
```

The noise of size = [100] (random vector with 100 values) is passed through the network and the output correspond to the number of pixels of one MNIST image (784)

```{r}
get_discriminator = function(){
 discriminator = keras_model_sequential()
 discriminator %>% 
 layer_dense(units = 200L, input_shape = c(784L)) %>% 
 layer_activation_leaky_relu() %>% 
 layer_dense(units = 100L) %>% 
 layer_activation_leaky_relu() %>% 
 layer_dense(units = 1L, activation = "sigmoid")
 return(discriminator)
}
```

```{r}
discriminator = get_discriminator()
discriminator(generator(tf$random$normal(c(1L, 100L))))
```

The normal architecture of a binary classifier (will get images as input)

Loss:

```{r}
ce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE)
loss_discriminator = function(real, fake){
 real_loss = ce(tf$ones_like(real), real)
 fake_loss = ce(tf$zeros_like(fake), fake)
 return(real_loss+fake_loss)
}
loss_generator = function(fake){
 return(ce(tf$ones_like(fake), fake))
}
```

Binary crossentropy as loss function.

However, we have to encode the true and predicted values for the two networks individually.

The discriminator will get two losses - one for identifying fake images as fake, and one for identifying real MNIST images as real images.

The generator will just get one loss - was it able to deceive the discriminator?

Each network will get its own optimizer (while a AE will be treated as one network, in a GAN the networks will be treated independently)

```{r}
gen_opt = tf$keras$optimizers$RMSprop(1e-4)
disc_opt = tf$keras$optimizers$RMSprop(1e-4)
```

We have to write here our own training loop (we cannot use the fit function). Let's define a training function:

```{r}
train_step = function(images){
 noise = tf$random$normal(c(32L, 100L))
 with(tf$GradientTape(persistent = TRUE) %as% tape,{
   gen_images = generator(noise)
   fake_output = discriminator(gen_images)
   real_output = discriminator(images)
   gen_loss = loss_generator(fake_output)
   disc_loss = loss_discriminator(real_output, fake_output)
 })
 gen_grads = tape$gradient(gen_loss, generator$weights)
 disc_grads = tape$gradient(disc_loss, discriminator$weights)
 rm(tape)
 gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights)))
 disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights)))
 return(c(gen_loss, disc_loss))
}
train_step = tf$`function`(reticulate::py_func(train_step))
```

In each iteration (for each batch) we will do the following (the GradientTape records computations to do automatic differenation):

1. sample noise
2. Generator creates images from the noise
3. Discriminator will make predictions for fake images and real images (response is a probability between [0,1])
4. Calculate loss for generator
5. Calculate loss for discriminator
6. Calculate gradients for weights and the loss
7. Update weights of generator
8. Update weights of discriminator
9. return losses

```{r, eval=FALSE}
generator = get_generator()
discriminator = get_discriminator()
epochs = 30L
steps = as.integer(nrow(train_x)/batch_size)
counter = 1
gen_loss = c()
disc_loss = c()

for(e in 1:epochs) {
  dat = reticulate::as_iterator(dataset$batch(batch_size))
  
   coro::loop(for (images in dat) {
      losses = train_step(images)
      gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy())
      disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy())
   })
   
  cat("Gen: ", mean(gen_loss), " Disc: ", mean(disc_loss), " \n")
  if(e %% 5 == 0) {
    noise = tf$random$normal(c(1L, 100L))
    imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), "Gen")
  }
}


# for(i in 1:(epochs*steps)){
#  images = get_batch()
#  losses = train_step(images)
#  gen_loss = tf$reduce_sum(losses[[1]])$numpy()
#  disc_loss = tf$reduce_sum(losses[[2]])$numpy()
#  if(i %% 50*steps == 0) {
#  noise = tf$random$normal(c(1L, 100L))
#  imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), "Gen")
#  }
#  if(i %% steps == 0){
#  counter = 1
#  cat("Gen: ", mean(gen_loss), " Disc: ", mean(disc_loss), " \n")
#  }
# }
```

The actual training loop:

1. Create networks
2. get batch of images
3. run train_step function
4. print losses
5. repeat step 2-4 for number of epochs


### Flower - GAN
```{r,eval=FALSE}
library(keras)
library(tidyverse)
data_files = list.files("flowers/", full.names = TRUE)
train = data_files[str_detect(data_files, "train")]
test = readRDS(file = "test.RDS")
train = lapply(train, readRDS)
train = abind::abind(train, along = 1L)
train = tf$concat(list(train, test), axis = 0L)$numpy()
train_x = array((train-127.5)/127.5, c(dim(train)))
get_generator = function(){
  generator = keras_model_sequential()
  generator %>% 
    layer_dense(units = 20L*20L*128L, input_shape = c(100L), use_bias = FALSE) %>% 
    layer_activation_leaky_relu() %>% 
    layer_reshape(c(20L, 20L, 128L)) %>% 
    layer_dropout(0.3) %>% 
    layer_conv_2d_transpose(filters = 256L, kernel_size = c(3L, 3L), padding = "same", strides = c(1L, 1L), use_bias = FALSE) %>% 
    layer_activation_leaky_relu() %>% 
    layer_dropout(0.3) %>% 
    layer_conv_2d_transpose(filters = 128L, kernel_size = c(5L, 5L), padding = "same", strides = c(1L, 1L), use_bias = FALSE) %>% 
    layer_activation_leaky_relu() %>% 
    layer_dropout(0.3) %>% 
    layer_conv_2d_transpose(filters = 64L, kernel_size = c(5L, 5L), padding = "same", strides = c(2L, 2L), use_bias = FALSE) %>%
    layer_activation_leaky_relu() %>% 
    layer_dropout(0.3) %>% 
    layer_conv_2d_transpose(filters =3L, kernel_size = c(5L, 5L), padding = "same", strides = c(2L, 2L), activation = "tanh", use_bias = FALSE)
  return(generator)
}
generator = get_generator()
image = generator(tf$random$normal(c(1L,100L)))$numpy()[1,,,]
image = scales::rescale(image, to = c(0, 255))
image %>% 
  image_to_array() %>%
  `/`(., 255) %>%
  as.raster() %>%
  plot()
get_discriminator = function(){
  discriminator = keras_model_sequential()
  discriminator %>% 
    layer_conv_2d(filters = 64L, kernel_size = c(5L, 5L), strides = c(2L, 2L), padding = "same", input_shape = c(80L, 80L, 3L)) %>%
    layer_activation_leaky_relu() %>% 
    layer_dropout(0.3) %>% 
    layer_conv_2d(filters = 128L, kernel_size = c(5L, 5L), strides = c(2L, 2L), padding = "same") %>% 
    layer_activation_leaky_relu() %>% 
    layer_dropout(0.3) %>% 
    layer_conv_2d(filters = 256L, kernel_size = c(3L, 3L), strides = c(2L, 2L), padding = "same") %>% 
    layer_activation_leaky_relu() %>% 
    layer_dropout(0.3) %>% 
    layer_flatten() %>% 
    layer_dense(units = 1L, activation = "sigmoid")
  return(discriminator)
}
discriminator = get_discriminator()
discriminator
discriminator(generator(tf$random$normal(c(1L, 100L))))
ce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE,label_smoothing = 0.1)
loss_discriminator = function(real, fake){
  real_loss = ce(tf$ones_like(real), real)
  fake_loss = ce(tf$zeros_like(fake), fake)
  return(real_loss+fake_loss)
}
loss_generator = function(fake){
  return(ce(tf$ones_like(fake), fake))
}
gen_opt = tf$keras$optimizers$RMSprop(1e-4)
disc_opt = tf$keras$optimizers$RMSprop(1e-4)
batch_size = 32L
get_batch = function(){
  indices = sample.int(nrow(train_x), batch_size)
  return(tf$constant(train_x[indices,,,,drop=FALSE], "float32"))
}
train_step = function(images){
  noise = tf$random$normal(c(32L, 100L))
  
  with(tf$GradientTape(persistent = TRUE) %as% tape,{
    gen_images = generator(noise)
    
    real_output = discriminator(images)
    fake_output = discriminator(gen_images)
    
    gen_loss = loss_generator(fake_output)
    disc_loss = loss_discriminator(real_output, fake_output)
    
  })
  
  gen_grads = tape$gradient(gen_loss, generator$weights)
  disc_grads = tape$gradient(disc_loss, discriminator$weights)
  rm(tape)
  
  gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights)))
  disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights)))
  
  return(c(gen_loss, disc_loss))
  
}
train_step = tf$`function`(reticulate::py_func(train_step))
epochs = 10L
steps = as.integer(nrow(train_x)/batch_size)
counter = 1
gen_loss = NULL
disc_loss = NULL
for(i in 1:(epochs*steps)){
  
  images = get_batch()
  losses = train_step(images)
  gen_loss[counter] = tf$reduce_sum(losses[[1]])$numpy()
  disc_loss[counter] = tf$reduce_sum(losses[[2]])$numpy()
  counter = counter+1
  if(i %% 10*steps == 0) {
    noise = tf$random$normal(c(1L, 100L))
    image = generator(noise)$numpy()[1,,,]
    image = scales::rescale(image, to = c(0, 255))
    image %>% 
      image_to_array() %>%
      `/`(., 255) %>%
      as.raster() %>%
      plot()
  }
  if(i %% steps == 0){
    counter = 1
    cat("Gen: ", mean(gen_loss), " Disc: ", mean(disc_loss), " \n")
  }
}


results = vector("list", 100L)
for(i in 1:100) {
  noise = tf$random$normal(c(1L, 100L))
  image = generator(noise)$numpy()[1,,,]
  image = scales::rescale(image, to = c(0, 255))
  image %>% 
    image_to_array() %>%
    `/`(., 255) %>%
    as.raster() %>%
    plot()
  results[[i]] = image
  imager::save.image(imager::as.cimg(image),quality = 1.0,file = paste0("images/flower",i, ".png"))
  imager::as.cimg(image)
}
saveRDS(abind::abind(results, along = 0L), file = "images/result.RDS")
```


```{r,echo=FALSE,out.width="300%",out.height="300%"}
knitr::include_graphics(c("images/flower2.png", "images/flower3.png", "images/flower4.png", "images/flower5.png"))
```


## Autoencoder
An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. 
### Autoencoder - DNN MNIST
Autoencoders consist of Encoder and a Decoder Networks. 

The encoder will compress the data into 2 dimensions and the decoder will reconstruct the original data:

```{r}
library(keras)
library(tensorflow)
rotate = function(x) t(apply(x, 2, rev))
imgPlot = function(img, title = ""){
 col=grey.colors(255)
 image(rotate(img), col = col, xlab = "", ylab = "", axes=FALSE, main = paste0("Label: ", as.character(title)))
}
data = tf$keras$datasets$mnist$load_data()
train = data[[1]]
train_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L))
test_x = array(test[[1]]/255, c(dim(test[[1]])[1], 784L))
## Dense autoencoder
### Inputs will be compromized to two dimensions
down_size_model = keras_model_sequential()
down_size_model %>% 
 layer_dense(units = 100L, input_shape = c(784L),activation = "relu") %>% 
 layer_dense(units = 20L, activation = "relu") %>% 
 layer_dense(units = 2L, activation = "linear")
### Reconstruction of the images
up_size_model = keras_model_sequential()
up_size_model %>% 
 layer_dense(units = 20L, input_shape = c(2L), activation = "relu") %>% 
 layer_dense(units = 100L, activation = "relu") %>% 
 layer_dense(units = 784L, activation = "sigmoid")
### Combine models into one
autoencoder = tf$keras$models$Model(inputs=down_size_model$input,  outputs=up_size_model(down_size_model$output))
autoencoder$compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.01))
image = autoencoder(train_x[1,,drop = FALSE])$numpy()
par(mfrow = c(1,2))
imgPlot(array(train_x[1,,drop = FALSE], c(28, 28)))
imgPlot(array(image, c(28, 28)))
autoencoder$fit(x = tf$constant(train_x), y = tf$constant(train_x), epochs = 5L, batch_size = 32L)
```

After training:
```{r}
pred_dim = down_size_model(test_x)
reconstr_pred = autoencoder(test_x)
imgPlot(array(reconstr_pred[10,]$numpy(), dim = c(28L, 28L)))
par(mfrow = c(1,1))
plot(pred_dim$numpy()[,1], pred_dim$numpy()[,2], col = test[[2]]+1L)
```

### Autoencoder - MNIST CNN
We can also use CNNs isntead of DNNs. There is also an inverse convolutional layer:

```{r,eval=FALSE}
data = tf$keras$datasets$mnist$load_data()
train = data[[1]]
train_x = array(train[[1]]/255, c(dim(train[[1]]), 1L))
test_x = array(data[[2]][[1/255/255, c(dim(data[[2]][[1/255), 1L))
down_size_model = keras_model_sequential()
down_size_model %>% 
 layer_conv_2d(filters = 32L, activation = "relu", kernel_size = c(2L,2L), 
                          input_shape = c(28L, 28L, 1L), strides = c(4L, 4L)) %>% 
 layer_conv_2d(filters = 16L, activation = "relu", 
                           kernel_size = c(7L,7L), strides = c(1L, 1L)) %>% 
 layer_flatten() %>% 
 layer_dense(units = 2L, activation = "linear")
up_size_model = keras_model_sequential()
up_size_model %>% 
 layer_dense(units = 8L, activation = "relu", input_shape = c(2L)) %>% 
 layer_reshape(target_shape = c(1L, 1L, 8L)) %>% 
 layer_conv_2d_transpose(filters = 16L, kernel_size = c(7,7), activation = "relu", strides = c(1L,1L)) %>% 
 layer_conv_2d_transpose(filters = 32L, activation = "relu", kernel_size = c(2,2), strides = c(4L,4L)) %>% 
 layer_conv_2d(filters = 1, kernel_size = c(1L, 1L), strides = c(1L, 1L), activation = "sigmoid")

autoencoder = tf$keras$models$Model(inputs = down_size_model$input, outputs = up_size_model(down_size_model$output))
autoencoder$compile(loss = loss_binary_crossentropy, optimizer = optimizer_rmsprop(0.001))
autoencoder$fit(x = tf$constant(train_x), y = tf$constant(train_x), epochs = 1L, batch_size = 64L)
pred_dim = down_size_model(tf$constant(test_x, "float32"))
reconstr_pred = autoencoder(tf$constant(test_x, "float32"))
imgPlot(reconstr_pred[10,,,]$numpy()[,,1])
plot(pred_dim[,1]$numpy(), pred_dim[,2]$numpy(), col = test[[2]]+1L)
## Generate new images!
new = matrix(c(10,10), 1, 2)
imgPlot(array(up_size_model(new)$numpy(), c(28L, 28L)))

```



## Varational Autoencoder
The difference to normal autoencoder is that we here try to fit latent variables which encode the images:

```{r, eval=FALSE}
library(tensorflow_probability)
data = tf$keras$datasets$mnist$load_data()
train = data[[1]]
train_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L))
tfp = reticulate::import("tensorflow_probability")
prior = tfp$distributions$Independent(tfp$distributions$Normal(loc=tf$zeros(shape(10L,4L)), scale=1.0),
 reinterpreted_batch_ndims=1L)
down_size_model = keras_model_sequential()
down_size_model %>% 
 layer_dense(units = 100L, input_shape = c(784L),activation = "relu") %>% 
 layer_dense(units = 20L, activation = "relu") %>% 
 layer_dense(units = 4L)
### Reconstruction of the images
up_size_model = keras_model_sequential()
up_size_model %>% 
 layer_dense(units = 20L, input_shape = c(2L), activation = "relu") %>% 
 layer_dense(units = 100L, activation = "relu") %>% 
 layer_dense(units = 784L, activation = "sigmoid")
### Combine models into one
batch_size = 32L
epochs = 10L
steps = as.integer(nrow(train_x)/32L * epochs)
prior = tfp$distributions$MultivariateNormalDiag(loc = tf$zeros(shape(batch_size, 2L), "float32"), scale_diag = tf$ones(2L, "float32"))
optimizer = tf$keras$optimizers$RMSprop(0.0001)
weights = c(down_size_model$weights, up_size_model$weights)
get_batch = function(){
 indices = sample.int(nrow(train_x), batch_size)
 return(train_x[indices,])
}
for(i in 1:steps){
 tmp_X = get_batch()
 with(tf$GradientTape() %as% tape, {
 encoded = down_size_model(tmp_X)
 
 dd = tfp$distributions$MultivariateNormalDiag(loc = encoded[,1:2], 
 scale_diag = 1.0/(0.01+ tf$math$softplus(encoded[,3:4])))
 samples = dd$sample()
 reconstructed = up_size_model(samples)
 
 KL_loss = dd$kl_divergence(prior) # constrain
 
 loss = tf$reduce_mean(tf$negative(tfp$distributions$Binomial(1L, logits = reconstructed)$log_prob(tmp_X)))+tf$reduce_mean(KL_loss)
 })
 gradients = tape$gradient(loss, weights)
 optimizer$apply_gradients(purrr::transpose(list(gradients, weights)))
 
 if(i %% as.integer(nrow(train_x)/10L) == 0) cat("Loss: ", loss$numpy(), "\n")
}
```


<!--chapter:end:05-GAN.Rmd-->

# Datasets
You can download the datasets we use in the course [here](http://rhsbio7.uni-regensburg.de:8500) (ignore browser warnings) or by installing the EcoData package:

```{r,eval=FALSE}
devtools::install_github(repo = "florianhartig/EcoData", subdir = "EcoData", 
dependencies = TRUE, build_vignettes = FALSE)
```


## Titanic 
The dataset is a collection of titanic passengers with information about their age, class, sex, and their survival status. The competition here is simple: train a ML model and predict the survival probability.

The titanic dataset is very well explored and serves as a stepping stone in many ML careers. For inspiration and data exploration notebooks, check out this kaggle competition: [](https://www.kaggle.com/c/titanic/data)

**Response variable:** 'survived'

A minimal working example:

1. Load dataset
```{r}
# load("datasets.RData")
library(EcoData)
data(titanic_ml)
titanic = titanic_ml
summary(titanic)
```

2. Impute missing values (not our response variable!)
```{r}
library(missRanger)
library(dplyr)
titanic_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)
titanic_imputed = missRanger::missRanger(data = titanic_imputed %>% select(-survived))
titanic_imputed$survived = titanic$survived
```

3. Split into training and testing
```{r}
train = titanic_imputed[!is.na(titanic$survived), ]
test = titanic_imputed[is.na(titanic$survived), ]
```

4. Train model
```{r}
model = glm(survived~., data=train, family = binomial())
```

5. Predictions
```{r}
preds = predict(model, data = test, type = "response")
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "glm.csv")
```

And submit the csv on [](http://rhsbio7.uni-regensburg.de:8500)

## Plant-pollinator database
The plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower).
We explored the advantage of ML algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look ![here](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13329).

```{r,echo=FALSE}
knitr::include_graphics(c("images/TM.png"))
```


**Response variable:** 'interaction'

A minimal working example:

1. Load dataset
```{r}
load("datasets.RData")
# library(EcoData)
# data(plantPollinator_df)
# plant_poll = plantPollinator_df
summary(plant_poll)
```

2. Impute missing values (not our response variable!)
We will select only a few predictors here (you can work with all predictors ofc).
```{r}
library(missRanger)
library(dplyr)
plant_poll_imputed = plant_poll %>% select(diameter, corolla, tongue, body, interaction)
plant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>% select(-interaction))
plant_poll_imputed$interaction = plant_poll$interaction
```

3. Split into training and testing
```{r}
train = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]
test = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ]
```

4. Train model
```{r}
model = glm(interaction~., data=train, family = binomial())
```

5. Predictions
```{r}
preds = predict(model, data = test, type = "response")
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "glm.csv")
```

## Wine
The dataset is a collection of wines of different quality. The aim is to predict the quality of the wine based on physochemical predictors. 

For inspiration and data exploration notebooks, check out this kaggle competition: [](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
For instance, check out this very nice [notebook](https://www.kaggle.com/aditimulye/red-wine-quality-assesment-starter-pack) which removes a few problems from the data. 

**Response variable:** 'quality'

We could use theoretically a regression model for this task but we will stick with a classification model

A minimal working example:

1. Load dataset
```{r}
load("datasets.RData")
# library(EcoData)
# data(wine)
summary(wine)
```

2. Impute missing values (not our response variable!)
```{r}
library(missRanger)
library(dplyr)
#wine_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)
wine_imputed = missRanger::missRanger(data = wine %>% select(-quality))
wine_imputed$quality = wine$quality
```

3. Split into training and testing
```{r}
train = wine_imputed[!is.na(wine$quality), ]
test = wine_imputed[is.na(wine$quality), ]
```

4. Train model
```{r}
library(ranger)
rf = ranger(quality~., data = train, classification = TRUE)
```

5. Predictions
```{r}
preds = predict(rf, data = test)$predictions
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "rf.csv")
```

## Nasa
A collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. 
For inspiration and data exploration notebooks, check out the kaggle competition: [](https://www.kaggle.com/shrutimehta/nasa-asteroids-classification)

**Response variable:** 'Hazardous'
1. Load dataset
```{r}
load("datasets.RData")
# library(EcoData)
# data(nasa)
summary(nasa)
```

2. Impute missing values (not our response variable!)
```{r}
library(missRanger)
library(dplyr)
#wine_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)
nasa_imputed = missRanger::missRanger(data = nasa %>% select(-Hazardous), maxiter = 1, num.trees=5L)
nasa_imputed$Hazardous = nasa$Hazardous
```

3. Split into training and testing
```{r}
train = nasa_imputed[!is.na(nasa$Hazardous), ]
test = nasa_imputed[is.na(nasa$Hazardous), ]
```

4. Train model
```{r}
library(ranger)
rf = ranger(Hazardous~., data = train, classification = TRUE, probability = TRUE)
```

5. Predictions
```{r}
preds = predict(rf, data = test)$predictions[,2]
head(preds)
```

6. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "rf.csv")
```


## Flower
A collection of over 4000 flower images of 5 plant species. The dataset is from [kaggle](https://www.kaggle.com/alxmamaev/flowers-recognition) but we downsampled the images from $320*240$ to $80*80$ pixels. 
You can download the dataset [here](http://rhsbio7.uni-regensburg.de:8500).

**Notes:**

- check out CNN notebooks on kaggle (they are often written in python but you can still copy the CNN architectures), e.g. [this one](https://www.kaggle.com/alirazaaliqadri/flower-recognition-tensorflow-keras-sequential)
- Last year's winner have used a transfer learning approach (they achieved around 70% accuracy), check out this [notebook](https://www.kaggle.com/stpeteishii/flower-name-classify-densenet201), see also the section about transfer learning \@ref(transfer)


**Response variable:** Plant species

1. Load dataset
The dataset requires pre-processing (we have to concatenate the train and test images):
```{r}
library(keras)
library(stringr)
data_files = list.files("flower/", full.names = TRUE)
train = data_files[str_detect(data_files, "train")]
test = readRDS(file = "flower/test.RDS")
train = lapply(train, readRDS)
train_classes = lapply(train, function(d) dim(d)[1])
train = abind::abind(train, along = 1L)
labels_train = rep(0:4, unlist(train_classes))

flower = EcoData::dataset_flower()
train = flower$train
test = flower$test
train_classes = flower$labels
```

Let's visualize a flower:
```{r}
train[100,,,] %>% 
 image_to_array() %>%
 `/`(., 255) %>%
 as.raster() %>%
 plot()
```

2. Build & train model:
```{r,eval=FALSE}
model = keras_model_sequential()
model %>% 
  layer_conv_2d(filters = 4L, kernel_size = 2L, input_shape = list( 80L, 80L, 3L)) %>% 
  layer_max_pooling_2d() %>% 
  layer_flatten() %>% 
  layer_dense(units = 5L, activation = "softmax")

model %>% 
  compile(optimizer = keras::optimizer_adamax(0.01), loss = keras::loss_categorical_crossentropy)


### Model fitting ###
epochs = 50L
batch_size = 25L
steps = floor(dim(train)[1]/batch_size)
generator = keras::flow_images_from_data(x = train/255 , y = keras::k_one_hot(labels_train, 5L), batch_size = batch_size)

optim = optimizer_adamax(0.01)
epoch_losses = c()
for(e in 1:epochs) {
  epoch_loss = c()
  for(s in 1:steps) {
    batch = reticulate::iter_next(generator)
    with(tf$GradientTape() %as% tape, {
        pred = model(batch[[1]])
        loss = keras::loss_categorical_crossentropy(batch[[2]], pred)
        loss = tf$reduce_mean(loss)
      })
    gradients = tape$gradient(target = loss, sources = model$weights)
    optim$apply_gradients(purrr::transpose(list(gradients, model$weights)))
    epoch_loss = c(epoch_loss, loss$numpy())
  }
  epoch_losses = c(epoch_losses, epoch_loss)
  cat("Epoch: ", e, " Loss: ", mean(epoch_losses), " \n")
}


```

3. Predictions
```{r,eval=FALSE}
preds = model %>% predict(test/255)
preds = apply(preds, 1, which.max)-1
head(preds)
```

4. Create submission csv
```{r,eval=FALSE}
write.csv(data.frame(y=preds), file = "cnn.csv")
```




<!--chapter:end:06-Datasets.Rmd-->

`r if (knitr::is_html_output()) '# References {-}'`

<!--chapter:end:07-Refs.Rmd-->

