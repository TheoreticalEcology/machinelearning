# Deep Learning {#Deep}

In this section, we will discuss both, different (deep) network architectures and different means to regularize and improve those deep architectures. 


## Network Architectures

### Deep Neural Networks (DNNs)

Deep neural networks are basically the same as simple artificial neural networks, only that they have more hidden layers.


### Convolutional Neural Networks (CNNs)

The main purpose of convolutional neural networks is image recognition. (Sound can be understood as an image as well!)
In a convolutional neural network, we have at least one convolution layer, additional to the normal, fully connected deep neural network layers. 

Neurons in a convolution layer are connected only to a small spatially contiguous area of the input layer (_receptive field_). We use this structure (_feature map_) to scan the **entire** features / neurons (e.g. picture). Think of the feature map as a _kernel_ or _filter_ (or imagine a sliding window with weighted pixels) that is used to scan the image. As the name is already indicating, this operation is a convolution in mathematics.
The kernel weights are optimized, but we use the same weights across the entire input neurons (_shared weights_).

The resulting (hidden) convolitional layer after training is called a _feature map_. You can think of the feature map as a map that shows you where the “shapes” expressed by the kernel appear in the input. One kernel / feature map will not be enough, we typically have many shapes that we want to recognize. Thus, the input layer is typically connected to several feature maps, which can be aggregated and followed by a second layer of feature maps, and so on.

You get one convolution map/layer for each kernel of one convolutional layer.


### Recurrent Neural Networks (RNNs)

Recurrent neural networks are used to model sequential data, i.e. a temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic:

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
  '<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/SEnXr6v2ifU"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```


### Natural Language Processing (NLP)

Natural language processing is actually more of a task than a network structure, but in the area of deep learning for natural language processing, particular network structures are used. The following video should give you an idea about what NLP is about.

```{r, eval=knitr::is_html_output(excludes = "epub"), results = 'asis', echo = F}
cat(
  '<iframe width="560" height="315" 
  src="https://www.youtube.com/embed/UFtXy0KRxVI"
  frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
  </iframe>'
)
```

See also the blog post linked with the Youtube video with accompanying code. Moreover, here is an <a href="https://nlpforhackers.io/keras-intro/" target="_blank" rel="noopener">article</a> that shows how natural language processing works with Keras, however, written in Python. As a challenge, you can take the code and implement it in R.


## Case Study: Dropout and Early Stopping in a Deep Neural Network 

Regularization in deep neural networks is very important because of the problem of overfitting. Standard regularization from statistics like $L1$ and $L2$ regularization are often fuzzy and require a lot of tuning. There are more stable and robust methods:

* Early stopping: Early stopping allows us to stop the training when for instance the test loss does not decrease anymore or the validation loss starts increasing.
* Dropout: The Dropout layer randomly sets input units to 0 with a frequency of a given rate at each step during training time, which helps prevent overfitting. Dropout is more robust than $L1$ and $L2$, and tuning of the dropout rate can be beneficial but a rate between $0.2-0.5$ often works quite well.

**Data preparation**

See \ref(mlr) for explanation about the preprocessing pipeline. 

```{r, message=FALSE}
#library(EcoData)
load("datasets.RData")
data(nasa)
str(nasa)

library(tidyverse)
library(mlr3)
library(mlr3pipelines)
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

data = nasa %>% select(-Orbit.Determination.Date,
                       -Close.Approach.Date, -Name, -Neo.Reference.ID)
data$Hazardous = as.factor(data$Hazardous)
task = TaskClassif$new(id = "nasa", backend = data,
                       target = "Hazardous", positive = "1")
preprocessing = po("imputeoor") %>>% po("scale") %>>% po("encode") 
data = preprocessing$train(task)[[1]]$data()

train = data[!is.na(data$Hazardous),]
submit = data[is.na(data$Hazardous),]

X = scale(train %>% select(-Hazardous))
Y = train %>% select(Hazardous)
Y = to_categorical(as.matrix(Y), 2)
```

**Early stopping**

```{r}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X)) %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dense(units = ncol(Y), activation = "softmax") 

model %>%
  compile(loss = loss_categorical_crossentropy,
          keras::optimizer_adamax(lr = 0.001))
summary(model)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 50L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4)
plot(model_history)
```

The validation loss first decreases but then starts increasing again, can you explain this behavior?
$\rightarrow$ Overfitting!

Let's try an $L1+L2$ regularization:

```{r}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X),
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = ncol(Y), activation = "softmax",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) 

model %>%
  compile(loss = loss_categorical_crossentropy,
          keras::optimizer_adamax(lr = 0.001))
summary(model)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4)
plot(model_history)
```

Better, but the validation loss still starts increasing after 40 epochs. But we can use early stopping to end the training before the validation loss starts increasing again!

```{r}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X),
              kernel_regularizer = regularizer_l1_l2( 0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = 50L, activation = "relu",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%
  layer_dense(units = ncol(Y), activation = "softmax",
              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) 

model %>%
  compile(loss = loss_categorical_crossentropy,
          keras::optimizer_adamax(lr = 0.001))
summary(model)

early = keras::callback_early_stopping(patience = 5L)

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4, callbacks = c(early))

plot(model_history)
```

Patience is the number of epochs to wait before aborting the training. 

**Dropout - another type of regularization**

@dropout suggests a dropout rate of 50% for internal hidden layers and 20% for the input layer. One advantage of dropout is that the training is more independent of the number of epochs i.e. the validation loss usually doesn't start to increase after several epochs. 

```{r}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

model = keras_model_sequential()
model %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 50L, activation = "relu", input_shape = ncol(X)) %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = 50L, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = ncol(Y), activation = "softmax") 

model %>%
  compile(loss = loss_categorical_crossentropy,
          keras::optimizer_adamax(lr = 0.001))

model_history =
  model %>%
    fit(x = X, y = Y, 
        epochs = 100L, batch_size = 20L, 
        shuffle = TRUE, validation_split = 0.4)
plot(model_history)
```

Of course, you can still combine early stopping and dropout, which is normally a good idea since it improves training efficiency (e.g. you could start with 1000 epochs and you know training will be aborted if it doesn't improve anymore).

<details>
  <summary>
    **<span style="color: #CC2FAA">Torch</span>**
  </summary>
  <p>
  
    Dropout and early stopping with Torch:

```{r}
library(torch)
torch_manual_seed(321L)
set.seed(123)

model_torch = nn_sequential(
  nn_dropout(0.2),
  nn_linear(ncol(X), 50L),
  nn_relu(),
  nn_dropout(0.5),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_dropout(0.5),
  nn_linear(50L, 50L),
  nn_relu(), 
  nn_dropout(0.5),
  nn_linear(50L, 2L)
)

YT = apply(Y, 1, which.max)

dataset_nasa = dataset(
  name = "nasa",
  initialize = function(nasa) {
    self$X = nasa$X
    self$Y = nasa$Y
  },
  .getitem = function(i) {
    X = self$X[i,,drop = FALSE] %>% torch_tensor()
    Y = self$Y[i] %>% torch_tensor()
    list(X, Y)
  },
  .length = function() {
    nrow(self$X)
  })

train_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), 
                      batch_size = 32, shuffle = TRUE)
test_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), 
                      batch_size = 32)

model_torch$train()

opt = optim_adam(model_torch$parameters, 0.01)

train_losses = c()
test_losses = c()
early_epoch = 0
min_loss = Inf
patience = 5
for(epoch in 1:50) {
  if(early_epoch >= patience){ break }
  
  train_loss = c()
  test_loss = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      loss$backward()
      opt$step()
      train_loss = c(train_loss, loss$item())
    }
  )
  
  coro::loop(
    for(batch in test_dl){
      pred = model_torch(batch[[1]]$squeeze())
      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = "mean")
      test_loss = c(test_loss, loss$item())
    }
  )
  
  ### Early stopping ###
  if(mean(test_loss) < min_loss) {
    min_loss = mean(test_loss)
    early_epoch = 0
  } else {
    early_epoch = early_epoch + 1
  }
  ###
  
  train_losses = c(train_losses, mean(train_loss))
  test_losses = c(test_losses, mean(test_loss))
  cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(train_loss)))
}

matplot(cbind(train_losses, test_losses), type = "o", pch = c(15, 16),
        col = c("darkblue", "darkred"), lty = 1, xlab = "Epoch",
        ylab = "Loss", las = 1)
legend("topright", bty = "n", col = c("darkblue", "darkred"),
       lty = 1, pch = c(15, 16), legend = c("Train loss", "Val loss") )
```

  </p>
</details>
<br/>


## Case Study - Fitting a Convolutional Neural Network on MNIST

We will show the use of convolutional neural networks with the MNIST data set. This data set is maybe one of the most famous image data sets. It consists of 60,000 handwritten digits from 0-9.

To do so, we define a few helper functions:

```{r}
library(keras)

rotate = function(x) t(apply(x, 2, rev))

imgPlot = function(img, title = ""){
  col = grey.colors(255)
  image(rotate(img), col = col, xlab = "", ylab = "", axes = FALSE,
     main = paste0("Label: ", as.character(title)))
}
```

The MNIST data set is so famous that there is an automatic download function in Keras:

```{r}
data = dataset_mnist()
train = data$train
test = data$test
```

Let's visualize a few digits:

```{r}
oldpar = par(mfrow = c(1, 3))
.n = sapply(1:3, function(x) imgPlot(train$x[x,,], train$y[x]))
par(oldpar)
```

Similar to the normal machine learning workflow, we have to scale the pixels (from 0-255) to the range of $[0, 1]$ and one hot encode the response. For scaling the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2D array/tensor is typically called a matrix.

```{r}
train_x = array(train$x/255, c(dim(train$x), 1))
test_x = array(test$x/255, c(dim(test$x), 1))
train_y = to_categorical(train$y, 10)
test_y = to_categorical(test$y, 10)
```

The last dimension denotes the number of channels in the image. In our case we have only one channel because the images are black and white.

Most times, we would have at least 3 color channels, for example RGB (red, green, blue) or HSV (hue, saturation, value), sometimes with several additional dimensions like transparency.

To build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size $2\times2$. These are 2D kernels because our images are 2D. For movies for example, one would use 3D kernels (the third dimension would correspond to time and not to the color channels).

```{r}
model = keras_model_sequential()
model %>% 
 layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,
               kernel_size = c(2L, 2L), activation = "relu") %>% 
 layer_max_pooling_2d() %>% 
 layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = "relu") %>% 
 layer_max_pooling_2d() %>% 
 layer_flatten() %>% 
 layer_dense(100L, activation = "relu") %>% 
 layer_dense(10L, activation = "softmax")
summary(model)
```

We additionally used a pooling layer for downsizing the resulting feature maps.
Without further specification, a $2\times2$ pooling layer is taken automatically.
Pooling layers take the input feature map and divide it into (in our case) parts of $2\times2$ size. Then the respective pooling operation is executed.
For every input map/layer, you get one (downsized) output map/layer.

As we are using the max pooling layer (there are sever other methods like the mean pooling), only the maximum value of these 4 parts is taken and forwarded further.
Example input:

```{}
1   2   |   5   8   |   3   6
6   5   |   2   4   |   8   1
------------------------------
9   4   |   3   7   |   2   5
0   3   |   2   7   |   4   9
```

We use max pooling for every field:

```{}
max(1, 2, 6, 5)   |   max(5, 8, 2, 4)   |   max(3, 6, 8, 1)
-----------------------------------------------------------
max(9, 4, 0, 3)   |   max(3, 7, 2, 7)   |   max(2, 5, 4, 9)
```

So the resulting pooled information is:

```{}
6   |   8   |   8
------------------
9   |   7   |   9
```

In this example, a $4\times6$ layer was transformed to a $2\times3$ layer and thus downsized.
This is similar to the biological process called _lateral inhibition_ where active neurons inhibit the activity of neighboring neurons.
It's a loss of information but often very useful for aggregating information and prevent overfitting.

After another convolutional and pooling layer we flatten the output. That means the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all weights from the last feature maps). You can imagine that like reshaping a matrix (2D) to a simple 1D vector. Then the full vector is used.
Having flattened the layer, we can simply use our typical output layer.

<details>
  <summary>
    **<span style="color: #CC2FAA">Torch</span>**
  </summary>
  <p>
  
    Prepare/download data:

```{r}
library(torch)
library(torchvision)

train_ds = mnist_dataset(
  ".",
  download = TRUE,
  train = TRUE,
  transform = transform_to_tensor
)

test_ds = mnist_dataset(
  ".",
  download = TRUE,
  train = FALSE,
  transform = transform_to_tensor
)
```

Build dataloader:

```{r}
train_dl = dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 32)
first_batch = train_dl$.iter()
df = first_batch$.next()

df$x$size()
```

Build convolutional neural network:
We have here to calculate the shapes of our layers on our own:

**We start with our input of shape (batch_size, 1, 28, 28)**

```{r}
sample = df$x
sample$size()
```

**First convolutional layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2)**

```{r}
conv1 = nn_conv2d(1, 16L, 2L, stride = 1L)
(sample %>% conv1)$size()
```

Output: batch_size = 32,  number of feature maps = 16, dimensions of each feature map = $(27 , 27)$
Wit a kernel size of two and stride = 1 we wil lose one pixel in each dimension...
Questions: 

* What happens if we increase the stride?
* What happens if we increase the kernel size?

**Pooling layer summarizes each feature map**

```{r}
(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()
```

kernel_size = 2L and stride = 2L halfs the pixel dimensions of our image

**Fully connected layer**

Now we have to flatten our final output of the convolutional neural network model to use a normal fully connected layer, but to do so we have to calculate the number of inputs for the fully connected layer:

```{r}
dims = (sample %>% conv1 %>%
          nnf_max_pool2d(kernel_size = 2L,stride = 2L))$size()
# Without the batch size of course.
final = prod(dims[-1]) 
print(final)
fc = nn_linear(final, 10L)
(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L)
  %>% torch_flatten(start_dim = 2L) %>% fc)$size()
```

Build the network:

```{r}
net = nn_module(
  "mnist",
  initialize = function() {
    self$conv1 = nn_conv2d(1, 16L, 2L)
    self$conv2 = nn_conv2d(16L, 16L, 3)
    self$fc1 = nn_linear(400L, 100L)
    self$fc2 = nn_linear(100L, 10L)
  },
  forward = function(x) {
    x %>% 
      self$conv1() %>%
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%         
      self$conv2() %>%
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      torch_flatten(start_dim = 2) %>%
      self$fc1() %>%
      nnf_relu() %>%
      self$fc2()
  }
)
```

  </p>
</details>
<br/>

The rest is as usual: First we compile the model.

```{r}
model %>% 
 compile(
 optimizer = keras::optimizer_adamax(0.01),
 loss = loss_categorical_crossentropy
 )
summary(model)
```

Then, we train the model:

```{r}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

epochs = 5L
batch_size = 32L
model %>% 
 fit(
 x = train_x, 
 y = train_y,
 epochs = epochs,
 batch_size = batch_size,
 shuffle = TRUE,
 validation_split = 0.2
)
```

<details>
  <summary>
    **<span style="color: #CC2FAA">Torch</span>**
  </summary>
  <p>

    Train model:

```{r}
library(torch)
torch_manual_seed(321L)
set.seed(123)

model_torch = net()
opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:3) {
  losses = c()
  coro::loop(for (batch in train_dl) {
    opt$zero_grad()
    pred = model_torch(batch[[1]])
    loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
    loss$backward()
    opt$step()
    losses = c(losses, loss$item())
  })
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}
```

    Evaluation:

```{r}
model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(
  for(b in test_dl){
    output = model_torch(b[[1]])
    labels = b[[2]]
    loss = nnf_cross_entropy(output, labels)
    test_losses = c(test_losses, loss$item())
    predicted = torch_max(output$data(), dim = 2)[[2]]
    total = total + labels$size(1)
    correct = correct + (predicted == labels)$sum()$item()
  }
)

mean(test_losses)
test_accuracy =  correct/total
test_accuracy
```

  </p>
</details>
<br/>


## Advanced Training Techniques 

### Data Augmentation

Having to train a convolutional neural network using very little data is a common problem. Data augmentation helps to artificially increase the number of images.

The idea is that a convolutional neural network learns specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures.

Luckily, it is very easy to use data augmentation in Keras.

To show this, we will use again the MNIST data set. We have to define a generator object (a specific object which infinitely draws samples from our data set). In the generator we can turn on the data augmentation. However, now we have to set the step size (steps_per_epoch) because the model does not know the first dimension of the image.

```{r, eval=FALSE}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

data = EcoData::dataset_flower()
train = data$train
test = data$test
labels = data$labels

model = keras_model_sequential()
model %>% 
  layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L),
                input_shape = c(80L, 80L, 3L), activation = "relu") %>% 
  layer_max_pooling_2d() %>% 
  layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L),
                activation = "relu") %>% 
  layer_max_pooling_2d() %>% 
  layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),
                strides = c(2L, 2L), activation = "relu") %>% 
  layer_max_pooling_2d() %>% 
  layer_flatten() %>% 
  layer_dropout(0.5) %>% 
  layer_dense(units = 5L, activation = "softmax")

  
# Data augmentation.
aug = image_data_generator(rotation_range = 90, 
                           zoom_range = c(0.3), 
                           horizontal_flip = TRUE, 
                           vertical_flip = TRUE)

# Data preparation / splitting.
indices = sample.int(nrow(train), 0.1*nrow(train))
generator = flow_images_from_data(train[-indices,,,]/255,
                                  k_one_hot(labels[-indices], num_classes = 5L),
                                  generator = aug,
                                  batch_size = 25L,
                                  shuffle = TRUE)

test = train[indices,,,]/255
test_labels = k_one_hot(labels[indices], num_classes = 5L)

# Our own training loop with early stopping:
epochs = 50L
batch_size = 25L
steps = floor(dim(train)[1]/batch_size)
optim = keras::optimizer_rmsprop()
max_patience = 5L
patience = 1L
min_val_loss = Inf
val_losses = c()
epoch_losses = c()

for(e in 1:epochs) {
  epoch_loss = c()
  for(s in 1:steps) {
    batch = reticulate::iter_next(generator)
    with(tf$GradientTape() %as% tape,
         {
            pred = model(batch[[1]], training = TRUE)
            loss = keras::loss_categorical_crossentropy(batch[[2]], pred)
            loss = tf$reduce_mean(loss)
          }
      )
    gradients = tape$gradient(target = loss,
                              sources = model$trainable_variables)
    optim$apply_gradients(purrr::transpose(list(gradients,
                                                model$trainable_variables)))
    epoch_loss = c(epoch_loss, loss$numpy())
  }
  epoch_losses = c(epoch_losses, epoch_loss)
  
  ## Test loss ##
  preds = model %>% predict(test)
  test_loss = keras::loss_categorical_crossentropy(test_labels, preds)
  val_losses = c(val_losses, tf$reduce_mean(test_loss)$numpy())
  
  cat("Epoch: ", e, " Train Loss: ", mean(epoch_losses),
      " Val Loss: ", val_losses[e],  " \n")
  
  if(val_losses[e] < min_val_loss) {
    min_val_loss = val_losses[e]
    patience = 1
  } else { patience = patience + 1 }
  if(patience == max_patience){ break }
}

preds = predict(model, data$test/255)
preds = apply(preds, 1, which.max)-1
```

So using data augmentation we can artificially increase the number of images.

<details>
  <summary>
    **<span style="color: #CC2FAA">Torch</span>**
  </summary>
  <p>

    In Torch, we have to change the transform function (but only for the train dataloader):

```{r}
library(torch)
torch_manual_seed(321L)
set.seed(123)

train_transforms = function(img) {
  img %>%
    transform_to_tensor() %>%
    transform_random_horizontal_flip(p = 0.3) %>% 
    transform_random_resized_crop(size = c(28L, 28L)) %>%
    transform_random_vertical_flip(0.3)
}

train_ds = mnist_dataset(".", download = TRUE, train = TRUE,
                         transform = train_transforms)
test_ds = mnist_dataset(".", download = TRUE, train = FALSE,
                        transform = transform_to_tensor)

train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 100L)

model_torch = net()
opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:1) {
  losses = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]])
      loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
      loss$backward()
      opt$step()
      losses = c(losses, loss$item())
    }
  )
  
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}

model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(
  for(b in test_dl){
    output = model_torch(b[[1]])
    labels = b[[2]]
    loss = nnf_cross_entropy(output, labels)
    test_losses = c(test_losses, loss$item())
    predicted = torch_max(output$data(), dim = 2)[[2]]
    total = total + labels$size(1)
    correct = correct + (predicted == labels)$sum()$item()
  }
)

test_accuracy =  correct/total
print(test_accuracy)
```

  </p>
</details>
<br/>


### Transfer Learning {#transfer}

Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning.

The main idea of transfer learning is that all the convolutional layers have mainly one task - learning to identify highly correlated neighbored features and therefore these learn structures such as edges in the image and only the top layer, the dense layer is the actual classifier of the convolutional neural network. Thus, one could think that we could only train the top layer as classifier. To do so, it will be confronted by sets of different edges/structures and has to decide the label based on these.

Again, this sounds very complicating but is again quite easy with Keras.

We will do this now with the CIFAR10 data set, so we have to prepare the data:

```{r}
data = keras::dataset_cifar10()
train = data$train
test = data$test

image = train$x[5,,,]
image %>% 
 image_to_array() %>%
 `/`(., 255) %>%
 as.raster() %>%
  plot()

train_x = array(train$x/255, c(dim(train$x)))
test_x = array(test$x/255, c(dim(test$x)))
train_y = to_categorical(train$y, 10)
test_y = to_categorical(test$y, 10)
```

Keras provides download functions for all famous architectures/convolutional neural network models which are already trained on the imagenet data set (another famous data set). These trained networks come already without their top layer, so we have to set include_top to false and change the input shape.

```{r, eval=FALSE}
densenet = application_densenet201(include_top = FALSE,
                                   input_shape  = c(32L, 32L, 3L))
```

Now, we will use not a sequential model but just a "keras_model" where we can specify the inputs and outputs. Thereby, the outputs are our own top layer, but the inputs are the densenet inputs, as these are already pre-trained.

```{r, eval=FALSE}
model = keras::keras_model(inputs = densenet$input, outputs = 
 layer_flatten(layer_dense(densenet$output, units = 10L,
                           activation = "softmax"))
 )
```


In the next step we want to freeze all layers except for our own last layer (with freezing I mean that these are not trained: we do not want to train the complete model, we only want to train the last layer). You can check the number of trainable weights via summary(model)

```{r, eval=FALSE}
model %>% freeze_weights(to = length(model$layers)-1)
summary(model)
```

And then the usual training:

```{r, eval=FALSE}
library(tensorflow)
library(keras)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

model %>% 
 compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax())
model %>% 
 fit(
 x = train_x, 
 y = train_y,
 epochs = 1L,
 batch_size = 32L,
 shuffle = T,
 validation_split = 0.2,
)
```

We have seen, that transfer learning can easily be done using Keras.

<details>
  <summary>
    **<span style="color: #CC2FAA">Torch</span>**
  </summary>
  <p>

    In Torch, we have to change the transform function (but only for the train dataloader):

```{r}
library(torchvision)
library(torch)
torch_manual_seed(321L)
set.seed(123)

train_ds = cifar10_dataset(".", download = TRUE, train = TRUE,
                           transform = transform_to_tensor)
test_ds = cifar10_dataset(".", download = TRUE, train = FALSE,
                          transform = transform_to_tensor)

train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)
test_dl = dataloader(test_ds, batch_size = 100L)

model_torch = model_resnet18(pretrained = TRUE)

# We will set all model parameters to constant values:
model_torch$parameters %>%
  purrr::walk(function(param) param$requires_grad_(FALSE))

# Let's replace the last layer (last layer is named 'fc') with our own layer:
inFeat = model_torch$fc$in_features
model_torch$fc = nn_linear(inFeat, out_features = 10L)

opt = optim_adam(params = model_torch$parameters, lr = 0.01)

for(e in 1:1) {
  losses = c()
  coro::loop(
    for(batch in train_dl){
      opt$zero_grad()
      pred = model_torch(batch[[1]])
      loss = nnf_cross_entropy(pred, batch[[2]], reduction = "mean")
      loss$backward()
      opt$step()
      losses = c(losses, loss$item())
    }
  )
  
  cat(sprintf("Loss at epoch %d: %3f\n", e, mean(losses)))
}

model_torch$eval()

test_losses = c()
total = 0
correct = 0

coro::loop(
  for(b in test_dl){
    output = model_torch(b[[1]])
    labels = b[[2]]
    loss = nnf_cross_entropy(output, labels)
    test_losses = c(test_losses, loss$item())
    predicted = torch_max(output$data(), dim = 2)[[2]]
    total = total + labels$size(1)
    correct = correct + (predicted == labels)$sum()$item()
  }
)

test_accuracy =  correct/total
print(test_accuracy)
```

  </p>
</details>
<br/>

**Flower data set**

Let's do that with our flower data set:

```{r, eval=FALSE}
library(keras)
library(tensorflow)
set_random_seed(321L, disable_gpu = FALSE)	#Already sets R's random seed.

data = EcoData::dataset_flower()
train = data$train
test = data$test
labels = data$labels

densenet = keras::application_densenet201(include_top = FALSE,
                                          input_shape = list(80L, 80L, 3L))

keras::freeze_weights(densenet)

model = keras_model(inputs = densenet$input, 
                    outputs = densenet$output %>% 
                      layer_flatten() %>% 
                      layer_dropout(0.2) %>% 
                      layer_dense(units = 200L) %>% 
                      layer_dropout(0.2) %>% 
                      layer_dense(units = 5L, activation = "softmax"))


# Data augmentation.
aug = image_data_generator(rotation_range = 180,zoom_range = 0.4,
                           width_shift_range = 0.2, height_shift_range = 0.2,
                           vertical_flip = TRUE, horizontal_flip = TRUE,
                           preprocessing_function = imagenet_preprocess_input)

# Data preparation / splitting.
indices = sample.int(nrow(train), 0.1*nrow(train))
generator = flow_images_from_data(train[-indices,,,]
                                  k_one_hot(labels[-indices], num_classes = 5L), 
                                  batch_size = 25L, shuffle = TRUE,
                                  generator = aug)

test = imagenet_preprocess_input(train[indices,,,])
test_labels = k_one_hot(labels[indices], num_classes = 5L)

# Our own training loop with early stopping:
epochs = 1L
batch_size = 45L
steps = floor(dim(train)[1]/batch_size)
optim = keras::optimizer_rmsprop(lr = 0.0005)
max_patience = 10L
patience = 1L
min_val_loss = Inf
val_losses = c()
epoch_losses = c()
for(e in 1:epochs) {
  epoch_loss = c()
  for(s in 1:steps) {
    batch = reticulate::iter_next(generator)
    with(tf$GradientTape() %as% tape, {
        pred = model(batch[[1]], training = TRUE)
        loss = keras::loss_categorical_crossentropy(batch[[2]], pred)
        loss = tf$reduce_mean(loss)
      })
    gradients = tape$gradient(target = loss,
                              sources = model$trainable_variables)
    optim$apply_gradients(purrr::transpose(list(gradients,
                                                model$trainable_variables)))
    epoch_loss = c(epoch_loss, loss$numpy())
  }
  epoch_losses = c(epoch_losses, epoch_loss)
  
  ## Test loss ##
  preds = model %>% predict(test)
  test_loss = keras::loss_categorical_crossentropy(test_labels, preds)
  val_losses = c(val_losses, tf$reduce_mean(test_loss)$numpy())
  
  cat("Epoch: ", e, " Train Loss: ", mean(epoch_losses),
      " Val Loss: ", val_losses[e],  " \n")
  
  if(val_losses[e] < min_val_loss) {
    min_val_loss = val_losses[e]
    patience = 1
  } else { patience = patience+1 }
  if(patience == max_patience) break
}

preds = predict(model, imagenet_preprocess_input(data$test))
```


