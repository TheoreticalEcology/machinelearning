<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Machine Learning and AI in TensorFlow and R</title>
  <meta name="description" content="Machine Learning and AI in TensorFlow and R" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Machine Learning and AI in TensorFlow and R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine Learning and AI in TensorFlow and R" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning and AI in TensorFlow and R" />
  
  <meta name="twitter:description" content="Machine Learning and AI in TensorFlow and R" />
  

<meta name="author" content="Maximilian Pichler and Florian Hartig" />


<meta name="date" content="2021-05-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Machine Learning and AI in TensorFlow and R</h1>
<p class="author"><em>Maximilian Pichler and Florian Hartig</em></p>
<p class="date" style="margin-top: 1.5em;"><em>2021-05-06</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#prerequisites"><span class="toc-section-number">1</span> Prerequisites</a></li>
<li><a href="#introduction"><span class="toc-section-number">2</span> Introduction to Machine Learning</a>
<ul>
<li><a href="#unsupervised-learning"><span class="toc-section-number">2.1</span> Unsupervised learning</a>
<ul>
<li><a href="#hierarchical-clustering"><span class="toc-section-number">2.1.1</span> Hierarchical clustering</a></li>
<li><a href="#k-means-clustering"><span class="toc-section-number">2.1.2</span> k-means clustering</a></li>
<li><a href="#density-based-clustering"><span class="toc-section-number">2.1.3</span> Density-based clustering</a></li>
<li><a href="#model-based-clustering"><span class="toc-section-number">2.1.4</span> Model-based clustering</a></li>
<li><a href="#ordination"><span class="toc-section-number">2.1.5</span> Ordination</a></li>
</ul></li>
<li><a href="#supervised-learning-regression-and-classification"><span class="toc-section-number">2.2</span> Supervised learning: regression and classification</a>
<ul>
<li><a href="#supervised-regression-using-random-forest"><span class="toc-section-number">2.2.1</span> Supervised regression using Random Forest</a></li>
<li><a href="#supervised-classification-using-random-forest"><span class="toc-section-number">2.2.2</span> Supervised classification using Random Forest</a></li>
</ul></li>
<li><a href="#introduction-to-tensorflow"><span class="toc-section-number">2.3</span> Introduction to Tensorflow</a>
<ul>
<li><a href="#tensorflow-data-containers"><span class="toc-section-number">2.3.1</span> Tensorflow data containers</a></li>
<li><a href="#tensorflow-data-types---good-practise-with-r-tf"><span class="toc-section-number">2.3.2</span> Tensorflow data types - good practise with R-TF</a></li>
</ul></li>
<li><a href="#introduction-to-pytorch"><span class="toc-section-number">2.4</span> Introduction to PyTorch</a>
<ul>
<li><a href="#pytorch-data-containers"><span class="toc-section-number">2.4.1</span> PyTorch data containers</a></li>
<li><a href="#torch-data-types---good-practise-with-r-tf"><span class="toc-section-number">2.4.2</span> Torch data types - good practise with R-TF</a></li>
</ul></li>
<li><a href="#first-steps-with-the-keras-framework"><span class="toc-section-number">2.5</span> First steps with the keras framework</a>
<ul>
<li><a href="#example-workflow-in-keras"><span class="toc-section-number">2.5.1</span> Example workflow in keras</a></li>
</ul></li>
</ul></li>
<li><a href="#fund"><span class="toc-section-number">3</span> Fundamental principles and techniques</a>
<ul>
<li><a href="#machine-learning-principles"><span class="toc-section-number">3.1</span> Machine learning principles</a>
<ul>
<li><a href="#optimization"><span class="toc-section-number">3.1.1</span> Optimization</a></li>
<li><a href="#regularization"><span class="toc-section-number">3.1.2</span> Regularization</a></li>
</ul></li>
<li><a href="#tree-based-ml-algorithms"><span class="toc-section-number">3.2</span> Tree-based ML algorithms</a>
<ul>
<li><a href="#classification-and-regression-trees"><span class="toc-section-number">3.2.1</span> Classification and Regression Trees</a></li>
<li><a href="#random-forest"><span class="toc-section-number">3.2.2</span> Random Forest</a></li>
<li><a href="#boosted-regression-trees"><span class="toc-section-number">3.2.3</span> Boosted regression trees</a></li>
</ul></li>
<li><a href="#distance-based-algorithms"><span class="toc-section-number">3.3</span> Distance-based algorithms</a>
<ul>
<li><a href="#k-nearest-neighbor"><span class="toc-section-number">3.3.1</span> k-nearest-neighbor</a></li>
<li><a href="#support-vector-machines-svm"><span class="toc-section-number">3.3.2</span> Support Vector Machines (SVM)</a></li>
</ul></li>
<li><a href="#artificial-neural-networks"><span class="toc-section-number">3.4</span> Artificial neural networks</a></li>
<li><a href="#the-standard-ml-pipeline-at-the-example-of-the-titanic-dataset"><span class="toc-section-number">3.5</span> The standard ML pipeline at the example of the titanic dataset</a>
<ul>
<li><a href="#data-cleaning"><span class="toc-section-number">3.5.1</span> Data cleaning</a></li>
<li><a href="#pre-processing-and-feature-selection"><span class="toc-section-number">3.5.2</span> Pre-processing and feature selection</a></li>
<li><a href="#split-data-for-training-and-testing"><span class="toc-section-number">3.5.3</span> Split data for training and testing</a></li>
<li><a href="#model-fitting"><span class="toc-section-number">3.5.4</span> Model fitting</a></li>
<li><a href="#model-evaluation"><span class="toc-section-number">3.5.5</span> Model evaluation</a></li>
<li><a href="#predictions-and-submission"><span class="toc-section-number">3.5.6</span> Predictions and submission</a></li>
</ul></li>
<li><a href="#mlr"><span class="toc-section-number">3.6</span> Bonus - ML pipelines with mlr3</a>
<ul>
<li><a href="#mlr3---the-basic-workflow"><span class="toc-section-number">3.6.1</span> mlr3 - the basic workflow</a></li>
<li><a href="#mlr3---hyper-parameter-tuning"><span class="toc-section-number">3.6.2</span> mlr3 - hyper-parameter tuning</a></li>
<li><a href="#mlr3---hyper-parameter-tuning-with-oversampling"><span class="toc-section-number">3.6.3</span> mlr3 - hyper-parameter tuning with oversampling</a></li>
</ul></li>
</ul></li>
<li><a href="#Deep"><span class="toc-section-number">4</span> Deep learning</a>
<ul>
<li><a href="#network-architectures"><span class="toc-section-number">4.1</span> Network architectures</a>
<ul>
<li><a href="#deep-neural-networks-dnns"><span class="toc-section-number">4.1.1</span> Deep neural networks (DNNs)</a></li>
<li><a href="#convolutional-neural-networks-dnns"><span class="toc-section-number">4.1.2</span> Convolutional neural networks (DNNs)</a></li>
<li><a href="#recurrent-neural-networks-rnns"><span class="toc-section-number">4.1.3</span> Recurrent neural networks (RNNs)</a></li>
<li><a href="#natural-language-processing-nlp"><span class="toc-section-number">4.1.4</span> Natural language processing (NLP)</a></li>
</ul></li>
<li><a href="#case-study-dropout-and-early-stopping-in-a-deep-neural-network"><span class="toc-section-number">4.2</span> Case study: dropout and early stopping in a deep neural network</a></li>
<li><a href="#case-study---fitting-a-convolutional-neural-networks-on-mnist"><span class="toc-section-number">4.3</span> Case study - fitting a Convolutional Neural Networks on MNIST</a></li>
<li><a href="#advanced-training-techniques"><span class="toc-section-number">4.4</span> Advanced training techniques</a>
<ul>
<li><a href="#data-augmentation"><span class="toc-section-number">4.4.1</span> Data Augmentation</a></li>
<li><a href="#transfer"><span class="toc-section-number">4.4.2</span> Transfer learning</a></li>
</ul></li>
</ul></li>
<li><a href="#interpretation-and-causality-with-machine-learning"><span class="toc-section-number">5</span> Interpretation and causality with machine learning</a>
<ul>
<li><a href="#explainable-ai"><span class="toc-section-number">5.1</span> Explainable AI</a>
<ul>
<li><a href="#a-practical-example"><span class="toc-section-number">5.1.1</span> A practical example</a></li>
<li><a href="#feature-importance"><span class="toc-section-number">5.1.2</span> Feature Importance</a></li>
<li><a href="#partial-dependencies"><span class="toc-section-number">5.1.3</span> Partial dependencies</a></li>
<li><a href="#accumulated-local-effects"><span class="toc-section-number">5.1.4</span> Accumulated local effects</a></li>
<li><a href="#friedmans-h-statistic"><span class="toc-section-number">5.1.5</span> Friedmans H-statistic</a></li>
<li><a href="#global-explainer---simplifying-the-ml-model"><span class="toc-section-number">5.1.6</span> Global explainer - Simplifying the ML model</a></li>
<li><a href="#local-explainer---lime-explaining-single-instances-observations"><span class="toc-section-number">5.1.7</span> Local explainer - LIME explaining single instances (observations)</a></li>
<li><a href="#local-explainer---shapley"><span class="toc-section-number">5.1.8</span> Local explainer - Shapley</a></li>
</ul></li>
<li><a href="#causal-inference-and-machine-learning"><span class="toc-section-number">5.2</span> Causal inference and machine learning</a>
<ul>
<li><a href="#causal-inference-on-static-data"><span class="toc-section-number">5.2.1</span> Causal inference on static data</a></li>
<li><a href="#structural-equation-models"><span class="toc-section-number">5.2.2</span> Structural equation models</a></li>
<li><a href="#automatic-causal-discovery"><span class="toc-section-number">5.2.3</span> Automatic causal discovery</a></li>
<li><a href="#causal-inference-on-dynamic-data"><span class="toc-section-number">5.2.4</span> Causal inference on dynamic data</a></li>
<li><a href="#outlook-for-machine-learning"><span class="toc-section-number">5.2.5</span> Outlook for machine learning</a></li>
</ul></li>
</ul></li>
<li><a href="#gans-vaes-and-reinforcement-learning"><span class="toc-section-number">6</span> GANs, VAEs, and Reinforcement learning</a>
<ul>
<li><a href="#generative-adversarial-network-gans"><span class="toc-section-number">6.1</span> Generative adversarial network (GANs)</a>
<ul>
<li><a href="#mnist---gan-based-on-dnns"><span class="toc-section-number">6.1.1</span> MNIST - GAN based on DNNs</a></li>
<li><a href="#flower---gan"><span class="toc-section-number">6.1.2</span> Flower - GAN</a></li>
</ul></li>
<li><a href="#autoencoder"><span class="toc-section-number">6.2</span> Autoencoder</a>
<ul>
<li><a href="#autoencoder---mnist-cnn"><span class="toc-section-number">6.2.1</span> Autoencoder - MNIST CNN</a></li>
</ul></li>
<li><a href="#varational-autoencoder"><span class="toc-section-number">6.3</span> Varational Autoencoder</a></li>
</ul></li>
<li><a href="#datasets"><span class="toc-section-number">7</span> Datasets</a>
<ul>
<li><a href="#titanic"><span class="toc-section-number">7.1</span> Titanic</a></li>
<li><a href="#plant-pollinator-database"><span class="toc-section-number">7.2</span> Plant-pollinator database</a></li>
<li><a href="#wine"><span class="toc-section-number">7.3</span> Wine</a></li>
<li><a href="#nasa"><span class="toc-section-number">7.4</span> Nasa</a></li>
<li><a href="#flower"><span class="toc-section-number">7.5</span> Flower</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning and AI in TensorFlow and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="prerequisites" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Prerequisites</h1>
<p><strong>R system</strong></p>
<p>Make sure you have a recent version of R (&gt;=3.6, ideally &gt;=4.0) and RStudio on your computers.</p>
<p><strong>Keras and tensorflow</strong></p>
<p>If you want to run the code on your own laptops, you also will need to install TensorFlow / Keras for R. For this, the following should work for most people:</p>
<p>Run in R:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;keras&quot;</span>, <span class="at">dependencies =</span> T)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>keras<span class="sc">::</span><span class="fu">install_keras</span>()</span></code></pre></div>
<p>This should work on most computers, in particular of all software is recent. Sometimes, however, things don’t work well, in particular the python distribution often makes problems. If the install does not work for you, we can look at it on Monday together. Also, we will provide some virtual machines in case your computers / laptops are too old or you don’t manage to install tensorflow.</p>
<p><strong>Torch for R</strong></p>
<p>We may also use Torch for R. This is an R frontend for the popular PyTorch framework. To install torch, type in R:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;torch&quot;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span></code></pre></div>
<p><strong>EcoData</strong></p>
<p>Finally, we may sometimes use datasets from the EcoData package. To install the package, run:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="at">repo =</span> <span class="st">&quot;florianhartig/EcoData&quot;</span>, <span class="at">subdir =</span> <span class="st">&quot;EcoData&quot;</span>, </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="at">dependencies =</span> <span class="cn">TRUE</span>, <span class="at">build_vignettes =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<!--chapter:end:index.Rmd-->
</div>
<div id="introduction" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Introduction to Machine Learning</h1>
<p>There are three basic ML tasks</p>
<ul>
<li>Unsupervised learning</li>
<li>Supervised learning</li>
<li>Reinforcement learning</li>
</ul>
<p><strong>Unsupervised learning</strong> is a technique, where one does not need to supervise the model. Instead, you allow the model to work on its own to discover information.</p>
<p>In <strong>supervised learning</strong>, you train an algorithm using labeled data, which means that you already know the correct answer for a part of the data (the so called tracings data).</p>
<p><strong>Reinforcement learning</strong> is a technique that emulates a game-like situation. The algorithm comes up with a solution by try and error and gets for the actions ether rewards or penalties. As in games, the goal is to maximize the rewards. We will talk on the last day more about this technique.</p>
<p>For the moment, we will focus on the first two tasks, supervised and unsupervised learning. To do so, we will first start with a small example, but before you start with the code, here a video to remind you of what we talked about in the class:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1AVrWvRvfxs" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<div id="unsupervised-learning" class="section level2" number="2.1">
<h2 number="2.1"><span class="header-section-number">2.1</span> Unsupervised learning</h2>
<p>In unsupervised learning, we want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider our iris dataset. Here, we have 150 observations of 4 floral traits</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>colors <span class="ot">=</span> <span class="fu">hcl.colors</span>(<span class="dv">3</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>traits <span class="ot">=</span> <span class="fu">as.matrix</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]) </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>species <span class="ot">=</span> iris<span class="sc">$</span>Species</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="at">y =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(species) , <span class="at">z =</span> traits, </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;Floral trait&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Individual&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-6-1.png" width="960" /></p>
<p>The observations are from 3 species, and indeed those species tend to have different traits, meaning that the observations form 3 clusters.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(traits, <span class="at">pch =</span> <span class="fu">as.integer</span>(species), <span class="at">col =</span> colors[<span class="fu">as.integer</span>(species)])</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>However, imagine we didn’t know what species are, which is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does.</p>
<div id="hierarchical-clustering" class="section level3" number="2.1.1">
<h3 number="2.1.1"><span class="header-section-number">2.1.1</span> Hierarchical clustering</h3>
<p>Build up a hierarchy (tree) between data points</p>
<ul>
<li>Agglomerative: start with each data point in their own cluster, merge them up hierarchically</li>
<li>Divisive: start with all data in one cluster, and split hierarchically</li>
</ul>
<p>Merges / splits are done according to linkage criterion, which measures distance between (potential) clusters. Cut the tree at a certain height to get clusters.</p>
<p>Here an example</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">dist</span>(traits)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">&quot;complete&quot;</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rect.hclust</span>(hc, <span class="at">k =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Same plot, but with colors for true species identity</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ape)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.phylo</span>(hc), </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">tip.color =</span> colors[<span class="fu">as.integer</span>(species)], </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">direction =</span> <span class="st">&quot;downwards&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>hcRes3 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc, <span class="at">k =</span> <span class="dv">3</span>)</span></code></pre></div>
<p>Calculate confusion matrix - note we switching labels here so that it fits to the species</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> hcRes3</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>tmp[hcRes3 <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>tmp[hcRes3 <span class="sc">==</span> <span class="dv">3</span>] <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>hcRes3 <span class="ot">&lt;-</span> tmp</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(hcRes3, species)</span></code></pre></div>
<pre><code>##       species
## hcRes3 setosa versicolor virginica
##      1     50          0         0
##      2      0         27         1
##      3      0         23        49</code></pre>
<p>Note that results might change if you choose a different agglomeration method, distance metric, or whether you scale your variables. Compare, e.g. to this example</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">&quot;ward.D2&quot;</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.phylo</span>(hc), </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">tip.color =</span> colors[<span class="fu">as.integer</span>(species)], </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">direction =</span> <span class="st">&quot;downwards&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>hcRes3 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(hcRes3, species)</span></code></pre></div>
<pre><code>##       species
## hcRes3 setosa versicolor virginica
##      1     50          0         0
##      2      0         49        15
##      3      0          1        35</code></pre>
<p>Which method is best?</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dendextend)</span></code></pre></div>
<pre><code>## 
## ---------------------
## Welcome to dendextend version 1.14.0
## Type citation(&#39;dendextend&#39;) for how to cite the package.
## 
## Type browseVignettes(package = &#39;dendextend&#39;) for the package vignette.
## The github page is: https://github.com/talgalili/dendextend/
## 
## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues
## Or contact: &lt;tal.galili@gmail.com&gt;
## 
##  To suppress this message use:  suppressPackageStartupMessages(library(dendextend))
## ---------------------</code></pre>
<pre><code>## 
## Attaching package: &#39;dendextend&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:ape&#39;:
## 
##     ladderize, rotate</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     cutree</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>methods <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;ward.D&quot;</span>, <span class="st">&quot;single&quot;</span>, <span class="st">&quot;complete&quot;</span>, <span class="st">&quot;average&quot;</span>, <span class="st">&quot;mcquitty&quot;</span>, <span class="st">&quot;median&quot;</span>, <span class="st">&quot;centroid&quot;</span>, <span class="st">&quot;ward.D2&quot;</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">dendlist</span>()</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq_along</span>(methods)) {</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> methods[i])   </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  out <span class="ot">&lt;-</span> <span class="fu">dendlist</span>(out, <span class="fu">as.dendrogram</span>(res))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(out) <span class="ot">&lt;-</span> methods</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>out</span></code></pre></div>
<pre><code>## $ward.D
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 199.6205 
## 
## $single
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 1.640122 
## 
## $complete
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 7.085196 
## 
## $average
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 4.062683 
## 
## $mcquitty
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 4.497283 
## 
## $median
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 2.82744 
## 
## $centroid
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 2.994307 
## 
## $ward.D2
## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 32.44761 
## 
## attr(,&quot;class&quot;)
## [1] &quot;dendlist&quot;</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>get_ordered_3_clusters <span class="ot">&lt;-</span> <span class="cf">function</span>(dend) {</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cutree</span>(dend, <span class="at">k =</span> <span class="dv">3</span>)[<span class="fu">order.dendrogram</span>(dend)]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>dend_3_clusters <span class="ot">&lt;-</span> <span class="fu">lapply</span>(out, get_ordered_3_clusters)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>compare_clusters_to_iris <span class="ot">&lt;-</span> <span class="cf">function</span>(clus) {<span class="fu">FM_index</span>(clus, <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> <span class="dv">50</span>), <span class="at">assume_sorted_vectors =</span> <span class="cn">TRUE</span>)}</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>clusters_performance <span class="ot">&lt;-</span> <span class="fu">sapply</span>(dend_3_clusters, compare_clusters_to_iris)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dotchart</span>(<span class="fu">sort</span>(clusters_performance), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fl">0.3</span>,<span class="dv">1</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;Fowlkes-Mallows index&quot;</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">main =</span> <span class="st">&quot;Performance of linkage methods </span><span class="sc">\n</span><span class="st"> in detecting the 3 species&quot;</span>,</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We might conclude here that ward.D2 works best. However, as we will learn later, optimizing the method without a hold-out for testing means that we may be overfitting. We should check this using cross-validation.</p>
</div>
<div id="k-means-clustering" class="section level3" number="2.1.2">
<h3 number="2.1.2"><span class="header-section-number">2.1.2</span> k-means clustering</h3>
<p>Another example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms.</p>
<p>A cluster refers to a collection of data points aggregated together because of certain similarities. In our example from above this similarities could be similar flowers aggregated together to a plant.</p>
<p>To start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the imaginary or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere and then adds each new data point to the cluster which minimizes the overall in-cluster sum of squares. After the algorithm has assigned a new data point to a cluster the centroid gets updated. By iterating this procedure for all data points and then starting again, the algorithm can find the optimum centroids and the data-points belonging to this cluster.</p>
<p>The k in K-means refers to the number of clusters and the ‘means’ refers to averaging of the data-points to find the centroids.</p>
<p>A typical pipeline for using kmeans clustering looks the same as for the other algortihms. After having visualized the data, we fit the model, visualize the results and have a look at the performance by use of the confusion matrix.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>kc <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(traits, <span class="dv">3</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>kc</span></code></pre></div>
<pre><code>## K-means clustering with 3 clusters of sizes 50, 62, 38
## 
## Cluster means:
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     5.006000    3.428000     1.462000    0.246000
## 2     5.901613    2.748387     4.393548    1.433871
## 3     6.850000    3.073684     5.742105    2.071053
## 
## Clustering vector:
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2
##  [60] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 2 3 3 3
## [119] 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2
## 
## Within cluster sum of squares by cluster:
## [1] 15.15100 39.82097 23.87947
##  (between_SS / total_SS =  88.4 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;</code></pre>
<p>Visualizing the results. Color codes true species identity, symbol shows cluster result</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris[<span class="fu">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>)], <span class="at">col =</span>  colors[<span class="fu">as.integer</span>(species)], <span class="at">pch =</span> kc<span class="sc">$</span>cluster)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(kc<span class="sc">$</span>centers[, <span class="fu">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>)], <span class="at">col =</span> colors, <span class="at">pch =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">cex =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We see that there are are some discrepancies. Confusion matrix:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(iris<span class="sc">$</span>Species, kc<span class="sc">$</span>cluster)</span></code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa     50  0  0
##   versicolor  0 48  2
##   virginica   0 14 36</code></pre>
<p>If you want to animate the clustering process, you could run</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(animation)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">saveGIF</span>(<span class="fu">kmeans.ani</span>(<span class="at">x =</span> traits[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="at">col =</span> colors), <span class="at">interval =</span> <span class="dv">1</span>, <span class="at">ani.width =</span> <span class="dv">800</span>, <span class="at">ani.height =</span> <span class="dv">800</span>)</span></code></pre></div>
<p>Ellbow technique to determine the number of clusters</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>getSumSq <span class="ot">&lt;-</span> <span class="cf">function</span>(k){<span class="fu">kmeans</span>(traits, k, <span class="at">nstart=</span><span class="dv">25</span>)<span class="sc">$</span>tot.withinss}</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>iris.kmeans1to10 <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, getSumSq)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, iris.kmeans1to10, <span class="at">type=</span><span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">frame =</span> <span class="cn">FALSE</span>, </span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Number of clusters K&quot;</span>,</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Total within-clusters sum of squares&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="density-based-clustering" class="section level3" number="2.1.3">
<h3 number="2.1.3"><span class="header-section-number">2.1.3</span> Density-based clustering</h3>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dbscan)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="fu">kNNdistplot</span>(traits, <span class="at">k =</span>  <span class="dv">4</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">0.4</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fpc package</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>dc <span class="ot">&lt;-</span> <span class="fu">dbscan</span>(traits, <span class="at">eps =</span> <span class="fl">0.4</span>, <span class="at">minPts =</span> <span class="dv">6</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>dc</span></code></pre></div>
<pre><code>## DBSCAN clustering for 150 objects.
## Parameters: eps = 0.4, minPts = 6
## The clustering contains 4 cluster(s) and 32 noise points.
## 
##  0  1  2  3  4 
## 32 46 36 14 22 
## 
## Available fields: cluster, eps, minPts</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span></code></pre></div>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(dc, traits, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">ggtheme =</span> <span class="fu">theme_light</span>())</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
</div>
<div id="model-based-clustering" class="section level3" number="2.1.4">
<h3 number="2.1.4"><span class="header-section-number">2.1.4</span> Model-based clustering</h3>
<p>The last class of methods for unsupervised clustering are so-called model-based clustering methods.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mclust)</span></code></pre></div>
<pre><code>##     __  ___________    __  _____________
##    /  |/  / ____/ /   / / / / ___/_  __/
##   / /|_/ / /   / /   / / / /\__ \ / /   
##  / /  / / /___/ /___/ /_/ /___/ // /    
## /_/  /_/\____/_____/\____//____//_/    version 5.4.7
## Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications.</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>mb <span class="ot">=</span> <span class="fu">Mclust</span>(traits)</span></code></pre></div>
<pre><code>## fitting ...
## 
  |                                                                                                                       
  |                                                                                                                 |   0%
  |                                                                                                                       
  |=                                                                                                                |   1%
  |                                                                                                                       
  |==                                                                                                               |   2%
  |                                                                                                                       
  |===                                                                                                              |   2%
  |                                                                                                                       
  |====                                                                                                             |   3%
  |                                                                                                                       
  |====                                                                                                             |   4%
  |                                                                                                                       
  |=====                                                                                                            |   5%
  |                                                                                                                       
  |======                                                                                                           |   6%
  |                                                                                                                       
  |=======                                                                                                          |   6%
  |                                                                                                                       
  |========                                                                                                         |   7%
  |                                                                                                                       
  |=========                                                                                                        |   8%
  |                                                                                                                       
  |==========                                                                                                       |   9%
  |                                                                                                                       
  |===========                                                                                                      |   9%
  |                                                                                                                       
  |============                                                                                                     |  10%
  |                                                                                                                       
  |============                                                                                                     |  11%
  |                                                                                                                       
  |=============                                                                                                    |  12%
  |                                                                                                                       
  |==============                                                                                                   |  13%
  |                                                                                                                       
  |===============                                                                                                  |  13%
  |                                                                                                                       
  |================                                                                                                 |  14%
  |                                                                                                                       
  |=================                                                                                                |  15%
  |                                                                                                                       
  |==================                                                                                               |  16%
  |                                                                                                                       
  |===================                                                                                              |  17%
  |                                                                                                                       
  |====================                                                                                             |  17%
  |                                                                                                                       
  |====================                                                                                             |  18%
  |                                                                                                                       
  |=====================                                                                                            |  19%
  |                                                                                                                       
  |======================                                                                                           |  20%
  |                                                                                                                       
  |=======================                                                                                          |  20%
  |                                                                                                                       
  |========================                                                                                         |  21%
  |                                                                                                                       
  |=========================                                                                                        |  22%
  |                                                                                                                       
  |==========================                                                                                       |  23%
  |                                                                                                                       
  |===========================                                                                                      |  24%
  |                                                                                                                       
  |============================                                                                                     |  24%
  |                                                                                                                       
  |============================                                                                                     |  25%
  |                                                                                                                       
  |=============================                                                                                    |  26%
  |                                                                                                                       
  |==============================                                                                                   |  27%
  |                                                                                                                       
  |===============================                                                                                  |  28%
  |                                                                                                                       
  |================================                                                                                 |  28%
  |                                                                                                                       
  |=================================                                                                                |  29%
  |                                                                                                                       
  |==================================                                                                               |  30%
  |                                                                                                                       
  |===================================                                                                              |  31%
  |                                                                                                                       
  |====================================                                                                             |  31%
  |                                                                                                                       
  |====================================                                                                             |  32%
  |                                                                                                                       
  |=====================================                                                                            |  33%
  |                                                                                                                       
  |======================================                                                                           |  34%
  |                                                                                                                       
  |=======================================                                                                          |  35%
  |                                                                                                                       
  |========================================                                                                         |  35%
  |                                                                                                                       
  |=========================================                                                                        |  36%
  |                                                                                                                       
  |==========================================                                                                       |  37%
  |                                                                                                                       
  |===========================================                                                                      |  38%
  |                                                                                                                       
  |============================================                                                                     |  39%
  |                                                                                                                       
  |=============================================                                                                    |  40%
  |                                                                                                                       
  |==============================================                                                                   |  41%
  |                                                                                                                       
  |===============================================                                                                  |  42%
  |                                                                                                                       
  |================================================                                                                 |  43%
  |                                                                                                                       
  |=================================================                                                                |  43%
  |                                                                                                                       
  |==================================================                                                               |  44%
  |                                                                                                                       
  |===================================================                                                              |  45%
  |                                                                                                                       
  |====================================================                                                             |  46%
  |                                                                                                                       
  |=====================================================                                                            |  47%
  |                                                                                                                       
  |======================================================                                                           |  48%
  |                                                                                                                       
  |=======================================================                                                          |  49%
  |                                                                                                                       
  |========================================================                                                         |  50%
  |                                                                                                                       
  |=========================================================                                                        |  50%
  |                                                                                                                       
  |==========================================================                                                       |  51%
  |                                                                                                                       
  |===========================================================                                                      |  52%
  |                                                                                                                       
  |============================================================                                                     |  53%
  |                                                                                                                       
  |=============================================================                                                    |  54%
  |                                                                                                                       
  |==============================================================                                                   |  55%
  |                                                                                                                       
  |===============================================================                                                  |  56%
  |                                                                                                                       
  |================================================================                                                 |  57%
  |                                                                                                                       
  |=================================================================                                                |  57%
  |                                                                                                                       
  |==================================================================                                               |  58%
  |                                                                                                                       
  |===================================================================                                              |  59%
  |                                                                                                                       
  |====================================================================                                             |  60%
  |                                                                                                                       
  |=====================================================================                                            |  61%
  |                                                                                                                       
  |======================================================================                                           |  62%
  |                                                                                                                       
  |=======================================================================                                          |  63%
  |                                                                                                                       
  |========================================================================                                         |  64%
  |                                                                                                                       
  |=========================================================================                                        |  65%
  |                                                                                                                       
  |==========================================================================                                       |  65%
  |                                                                                                                       
  |===========================================================================                                      |  66%
  |                                                                                                                       
  |============================================================================                                     |  67%
  |                                                                                                                       
  |=============================================================================                                    |  68%
  |                                                                                                                       
  |=============================================================================                                    |  69%
  |                                                                                                                       
  |==============================================================================                                   |  69%
  |                                                                                                                       
  |===============================================================================                                  |  70%
  |                                                                                                                       
  |================================================================================                                 |  71%
  |                                                                                                                       
  |=================================================================================                                |  72%
  |                                                                                                                       
  |==================================================================================                               |  72%
  |                                                                                                                       
  |===================================================================================                              |  73%
  |                                                                                                                       
  |====================================================================================                             |  74%
  |                                                                                                                       
  |=====================================================================================                            |  75%
  |                                                                                                                       
  |=====================================================================================                            |  76%
  |                                                                                                                       
  |======================================================================================                           |  76%
  |                                                                                                                       
  |=======================================================================================                          |  77%
  |                                                                                                                       
  |========================================================================================                         |  78%
  |                                                                                                                       
  |=========================================================================================                        |  79%
  |                                                                                                                       
  |==========================================================================================                       |  80%
  |                                                                                                                       
  |===========================================================================================                      |  80%
  |                                                                                                                       
  |============================================================================================                     |  81%
  |                                                                                                                       
  |=============================================================================================                    |  82%
  |                                                                                                                       
  |=============================================================================================                    |  83%
  |                                                                                                                       
  |==============================================================================================                   |  83%
  |                                                                                                                       
  |===============================================================================================                  |  84%
  |                                                                                                                       
  |================================================================================================                 |  85%
  |                                                                                                                       
  |=================================================================================================                |  86%
  |                                                                                                                       
  |==================================================================================================               |  87%
  |                                                                                                                       
  |===================================================================================================              |  87%
  |                                                                                                                       
  |====================================================================================================             |  88%
  |                                                                                                                       
  |=====================================================================================================            |  89%
  |                                                                                                                       
  |=====================================================================================================            |  90%
  |                                                                                                                       
  |======================================================================================================           |  91%
  |                                                                                                                       
  |=======================================================================================================          |  91%
  |                                                                                                                       
  |========================================================================================================         |  92%
  |                                                                                                                       
  |=========================================================================================================        |  93%
  |                                                                                                                       
  |==========================================================================================================       |  94%
  |                                                                                                                       
  |===========================================================================================================      |  94%
  |                                                                                                                       
  |============================================================================================================     |  95%
  |                                                                                                                       
  |=============================================================================================================    |  96%
  |                                                                                                                       
  |=============================================================================================================    |  97%
  |                                                                                                                       
  |==============================================================================================================   |  98%
  |                                                                                                                       
  |===============================================================================================================  |  98%
  |                                                                                                                       
  |================================================================================================================ |  99%
  |                                                                                                                       
  |=================================================================================================================| 100%</code></pre>
<p>Mclust automatically compares a number of candidate models (#clusters, shape) according to BIC. We can look at the selected model via</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>mb<span class="sc">$</span>G <span class="co"># two clusters</span></span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>mb<span class="sc">$</span>modelName <span class="co"># &gt; ellipsoidal, equal shape</span></span></code></pre></div>
<pre><code>## [1] &quot;VEV&quot;</code></pre>
<p>We see that the algorithm prefers to have 2 clusters. For better comparability to the other 2 methods, we will overrule this by setting:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>mb3 <span class="ot">=</span> <span class="fu">Mclust</span>(traits, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## fitting ...
## 
  |                                                                                                                       
  |                                                                                                                 |   0%
  |                                                                                                                       
  |========                                                                                                         |   7%
  |                                                                                                                       
  |===============                                                                                                  |  13%
  |                                                                                                                       
  |=======================                                                                                          |  20%
  |                                                                                                                       
  |==============================                                                                                   |  27%
  |                                                                                                                       
  |======================================                                                                           |  33%
  |                                                                                                                       
  |=============================================                                                                    |  40%
  |                                                                                                                       
  |=====================================================                                                            |  47%
  |                                                                                                                       
  |============================================================                                                     |  53%
  |                                                                                                                       
  |====================================================================                                             |  60%
  |                                                                                                                       
  |===========================================================================                                      |  67%
  |                                                                                                                       
  |===================================================================================                              |  73%
  |                                                                                                                       
  |==========================================================================================                       |  80%
  |                                                                                                                       
  |==================================================================================================               |  87%
  |                                                                                                                       
  |=========================================================================================================        |  93%
  |                                                                                                                       
  |=================================================================================================================| 100%</code></pre>
<p>Result in terms of the predicted densities for the 3 clusters</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mb3, <span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Predicted clusters</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mb3, <span class="at">what=</span><span class="fu">c</span>(<span class="st">&quot;classification&quot;</span>), <span class="at">add =</span> T)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Confusion matrix</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(iris<span class="sc">$</span>Species, mb3<span class="sc">$</span>classification)</span></code></pre></div>
<pre><code>##             
##               1  2  3
##   setosa     50  0  0
##   versicolor  0 45  5
##   virginica   0  0 50</code></pre>
</div>
<div id="ordination" class="section level3" number="2.1.5">
<h3 number="2.1.5"><span class="header-section-number">2.1.5</span> Ordination</h3>
<p>Note the relationship between clustering and ordination. Here a PCA ordination on on the</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>pcTraits <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(traits, <span class="at">center =</span> <span class="cn">TRUE</span>,<span class="at">scale. =</span> <span class="cn">TRUE</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="fu">biplot</span>(pcTraits, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.25</span>,<span class="fl">0.25</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.25</span>,<span class="fl">0.25</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>You can cluster the results of this ordination, ordinate before clustering, or superimpose one on the other.</p>
</div>
</div>
<div id="supervised-learning-regression-and-classification" class="section level2" number="2.2">
<h2 number="2.2"><span class="header-section-number">2.2</span> Supervised learning: regression and classification</h2>
<p>The two most prominent branches of supervised learning are regression and classification. Fundamentally, classification is about predicting a label and regression is about predicting a quantity. The following video explains that in more depth:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/i04Pfrb71vk" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<div id="supervised-regression-using-random-forest" class="section level3" number="2.2.1">
<h3 number="2.2.1"><span class="header-section-number">2.2.1</span> Supervised regression using Random Forest</h3>
<p>The random forest (RF) algorithm is possibly the most widely used ML algorithm and can be used for regression and classification. We will talk more about the algorithm on Day 2.</p>
<p>For the moment, we want to go through typical workflow for a supervised regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. We will again use the iris dataset that we used before. The goal is now to predict Sepal.Length based on the infomration about the other variables (including species).</p>
<p>Fitting the model</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span></code></pre></div>
<pre><code>## randomForest 4.6-14</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Sepal.Length <span class="sc">~</span> ., <span class="at">data =</span> iris)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># str(m1)</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># m1$type</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co"># predict(m1)</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = Sepal.Length ~ ., data = iris) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 1
## 
##           Mean of squared residuals: 0.1364625
##                     % Var explained: 79.97</code></pre>
<p>Visualization of the results</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(m1), iris<span class="sc">$</span>Sepal.Length, <span class="at">xlab =</span> <span class="st">&quot;predicted&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;observed&quot;</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(m1)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-1.png" width="672" />
To understand, the structure of a RF in more detail, we can use a package from GitHub</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github(&#39;araastat/reprtree&#39;)</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>reprtree<span class="sc">:::</span><span class="fu">plot.getTree</span>(m1, iris)</span></code></pre></div>
<pre><code>## Loading required package: plotrix</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
<div id="supervised-classification-using-random-forest" class="section level3" number="2.2.2">
<h3 number="2.2.2"><span class="header-section-number">2.2.2</span> Supervised classification using Random Forest</h3>
<p>With the RF, we can also do classification. The steps are the same as for regression tasks, but we can additionally, see how well it performed by looking at the so called confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represent the instances in an actual class. Thus the diagonals are the correctly predicted classes and the off-diagnoal elements are the falsly classified elements.</p>
<p>Fitting the model:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># str(m1)</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># m1$type</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co"># predict(m1)</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = Species ~ ., data = iris) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 4.67%
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         50          0         0        0.00
## versicolor      0         47         3        0.06
## virginica       0          4        46        0.08</code></pre>
<p>Visualizing the fitted model:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>reprtree<span class="sc">:::</span><span class="fu">plot.getTree</span>(m1, iris)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Visualizing results ecologically:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>oldpar <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris<span class="sc">$</span>Petal.Width, iris<span class="sc">$</span>Petal.Length, <span class="at">col =</span> iris<span class="sc">$</span>Species, <span class="at">main =</span> <span class="st">&quot;observed&quot;</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris<span class="sc">$</span>Petal.Width, iris<span class="sc">$</span>Petal.Length, <span class="at">col =</span> <span class="fu">predict</span>(m1), <span class="at">main =</span> <span class="st">&quot;predicted&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Confusion matrix:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">predict</span>(m1),iris<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>##             
##              setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         47         4
##   virginica       0          3        46</code></pre>
</div>
</div>
<div id="introduction-to-tensorflow" class="section level2" number="2.3">
<h2 number="2.3"><span class="header-section-number">2.3</span> Introduction to Tensorflow</h2>
<p>All operations in TF are written in C++ and are highly optimized. But dont worry, we don’t have to use C++ to use TF because there are several bindings for other languages. TensorFlow officialy supports a Python API, but meanwhile there are several community carried APIs for other languages:</p>
<ul>
<li>R</li>
<li>Go</li>
<li>Rust</li>
<li>Swift</li>
<li>JavaScript</li>
</ul>
<p>In this course we will use TF with the <a href="https://tensorflow.rstudio.com/" class="uri">https://tensorflow.rstudio.com/</a> binding, that was developed and published 2017 by the RStudio
Team. They developed first a R package (reticulate) to call python in R. Actually, we are using in R the python TF module (more about this later).
TF offers different levels of API. We could implement a neural network completly by ourselves, or we could use Keras which is provided by TF as a submodule. Keras is a powerful module for building and training neural networks. It allows us to build and train neural networks in a few lines of codes. Since the end of 2018, Keras and TF are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras
for neural networks but also how we can use the TF’s automatic differenation for using complex objective functions.</p>
<p>One of the most commonly used frameworks for machine learning is TensorFlow. TensorFlow is a open source linear algebra library with a focus on neural networks, published by Google in 2015. TF supports several interesting features, im particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization.</p>
<p>These advantages are nicely explained in the following video:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MotG3XI2qSs" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>To sum the most important points of the video up:</p>
<ul>
<li>TF is a math library which is highly optimized for neural networks</li>
<li>If a GPU is available, computations can be easily run on the GPU but even on a CPU is TF still very fast</li>
<li>The “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for the GPU)</li>
<li>The interface (the part of TF that we use) is written in python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend.</li>
</ul>
<p>All operations in TF are written in C++ and are highly optimized. But dont worry, we don’t have to use C++ to use TF, because there are several bindings for other languages. Officially, TensorFlow only supports a Python API, but meanwhile there are several community carried APIs for other languages, including R, Go, Rust, Swift or JavaScript. In this book, we will use TF with the <a href="https://tensorflow.rstudio.com/" class="uri">https://tensorflow.rstudio.com/</a> binding that was developed and published 2017 by the RStudio Team. They developed first a R package (reticulate) to call python in R. Actually, we are using in R the python TF module (more about this later).</p>
<p>Useful links:</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf">TensorFlow documentation</a> (which is for the python API, but just replace the ‘.’ with ‘$’)</li>
<li><a href="https://tensorflow.rstudio.com/">Rstudio tensorflow website</a></li>
</ul>
<div id="tensorflow-data-containers" class="section level3" number="2.3.1">
<h3 number="2.3.1"><span class="header-section-number">2.3.1</span> Tensorflow data containers</h3>
<p>TF has two data containers (structures):</p>
<ul>
<li>constant (tf$constant) :creates a constant (immutable) value in the computation graph</li>
<li>variable (tf$Variable): creates a mutable value in the computation graph (used as parameter/weight in models)</li>
</ul>
<p>To get started with tensorflow, we have to load the library and check if the installation worked.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Don&#39;t worry about weird messages. TF supports additional optimizations</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="fu">exists</span>(<span class="st">&quot;tf&quot;</span>)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Don’t worry about weird messages (they will only appear once at the start of the session).</p>
<p>We now can define the variables and do some math with them:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">constant</span>(<span class="dv">5</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">constant</span>(<span class="dv">10</span>)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(a)</span></code></pre></div>
<pre><code>## tf.Tensor(5.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(b)</span></code></pre></div>
<pre><code>## tf.Tensor(10.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">add</span>(a, b)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(c)</span></code></pre></div>
<pre><code>## tf.Tensor(15.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>tf<span class="sc">$</span><span class="fu">print</span>(c)</span></code></pre></div>
<p>Normal R methods such as print() are provided by the R package “tensorflow.”</p>
<p>The tensorflow library (created by the RStudio team) built R methods for all common operations:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="st">`</span><span class="at">+.tensorflow.tensor</span><span class="st">`</span> <span class="ot">=</span> <span class="cf">function</span>(a, b) <span class="fu">return</span>(tf<span class="sc">$</span><span class="fu">add</span>(a,b))</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>tf<span class="sc">$</span><span class="fu">print</span>(a<span class="sc">+</span>b)</span></code></pre></div>
<p>Their operators also transfrom automatically R numbers into constant tensors when attempting to add a tensor to a R number:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> c <span class="sc">+</span> <span class="dv">5</span>  <span class="co"># 5 is automatically converted to a tensor</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(d)</span></code></pre></div>
<pre><code>## tf.Tensor(20.0, shape=(), dtype=float32)</code></pre>
<p>TF container are objects, which means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object)
For instance, there is a method to transform the tensor object back to a R object:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(d)</span></code></pre></div>
<pre><code>## [1] &quot;tensorflow.tensor&quot;                                &quot;tensorflow.python.framework.ops.EagerTensor&quot;     
## [3] &quot;tensorflow.python.framework.ops._EagerTensorBase&quot; &quot;tensorflow.python.framework.ops.Tensor&quot;          
## [5] &quot;tensorflow.python.types.internal.NativeObject&quot;    &quot;tensorflow.python.types.core.Tensor&quot;             
## [7] &quot;python.builtin.object&quot;</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(d<span class="sc">$</span><span class="fu">numpy</span>())</span></code></pre></div>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
</div>
<div id="tensorflow-data-types---good-practise-with-r-tf" class="section level3" number="2.3.2">
<h3 number="2.3.2"><span class="header-section-number">2.3.2</span> Tensorflow data types - good practise with R-TF</h3>
<p>R uses dynamic typing, which means you can assign to a variable a number, character, function or whatever, and the the type is automatically infered.
In other languages you have to state explicitly the type, e.g. in C: int a = 5; float a = 5.0; char a = “a”;
While TF tries to infer dynamically the type, often you must state it explicitly.
Common important types:
- float32 (floating point number with 32bits, “single precision”)
- float64 (floating point number with 64bits, “double precision”)
- int8 (integer with 8bits)
The reason why TF is so explicit about the types is that many GPUs (e.g. the NVIDIA geforces) can handle only up to 32bit numbers! (you do not need high precision in graphical modeling)</p>
<p>But let us see in practice, what we have to do with these types and how to specifcy them:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>r_matrix <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">10</span><span class="sc">*</span><span class="dv">10</span>), <span class="dv">10</span>,<span class="dv">10</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">constant</span>(r_matrix, <span class="at">dtype =</span> <span class="st">&quot;float32&quot;</span>) </span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">constant</span>(<span class="fl">2.0</span>, <span class="at">dtype =</span> <span class="st">&quot;float64&quot;</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> m <span class="sc">/</span> b <span class="co"># doesn&#39;t work! we try to divide float32/float64</span></span></code></pre></div>
<p>So what went wrong here: we tried to divide a float32 to a float64 number, but, we can only divide numbers of the same type!</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>r_matrix <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">10</span><span class="sc">*</span><span class="dv">10</span>), <span class="dv">10</span>,<span class="dv">10</span>)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">constant</span>(r_matrix, <span class="at">dtype =</span> <span class="st">&quot;float64&quot;</span>)</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">constant</span>(<span class="fl">2.0</span>, <span class="at">dtype =</span> <span class="st">&quot;float64&quot;</span>)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> m <span class="sc">/</span> b <span class="co"># now it works</span></span></code></pre></div>
<p>We can also specify the type of the object by providing an object e.g. tf$float64.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>r_matrix <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">10</span><span class="sc">*</span><span class="dv">10</span>), <span class="dv">10</span>,<span class="dv">10</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">constant</span>(r_matrix, <span class="at">dtype =</span> tf<span class="sc">$</span>float64)</span></code></pre></div>
<p>Tensorflow arguments often require exact/explicit data types:
TF often expects for arguments integers. In R however an integer is normally saved as float.
Thus, we have to use a “L” after an integer to tell the R interpreter that it should be treated as an integer:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">is.integer</span>(<span class="dv">5</span>)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">is.integer</span>(5L)</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="fu">matrix</span>(<span class="fu">t</span>(r_matrix), <span class="dv">5</span>, <span class="dv">20</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>tf<span class="sc">$</span><span class="fu">reshape</span>(r_matrix, <span class="at">shape =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">20</span>))<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>tf<span class="sc">$</span><span class="fu">reshape</span>(r_matrix, <span class="at">shape =</span> <span class="fu">c</span>(5L, 20L))<span class="sc">$</span><span class="fu">numpy</span>()</span></code></pre></div>
<p>Skipping the “L” is one of the most common errors when using R-TF!</p>
</div>
</div>
<div id="introduction-to-pytorch" class="section level2" number="2.4">
<h2 number="2.4"><span class="header-section-number">2.4</span> Introduction to PyTorch</h2>
<p>PyTorch is another famous library for deep learning. As for tensorflow, torch itself is written in c++ but the API in python. Last year, the RStudio team released R-torch, and while r-tensorflow calls the python API in the background, the r-torch API is built directly on the c++ torch library!</p>
<p>Useful links:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a> (which is for the python API, bust just replace the ‘.’ with ‘$’)</li>
<li><a href="https://torch.mlverse.org/">R-torch website</a></li>
</ul>
<div id="pytorch-data-containers" class="section level3" number="2.4.1">
<h3 number="2.4.1"><span class="header-section-number">2.4.1</span> PyTorch data containers</h3>
<p>TF has two data containers (structures):</p>
<ul>
<li>constant (tf_tensor(…)) :creates a constant (immutable) value in the computation graph</li>
<li>variable (tf_$Variable_tensor(…, requires_grad=TRUE)): creates a mutable value in the computation graph (used as parameter/weight in models)</li>
</ul>
<p>To get started with torch, we have to load the library and check if the installation worked.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span></code></pre></div>
<pre><code>## Warning: Torch failed to start, restart your R session to try again. /home/maxpichler/R/x86_64-pc-linux-gnu-library/3.6/
## torch/deps/liblantern.so - libcudart.so.10.1: cannot open shared object file: No such file or directory</code></pre>
<p>Don’t worry about weird messages (they will only appear once at the start of the session).</p>
<p>We now can define the variables and do some math with them:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fl">5.</span>)</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fl">10.</span>)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(a)</span></code></pre></div>
<pre><code>## torch_tensor
##  5
## [ CPUFloatType{1} ]</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(b)</span></code></pre></div>
<pre><code>## torch_tensor
##  10
## [ CPUFloatType{1} ]</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> a<span class="sc">$</span><span class="fu">add</span>( b )</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(c)</span></code></pre></div>
<pre><code>## torch_tensor
##  15
## [ CPUFloatType{1} ]</code></pre>
<p>The r-torch package provides all common methods (an advantage over tensorflow)</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fl">5.</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fl">10.</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(a<span class="sc">+</span>b)</span></code></pre></div>
<pre><code>## torch_tensor
##  15
## [ CPUFloatType{1} ]</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(a<span class="sc">/</span>b)</span></code></pre></div>
<pre><code>## torch_tensor
##  0.5000
## [ CPUFloatType{1} ]</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(a<span class="sc">*</span>b)</span></code></pre></div>
<pre><code>## torch_tensor
##  50
## [ CPUFloatType{1} ]</code></pre>
<p>Their operators also transfrom automatically R numbers into tensors when attempting to add a tensor to a R number:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> a <span class="sc">+</span> <span class="dv">5</span>  <span class="co"># 5 is automatically converted to a tensor</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(d)</span></code></pre></div>
<pre><code>## torch_tensor
##  10
## [ CPUFloatType{1} ]</code></pre>
<p>As for tensorflow, we have to explicitly transform the tensors back to R:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(d)</span></code></pre></div>
<pre><code>## [1] &quot;torch_tensor&quot; &quot;R7&quot;</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(<span class="fu">as.numeric</span>(d))</span></code></pre></div>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
</div>
<div id="torch-data-types---good-practise-with-r-tf" class="section level3" number="2.4.2">
<h3 number="2.4.2"><span class="header-section-number">2.4.2</span> Torch data types - good practise with R-TF</h3>
<p>Similar to tensorflow:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>r_matrix <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">10</span><span class="sc">*</span><span class="dv">10</span>), <span class="dv">10</span>,<span class="dv">10</span>)</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="fu">torch_tensor</span>(r_matrix, <span class="at">dtype =</span> <span class="fu">torch_float32</span>()) </span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fl">2.0</span>, <span class="at">dtype =</span> <span class="fu">torch_float64</span>())</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> m <span class="sc">/</span> b </span></code></pre></div>
<p>But here’s a difference! With tensorfow we would get an error, but with r-torch, m is automatically casted to a double (float64). However, this is still bad practise!</p>
<p>During the course we will try to provide for all keras/tensorflow examples the corresponding pytorch code snippets.</p>
</div>
</div>
<div id="first-steps-with-the-keras-framework" class="section level2" number="2.5">
<h2 number="2.5"><span class="header-section-number">2.5</span> First steps with the keras framework</h2>
<p>We have seen that we can use TF directly from R, and we could use this knowledge to implement a neural network in TF directly from R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us with implementing the machine learning models in TF. The most common of those is Keras.</p>
<p>Keras is a powerful framework for building and training neural networks with a few lines of codes. Since the end of 2018, Keras and TF are completely interoperable, allowing us to utilize the best of both.</p>
<p>The objective of this lesson is to familiarize yourself with keras. If you have TF installed, Keras can be found within TF: tf.keras. However, the RStudio team has built an R package on top of tf.keras, and it is more convenient to use this. To load the keras package, type</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span></code></pre></div>
<div id="example-workflow-in-keras" class="section level3" number="2.5.1">
<h3 number="2.5.1"><span class="header-section-number">2.5.1</span> Example workflow in keras</h3>
<p>To show how keras works, we will now build a small classifier in keras to predict the three species of the iris dataset. Load the necessary packages and datasets:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(iris)</span></code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<p>It is beneficial for neural networks to scale the predictors (scaling = centering and standardization, see ?scale)
We also split our data into the predictors (X) and the response (Y = the three species).</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> iris[,<span class="dv">5</span>]</span></code></pre></div>
<p>Additionally, keras/tf cannot handle factors and we have to create contrasts (one-hot encoding):
To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like python and C++ on which TF is built, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Therefore, we have to substract it.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">to_categorical</span>(<span class="fu">as.integer</span>(Y)<span class="sc">-</span>1L, <span class="dv">3</span>)</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Y) <span class="co"># 3 colums, one for each level in the response</span></span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    1    0    0
## [3,]    1    0    0
## [4,]    1    0    0
## [5,]    1    0    0
## [6,]    1    0    0</code></pre>
<p>After having prepared the data, we will now see a typical workflow to specify a model in keras.</p>
<p><strong>1. Initiliaze a sequential model in keras:</strong></p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span></code></pre></div>
<p>A sequential keras model is a higher order type of model within keras and consists of one input and one output model.</p>
<p><strong>2. Add hidden layers to the model (we will learn more about hidden layers during the next days).</strong>
When specifiying the hidden layers, we also have to specify a so called activation function and their shape.
You can think of the activation function as decisive for what is forwarded to the next neuron (but we will learn more about it later). The shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(4L)) <span class="sc">%&gt;%</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L) <span class="sc">%&gt;%</span></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L) <span class="sc">%&gt;%</span></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 3L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>) </span></code></pre></div>
<ul>
<li>softmax scales a potential multidimensional vector to the interval (0,1]</li>
</ul>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>The torch syntax is very similar, we will give a list of layers to ‘nn_sequential’ function. Here, we have to specify the softmax activation function as an extra layer:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> </span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_sequential</span>(</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(4L, 20L),</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(20L, 20L),</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(20L, 20L),</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_linear</span>(20L, 3L),</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nn_softmax</span>(<span class="dv">2</span>)</span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
</details>
<p><br/></p>
<p><strong>3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).</strong></p>
<p>We will leaern about other options later, so for now, do not worry about the “lr” argument, crossentropy or the optimizer.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense (Dense)                                          (None, 20)                                       100                
## ___________________________________________________________________________________________________________________________
## dense_1 (Dense)                                        (None, 20)                                       420                
## ___________________________________________________________________________________________________________________________
## dense_2 (Dense)                                        (None, 20)                                       420                
## ___________________________________________________________________________________________________________________________
## dense_3 (Dense)                                        (None, 3)                                        63                 
## ===========================================================================================================================
## Total params: 1,003
## Trainable params: 1,003
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Specify optimizer and the parameters which will be trained (in our case the parameters of the network)</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>optimizer_torch <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span></code></pre></div>
</details>
<p><br/></p>
<p><strong>4. Fit model in 30 iterations(epochs)</strong></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> <span class="fu">apply</span>(Y,<span class="dv">2</span>,as.integer), <span class="at">epochs =</span> 30L, <span class="at">batch_size =</span> 20L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>In torch, we jump directly to the training loop, however, here we have to write our own training loop:</p>
<ol style="list-style-type: decimal">
<li>get a batch of data</li>
<li>predict on batch</li>
<li>calculate loss between predictions and true labels</li>
<li>backpropagate error</li>
<li>update weights</li>
<li>go to step 1 and repeat</li>
</ol>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate number of training steps</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> <span class="dv">30</span></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> <span class="dv">20</span></span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>steps <span class="ot">=</span> <span class="fu">round</span>(<span class="fu">nrow</span>(X)<span class="sc">/</span>batch_size<span class="sc">*</span><span class="dv">30</span>)</span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a>X_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(X)</span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a>Y_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fu">apply</span>(Y, <span class="dv">1</span>, which.max)) </span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a><span class="co"># set model into training status</span></span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">train</span>()</span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true" tabindex="-1"></a>log_losses <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-14"><a href="#cb121-14" aria-hidden="true" tabindex="-1"></a><span class="co"># training loop</span></span>
<span id="cb121-15"><a href="#cb121-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>steps) {</span>
<span id="cb121-16"><a href="#cb121-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get batch</span></span>
<span id="cb121-17"><a href="#cb121-17" aria-hidden="true" tabindex="-1"></a>  indices <span class="ot">=</span> <span class="fu">sample.int</span>( <span class="fu">nrow</span>(X), batch_size)</span>
<span id="cb121-18"><a href="#cb121-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb121-19"><a href="#cb121-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># reset backpropagation</span></span>
<span id="cb121-20"><a href="#cb121-20" aria-hidden="true" tabindex="-1"></a>  optimizer_torch<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb121-21"><a href="#cb121-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb121-22"><a href="#cb121-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># predict and calculate loss</span></span>
<span id="cb121-23"><a href="#cb121-23" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">model_torch</span>(X_torch[indices, ])</span>
<span id="cb121-24"><a href="#cb121-24" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, Y_torch[indices])</span>
<span id="cb121-25"><a href="#cb121-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb121-26"><a href="#cb121-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># backprop and weight update</span></span>
<span id="cb121-27"><a href="#cb121-27" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb121-28"><a href="#cb121-28" aria-hidden="true" tabindex="-1"></a>  optimizer_torch<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb121-29"><a href="#cb121-29" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb121-30"><a href="#cb121-30" aria-hidden="true" tabindex="-1"></a>  log_losses[i] <span class="ot">=</span> <span class="fu">as.numeric</span>(loss)</span>
<span id="cb121-31"><a href="#cb121-31" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</details>
<p><br/></p>
<p><strong>5. Plot training history:</strong></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(log_losses, <span class="at">xlab =</span> <span class="st">&quot;steps&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;loss&quot;</span>, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<img src="_main_files/figure-html/unnamed-chunk-63-1.png" width="672" />
</details>
<p><br/></p>
<p><strong>6. Create predictions:</strong></p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">=</span> <span class="fu">predict</span>(model, X) <span class="co"># probabilities for each class</span></span></code></pre></div>
<p>We will get probabilites:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(predictions) <span class="co"># quasi-probabilities for each species</span></span></code></pre></div>
<pre><code>##           [,1]        [,2]        [,3]
## [1,] 0.9817764 0.012261051 0.005962546
## [2,] 0.9656658 0.029913722 0.004420638
## [3,] 0.9845085 0.012120169 0.003371367
## [4,] 0.9811534 0.015199146 0.003647536
## [5,] 0.9861354 0.008666792 0.005197912
## [6,] 0.9751995 0.012824436 0.011976076</code></pre>
<p>For each plant, we want to know for which species we got the highest probability:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">apply</span>(predictions, <span class="dv">1</span>, which.max) </span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(preds)</span></code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2
##  [60] 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 3 2 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 2 3 3 3 3
## [119] 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</code></pre>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>The torch syntax is very similar, we will give a list of layers to ‘nn_sequential’ function. Here, we have to specify the softmax activation function as an extra layer:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>preds_torch <span class="ot">=</span> <span class="fu">model_torch</span>(<span class="fu">torch_tensor</span>(X))</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>preds_torch <span class="ot">=</span> <span class="fu">apply</span>(preds_torch, <span class="dv">1</span>, which.max) </span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(preds_torch)</span></code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2
##  [60] 2 2 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [119] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</code></pre>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds_torch <span class="sc">==</span> <span class="fu">as.integer</span>(iris<span class="sc">$</span>Species))</span></code></pre></div>
<pre><code>## [1] 0.9666667</code></pre>
</details>
<p><br/></p>
<p><strong>7. Calculate Accuracy (how often we have been correct):</strong></p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds <span class="sc">==</span> <span class="fu">as.integer</span>(iris<span class="sc">$</span>Species))</span></code></pre></div>
<pre><code>## [1] 0.9</code></pre>
<p><strong>8. Plot predictions, to see if we have done a good job:</strong></p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>oldpar <span class="ot">=</span> <span class="fu">par</span>()</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris<span class="sc">$</span>Sepal.Length, iris<span class="sc">$</span>Petal.Length, <span class="at">col =</span> iris<span class="sc">$</span>Species, <span class="at">main =</span> <span class="st">&quot;Observed&quot;</span>)</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris<span class="sc">$</span>Sepal.Length, iris<span class="sc">$</span>Petal.Length, <span class="at">col =</span> preds, <span class="at">main =</span> <span class="st">&quot;Predicted&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<p>So you see, building a neural network is with keras very easy and you can already do it on your own.</p>
<!--chapter:end:01-intro.Rmd-->
</div>
</div>
</div>
<div id="fund" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> Fundamental principles and techniques</h1>
<div id="machine-learning-principles" class="section level2" number="3.1">
<h2 number="3.1"><span class="header-section-number">3.1</span> Machine learning principles</h2>
<div id="optimization" class="section level3" number="3.1.1">
<h3 number="3.1.1"><span class="header-section-number">3.1.1</span> Optimization</h3>
<p>from wikipedia: " an optimization problem is the problem of finding the best solution from all feasible solutions"</p>
<p>Why do we need this “optimization?”</p>
<p>We need to somehow tell the algorithm what it should learn. To do so we have the so called loss-function, which expresses what our goal is. But we also need to somewhow find the configurations for which the loss function is
minimized. This is the job of the optimizer. Thus, an optimization consists of:</p>
<ul>
<li><p>A loss function (e.g. we tell in each training step the algorithm how many observations were miss-classified) guides the training of ML algorithms</p></li>
<li><p>The optimizer, which tries to update the weights of the ML algorithms in a way that the loss function is minimized</p></li>
</ul>
<p>Calculating analytically the global optima is a non-trivial problem and thus a bunch of diverse optimization algorithms evolved</p>
<p>Some optimization algorithms are inspired by biological systems e.g. Ants, Bee, or even slime algorithms. These optimizers are explained int the following video, have a look:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/X-iSQQgOd1A" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<div id="small-optimization-example" class="section level4" number="3.1.1.1">
<h4 number="3.1.1.1"><span class="header-section-number">3.1.1.1</span> Small optimization example</h4>
<p>As an easy example for optimization we can think of a quadratic function:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>func <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">return</span>(x<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>This function is so easy, we can randomly prob it and identify the optimum by plotting</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(a, <span class="fu">func</span>(a))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<p>The smallest value is at x = 0 (to be honest, we can calculate this for this simple case analytically)</p>
<p>We can also use an optimizer with the optim-function (the first argument is the starting value)</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim</span>(<span class="fl">1.0</span>, func)</span></code></pre></div>
<pre><code>## Warning in optim(1, func): one-dimensional optimization by Nelder-Mead is unreliable:
## use &quot;Brent&quot; or optimize() directly</code></pre>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(opt<span class="sc">$</span>par)</span></code></pre></div>
<pre><code>## [1] -8.881784e-16</code></pre>
<p>opt$par will return the best values found by the optimizer, which is really close to zeor :)</p>
</div>
<div id="advanced-optimization-example" class="section level4" number="3.1.1.2">
<h4 number="3.1.1.2"><span class="header-section-number">3.1.1.2</span> Advanced optimization example</h4>
<p>Optimization is also done when fitting a linear regression model. Thereby, we optimize the weights (intercept and slope). But using lm (y~x) is too simple, we would like to do this by hand to also better understand what optimization is and how it works.</p>
<p>As an example we take the airquality data set. First, we have to be sure to have no NAs in there. Then we split into response (Ozone) and predictors (Month, Day, Solar.R, Wind, Temp).Additionally it is beneficial for the optimizer, when the different predictors have the same support, and thus we scale them.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> airquality[<span class="fu">complete.cases</span>(airquality<span class="sc">$</span>Ozone) <span class="sc">&amp;</span> <span class="fu">complete.cases</span>(airquality<span class="sc">$</span>Solar.R),]</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> data<span class="sc">$</span>Ozone</span></code></pre></div>
<p>The model we want to optimize: <span class="math inline">\(ozone = Solar.R*X1 + Wind*X2 + Temp*X3 + Month*X4 + Day*X5 + X6\)</span></p>
<p>As the we assume that the residuals are normally distributed, our loss function is the mean squared errors: mean(predicted ozone - true ozone)^2)</p>
<p>Our task is now to find the parameters X1-X6 for which this loss function is the smallest. Therefore, we implement a function, that takes parameters and returns the loss.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>linear_regression <span class="ot">=</span> <span class="cf">function</span>(w) {</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> w[<span class="dv">1</span>]<span class="sc">*</span>X[,<span class="dv">1</span>] <span class="sc">+</span> <span class="co"># Solar.R</span></span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">2</span>]<span class="sc">*</span>X[,<span class="dv">2</span>] <span class="sc">+</span> <span class="co"># Wind</span></span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">3</span>]<span class="sc">*</span>X[,<span class="dv">3</span>] <span class="sc">+</span> <span class="co"># Temp</span></span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">4</span>]<span class="sc">*</span>X[,<span class="dv">4</span>] <span class="sc">+</span> <span class="co"># Month</span></span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">5</span>]<span class="sc">*</span>X[,<span class="dv">5</span>] <span class="sc">+</span></span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">6</span>]         <span class="co"># or X %*% w[1:5] + w[6]</span></span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># loss  = MSE, we want to find the optimal weights </span></span>
<span id="cb144-9"><a href="#cb144-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># to minimize the sum of squared residuals</span></span>
<span id="cb144-10"><a href="#cb144-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">mean</span>((pred <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb144-11"><a href="#cb144-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(loss)</span>
<span id="cb144-12"><a href="#cb144-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>For example we can sample some weights and see what the loss with this weights is.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="fu">linear_regression</span>(<span class="fu">runif</span>(<span class="dv">6</span>))</span></code></pre></div>
<pre><code>## [1] 2807.74</code></pre>
<p>We can try to find the optimum bruteforce (which means we will use a random set of weights and see for which the loss function is smallest):</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>random_search <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">6</span><span class="sc">*</span><span class="dv">5000</span>,<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>), <span class="dv">5000</span>, <span class="dv">6</span>)</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a>losses <span class="ot">=</span> <span class="fu">apply</span>(random_search, <span class="dv">1</span>, linear_regression)</span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(losses, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>random_search[<span class="fu">which.min</span>(losses),]</span></code></pre></div>
<pre><code>## [1]  4.847733 -9.630324  6.326885 -2.982961  1.211288  9.963248</code></pre>
<p>Bruteforce isn’t a good approach, it might work well with only a few parameters, but with increasing complexity and more parameters it will take a long time.</p>
<p>In R the optim function helps to get faster to the optimum.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim</span>(<span class="fu">runif</span>(<span class="dv">6</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), linear_regression)</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>opt<span class="sc">$</span>par</span></code></pre></div>
<pre><code>## [1]   0.6473966 -20.0175388  21.7380624 -10.2651763  -8.4418507  25.5586780</code></pre>
<p>By comparing the weights from the optimizer to the estimated weights of the lm() function, we see that our self-written code obtains the same weights as the lm.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
</div>
</div>
<div id="regularization" class="section level3" number="3.1.2">
<h3 number="3.1.2"><span class="header-section-number">3.1.2</span> Regularization</h3>
<p>Regularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a ML model. The most important distinction is between shrinkage estimators and estimators based on model averaging.</p>
<p><strong>Shrikage estimators</strong> are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are <em>“shrunk”</em> to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penality is proportional to the absolute deviation (L1 penalty), and the Tikhonov regularization aka ridge regression, where the penalty is proportional to the squared distance from the reference (L2 penalty). Thus, the loss function that we optimize is thus given by</p>
<p><span class="math display">\[
loss = fit - \lambda \cdot d
\]</span>
where fit refers to the standard loss function, <span class="math inline">\(\lambda\)</span> is the strength of the regularization, and <span class="math inline">\(d\)</span> is the chosen metrics, e.g. L1 or L2:
<span class="math display">\[
loss_{L1} = fit - \lambda \cdot \Vert weights \Vert_1
\]</span>
<span class="math display">\[
loss_{L2} = fit - \lambda \cdot \Vert weights \Vert_2
\]</span>
<span class="math inline">\(\lambda\)</span> and possibly d are typically optimized under cross-validation. L1 and L2 can be also combined which is then called elastic net (see <span class="citation"><a href="#ref-zou2005" role="doc-biblioref">Zou and Hastie</a> (<a href="#ref-zou2005" role="doc-biblioref">2005</a>)</span>)</p>
<p><strong>Model averaging</strong> refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it often more efficient to have many simpler models and average them, than to have one “super model.” The reasons are complicated, and explained in more detail in <span class="citation"><a href="#ref-dormann2018" role="doc-biblioref">Dormann et al.</a> (<a href="#ref-dormann2018" role="doc-biblioref">2018</a>)</span>.</p>
<p>A particular important application of averaging is boosting, where the principle is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap the data, and average the boot-strapped predictions.</p>
<p>To see how these techniques work in practice, let’s first focus on lasso and ridge regularization for weights in neural networks. We can imagine that the lasso and ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunk) to zero.</p>
<p>Lasso (penalty ~ abs(sum(Weights))) and ridge (penalty ~ (sum(Weights))^2) have slightly different properties, which are best understood if we express those as the effective prior preference that they create on the parameters:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-80-1.png" width="672" /></p>
<p>As you can see, the Lasso creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the ridge. For this reason, Lasso is often interpreted more as a model selection method.</p>
<p>The Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.</p>
<p>We can implement the linear regression also in keras, when we do not specify any hidden layers</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> airquality[<span class="fu">complete.cases</span>(airquality),]</span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> data<span class="sc">$</span>Ozone</span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="co"># l1/l2 on linear model</span></span>
<span id="cb154-6"><a href="#cb154-6" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb154-7"><a href="#cb154-7" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb154-8"><a href="#cb154-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="fu">dim</span>(X)[<span class="dv">2</span>]))</span>
<span id="cb154-9"><a href="#cb154-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_1&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense_4 (Dense)                                        (None, 1)                                        6                  
## ===========================================================================================================================
## Total params: 6
## Trainable params: 6
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(<span class="at">loss =</span> loss_mean_squared_error, <span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.5</span>))</span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a> model <span class="sc">%&gt;%</span></span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, <span class="at">epochs =</span> 50L, <span class="at">batch_size =</span> 20L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a>unconstrained <span class="ot">=</span> model<span class="sc">$</span><span class="fu">get_weights</span>()</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.014 -12.284  -3.302   8.454  95.348 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.980  21.264  &lt; 2e-16 ***
## XSolar.R       4.583      2.135   2.147   0.0341 *  
## XWind        -11.806      2.293  -5.149 1.23e-06 ***
## XTemp         18.067      2.610   6.922 3.66e-10 ***
## XMonth        -4.479      2.230  -2.009   0.0471 *  
## XDay           2.385      2.000   1.192   0.2358    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20.86 on 105 degrees of freedom
## Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 
## F-statistic: 34.99 on 5 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>], <span class="at">out_features =</span> 1L)</span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.5</span>)</span>
<span id="cb160-6"><a href="#cb160-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-7"><a href="#cb160-7" aria-hidden="true" tabindex="-1"></a>X_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(X)</span>
<span id="cb160-8"><a href="#cb160-8" aria-hidden="true" tabindex="-1"></a>Y_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fu">matrix</span>(Y, <span class="at">ncol =</span> 1L), <span class="at">dtype =</span> <span class="fu">torch_float32</span>())</span>
<span id="cb160-9"><a href="#cb160-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>) {</span>
<span id="cb160-10"><a href="#cb160-10" aria-hidden="true" tabindex="-1"></a>  indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(X), 20L)</span>
<span id="cb160-11"><a href="#cb160-11" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb160-12"><a href="#cb160-12" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">model_torch</span>(X_torch[indices, ])</span>
<span id="cb160-13"><a href="#cb160-13" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_mse_loss</span>(pred, Y_torch[indices,,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb160-14"><a href="#cb160-14" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb160-15"><a href="#cb160-15" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb160-16"><a href="#cb160-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb160-17"><a href="#cb160-17" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span>parameters</span></code></pre></div>
<pre><code>## $`0.weight`
## torch_tensor
##   4.6999 -11.6531  19.4547  -4.1804   0.9807
## [ CPUFloatType{1,5} ]
## 
## $`0.bias`
## torch_tensor
##  43.1719
## [ CPUFloatType{1} ]</code></pre>
</details>
<p><br/></p>
<p>But keras also allows use to use lasso and ridge on the weights.
Lets see what happens when we put a l1 (lasso) regularization on the weights:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="fu">dim</span>(X)[<span class="dv">2</span>]), </span>
<span id="cb164-4"><a href="#cb164-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(<span class="dv">10</span>), <span class="at">bias_regularizer =</span> <span class="fu">regularizer_l1</span>(<span class="dv">10</span>))</span>
<span id="cb164-5"><a href="#cb164-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_2&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense_5 (Dense)                                        (None, 1)                                        6                  
## ===========================================================================================================================
## Total params: 6
## Trainable params: 6
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb166-2"><a href="#cb166-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_mean_squared_error, <span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.5</span>), <span class="at">metrics =</span> <span class="fu">c</span>(metric_mean_squared_error))</span>
<span id="cb166-3"><a href="#cb166-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb166-4"><a href="#cb166-4" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb166-5"><a href="#cb166-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, <span class="at">epochs =</span> 30L, <span class="at">batch_size =</span> 20L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb166-6"><a href="#cb166-6" aria-hidden="true" tabindex="-1"></a>l1 <span class="ot">=</span> model<span class="sc">$</span><span class="fu">get_weights</span>()</span>
<span id="cb166-7"><a href="#cb166-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.014 -12.284  -3.302   8.454  95.348 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.980  21.264  &lt; 2e-16 ***
## XSolar.R       4.583      2.135   2.147   0.0341 *  
## XWind        -11.806      2.293  -5.149 1.23e-06 ***
## XTemp         18.067      2.610   6.922 3.66e-10 ***
## XMonth        -4.479      2.230  -2.009   0.0471 *  
## XDay           2.385      2.000   1.192   0.2358    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20.86 on 105 degrees of freedom
## Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 
## F-statistic: 34.99 on 5 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(<span class="fu">unlist</span>(l1), <span class="fu">unlist</span>(unconstrained))</span></code></pre></div>
<pre><code>##              [,1]       [,2]
## [1,]  1.931289077   4.800424
## [2,] -8.750296593 -11.806220
## [3,] 12.109655380  17.536272
## [4,] -0.004215715  -4.151731
## [5,] -0.023838159   2.253319
## [6,] 32.841133118  40.950207</code></pre>
<p>One can clearly see that parameters are pulled towards zero because of the regularization.</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>In torch, we have to specify the regularization on our own when calculating the loss.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>], <span class="at">out_features =</span> 1L)</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.5</span>)</span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a>X_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(X)</span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>Y_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fu">matrix</span>(Y, <span class="at">ncol =</span> 1L), <span class="at">dtype =</span> <span class="fu">torch_float32</span>())</span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>) {</span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a>  indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(X), 20L)</span>
<span id="cb172-10"><a href="#cb172-10" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb172-11"><a href="#cb172-11" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">model_torch</span>(X_torch[indices, ])</span>
<span id="cb172-12"><a href="#cb172-12" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_mse_loss</span>(pred, Y_torch[indices,,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb172-13"><a href="#cb172-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb172-14"><a href="#cb172-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Add l1:</span></span>
<span id="cb172-15"><a href="#cb172-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(model_torch<span class="sc">$</span>parameters)) loss <span class="ot">=</span> loss <span class="sc">+</span> model_torch<span class="sc">$</span>parameters[[i]]<span class="sc">$</span><span class="fu">abs</span>()<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">*</span><span class="fl">10.0</span></span>
<span id="cb172-16"><a href="#cb172-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb172-17"><a href="#cb172-17" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb172-18"><a href="#cb172-18" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb172-19"><a href="#cb172-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb172-20"><a href="#cb172-20" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span>parameters</span></code></pre></div>
<pre><code>## $`0.weight`
## torch_tensor
##   0.3801  -5.9210  13.1664  -0.1836   0.7057
## [ CPUFloatType{1,5} ]
## 
## $`0.bias`
## torch_tensor
##  36.6508
## [ CPUFloatType{1} ]</code></pre>
</details>
<p><br/></p>
</div>
</div>
<div id="tree-based-ml-algorithms" class="section level2" number="3.2">
<h2 number="3.2"><span class="header-section-number">3.2</span> Tree-based ML algorithms</h2>
<p>Famous ML algorithms such as random Forest and gradient boosted trees are based on classification- and regression trees.</p>
<div id="classification-and-regression-trees" class="section level3" number="3.2.1">
<h3 number="3.2.1"><span class="header-section-number">3.2.1</span> Classification and Regression Trees</h3>
<p>Tree-based models in general use a series of if-then rules to generate predictions from one or more decision trees.
In this lecture, we will explore regression and classifaction trees at the example of the airquality data set. There is one important hyper-parameter for regression trees: minsplit</p>
<ul>
<li>it controls the depth of tree (see the help of rpart for a description)</li>
<li>it controls the complexity of the tree and thus also be seen as a regularization parameter</li>
</ul>
<p>We first prepare and visualize the data and afterwards fit a decision tree.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;rpart&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dendextend&#39;:
## 
##     prune</code></pre>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a>data<span class="ot">=</span>airquality[<span class="fu">complete.cases</span>(airquality),]</span></code></pre></div>
<p>Fit and visualize a regression tree:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>rt <span class="ot">=</span> <span class="fu">rpart</span>(Ozone<span class="sc">~</span>., <span class="at">data =</span> data,<span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">10</span>))</span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(rt)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<p>Visualize the predictions:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(rt, data)</span>
<span id="cb181-2"><a href="#cb181-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(data<span class="sc">$</span>Temp, data<span class="sc">$</span>Ozone)</span>
<span id="cb181-3"><a href="#cb181-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data<span class="sc">$</span>Temp[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], pred[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<p>The angular form of the prediction line is typical for regression trees and is a weakness of it.</p>
</div>
<div id="random-forest" class="section level3" number="3.2.2">
<h3 number="3.2.2"><span class="header-section-number">3.2.2</span> Random Forest</h3>
<p>To overcome this weakness, a random forest uses an ensemble of regression/classification trees. Thus, the random forest is in principle nothing else than a normal regression/classification tree, but it uses the idea of the “wisdom of the crowd”: By asking many people (regression/classification trees) one can make a more informed decision (prediction/classification). When you buy a new phone for example you would also no directly go into the shop, but search in the internet and ask your friends and family.</p>
<p>There are two randomization steps with the RF that are responsible for the success of RF:</p>
<ul>
<li>bootstrap sample for each tree (we will sample observations with replacement from the dataset, for the phone this is like that not everyone has experience about each phone)</li>
<li>at each split, we will sample a subset of predictors which are then considered as potential splitting criterion (for the phone this is like that not everyone has the same decision criteria).</li>
</ul>
<p>Applying the random forest follows the same principle as for the methods before: we visualize the data (we have already done this so often for the airquality data set, thus we skip it here), fit the algorithm and then plot the outcomes.</p>
<p>Fit a RF and visualize the predictions:</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">=</span> <span class="fu">randomForest</span>(Ozone<span class="sc">~</span>., <span class="at">data =</span> data)</span>
<span id="cb182-3"><a href="#cb182-3" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(rf, data)</span>
<span id="cb182-4"><a href="#cb182-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone<span class="sc">~</span>Temp, <span class="at">data =</span> data)</span>
<span id="cb182-5"><a href="#cb182-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data<span class="sc">$</span>Temp[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], pred[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>One advantage of RF is that we will get a variable importance. At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Thus the variable importance shows us how important a variable is averaged over all trees.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>rf<span class="sc">$</span>importance</span></code></pre></div>
<pre><code>##         IncNodePurity
## Solar.R      18320.95
## Wind         31075.95
## Temp         34020.46
## Month        10806.85
## Day          14989.52</code></pre>
<p>There are several important hyperparameters in a random forest, that we can tune to get better results:</p>
<ul>
<li>Similar to the minsplit parameter in regression and classification trees, the hyper parameter nodesize controls for complexity -&gt; Minimum size of terminal nodes in the tree. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).</li>
<li>mtry - Number of features randomly sampled as candidates at each split.</li>
</ul>
</div>
<div id="boosted-regression-trees" class="section level3" number="3.2.3">
<h3 number="3.2.3"><span class="header-section-number">3.2.3</span> Boosted regression trees</h3>
<p>RF fits hundreds of trees independent of each other. Here, the idea of a boosted regression tree comes in. Maybe we could learn from the errors the previous weak learners make and thus enhance the performance of the algorithm.</p>
<p>Thus, a boosted regression tree (BRT) starts with a simple regression tree (weak learner) and then fits sequentially additional trees to improve the results.
There are two different strategies to do so:</p>
<ul>
<li>AdaBoost, wrong classified observations (by the previous tree) will get a higher weight and therefore the next trees will focus on difficult/missclassified observations.</li>
<li>Gradient boosting (state of the art), each sequential model will be fit on the residual errors of the previous model.</li>
</ul>
<p>We can fit a BRT using xgboost, but before we have to transform the data into a xgb.Dmatrix.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a>data_xg <span class="ot">=</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])), <span class="at">label =</span> data<span class="sc">$</span>Ozone)</span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a>brt <span class="ot">=</span> <span class="fu">xgboost</span>(data_xg, <span class="at">nrounds =</span> 16L, <span class="at">nthreads =</span> 4L)</span></code></pre></div>
<pre><code>## [23:37:55] WARNING: amalgamation/../src/learner.cc:516: 
## Parameters: { nthreads } might not be used.
## 
##   This may not be accurate due to some parameters are only used in language bindings but
##   passed down to XGBoost core.  Or some parameters are not used but slip through this
##   verification. Please open an issue if you find above cases.
## 
## 
## [1]  train-rmse:39.724625 
## [2]  train-rmse:30.225760 
## [3]  train-rmse:23.134842 
## [4]  train-rmse:17.899178 
## [5]  train-rmse:14.097784 
## [6]  train-rmse:11.375458 
## [7]  train-rmse:9.391275 
## [8]  train-rmse:7.889689 
## [9]  train-rmse:6.646585 
## [10] train-rmse:5.804859 
## [11] train-rmse:5.128438 
## [12] train-rmse:4.456416 
## [13] train-rmse:4.069464 
## [14] train-rmse:3.674615 
## [15] train-rmse:3.424578 
## [16] train-rmse:3.191301</code></pre>
<p>The nrounds controls how many sequantial trees we fit, in our example this was 16. When we predict to new data, we can limit the number of trees used to prevent overfitting (remeber: each new tree tries to improve the predictions of the previous trees).</p>
<p>Let us visualize the predictions for different number of trees:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb187-3"><a href="#cb187-3" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">predict</span>(brt, <span class="at">newdata =</span> data_xg, <span class="at">ntreelimit =</span> i)</span>
<span id="cb187-4"><a href="#cb187-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(data<span class="sc">$</span>Temp, data<span class="sc">$</span>Ozone, <span class="at">main =</span> i)</span>
<span id="cb187-5"><a href="#cb187-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(data<span class="sc">$</span>Temp[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], pred[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb187-6"><a href="#cb187-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="_main_files/figure-html/BRT2-1.png" width="672" />
There are also other ways to control for complexity of the BRT algorithm:</p>
<ul>
<li>max_depth, depth of each tree</li>
<li>shrinkage (each tree will get a weight and the weight will decrease with the number of trees)</li>
</ul>
<p>When having specified the final model, we can as for random forests get a variable importance:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>xgboost<span class="sc">::</span><span class="fu">xgb.importance</span>(<span class="at">model =</span> brt)</span></code></pre></div>
<pre><code>##    Feature        Gain     Cover  Frequency
## 1:    Temp 0.570071875 0.2958229 0.24836601
## 2:    Wind 0.348230710 0.3419576 0.24183007
## 3: Solar.R 0.058795559 0.1571072 0.30718954
## 4:     Day 0.019530002 0.1779925 0.16993464
## 5:   Month 0.003371853 0.0271197 0.03267974</code></pre>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((data<span class="sc">$</span>Ozone <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># RMSE</span></span></code></pre></div>
<pre><code>## [1] 17.89918</code></pre>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a>data_xg <span class="ot">=</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])), <span class="at">label =</span> data<span class="sc">$</span>Ozone)</span></code></pre></div>
<p>One important strength of xgboost is that we can directly do a cross-validation (which is indepdent on the BRT itself!) and specify its properties with nfold (the original dataset is randomly partitioned intonfoldequal size subsamples and each time one of these data sets is used for predictions to judge the performance):</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>brt <span class="ot">=</span> <span class="fu">xgboost</span>(data_xg, <span class="at">nrounds =</span> 5L)</span></code></pre></div>
<pre><code>## [1]  train-rmse:39.724625 
## [2]  train-rmse:30.225761 
## [3]  train-rmse:23.134842 
## [4]  train-rmse:17.899178 
## [5]  train-rmse:14.097784</code></pre>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>brt_cv <span class="ot">=</span> xgboost<span class="sc">::</span><span class="fu">xgb.cv</span>(<span class="at">data =</span> data_xg, <span class="at">nfold =</span> 3L, <span class="at">nrounds =</span> 3L, <span class="at">nthreads =</span> 4L)</span></code></pre></div>
<pre><code>## [1]  train-rmse:39.886848+1.148135   test-rmse:40.835787+4.206757 
## [2]  train-rmse:30.437888+0.922479   test-rmse:32.590283+4.841707 
## [3]  train-rmse:23.503598+0.751941   test-rmse:27.267021+5.103056</code></pre>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(brt_cv)</span></code></pre></div>
<pre><code>## ##### xgb.cv 3-folds
##  iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std
##     1        39.88685      1.1481345       40.83579      4.206757
##     2        30.43789      0.9224789       32.59028      4.841707
##     3        23.50360      0.7519412       27.26702      5.103056</code></pre>
<p>If we do three-folded CV, we actually fit three different BRT models (xgboost models)</p>
<p>This now tells us how well the model performed.</p>
</div>
</div>
<div id="distance-based-algorithms" class="section level2" number="3.3">
<h2 number="3.3"><span class="header-section-number">3.3</span> Distance-based algorithms</h2>
<p>In this chapter, we introduce support-vector machines (SVMs) and other distance-based methods.</p>
<div id="k-nearest-neighbor" class="section level3" number="3.3.1">
<h3 number="3.3.1"><span class="header-section-number">3.3.1</span> k-nearest-neighbor</h3>
<p>K Nearest Neighbour (kNN) is a simple algorithm that stores all the available cases and classifies the new data based on a similarity measure. It is mostly used to classifies a data point based on how its k nearest neighbours are classified.</p>
<p>Let us first see an example:</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> iris[,<span class="dv">5</span>]</span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[<span class="sc">-</span><span class="dv">100</span>,<span class="dv">1</span>], X[<span class="sc">-</span><span class="dv">100</span>,<span class="dv">3</span>], <span class="at">col =</span> Y)</span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X[<span class="dv">100</span>,<span class="dv">1</span>], X[<span class="dv">100</span>,<span class="dv">3</span>], <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">18</span>, <span class="at">cex =</span> <span class="fl">1.3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<p>Which class would you decide for the blue point? What are the classes of the nearest points? Well this procedure is used by the kNN and thus there is actually no “real” learning in a kNN.</p>
<p>For applying a kNN, we first have to scale teh data set, because we deal with distances and a priori want the same influence of all predictors (image one variable has values from -10.000 to 10.000 and one from -1 to 1, then the influence of the first variable on the distance to the other points is stronger than the second variable). As in the iris-data set there are no real test, we also have to split the data into train and test. Then we will follow the usual pipeline.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> iris</span>
<span id="cb200-2"><a href="#cb200-2" aria-hidden="true" tabindex="-1"></a>data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">=</span> <span class="fu">apply</span>(data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>],<span class="dv">2</span>, scale)</span>
<span id="cb200-3"><a href="#cb200-3" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(data), <span class="fl">0.7</span><span class="sc">*</span><span class="fu">nrow</span>(data))</span>
<span id="cb200-4"><a href="#cb200-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[indices,]</span>
<span id="cb200-5"><a href="#cb200-5" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data[<span class="sc">-</span>indices,]</span></code></pre></div>
<p>Fit model and create predictions:</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kknn)</span>
<span id="cb201-2"><a href="#cb201-2" aria-hidden="true" tabindex="-1"></a>knn <span class="ot">=</span> <span class="fu">kknn</span>(Species<span class="sc">~</span>., <span class="at">train =</span> train, <span class="at">test =</span> test)</span>
<span id="cb201-3"><a href="#cb201-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(knn)</span></code></pre></div>
<pre><code>## 
## Call:
## kknn(formula = Species ~ ., train = train, test = test)
## 
## Response: &quot;nominal&quot;
##           fit prob.setosa prob.versicolor prob.virginica
## 1      setosa           1       0.0000000     0.00000000
## 2      setosa           1       0.0000000     0.00000000
## 3      setosa           1       0.0000000     0.00000000
## 4      setosa           1       0.0000000     0.00000000
## 5      setosa           1       0.0000000     0.00000000
## 6      setosa           1       0.0000000     0.00000000
## 7      setosa           1       0.0000000     0.00000000
## 8      setosa           1       0.0000000     0.00000000
## 9      setosa           1       0.0000000     0.00000000
## 10     setosa           1       0.0000000     0.00000000
## 11     setosa           1       0.0000000     0.00000000
## 12     setosa           1       0.0000000     0.00000000
## 13     setosa           1       0.0000000     0.00000000
## 14     setosa           1       0.0000000     0.00000000
## 15     setosa           1       0.0000000     0.00000000
## 16     setosa           1       0.0000000     0.00000000
## 17     setosa           1       0.0000000     0.00000000
## 18 versicolor           0       0.9843084     0.01569160
## 19 versicolor           0       1.0000000     0.00000000
## 20 versicolor           0       0.7626128     0.23738721
## 21 versicolor           0       0.9843084     0.01569160
## 22 versicolor           0       0.9511855     0.04881448
## 23 versicolor           0       1.0000000     0.00000000
## 24 versicolor           0       1.0000000     0.00000000
## 25 versicolor           0       1.0000000     0.00000000
## 26 versicolor           0       0.8271189     0.17288113
## 27 versicolor           0       0.9843084     0.01569160
## 28  virginica           0       0.3205816     0.67941842
## 29  virginica           0       0.2266030     0.77339705
## 30  virginica           0       0.0000000     1.00000000
## 31  virginica           0       0.1008186     0.89918140
## 32  virginica           0       0.0000000     1.00000000
## 33  virginica           0       0.1257844     0.87421565
## 34  virginica           0       0.0000000     1.00000000
## 35  virginica           0       0.0000000     1.00000000
## 36  virginica           0       0.2373872     0.76261279
## 37  virginica           0       0.0000000     1.00000000
## 38  virginica           0       0.3569042     0.64309579
## 39  virginica           0       0.0000000     1.00000000
## 40 versicolor           0       0.6274042     0.37259581
## 41  virginica           0       0.0000000     1.00000000
## 42  virginica           0       0.2736997     0.72630027
## 43  virginica           0       0.3205816     0.67941842
## 44  virginica           0       0.3850877     0.61491234
## 45  virginica           0       0.0000000     1.00000000</code></pre>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(test<span class="sc">$</span>Species, <span class="fu">fitted</span>(knn))</span></code></pre></div>
<pre><code>##             
##              setosa versicolor virginica
##   setosa         17          0         0
##   versicolor      0         10         0
##   virginica       0          1        17</code></pre>
</div>
<div id="support-vector-machines-svm" class="section level3" number="3.3.2">
<h3 number="3.3.2"><span class="header-section-number">3.3.2</span> Support Vector Machines (SVM)</h3>
<p>Support vectors machines have a different approach. They try to divide the predictor space into spaces sectors for each class. To do so a SVM fits the parameters of a hyperplane (a n-1 dimensional subspace in a n-dimensional space) in the predictor space by optimizing the distance between the hyperlane and the nearest point from each class.</p>
<p>Fitting a SVM:</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb205-2"><a href="#cb205-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> iris</span>
<span id="cb205-3"><a href="#cb205-3" aria-hidden="true" tabindex="-1"></a>data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">=</span> <span class="fu">apply</span>(data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>],<span class="dv">2</span>, scale)</span>
<span id="cb205-4"><a href="#cb205-4" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(data), <span class="fl">0.7</span><span class="sc">*</span><span class="fu">nrow</span>(data))</span>
<span id="cb205-5"><a href="#cb205-5" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[indices,]</span>
<span id="cb205-6"><a href="#cb205-6" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data[<span class="sc">-</span>indices,]</span>
<span id="cb205-7"><a href="#cb205-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-8"><a href="#cb205-8" aria-hidden="true" tabindex="-1"></a>sm <span class="ot">=</span> <span class="fu">svm</span>(Species<span class="sc">~</span>., <span class="at">data =</span> train, <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>)</span>
<span id="cb205-9"><a href="#cb205-9" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(sm, <span class="at">newdata =</span> test)</span></code></pre></div>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="#cb206-1" aria-hidden="true" tabindex="-1"></a>oldpar <span class="ot">=</span> <span class="fu">par</span>()</span>
<span id="cb206-2"><a href="#cb206-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb206-3"><a href="#cb206-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(test<span class="sc">$</span>Sepal.Length, test<span class="sc">$</span>Petal.Length, <span class="at">col =</span>  pred, <span class="at">main =</span> <span class="st">&quot;predicted&quot;</span>)</span>
<span id="cb206-4"><a href="#cb206-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(test<span class="sc">$</span>Sepal.Length, test<span class="sc">$</span>Petal.Length, <span class="at">col =</span>  test<span class="sc">$</span>Species, <span class="at">main =</span> <span class="st">&quot;observed&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-94-1.png" width="672" /></p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(oldpar)</span></code></pre></div>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;cin&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;cra&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;csi&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;cxy&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;din&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;page&quot; cannot be set</code></pre>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(pred<span class="sc">==</span>test<span class="sc">$</span>Species) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.9555556</code></pre>
<p>SVM can only work on linear separable problems (A problem is called linearly separable if there exists at least one line in the plane with all of the points of one class on one side of the hyperplane and all the points of the others classes on the other side).</p>
<p>If this is not possible, we however, can use the so called kernel trick, which maps the predictor space into a (higher dimensional) space in which the problem is linear separable. After having identified the boundaries in the higher-dimensional space, we can project them back into the original dimensions.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb216-2"><a href="#cb216-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb216-3"><a href="#cb216-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb216-4"><a href="#cb216-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">expand.grid</span>(x1, x2)</span>
<span id="cb216-5"><a href="#cb216-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">apply</span>(X, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">exp</span>(<span class="sc">-</span>x[<span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> x[<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb216-6"><a href="#cb216-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>y)) <span class="sc">&lt;</span> <span class="fl">0.62</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb216-7"><a href="#cb216-7" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">matrix</span>(y, <span class="dv">100</span>, <span class="dv">100</span>))</span>
<span id="cb216-8"><a href="#cb216-8" aria-hidden="true" tabindex="-1"></a>animation<span class="sc">::</span><span class="fu">saveGIF</span>({</span>
<span id="cb216-9"><a href="#cb216-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">c</span>(<span class="st">&quot;truth&quot;</span>,<span class="st">&quot;linear&quot;</span>, <span class="st">&quot;radial&quot;</span>, <span class="st">&quot;sigmoid&quot;</span>)) {</span>
<span id="cb216-10"><a href="#cb216-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="st">&quot;truth&quot;</span>){</span>
<span id="cb216-11"><a href="#cb216-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">image</span>(<span class="fu">matrix</span>(y, <span class="dv">100</span>,<span class="dv">100</span>),<span class="at">main =</span> <span class="st">&quot;Ground truth&quot;</span>,<span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb216-12"><a href="#cb216-12" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb216-13"><a href="#cb216-13" aria-hidden="true" tabindex="-1"></a>      sv <span class="ot">=</span> e1071<span class="sc">::</span><span class="fu">svm</span>(<span class="at">x =</span> X, <span class="at">y =</span> <span class="fu">factor</span>(y), <span class="at">kernel =</span> i)</span>
<span id="cb216-14"><a href="#cb216-14" aria-hidden="true" tabindex="-1"></a>      <span class="fu">image</span>(<span class="fu">matrix</span>(<span class="fu">as.numeric</span>(<span class="fu">as.character</span>(<span class="fu">predict</span>(sv, X))), <span class="dv">100</span>,<span class="dv">100</span>),<span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Kernel: &quot;</span>, i),<span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb216-15"><a href="#cb216-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="at">labels =</span> <span class="fu">round</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="dv">1</span>))</span>
<span id="cb216-16"><a href="#cb216-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">axis</span>(<span class="dv">2</span>, <span class="at">at =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="at">labels =</span> <span class="fu">round</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="dv">1</span>), <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb216-17"><a href="#cb216-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb216-18"><a href="#cb216-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb216-19"><a href="#cb216-19" aria-hidden="true" tabindex="-1"></a>},<span class="at">movie.name =</span> <span class="st">&quot;svm.gif&quot;</span>, <span class="at">autobrowse =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="images/svm.gif" /><!-- --></p>
<p>As you have seen this does not work with each kernel. Thus, the problem is to find the actual correct kernel, which is again an optimization procedure and can thus be approximated.</p>
</div>
</div>
<div id="artificial-neural-networks" class="section level2" number="3.4">
<h2 number="3.4"><span class="header-section-number">3.4</span> Artificial neural networks</h2>
<p>Now, we will come to artificial neural networks (ANNs), for which the topic of regularization is also important. We can specify the regularization in each layer via the kernel_regularization argument.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> airquality</span>
<span id="cb217-3"><a href="#cb217-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      Ozone           Solar.R           Wind             Temp           Month            Day      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   Min.   :5.000   Min.   : 1.0  
##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   1st Qu.:6.000   1st Qu.: 8.0  
##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   Median :7.000   Median :16.0  
##  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   Mean   :6.993   Mean   :15.8  
##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   Max.   :9.000   Max.   :31.0  
##  NA&#39;s   :37       NA&#39;s   :7</code></pre>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> data[<span class="fu">complete.cases</span>(data),] <span class="co"># remove NAs</span></span>
<span id="cb219-2"><a href="#cb219-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      Ozone          Solar.R           Wind            Temp           Month            Day       
##  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00   Min.   :5.000   Min.   : 1.00  
##  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00   1st Qu.:6.000   1st Qu.: 9.00  
##  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00   Median :7.000   Median :16.00  
##  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79   Mean   :7.216   Mean   :15.95  
##  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50   3rd Qu.:9.000   3rd Qu.:22.50  
##  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00   Max.   :9.000   Max.   :31.00</code></pre>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="#cb221-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(data[,<span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>])</span>
<span id="cb221-2"><a href="#cb221-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> data[,<span class="dv">1</span>]</span>
<span id="cb221-3"><a href="#cb221-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb221-4"><a href="#cb221-4" aria-hidden="true" tabindex="-1"></a>penalty <span class="ot">=</span> <span class="fl">0.1</span></span>
<span id="cb221-5"><a href="#cb221-5" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb221-6"><a href="#cb221-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(5L), <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty)) <span class="sc">%&gt;%</span></span>
<span id="cb221-7"><a href="#cb221-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty) ) <span class="sc">%&gt;%</span></span>
<span id="cb221-8"><a href="#cb221-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty)) <span class="sc">%&gt;%</span></span>
<span id="cb221-9"><a href="#cb221-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty)) <span class="co"># one output dimension with a linear activation function</span></span>
<span id="cb221-10"><a href="#cb221-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_3&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense_6 (Dense)                                        (None, 100)                                      600                
## ___________________________________________________________________________________________________________________________
## dense_7 (Dense)                                        (None, 100)                                      10100              
## ___________________________________________________________________________________________________________________________
## dense_8 (Dense)                                        (None, 100)                                      10100              
## ___________________________________________________________________________________________________________________________
## dense_9 (Dense)                                        (None, 1)                                        101                
## ===========================================================================================================================
## Total params: 20,901
## Trainable params: 20,901
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="#cb223-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb223-2"><a href="#cb223-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(<span class="at">loss =</span> loss_mean_squared_error, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="fl">0.1</span>))</span>
<span id="cb223-3"><a href="#cb223-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb223-4"><a href="#cb223-4" aria-hidden="true" tabindex="-1"></a> model <span class="sc">%&gt;%</span></span>
<span id="cb223-5"><a href="#cb223-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> <span class="fu">matrix</span>(Y, <span class="at">ncol =</span> 1L), <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split =</span> <span class="fl">0.2</span>)</span>
<span id="cb223-6"><a href="#cb223-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="#cb225-1" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">=</span> <span class="fu">lapply</span>(model<span class="sc">$</span>weights, <span class="cf">function</span>(w) w<span class="sc">$</span><span class="fu">numpy</span>() )</span>
<span id="cb225-2"><a href="#cb225-2" aria-hidden="true" tabindex="-1"></a>fields<span class="sc">::</span><span class="fu">image.plot</span>(weights[[<span class="dv">1</span>]])</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-97-2.png" width="672" /></p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Again, we have to do the regularization on our own:</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>], <span class="at">out_features =</span> 100L),</span>
<span id="cb226-3"><a href="#cb226-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb226-4"><a href="#cb226-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(100L, 100L),</span>
<span id="cb226-5"><a href="#cb226-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb226-6"><a href="#cb226-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(100L, 100L),</span>
<span id="cb226-7"><a href="#cb226-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb226-8"><a href="#cb226-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(100L, 1L),</span>
<span id="cb226-9"><a href="#cb226-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb226-10"><a href="#cb226-10" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.1</span>)</span>
<span id="cb226-11"><a href="#cb226-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-12"><a href="#cb226-12" aria-hidden="true" tabindex="-1"></a>X_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(X)</span>
<span id="cb226-13"><a href="#cb226-13" aria-hidden="true" tabindex="-1"></a>Y_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(<span class="fu">matrix</span>(Y, <span class="at">ncol =</span> 1L), <span class="at">dtype =</span> <span class="fu">torch_float32</span>())</span>
<span id="cb226-14"><a href="#cb226-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>) {</span>
<span id="cb226-15"><a href="#cb226-15" aria-hidden="true" tabindex="-1"></a>  indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(X), 20L)</span>
<span id="cb226-16"><a href="#cb226-16" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb226-17"><a href="#cb226-17" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">model_torch</span>(X_torch[indices, ])</span>
<span id="cb226-18"><a href="#cb226-18" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_mse_loss</span>(pred, Y_torch[indices,,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb226-19"><a href="#cb226-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb226-20"><a href="#cb226-20" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Add l1 (only on the &#39;kernel weights&#39;):</span></span>
<span id="cb226-21"><a href="#cb226-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">8</span>, <span class="at">by =</span> <span class="dv">2</span>)) loss <span class="ot">=</span> loss <span class="sc">+</span> model_torch<span class="sc">$</span>parameters[[i]]<span class="sc">$</span><span class="fu">abs</span>()<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">*</span><span class="fl">0.1</span></span>
<span id="cb226-22"><a href="#cb226-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb226-23"><a href="#cb226-23" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb226-24"><a href="#cb226-24" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb226-25"><a href="#cb226-25" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Let’s visualize the first (input layer):</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="#cb227-1" aria-hidden="true" tabindex="-1"></a>fields<span class="sc">::</span><span class="fu">image.plot</span>(<span class="fu">as.matrix</span>(model_torch<span class="sc">$</span>parameters<span class="sc">$</span><span class="st">`</span><span class="at">0.weight</span><span class="st">`</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
</details>
<p><br/></p>
<p>Additionally to the usual l1 and l2 regularisation there is an additional regularisation: the so called dropout-layer (we will learn about this in more detail later).</p>
<p>Before we specialise on any tuning it is important to understand that ML always consists of a pipeline of actions.</p>
</div>
<div id="the-standard-ml-pipeline-at-the-example-of-the-titanic-dataset" class="section level2" number="3.5">
<h2 number="3.5"><span class="header-section-number">3.5</span> The standard ML pipeline at the example of the titanic dataset</h2>
<p>The typical ML workflow consist of:</p>
<ul>
<li>Data cleaning and exploration (EDA=explorative data analysis) with tidyverse</li>
<li>Pre-processing and feature selection</li>
<li>Splitting dataset into train and test set for evaluation</li>
<li>Model fitting</li>
<li>Model evaluation</li>
<li>New predictions
Here is an (optional) video that explains the entire pipeline from a slightly different perspective</li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/nKW8Ndu7Mjw" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>In the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics can be found here</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/nRtp7wSEtJA" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Another good reference is R for data science by Hadley <a href="https://r4ds.had.co.nz/"></a></p>
<p>For this lecture you need the titanic dataset provided by us. You can find it in GRIPS (datasets.RData in the dataset and submission section) or at <a href="http://rhsbio6.uni-regensburg.de:8500"></a>.</p>
<p>We have split the dataset already into training and testing datasets (the test split has one column less than the train split, as the result is not known a priori for the test)</p>
<div id="data-cleaning" class="section level3" number="3.5.1">
<h3 number="3.5.1"><span class="header-section-number">3.5.1</span> Data cleaning</h3>
<p>Load necessary libraries:</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb228-2"><a href="#cb228-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb228-3"><a href="#cb228-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<p>Load dataset:</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="#cb229-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;datasets.RData&quot;</span>)</span>
<span id="cb229-2"><a href="#cb229-2" aria-hidden="true" tabindex="-1"></a><span class="co"># library(EcoData)</span></span>
<span id="cb229-3"><a href="#cb229-3" aria-hidden="true" tabindex="-1"></a><span class="co"># data(titanic_ml)</span></span>
<span id="cb229-4"><a href="#cb229-4" aria-hidden="true" tabindex="-1"></a><span class="co"># titanic = titanic_ml</span></span>
<span id="cb229-5"><a href="#cb229-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> titanic</span></code></pre></div>
<p>Standard summaries:</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="#cb230-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1309 obs. of  14 variables:
##  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...
##  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...
##  $ name     : chr  &quot;Sinkkonen, Miss. Anna&quot; &quot;Woolner, Mr. Hugh&quot; &quot;Sage, Mr. Douglas Bullen&quot; &quot;Palsson, Master. Paul Folke&quot; ...
##  $ sex      : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ...
##  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...
##  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...
##  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...
##  $ ticket   : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 221 123 779 542 589 873 472 823 588 834 ...
##  $ fare     : num  13 35.5 69.55 21.07 8.05 ...
##  $ cabin    : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 1 94 1 1 1 1 1 1 1 1 ...
##  $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 4 4 4 4 4 4 2 4 2 ...
##  $ boat     : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 3 28 1 1 1 1 1 19 1 15 ...
##  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...
##  $ home.dest: Factor w/ 370 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ...</code></pre>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      pclass         survived          name               sex           age              sibsp            parch      
##  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667   Min.   :0.0000   Min.   :0.000  
##  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000  
##  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000   Median :0.0000   Median :0.000  
##  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811   Mean   :0.4989   Mean   :0.385  
##  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000  
##  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000   Max.   :8.0000   Max.   :9.000  
##                  NA&#39;s   :655                                      NA&#39;s   :263                                       
##       ticket          fare                     cabin      embarked      boat          body      
##  CA. 2343:  11   Min.   :  0.000                  :1014    :  2           :823   Min.   :  1.0  
##  1601    :   8   1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0  
##  CA 2144 :   8   Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0  
##  3101295 :   7   Mean   : 33.295   G6             :   5   S:914    15     : 37   Mean   :160.8  
##  347077  :   7   3rd Qu.: 31.275   B96 B98        :   4            14     : 33   3rd Qu.:256.0  
##  347082  :   7   Max.   :512.329   C22 C26        :   4            4      : 31   Max.   :328.0  
##  (Other) :1261   NA&#39;s   :1         (Other)        : 271            (Other):308   NA&#39;s   :1188   
##                 home.dest  
##                      :564  
##  New York, NY        : 64  
##  London              : 14  
##  Montreal, PQ        : 10  
##  Cornwall / Akron, OH:  9  
##  Paris, France       :  9  
##  (Other)             :639</code></pre>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="#cb234-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)</span></code></pre></div>
<pre><code>##      pclass survived                         name    sex  age sibsp parch             ticket   fare cabin embarked boat
## 561       2        1        Sinkkonen, Miss. Anna female 30.0     0     0             250648 13.000              S   10
## 321       1        1            Woolner, Mr. Hugh   male   NA     0     0              19947 35.500   C52        S    D
## 1177      3        0     Sage, Mr. Douglas Bullen   male   NA     8     2           CA. 2343 69.550              S     
## 1098      3        0  Palsson, Master. Paul Folke   male  6.0     3     1             349909 21.075              S     
## 1252      3        0   Tomlin, Mr. Ernest Portage   male 30.5     0     0             364499  8.050              S     
## 1170      3        0 Saether, Mr. Simon Sivertsen   male 38.5     0     0 SOTON/O.Q. 3101262  7.250              S     
##      body                home.dest
## 561    NA Finland / Washington, DC
## 321    NA          London, England
## 1177   NA                         
## 1098   NA                         
## 1252   50                         
## 1170   32</code></pre>
<p>The name variable consists of 1309 unique factors (there are 1309 observations…):</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="#cb236-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>name))</span></code></pre></div>
<pre><code>## [1] 1307</code></pre>
<p>However, there is a title in each name. Let’s extract the titles:</p>
<ol style="list-style-type: decimal">
<li>we will extract all names and split each name after each comma “,”</li>
<li>we will split the second split of the name after a point “.” and extract the titles</li>
</ol>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="#cb238-1" aria-hidden="true" tabindex="-1"></a>first_split <span class="ot">=</span> <span class="fu">sapply</span>(data<span class="sc">$</span>name, <span class="cf">function</span>(x) stringr<span class="sc">::</span><span class="fu">str_split</span>(x, <span class="at">pattern =</span> <span class="st">&quot;,&quot;</span>)[[<span class="dv">1</span>]][<span class="dv">2</span>])</span>
<span id="cb238-2"><a href="#cb238-2" aria-hidden="true" tabindex="-1"></a>titles <span class="ot">=</span> <span class="fu">sapply</span>(first_split, <span class="cf">function</span>(x) <span class="fu">strsplit</span>(x, <span class="st">&quot;.&quot;</span>,<span class="at">fixed =</span> <span class="cn">TRUE</span>)[[<span class="dv">1</span>]][<span class="dv">1</span>])</span></code></pre></div>
<p>We get 18 unique titles:</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="#cb239-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(titles)</span></code></pre></div>
<pre><code>## titles
##          Capt           Col           Don          Dona            Dr      Jonkheer          Lady         Major 
##             1             4             1             1             8             1             1             2 
##        Master          Miss          Mlle           Mme            Mr           Mrs            Ms           Rev 
##            61           260             2             1           757           197             2             8 
##           Sir  the Countess 
##             1             1</code></pre>
<p>A few titles have a very low occurrence rate:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="#cb241-1" aria-hidden="true" tabindex="-1"></a>titles <span class="ot">=</span> stringr<span class="sc">::</span><span class="fu">str_trim</span>((titles))</span>
<span id="cb241-2"><a href="#cb241-2" aria-hidden="true" tabindex="-1"></a>titles <span class="sc">%&gt;%</span></span>
<span id="cb241-3"><a href="#cb241-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">fct_count</span>()</span></code></pre></div>
<pre><code>## # A tibble: 18 x 2
##    f                n
##    &lt;fct&gt;        &lt;int&gt;
##  1 Capt             1
##  2 Col              4
##  3 Don              1
##  4 Dona             1
##  5 Dr               8
##  6 Jonkheer         1
##  7 Lady             1
##  8 Major            2
##  9 Master          61
## 10 Miss           260
## 11 Mlle             2
## 12 Mme              1
## 13 Mr             757
## 14 Mrs            197
## 15 Ms               2
## 16 Rev              8
## 17 Sir              1
## 18 the Countess     1</code></pre>
<p>We will collapse titles with low occurrences into one title, which we can easily do with the forcats package.</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="#cb243-1" aria-hidden="true" tabindex="-1"></a>titles2 <span class="ot">=</span></span>
<span id="cb243-2"><a href="#cb243-2" aria-hidden="true" tabindex="-1"></a>  forcats<span class="sc">::</span><span class="fu">fct_collapse</span>(titles,</span>
<span id="cb243-3"><a href="#cb243-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">officer =</span> <span class="fu">c</span>(<span class="st">&quot;Capt&quot;</span>, <span class="st">&quot;Col&quot;</span>, <span class="st">&quot;Major&quot;</span>, <span class="st">&quot;Dr&quot;</span>, <span class="st">&quot;Rev&quot;</span>),</span>
<span id="cb243-4"><a href="#cb243-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">royal =</span> <span class="fu">c</span>(<span class="st">&quot;Jonkheer&quot;</span>, <span class="st">&quot;Don&quot;</span>, <span class="st">&quot;Sir&quot;</span>, <span class="st">&quot;the Countess&quot;</span>, <span class="st">&quot;Dona&quot;</span>, <span class="st">&quot;Lady&quot;</span>),</span>
<span id="cb243-5"><a href="#cb243-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">miss =</span> <span class="fu">c</span>(<span class="st">&quot;Miss&quot;</span>, <span class="st">&quot;Mlle&quot;</span>),</span>
<span id="cb243-6"><a href="#cb243-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mrs =</span> <span class="fu">c</span>(<span class="st">&quot;Mrs&quot;</span>, <span class="st">&quot;Mme&quot;</span>, <span class="st">&quot;Ms&quot;</span>)</span>
<span id="cb243-7"><a href="#cb243-7" aria-hidden="true" tabindex="-1"></a>                        )</span></code></pre></div>
<p>We can count titles again to see the new number of titles</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="#cb244-1" aria-hidden="true" tabindex="-1"></a>titles2 <span class="sc">%&gt;%</span>  </span>
<span id="cb244-2"><a href="#cb244-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fct_count</span>()</span></code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   f           n
##   &lt;fct&gt;   &lt;int&gt;
## 1 officer    23
## 2 royal       6
## 3 Master     61
## 4 miss      262
## 5 mrs       200
## 6 Mr        757</code></pre>
<p>Add new title variable to dataset:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="#cb246-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span></span>
<span id="cb246-2"><a href="#cb246-2" aria-hidden="true" tabindex="-1"></a>  data <span class="sc">%&gt;%</span></span>
<span id="cb246-3"><a href="#cb246-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">title =</span> titles2)</span></code></pre></div>
<p>As a second example, we will explore and clean the numeric “age” variable:</p>
<p>Explore the variable:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      pclass         survived          name               sex           age              sibsp            parch      
##  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667   Min.   :0.0000   Min.   :0.000  
##  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000  
##  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000   Median :0.0000   Median :0.000  
##  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811   Mean   :0.4989   Mean   :0.385  
##  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000  
##  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000   Max.   :8.0000   Max.   :9.000  
##                  NA&#39;s   :655                                      NA&#39;s   :263                                       
##       ticket          fare                     cabin      embarked      boat          body      
##  CA. 2343:  11   Min.   :  0.000                  :1014    :  2           :823   Min.   :  1.0  
##  1601    :   8   1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0  
##  CA 2144 :   8   Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0  
##  3101295 :   7   Mean   : 33.295   G6             :   5   S:914    15     : 37   Mean   :160.8  
##  347077  :   7   3rd Qu.: 31.275   B96 B98        :   4            14     : 33   3rd Qu.:256.0  
##  347082  :   7   Max.   :512.329   C22 C26        :   4            4      : 31   Max.   :328.0  
##  (Other) :1261   NA&#39;s   :1         (Other)        : 271            (Other):308   NA&#39;s   :1188   
##                 home.dest       title    
##                      :564   officer: 23  
##  New York, NY        : 64   royal  :  6  
##  London              : 14   Master : 61  
##  Montreal, PQ        : 10   miss   :262  
##  Cornwall / Akron, OH:  9   mrs    :200  
##  Paris, France       :  9   Mr     :757  
##  (Other)             :639</code></pre>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="#cb249-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(data<span class="sc">$</span>age))<span class="sc">/</span><span class="fu">nrow</span>(data)</span></code></pre></div>
<pre><code>## [1] 0.2009167</code></pre>
<p>20% NAs!
Either we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups.
In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title).
After grouping, all operations (such as our median(age….)) will be done within the specified groups.</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="#cb251-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span></span>
<span id="cb251-2"><a href="#cb251-2" aria-hidden="true" tabindex="-1"></a>  data <span class="sc">%&gt;%</span></span>
<span id="cb251-3"><a href="#cb251-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(sex, pclass, title) <span class="sc">%&gt;%</span></span>
<span id="cb251-4"><a href="#cb251-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">age2 =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(age), <span class="fu">median</span>(age, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), age)) <span class="sc">%&gt;%</span></span>
<span id="cb251-5"><a href="#cb251-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">fare2 =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(fare), <span class="fu">median</span>(fare, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), fare)) <span class="sc">%&gt;%</span></span>
<span id="cb251-6"><a href="#cb251-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ungroup</span>()</span></code></pre></div>
</div>
<div id="pre-processing-and-feature-selection" class="section level3" number="3.5.2">
<h3 number="3.5.2"><span class="header-section-number">3.5.2</span> Pre-processing and feature selection</h3>
<p>We want to you keras in our example, but it cannot handle factors and requires scaled the data.</p>
<p>Normally, one would do this for all predictors, but as we here only showe the pipeline, we have sub-selected a bunch of predictors and do this only for them.</p>
<p>We first scale the numeric predictors abd change the factors with only two groups/levels into integer (this can be handled from keras)</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="#cb252-1" aria-hidden="true" tabindex="-1"></a>data_sub <span class="ot">=</span></span>
<span id="cb252-2"><a href="#cb252-2" aria-hidden="true" tabindex="-1"></a>  data <span class="sc">%&gt;%</span></span>
<span id="cb252-3"><a href="#cb252-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(survived, sex, age2, fare2, title, pclass) <span class="sc">%&gt;%</span></span>
<span id="cb252-4"><a href="#cb252-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">age2 =</span> scales<span class="sc">::</span><span class="fu">rescale</span>(age2, <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)), <span class="at">fare2 =</span> scales<span class="sc">::</span><span class="fu">rescale</span>(fare2, <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb252-5"><a href="#cb252-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">sex =</span> <span class="fu">as.integer</span>(sex) <span class="sc">-</span> 1L, <span class="at">title =</span> <span class="fu">as.integer</span>(title) <span class="sc">-</span> 1L, <span class="at">pclass =</span> <span class="fu">as.integer</span>(pclass <span class="sc">-</span> 1L))</span></code></pre></div>
<p>Factors with more than two levels, should be one hot encoded:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="#cb253-1" aria-hidden="true" tabindex="-1"></a>one_title <span class="ot">=</span> <span class="fu">k_one_hot</span>(data_sub<span class="sc">$</span>title, <span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>title)))<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb253-2"><a href="#cb253-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(one_title) <span class="ot">=</span> <span class="fu">levels</span>(data<span class="sc">$</span>title)</span>
<span id="cb253-3"><a href="#cb253-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb253-4"><a href="#cb253-4" aria-hidden="true" tabindex="-1"></a>one_sex <span class="ot">=</span> <span class="fu">k_one_hot</span>(data_sub<span class="sc">$</span>sex, <span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>sex)))<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb253-5"><a href="#cb253-5" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(one_sex) <span class="ot">=</span> <span class="fu">levels</span>(data<span class="sc">$</span>sex)</span>
<span id="cb253-6"><a href="#cb253-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb253-7"><a href="#cb253-7" aria-hidden="true" tabindex="-1"></a>one_pclass <span class="ot">=</span> <span class="fu">k_one_hot</span>(data_sub<span class="sc">$</span>pclass,  <span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>pclass)))<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb253-8"><a href="#cb253-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(one_pclass) <span class="ot">=</span> <span class="fu">paste0</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>pclass)), <span class="st">&quot;pclass&quot;</span>)</span></code></pre></div>
<p>And we have to add the dummy encoded variables to the dataset:</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="#cb254-1" aria-hidden="true" tabindex="-1"></a>data_sub <span class="ot">=</span> <span class="fu">cbind</span>(<span class="fu">data.frame</span>(<span class="at">survived=</span> data_sub<span class="sc">$</span>survived), one_title, one_sex, <span class="at">age =</span> data_sub<span class="sc">$</span>age2, <span class="at">fare =</span> data_sub<span class="sc">$</span>fare2, one_pclass)</span>
<span id="cb254-2"><a href="#cb254-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_sub)</span></code></pre></div>
<pre><code>##   survived officer royal Master miss mrs Mr female male        age       fare 1pclass 2pclass 3pclass
## 1        1       0     0      0    1   0  0      1    0 0.37369494 0.02537431       0       1       0
## 2        1       0     0      0    0   0  1      0    1 0.51774510 0.06929139       1       0       0
## 3        0       0     0      0    0   0  1      0    1 0.32359053 0.13575256       0       0       1
## 4        0       0     0      1    0   0  0      0    1 0.07306851 0.04113566       0       0       1
## 5        0       0     0      0    0   0  1      0    1 0.37995799 0.01571255       0       0       1
## 6        0       0     0      0    0   0  1      0    1 0.48016680 0.01415106       0       0       1</code></pre>
</div>
<div id="split-data-for-training-and-testing" class="section level3" number="3.5.3">
<h3 number="3.5.3"><span class="header-section-number">3.5.3</span> Split data for training and testing</h3>
<p>The splitting consists of two splits:</p>
<ul>
<li>an outer split (the original split, remember we got a train and test split without the response “survived”)</li>
<li>an inner split (we will split further the train dataset into another train and test split with known response)
The inner split is important because to assess the model’s performance and potential overfitting</li>
</ul>
<p>Outer split:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="#cb256-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data_sub[<span class="sc">!</span><span class="fu">is.na</span>(data_sub<span class="sc">$</span>survived),]</span>
<span id="cb256-2"><a href="#cb256-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data_sub[<span class="fu">is.na</span>(data_sub<span class="sc">$</span>survived),]</span></code></pre></div>
<p>Inner split:</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="#cb257-1" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(train), <span class="fl">0.7</span><span class="sc">*</span><span class="fu">nrow</span>(train))</span>
<span id="cb257-2"><a href="#cb257-2" aria-hidden="true" tabindex="-1"></a>sub_train <span class="ot">=</span> train[indices,]</span>
<span id="cb257-3"><a href="#cb257-3" aria-hidden="true" tabindex="-1"></a>sub_test <span class="ot">=</span> train[<span class="sc">-</span>indices,]</span></code></pre></div>
<p>What is the difference between the two splits? (Tip: have a look at the variable survived)</p>
</div>
<div id="model-fitting" class="section level3" number="3.5.4">
<h3 number="3.5.4"><span class="header-section-number">3.5.4</span> Model fitting</h3>
<p>In the next step we will fit a keras model on the train data of the inner split:</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="#cb258-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb258-2"><a href="#cb258-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb258-3"><a href="#cb258-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">input_shape =</span> <span class="fu">ncol</span>(sub_train) <span class="sc">-</span> 1L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb258-4"><a href="#cb258-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb258-5"><a href="#cb258-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb258-6"><a href="#cb258-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 2L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb258-7"><a href="#cb258-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_4&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense_10 (Dense)                                       (None, 20)                                       280                
## ___________________________________________________________________________________________________________________________
## dense_11 (Dense)                                       (None, 20)                                       420                
## ___________________________________________________________________________________________________________________________
## dense_12 (Dense)                                       (None, 20)                                       420                
## ___________________________________________________________________________________________________________________________
## dense_13 (Dense)                                       (None, 2)                                        42                 
## ===========================================================================================================================
## Total params: 1,162
## Trainable params: 1,162
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="#cb260-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb260-2"><a href="#cb260-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb260-3"><a href="#cb260-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, <span class="at">optimizer =</span> keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="fl">0.01</span>))</span>
<span id="cb260-4"><a href="#cb260-4" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb260-5"><a href="#cb260-5" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb260-6"><a href="#cb260-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(sub_train[,<span class="sc">-</span><span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">to_categorical</span>(sub_train[,<span class="dv">1</span>],<span class="at">num_classes =</span> 2L), <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 32L, <span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb260-7"><a href="#cb260-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-8"><a href="#cb260-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-119-1.png" width="672" /></p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="#cb262-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb262-2"><a href="#cb262-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="fu">dim</span>(sub_train[,<span class="sc">-</span><span class="dv">1</span>])[<span class="dv">2</span>], <span class="at">out_features =</span> 20L),</span>
<span id="cb262-3"><a href="#cb262-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb262-4"><a href="#cb262-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(20L, 20L),</span>
<span id="cb262-5"><a href="#cb262-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb262-6"><a href="#cb262-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(20L, 2L)</span>
<span id="cb262-7"><a href="#cb262-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb262-8"><a href="#cb262-8" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb262-9"><a href="#cb262-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb262-10"><a href="#cb262-10" aria-hidden="true" tabindex="-1"></a>X_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>( <span class="fu">as.matrix</span>(sub_train[,<span class="sc">-</span><span class="dv">1</span>])) </span>
<span id="cb262-11"><a href="#cb262-11" aria-hidden="true" tabindex="-1"></a>Y_torch <span class="ot">=</span> <span class="fu">torch_tensor</span>(sub_train[,<span class="dv">1</span>]<span class="sc">+</span><span class="dv">1</span>, <span class="at">dtype=</span> <span class="fu">torch_long</span>())</span>
<span id="cb262-12"><a href="#cb262-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>) {</span>
<span id="cb262-13"><a href="#cb262-13" aria-hidden="true" tabindex="-1"></a>  indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(sub_train), 20L)</span>
<span id="cb262-14"><a href="#cb262-14" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb262-15"><a href="#cb262-15" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">model_torch</span>(X_torch[indices, ])</span>
<span id="cb262-16"><a href="#cb262-16" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, Y_torch[indices], <span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb262-17"><a href="#cb262-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(loss)</span>
<span id="cb262-18"><a href="#cb262-18" aria-hidden="true" tabindex="-1"></a>  loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb262-19"><a href="#cb262-19" aria-hidden="true" tabindex="-1"></a>  opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb262-20"><a href="#cb262-20" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## torch_tensor
## 0.674505
## [ CPUFloatType{} ]
## torch_tensor
## 0.687759
## [ CPUFloatType{} ]
## torch_tensor
## 0.659471
## [ CPUFloatType{} ]
## torch_tensor
## 0.629493
## [ CPUFloatType{} ]
## torch_tensor
## 0.663918
## [ CPUFloatType{} ]
## torch_tensor
## 0.668542
## [ CPUFloatType{} ]
## torch_tensor
## 0.618048
## [ CPUFloatType{} ]
## torch_tensor
## 0.537068
## [ CPUFloatType{} ]
## torch_tensor
## 0.616329
## [ CPUFloatType{} ]
## torch_tensor
## 0.625666
## [ CPUFloatType{} ]
## torch_tensor
## 0.555079
## [ CPUFloatType{} ]
## torch_tensor
## 0.617161
## [ CPUFloatType{} ]
## torch_tensor
## 0.681252
## [ CPUFloatType{} ]
## torch_tensor
## 0.481454
## [ CPUFloatType{} ]
## torch_tensor
## 0.551633
## [ CPUFloatType{} ]
## torch_tensor
## 0.516082
## [ CPUFloatType{} ]
## torch_tensor
## 0.484434
## [ CPUFloatType{} ]
## torch_tensor
## 0.547698
## [ CPUFloatType{} ]
## torch_tensor
## 0.43127
## [ CPUFloatType{} ]
## torch_tensor
## 0.626385
## [ CPUFloatType{} ]
## torch_tensor
## 0.511114
## [ CPUFloatType{} ]
## torch_tensor
## 0.372383
## [ CPUFloatType{} ]
## torch_tensor
## 0.432114
## [ CPUFloatType{} ]
## torch_tensor
## 0.499551
## [ CPUFloatType{} ]
## torch_tensor
## 0.453553
## [ CPUFloatType{} ]
## torch_tensor
## 0.466136
## [ CPUFloatType{} ]
## torch_tensor
## 0.448839
## [ CPUFloatType{} ]
## torch_tensor
## 0.759023
## [ CPUFloatType{} ]
## torch_tensor
## 0.388911
## [ CPUFloatType{} ]
## torch_tensor
## 0.495784
## [ CPUFloatType{} ]
## torch_tensor
## 0.54462
## [ CPUFloatType{} ]
## torch_tensor
## 0.506348
## [ CPUFloatType{} ]
## torch_tensor
## 0.367066
## [ CPUFloatType{} ]
## torch_tensor
## 0.646748
## [ CPUFloatType{} ]
## torch_tensor
## 0.539941
## [ CPUFloatType{} ]
## torch_tensor
## 0.511312
## [ CPUFloatType{} ]
## torch_tensor
## 0.513487
## [ CPUFloatType{} ]
## torch_tensor
## 0.458311
## [ CPUFloatType{} ]
## torch_tensor
## 0.410686
## [ CPUFloatType{} ]
## torch_tensor
## 0.677111
## [ CPUFloatType{} ]
## torch_tensor
## 0.419904
## [ CPUFloatType{} ]
## torch_tensor
## 0.578236
## [ CPUFloatType{} ]
## torch_tensor
## 0.610959
## [ CPUFloatType{} ]
## torch_tensor
## 0.250276
## [ CPUFloatType{} ]
## torch_tensor
## 0.931519
## [ CPUFloatType{} ]
## torch_tensor
## 0.437932
## [ CPUFloatType{} ]
## torch_tensor
## 0.525844
## [ CPUFloatType{} ]
## torch_tensor
## 0.318728
## [ CPUFloatType{} ]
## torch_tensor
## 0.522585
## [ CPUFloatType{} ]
## torch_tensor
## 0.491841
## [ CPUFloatType{} ]
## torch_tensor
## 0.431945
## [ CPUFloatType{} ]
## torch_tensor
## 0.403163
## [ CPUFloatType{} ]
## torch_tensor
## 0.501861
## [ CPUFloatType{} ]
## torch_tensor
## 0.464848
## [ CPUFloatType{} ]
## torch_tensor
## 0.602553
## [ CPUFloatType{} ]
## torch_tensor
## 0.580899
## [ CPUFloatType{} ]
## torch_tensor
## 0.530852
## [ CPUFloatType{} ]
## torch_tensor
## 0.489577
## [ CPUFloatType{} ]
## torch_tensor
## 0.326926
## [ CPUFloatType{} ]
## torch_tensor
## 0.416253
## [ CPUFloatType{} ]
## torch_tensor
## 0.569155
## [ CPUFloatType{} ]
## torch_tensor
## 0.545184
## [ CPUFloatType{} ]
## torch_tensor
## 0.417683
## [ CPUFloatType{} ]
## torch_tensor
## 0.370531
## [ CPUFloatType{} ]
## torch_tensor
## 0.656358
## [ CPUFloatType{} ]
## torch_tensor
## 0.495314
## [ CPUFloatType{} ]
## torch_tensor
## 0.467925
## [ CPUFloatType{} ]
## torch_tensor
## 0.597065
## [ CPUFloatType{} ]
## torch_tensor
## 0.836428
## [ CPUFloatType{} ]
## torch_tensor
## 0.446705
## [ CPUFloatType{} ]
## torch_tensor
## 0.573751
## [ CPUFloatType{} ]
## torch_tensor
## 0.389211
## [ CPUFloatType{} ]
## torch_tensor
## 0.547818
## [ CPUFloatType{} ]
## torch_tensor
## 0.518562
## [ CPUFloatType{} ]
## torch_tensor
## 0.572411
## [ CPUFloatType{} ]
## torch_tensor
## 0.575764
## [ CPUFloatType{} ]
## torch_tensor
## 0.511749
## [ CPUFloatType{} ]
## torch_tensor
## 0.546953
## [ CPUFloatType{} ]
## torch_tensor
## 0.547896
## [ CPUFloatType{} ]
## torch_tensor
## 0.51876
## [ CPUFloatType{} ]
## torch_tensor
## 0.571322
## [ CPUFloatType{} ]
## torch_tensor
## 0.560785
## [ CPUFloatType{} ]
## torch_tensor
## 0.509102
## [ CPUFloatType{} ]
## torch_tensor
## 0.503276
## [ CPUFloatType{} ]
## torch_tensor
## 0.358027
## [ CPUFloatType{} ]
## torch_tensor
## 0.602658
## [ CPUFloatType{} ]
## torch_tensor
## 0.31295
## [ CPUFloatType{} ]
## torch_tensor
## 0.372316
## [ CPUFloatType{} ]
## torch_tensor
## 0.410143
## [ CPUFloatType{} ]
## torch_tensor
## 0.479437
## [ CPUFloatType{} ]
## torch_tensor
## 0.362764
## [ CPUFloatType{} ]
## torch_tensor
## 0.373629
## [ CPUFloatType{} ]
## torch_tensor
## 0.506555
## [ CPUFloatType{} ]
## torch_tensor
## 0.357655
## [ CPUFloatType{} ]
## torch_tensor
## 0.435401
## [ CPUFloatType{} ]
## torch_tensor
## 0.468753
## [ CPUFloatType{} ]
## torch_tensor
## 0.450491
## [ CPUFloatType{} ]
## torch_tensor
## 0.570343
## [ CPUFloatType{} ]
## torch_tensor
## 0.358519
## [ CPUFloatType{} ]
## torch_tensor
## 0.36841
## [ CPUFloatType{} ]
## torch_tensor
## 0.38052
## [ CPUFloatType{} ]
## torch_tensor
## 0.436957
## [ CPUFloatType{} ]
## torch_tensor
## 0.505041
## [ CPUFloatType{} ]
## torch_tensor
## 0.462252
## [ CPUFloatType{} ]
## torch_tensor
## 0.277778
## [ CPUFloatType{} ]
## torch_tensor
## 0.514669
## [ CPUFloatType{} ]
## torch_tensor
## 0.511981
## [ CPUFloatType{} ]
## torch_tensor
## 0.63429
## [ CPUFloatType{} ]
## torch_tensor
## 0.329449
## [ CPUFloatType{} ]
## torch_tensor
## 0.416236
## [ CPUFloatType{} ]
## torch_tensor
## 0.321375
## [ CPUFloatType{} ]
## torch_tensor
## 0.416791
## [ CPUFloatType{} ]
## torch_tensor
## 0.769134
## [ CPUFloatType{} ]
## torch_tensor
## 0.383466
## [ CPUFloatType{} ]
## torch_tensor
## 0.251347
## [ CPUFloatType{} ]
## torch_tensor
## 0.386374
## [ CPUFloatType{} ]
## torch_tensor
## 0.635554
## [ CPUFloatType{} ]
## torch_tensor
## 0.398997
## [ CPUFloatType{} ]
## torch_tensor
## 0.586117
## [ CPUFloatType{} ]
## torch_tensor
## 0.367813
## [ CPUFloatType{} ]
## torch_tensor
## 0.403955
## [ CPUFloatType{} ]
## torch_tensor
## 0.469602
## [ CPUFloatType{} ]
## torch_tensor
## 0.420949
## [ CPUFloatType{} ]
## torch_tensor
## 0.527529
## [ CPUFloatType{} ]
## torch_tensor
## 0.408701
## [ CPUFloatType{} ]
## torch_tensor
## 0.368456
## [ CPUFloatType{} ]
## torch_tensor
## 0.347138
## [ CPUFloatType{} ]
## torch_tensor
## 0.628231
## [ CPUFloatType{} ]
## torch_tensor
## 0.271172
## [ CPUFloatType{} ]
## torch_tensor
## 0.377168
## [ CPUFloatType{} ]
## torch_tensor
## 0.371066
## [ CPUFloatType{} ]
## torch_tensor
## 0.296142
## [ CPUFloatType{} ]
## torch_tensor
## 0.498283
## [ CPUFloatType{} ]
## torch_tensor
## 0.596838
## [ CPUFloatType{} ]
## torch_tensor
## 0.564676
## [ CPUFloatType{} ]
## torch_tensor
## 0.461467
## [ CPUFloatType{} ]
## torch_tensor
## 0.667467
## [ CPUFloatType{} ]
## torch_tensor
## 0.407933
## [ CPUFloatType{} ]
## torch_tensor
## 0.514451
## [ CPUFloatType{} ]
## torch_tensor
## 0.518118
## [ CPUFloatType{} ]
## torch_tensor
## 0.5676
## [ CPUFloatType{} ]
## torch_tensor
## 0.566754
## [ CPUFloatType{} ]
## torch_tensor
## 0.382403
## [ CPUFloatType{} ]
## torch_tensor
## 0.456706
## [ CPUFloatType{} ]
## torch_tensor
## 0.329443
## [ CPUFloatType{} ]
## torch_tensor
## 0.257806
## [ CPUFloatType{} ]
## torch_tensor
## 0.293806
## [ CPUFloatType{} ]
## torch_tensor
## 0.660493
## [ CPUFloatType{} ]
## torch_tensor
## 0.581207
## [ CPUFloatType{} ]
## torch_tensor
## 0.467293
## [ CPUFloatType{} ]
## torch_tensor
## 0.303946
## [ CPUFloatType{} ]
## torch_tensor
## 0.350987
## [ CPUFloatType{} ]
## torch_tensor
## 0.372797
## [ CPUFloatType{} ]
## torch_tensor
## 0.295605
## [ CPUFloatType{} ]
## torch_tensor
## 0.621997
## [ CPUFloatType{} ]
## torch_tensor
## 0.542493
## [ CPUFloatType{} ]
## torch_tensor
## 0.619512
## [ CPUFloatType{} ]
## torch_tensor
## 0.388555
## [ CPUFloatType{} ]
## torch_tensor
## 0.422618
## [ CPUFloatType{} ]
## torch_tensor
## 0.338808
## [ CPUFloatType{} ]
## torch_tensor
## 0.257067
## [ CPUFloatType{} ]
## torch_tensor
## 0.539933
## [ CPUFloatType{} ]
## torch_tensor
## 0.448841
## [ CPUFloatType{} ]
## torch_tensor
## 0.302748
## [ CPUFloatType{} ]
## torch_tensor
## 0.385826
## [ CPUFloatType{} ]
## torch_tensor
## 0.378119
## [ CPUFloatType{} ]
## torch_tensor
## 0.533214
## [ CPUFloatType{} ]
## torch_tensor
## 0.297191
## [ CPUFloatType{} ]
## torch_tensor
## 0.487741
## [ CPUFloatType{} ]
## torch_tensor
## 0.622083
## [ CPUFloatType{} ]
## torch_tensor
## 0.473199
## [ CPUFloatType{} ]
## torch_tensor
## 0.404346
## [ CPUFloatType{} ]
## torch_tensor
## 0.235299
## [ CPUFloatType{} ]
## torch_tensor
## 0.549628
## [ CPUFloatType{} ]
## torch_tensor
## 0.277513
## [ CPUFloatType{} ]
## torch_tensor
## 0.344548
## [ CPUFloatType{} ]
## torch_tensor
## 0.446229
## [ CPUFloatType{} ]
## torch_tensor
## 0.357351
## [ CPUFloatType{} ]
## torch_tensor
## 0.4031
## [ CPUFloatType{} ]
## torch_tensor
## 0.234357
## [ CPUFloatType{} ]
## torch_tensor
## 0.184081
## [ CPUFloatType{} ]
## torch_tensor
## 0.405989
## [ CPUFloatType{} ]
## torch_tensor
## 0.233916
## [ CPUFloatType{} ]
## torch_tensor
## 0.453483
## [ CPUFloatType{} ]
## torch_tensor
## 0.394642
## [ CPUFloatType{} ]
## torch_tensor
## 0.368567
## [ CPUFloatType{} ]
## torch_tensor
## 0.567078
## [ CPUFloatType{} ]
## torch_tensor
## 0.400745
## [ CPUFloatType{} ]
## torch_tensor
## 0.249258
## [ CPUFloatType{} ]
## torch_tensor
## 0.523745
## [ CPUFloatType{} ]
## torch_tensor
## 0.428694
## [ CPUFloatType{} ]
## torch_tensor
## 0.588379
## [ CPUFloatType{} ]
## torch_tensor
## 0.187199
## [ CPUFloatType{} ]
## torch_tensor
## 0.47809
## [ CPUFloatType{} ]
## torch_tensor
## 0.448054
## [ CPUFloatType{} ]
## torch_tensor
## 0.457262
## [ CPUFloatType{} ]
## torch_tensor
## 0.68591
## [ CPUFloatType{} ]
## torch_tensor
## 0.259515
## [ CPUFloatType{} ]
## torch_tensor
## 0.414076
## [ CPUFloatType{} ]
## torch_tensor
## 0.341963
## [ CPUFloatType{} ]
## torch_tensor
## 0.421827
## [ CPUFloatType{} ]
## torch_tensor
## 0.434542
## [ CPUFloatType{} ]
## torch_tensor
## 0.435403
## [ CPUFloatType{} ]
## torch_tensor
## 0.372657
## [ CPUFloatType{} ]
## torch_tensor
## 0.357822
## [ CPUFloatType{} ]
## torch_tensor
## 0.400233
## [ CPUFloatType{} ]
## torch_tensor
## 0.564408
## [ CPUFloatType{} ]
## torch_tensor
## 0.406186
## [ CPUFloatType{} ]
## torch_tensor
## 0.450136
## [ CPUFloatType{} ]
## torch_tensor
## 0.445251
## [ CPUFloatType{} ]
## torch_tensor
## 0.34423
## [ CPUFloatType{} ]
## torch_tensor
## 0.401276
## [ CPUFloatType{} ]
## torch_tensor
## 0.348633
## [ CPUFloatType{} ]
## torch_tensor
## 0.412551
## [ CPUFloatType{} ]
## torch_tensor
## 0.336446
## [ CPUFloatType{} ]
## torch_tensor
## 0.345295
## [ CPUFloatType{} ]
## torch_tensor
## 0.520683
## [ CPUFloatType{} ]
## torch_tensor
## 0.336146
## [ CPUFloatType{} ]
## torch_tensor
## 0.322018
## [ CPUFloatType{} ]
## torch_tensor
## 0.331209
## [ CPUFloatType{} ]
## torch_tensor
## 0.445452
## [ CPUFloatType{} ]
## torch_tensor
## 0.480105
## [ CPUFloatType{} ]
## torch_tensor
## 0.19954
## [ CPUFloatType{} ]
## torch_tensor
## 0.232235
## [ CPUFloatType{} ]
## torch_tensor
## 0.53529
## [ CPUFloatType{} ]
## torch_tensor
## 0.519431
## [ CPUFloatType{} ]
## torch_tensor
## 0.357285
## [ CPUFloatType{} ]
## torch_tensor
## 0.771656
## [ CPUFloatType{} ]
## torch_tensor
## 0.394094
## [ CPUFloatType{} ]
## torch_tensor
## 0.242035
## [ CPUFloatType{} ]
## torch_tensor
## 0.69783
## [ CPUFloatType{} ]
## torch_tensor
## 0.289633
## [ CPUFloatType{} ]
## torch_tensor
## 0.475387
## [ CPUFloatType{} ]
## torch_tensor
## 0.68816
## [ CPUFloatType{} ]
## torch_tensor
## 0.484704
## [ CPUFloatType{} ]
## torch_tensor
## 0.49786
## [ CPUFloatType{} ]
## torch_tensor
## 0.640882
## [ CPUFloatType{} ]
## torch_tensor
## 0.351956
## [ CPUFloatType{} ]
## torch_tensor
## 0.532307
## [ CPUFloatType{} ]
## torch_tensor
## 0.531353
## [ CPUFloatType{} ]
## torch_tensor
## 0.560007
## [ CPUFloatType{} ]
## torch_tensor
## 0.482566
## [ CPUFloatType{} ]
## torch_tensor
## 0.412804
## [ CPUFloatType{} ]
## torch_tensor
## 0.434385
## [ CPUFloatType{} ]
## torch_tensor
## 0.381985
## [ CPUFloatType{} ]
## torch_tensor
## 0.615963
## [ CPUFloatType{} ]
## torch_tensor
## 0.396546
## [ CPUFloatType{} ]
## torch_tensor
## 0.375409
## [ CPUFloatType{} ]
## torch_tensor
## 0.471728
## [ CPUFloatType{} ]
## torch_tensor
## 0.372322
## [ CPUFloatType{} ]
## torch_tensor
## 0.208726
## [ CPUFloatType{} ]
## torch_tensor
## 0.425016
## [ CPUFloatType{} ]
## torch_tensor
## 0.504991
## [ CPUFloatType{} ]
## torch_tensor
## 0.531008
## [ CPUFloatType{} ]
## torch_tensor
## 0.597863
## [ CPUFloatType{} ]
## torch_tensor
## 0.451041
## [ CPUFloatType{} ]
## torch_tensor
## 0.390388
## [ CPUFloatType{} ]
## torch_tensor
## 0.412676
## [ CPUFloatType{} ]
## torch_tensor
## 0.305003
## [ CPUFloatType{} ]
## torch_tensor
## 0.243376
## [ CPUFloatType{} ]
## torch_tensor
## 0.602575
## [ CPUFloatType{} ]
## torch_tensor
## 0.221464
## [ CPUFloatType{} ]
## torch_tensor
## 0.339891
## [ CPUFloatType{} ]
## torch_tensor
## 0.39274
## [ CPUFloatType{} ]
## torch_tensor
## 0.363203
## [ CPUFloatType{} ]
## torch_tensor
## 0.613401
## [ CPUFloatType{} ]
## torch_tensor
## 0.518743
## [ CPUFloatType{} ]
## torch_tensor
## 0.428977
## [ CPUFloatType{} ]
## torch_tensor
## 0.498624
## [ CPUFloatType{} ]
## torch_tensor
## 0.335959
## [ CPUFloatType{} ]
## torch_tensor
## 0.679345
## [ CPUFloatType{} ]
## torch_tensor
## 0.505854
## [ CPUFloatType{} ]
## torch_tensor
## 0.30708
## [ CPUFloatType{} ]
## torch_tensor
## 0.365738
## [ CPUFloatType{} ]
## torch_tensor
## 0.489005
## [ CPUFloatType{} ]
## torch_tensor
## 0.429391
## [ CPUFloatType{} ]
## torch_tensor
## 0.595303
## [ CPUFloatType{} ]
## torch_tensor
## 0.499801
## [ CPUFloatType{} ]
## torch_tensor
## 0.496359
## [ CPUFloatType{} ]
## torch_tensor
## 0.459177
## [ CPUFloatType{} ]
## torch_tensor
## 0.460044
## [ CPUFloatType{} ]
## torch_tensor
## 0.503074
## [ CPUFloatType{} ]
## torch_tensor
## 0.482042
## [ CPUFloatType{} ]
## torch_tensor
## 0.533078
## [ CPUFloatType{} ]
## torch_tensor
## 0.381436
## [ CPUFloatType{} ]
## torch_tensor
## 0.478166
## [ CPUFloatType{} ]
## torch_tensor
## 0.384332
## [ CPUFloatType{} ]
## torch_tensor
## 0.256755
## [ CPUFloatType{} ]
## torch_tensor
## 0.51086
## [ CPUFloatType{} ]
## torch_tensor
## 0.33079
## [ CPUFloatType{} ]
## torch_tensor
## 0.424699
## [ CPUFloatType{} ]
## torch_tensor
## 0.695894
## [ CPUFloatType{} ]
## torch_tensor
## 0.478257
## [ CPUFloatType{} ]
## torch_tensor
## 0.399054
## [ CPUFloatType{} ]
## torch_tensor
## 0.422863
## [ CPUFloatType{} ]
## torch_tensor
## 0.405214
## [ CPUFloatType{} ]
## torch_tensor
## 0.504448
## [ CPUFloatType{} ]
## torch_tensor
## 0.376438
## [ CPUFloatType{} ]
## torch_tensor
## 0.345226
## [ CPUFloatType{} ]
## torch_tensor
## 0.582989
## [ CPUFloatType{} ]
## torch_tensor
## 0.551982
## [ CPUFloatType{} ]
## torch_tensor
## 0.594883
## [ CPUFloatType{} ]
## torch_tensor
## 0.495424
## [ CPUFloatType{} ]
## torch_tensor
## 0.395951
## [ CPUFloatType{} ]
## torch_tensor
## 0.382973
## [ CPUFloatType{} ]
## torch_tensor
## 0.55449
## [ CPUFloatType{} ]
## torch_tensor
## 0.589461
## [ CPUFloatType{} ]
## torch_tensor
## 0.53215
## [ CPUFloatType{} ]
## torch_tensor
## 0.336653
## [ CPUFloatType{} ]
## torch_tensor
## 0.542423
## [ CPUFloatType{} ]
## torch_tensor
## 0.42679
## [ CPUFloatType{} ]
## torch_tensor
## 0.4224
## [ CPUFloatType{} ]
## torch_tensor
## 0.47332
## [ CPUFloatType{} ]
## torch_tensor
## 0.487384
## [ CPUFloatType{} ]
## torch_tensor
## 0.308228
## [ CPUFloatType{} ]
## torch_tensor
## 0.379888
## [ CPUFloatType{} ]
## torch_tensor
## 0.668328
## [ CPUFloatType{} ]
## torch_tensor
## 0.696254
## [ CPUFloatType{} ]
## torch_tensor
## 0.555597
## [ CPUFloatType{} ]
## torch_tensor
## 0.437693
## [ CPUFloatType{} ]
## torch_tensor
## 0.346176
## [ CPUFloatType{} ]
## torch_tensor
## 0.461245
## [ CPUFloatType{} ]
## torch_tensor
## 0.538978
## [ CPUFloatType{} ]
## torch_tensor
## 0.397133
## [ CPUFloatType{} ]
## torch_tensor
## 0.407224
## [ CPUFloatType{} ]
## torch_tensor
## 0.43588
## [ CPUFloatType{} ]
## torch_tensor
## 0.352819
## [ CPUFloatType{} ]
## torch_tensor
## 0.507514
## [ CPUFloatType{} ]
## torch_tensor
## 0.415583
## [ CPUFloatType{} ]
## torch_tensor
## 0.500623
## [ CPUFloatType{} ]
## torch_tensor
## 0.4714
## [ CPUFloatType{} ]
## torch_tensor
## 0.227648
## [ CPUFloatType{} ]
## torch_tensor
## 0.457516
## [ CPUFloatType{} ]
## torch_tensor
## 0.316914
## [ CPUFloatType{} ]
## torch_tensor
## 0.63722
## [ CPUFloatType{} ]
## torch_tensor
## 0.226939
## [ CPUFloatType{} ]
## torch_tensor
## 0.25717
## [ CPUFloatType{} ]
## torch_tensor
## 0.512327
## [ CPUFloatType{} ]
## torch_tensor
## 0.310883
## [ CPUFloatType{} ]
## torch_tensor
## 0.45836
## [ CPUFloatType{} ]
## torch_tensor
## 0.326464
## [ CPUFloatType{} ]
## torch_tensor
## 0.270925
## [ CPUFloatType{} ]
## torch_tensor
## 0.411667
## [ CPUFloatType{} ]
## torch_tensor
## 0.461355
## [ CPUFloatType{} ]
## torch_tensor
## 0.310854
## [ CPUFloatType{} ]
## torch_tensor
## 0.378207
## [ CPUFloatType{} ]
## torch_tensor
## 0.625009
## [ CPUFloatType{} ]
## torch_tensor
## 0.388278
## [ CPUFloatType{} ]
## torch_tensor
## 0.516239
## [ CPUFloatType{} ]
## torch_tensor
## 0.393094
## [ CPUFloatType{} ]
## torch_tensor
## 0.667067
## [ CPUFloatType{} ]
## torch_tensor
## 0.308425
## [ CPUFloatType{} ]
## torch_tensor
## 0.271553
## [ CPUFloatType{} ]
## torch_tensor
## 0.424099
## [ CPUFloatType{} ]
## torch_tensor
## 0.329076
## [ CPUFloatType{} ]
## torch_tensor
## 0.456936
## [ CPUFloatType{} ]
## torch_tensor
## 0.395035
## [ CPUFloatType{} ]
## torch_tensor
## 0.368807
## [ CPUFloatType{} ]
## torch_tensor
## 0.242131
## [ CPUFloatType{} ]
## torch_tensor
## 0.54161
## [ CPUFloatType{} ]
## torch_tensor
## 0.332309
## [ CPUFloatType{} ]
## torch_tensor
## 0.377618
## [ CPUFloatType{} ]
## torch_tensor
## 0.494565
## [ CPUFloatType{} ]
## torch_tensor
## 0.294066
## [ CPUFloatType{} ]
## torch_tensor
## 0.27493
## [ CPUFloatType{} ]
## torch_tensor
## 0.302859
## [ CPUFloatType{} ]
## torch_tensor
## 0.501485
## [ CPUFloatType{} ]
## torch_tensor
## 0.392607
## [ CPUFloatType{} ]
## torch_tensor
## 0.362632
## [ CPUFloatType{} ]
## torch_tensor
## 0.357129
## [ CPUFloatType{} ]
## torch_tensor
## 0.584508
## [ CPUFloatType{} ]
## torch_tensor
## 0.551174
## [ CPUFloatType{} ]
## torch_tensor
## 0.512529
## [ CPUFloatType{} ]
## torch_tensor
## 0.374158
## [ CPUFloatType{} ]
## torch_tensor
## 0.355655
## [ CPUFloatType{} ]
## torch_tensor
## 0.307425
## [ CPUFloatType{} ]
## torch_tensor
## 0.559684
## [ CPUFloatType{} ]
## torch_tensor
## 0.56158
## [ CPUFloatType{} ]
## torch_tensor
## 0.348544
## [ CPUFloatType{} ]
## torch_tensor
## 0.415959
## [ CPUFloatType{} ]
## torch_tensor
## 0.446288
## [ CPUFloatType{} ]
## torch_tensor
## 0.523871
## [ CPUFloatType{} ]
## torch_tensor
## 0.345746
## [ CPUFloatType{} ]
## torch_tensor
## 0.462541
## [ CPUFloatType{} ]
## torch_tensor
## 0.381615
## [ CPUFloatType{} ]
## torch_tensor
## 0.307486
## [ CPUFloatType{} ]
## torch_tensor
## 0.38747
## [ CPUFloatType{} ]
## torch_tensor
## 0.242408
## [ CPUFloatType{} ]
## torch_tensor
## 0.354356
## [ CPUFloatType{} ]
## torch_tensor
## 0.40179
## [ CPUFloatType{} ]
## torch_tensor
## 0.488963
## [ CPUFloatType{} ]
## torch_tensor
## 0.511835
## [ CPUFloatType{} ]
## torch_tensor
## 0.388213
## [ CPUFloatType{} ]
## torch_tensor
## 0.474527
## [ CPUFloatType{} ]
## torch_tensor
## 0.398443
## [ CPUFloatType{} ]
## torch_tensor
## 0.589918
## [ CPUFloatType{} ]
## torch_tensor
## 0.491081
## [ CPUFloatType{} ]
## torch_tensor
## 0.399122
## [ CPUFloatType{} ]
## torch_tensor
## 0.322369
## [ CPUFloatType{} ]
## torch_tensor
## 0.679908
## [ CPUFloatType{} ]
## torch_tensor
## 0.190278
## [ CPUFloatType{} ]
## torch_tensor
## 0.290614
## [ CPUFloatType{} ]
## torch_tensor
## 0.305845
## [ CPUFloatType{} ]
## torch_tensor
## 0.356521
## [ CPUFloatType{} ]
## torch_tensor
## 0.32244
## [ CPUFloatType{} ]
## torch_tensor
## 0.43916
## [ CPUFloatType{} ]
## torch_tensor
## 0.475629
## [ CPUFloatType{} ]
## torch_tensor
## 0.266565
## [ CPUFloatType{} ]
## torch_tensor
## 0.239379
## [ CPUFloatType{} ]
## torch_tensor
## 0.459647
## [ CPUFloatType{} ]
## torch_tensor
## 0.335789
## [ CPUFloatType{} ]
## torch_tensor
## 0.490906
## [ CPUFloatType{} ]
## torch_tensor
## 0.477898
## [ CPUFloatType{} ]
## torch_tensor
## 0.402575
## [ CPUFloatType{} ]
## torch_tensor
## 0.600777
## [ CPUFloatType{} ]
## torch_tensor
## 0.258597
## [ CPUFloatType{} ]
## torch_tensor
## 0.438686
## [ CPUFloatType{} ]
## torch_tensor
## 0.531851
## [ CPUFloatType{} ]
## torch_tensor
## 0.253977
## [ CPUFloatType{} ]
## torch_tensor
## 0.403474
## [ CPUFloatType{} ]
## torch_tensor
## 0.336894
## [ CPUFloatType{} ]
## torch_tensor
## 0.436859
## [ CPUFloatType{} ]
## torch_tensor
## 0.363432
## [ CPUFloatType{} ]
## torch_tensor
## 0.248285
## [ CPUFloatType{} ]
## torch_tensor
## 0.496528
## [ CPUFloatType{} ]
## torch_tensor
## 0.261794
## [ CPUFloatType{} ]
## torch_tensor
## 0.36703
## [ CPUFloatType{} ]
## torch_tensor
## 0.401191
## [ CPUFloatType{} ]
## torch_tensor
## 0.301032
## [ CPUFloatType{} ]
## torch_tensor
## 0.722476
## [ CPUFloatType{} ]
## torch_tensor
## 0.455946
## [ CPUFloatType{} ]
## torch_tensor
## 0.334515
## [ CPUFloatType{} ]
## torch_tensor
## 0.270791
## [ CPUFloatType{} ]
## torch_tensor
## 0.262766
## [ CPUFloatType{} ]
## torch_tensor
## 0.17537
## [ CPUFloatType{} ]
## torch_tensor
## 0.337111
## [ CPUFloatType{} ]
## torch_tensor
## 0.722401
## [ CPUFloatType{} ]
## torch_tensor
## 0.646571
## [ CPUFloatType{} ]
## torch_tensor
## 0.605239
## [ CPUFloatType{} ]
## torch_tensor
## 0.173813
## [ CPUFloatType{} ]
## torch_tensor
## 0.394196
## [ CPUFloatType{} ]
## torch_tensor
## 0.279225
## [ CPUFloatType{} ]
## torch_tensor
## 0.380039
## [ CPUFloatType{} ]
## torch_tensor
## 0.31053
## [ CPUFloatType{} ]
## torch_tensor
## 0.377129
## [ CPUFloatType{} ]
## torch_tensor
## 0.407474
## [ CPUFloatType{} ]
## torch_tensor
## 0.500155
## [ CPUFloatType{} ]
## torch_tensor
## 0.517789
## [ CPUFloatType{} ]
## torch_tensor
## 0.402987
## [ CPUFloatType{} ]
## torch_tensor
## 0.584422
## [ CPUFloatType{} ]
## torch_tensor
## 0.552237
## [ CPUFloatType{} ]
## torch_tensor
## 0.589956
## [ CPUFloatType{} ]
## torch_tensor
## 0.362919
## [ CPUFloatType{} ]
## torch_tensor
## 0.406081
## [ CPUFloatType{} ]
## torch_tensor
## 0.444699
## [ CPUFloatType{} ]
## torch_tensor
## 0.481884
## [ CPUFloatType{} ]
## torch_tensor
## 0.397068
## [ CPUFloatType{} ]
## torch_tensor
## 0.57274
## [ CPUFloatType{} ]
## torch_tensor
## 0.389938
## [ CPUFloatType{} ]
## torch_tensor
## 0.385996
## [ CPUFloatType{} ]
## torch_tensor
## 0.421732
## [ CPUFloatType{} ]
## torch_tensor
## 0.209541
## [ CPUFloatType{} ]
## torch_tensor
## 0.384812
## [ CPUFloatType{} ]
## torch_tensor
## 0.299939
## [ CPUFloatType{} ]
## torch_tensor
## 0.413445
## [ CPUFloatType{} ]
## torch_tensor
## 0.386156
## [ CPUFloatType{} ]
## torch_tensor
## 0.386136
## [ CPUFloatType{} ]
## torch_tensor
## 0.456723
## [ CPUFloatType{} ]
## torch_tensor
## 0.689384
## [ CPUFloatType{} ]
## torch_tensor
## 0.627301
## [ CPUFloatType{} ]
## torch_tensor
## 0.297648
## [ CPUFloatType{} ]
## torch_tensor
## 0.372408
## [ CPUFloatType{} ]
## torch_tensor
## 0.3924
## [ CPUFloatType{} ]
## torch_tensor
## 0.313157
## [ CPUFloatType{} ]
## torch_tensor
## 0.286451
## [ CPUFloatType{} ]
## torch_tensor
## 0.503854
## [ CPUFloatType{} ]
## torch_tensor
## 0.5197
## [ CPUFloatType{} ]
## torch_tensor
## 0.367511
## [ CPUFloatType{} ]
## torch_tensor
## 0.384413
## [ CPUFloatType{} ]
## torch_tensor
## 0.385102
## [ CPUFloatType{} ]
## torch_tensor
## 0.676447
## [ CPUFloatType{} ]
## torch_tensor
## 0.533545
## [ CPUFloatType{} ]
## torch_tensor
## 0.428157
## [ CPUFloatType{} ]
## torch_tensor
## 0.625363
## [ CPUFloatType{} ]
## torch_tensor
## 0.435077
## [ CPUFloatType{} ]
## torch_tensor
## 0.440182
## [ CPUFloatType{} ]
## torch_tensor
## 0.595799
## [ CPUFloatType{} ]
## torch_tensor
## 0.364245
## [ CPUFloatType{} ]
## torch_tensor
## 0.393755
## [ CPUFloatType{} ]
## torch_tensor
## 0.528594
## [ CPUFloatType{} ]
## torch_tensor
## 0.527712
## [ CPUFloatType{} ]
## torch_tensor
## 0.556635
## [ CPUFloatType{} ]
## torch_tensor
## 0.405661
## [ CPUFloatType{} ]
## torch_tensor
## 0.531449
## [ CPUFloatType{} ]
## torch_tensor
## 0.403637
## [ CPUFloatType{} ]
## torch_tensor
## 0.544648
## [ CPUFloatType{} ]
## torch_tensor
## 0.415399
## [ CPUFloatType{} ]
## torch_tensor
## 0.659341
## [ CPUFloatType{} ]
## torch_tensor
## 0.300895
## [ CPUFloatType{} ]
## torch_tensor
## 0.467048
## [ CPUFloatType{} ]</code></pre>
Note: the ‘nnf_cross_entropy’ expects predictions on the scale of the linear predictors (the loss function itself will apply the softmax!)
</details>
<p><br/></p>
</div>
<div id="model-evaluation" class="section level3" number="3.5.5">
<h3 number="3.5.5"><span class="header-section-number">3.5.5</span> Model evaluation</h3>
<p>We will predict survived for the test data of the inner split and calculate the accuracy:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="#cb264-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span></span>
<span id="cb264-2"><a href="#cb264-2" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb264-3"><a href="#cb264-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(sub_test[,<span class="sc">-</span><span class="dv">1</span>]))</span>
<span id="cb264-4"><a href="#cb264-4" aria-hidden="true" tabindex="-1"></a>predicted <span class="ot">=</span> <span class="fu">ifelse</span>(preds[,<span class="dv">2</span>] <span class="sc">&lt;</span> <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb264-5"><a href="#cb264-5" aria-hidden="true" tabindex="-1"></a>observed <span class="ot">=</span> sub_test[,<span class="dv">1</span>]</span>
<span id="cb264-6"><a href="#cb264-6" aria-hidden="true" tabindex="-1"></a>(<span class="at">accuracy =</span> <span class="fu">mean</span>(predicted <span class="sc">==</span> observed))</span></code></pre></div>
<pre><code>## [1] 0.7461929</code></pre>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="#cb266-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb266-2"><a href="#cb266-2" aria-hidden="true" tabindex="-1"></a>preds_torch <span class="ot">=</span> <span class="fu">nnf_softmax</span>( <span class="fu">model_torch</span>(<span class="fu">torch_tensor</span>(<span class="fu">as.matrix</span>(sub_test[,<span class="sc">-</span><span class="dv">1</span>]))) , <span class="at">dim =</span> 2L)</span>
<span id="cb266-3"><a href="#cb266-3" aria-hidden="true" tabindex="-1"></a>preds_torch <span class="ot">=</span> <span class="fu">as.matrix</span>(preds_torch)</span>
<span id="cb266-4"><a href="#cb266-4" aria-hidden="true" tabindex="-1"></a>preds_torch <span class="ot">=</span> <span class="fu">apply</span>(preds_torch, <span class="dv">1</span>, which.max)</span>
<span id="cb266-5"><a href="#cb266-5" aria-hidden="true" tabindex="-1"></a>(<span class="at">accuracy =</span> <span class="fu">mean</span>(preds_torch<span class="dv">-1</span> <span class="sc">==</span> observed))</span></code></pre></div>
<pre><code>## [1] 0.7563452</code></pre>
Now we have to use the softmax function.
</details>
<p><br/></p>
</div>
<div id="predictions-and-submission" class="section level3" number="3.5.6">
<h3 number="3.5.6"><span class="header-section-number">3.5.6</span> Predictions and submission</h3>
<p>When we are satisfied with the performance of our model in the inner split, we will create predictions for the test data of the outer split:</p>
<p>To do so, we select all observations that belong to the outer test split (use the filter function) and remove the survived (NAs) columns</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="#cb268-1" aria-hidden="true" tabindex="-1"></a>submit <span class="ot">=</span> </span>
<span id="cb268-2"><a href="#cb268-2" aria-hidden="true" tabindex="-1"></a>  test <span class="sc">%&gt;%</span> </span>
<span id="cb268-3"><a href="#cb268-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(<span class="sc">-</span>survived)</span></code></pre></div>
<p>We cannot assess the performance on the test split because the true survived ratio is unknown, however, we can now submit our predictions to the submission server at
<a href="http://rhsbio7.uni-regensburg.de:8500" class="uri">http://rhsbio7.uni-regensburg.de:8500</a>
To do so, we have to transform our survived probabilities into actual 0/1 predictions (probabilities are not allowed) and create a csv:</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="#cb269-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> model <span class="sc">%&gt;%</span> </span>
<span id="cb269-2"><a href="#cb269-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(<span class="fu">as.matrix</span>(submit))</span></code></pre></div>
<p>All values &gt; 0.5 will be set to 1 and values &lt; 0.5 to zero.
For the submission it is critical to change the predictions into a data.frame, select the second column (the probablity to survive), and save it with the write.csv function:</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="#cb270-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(<span class="fu">data.frame</span>(<span class="at">y=</span>pred[,<span class="dv">2</span>]), <span class="at">file =</span> <span class="st">&quot;Max_1.csv&quot;</span>)</span></code></pre></div>
<p>The file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself.</p>
</div>
</div>
<div id="mlr" class="section level2" number="3.6">
<h2 number="3.6"><span class="header-section-number">3.6</span> Bonus - ML pipelines with mlr3</h2>
<p>As we have seen today, many of the ML algorithms are distributed over several packages but the general ML pipeline is very similar for all models: feature engineering, feature selection?, hyper-parameter tuning and cross validation.</p>
<p>The idea of the mlr3 framework is now to provide a general ML interface which you can use to build reproducible and automatic ML pipelines. The key features of mlr3 are:</p>
<ul>
<li>All common ML packages are integrated into mlr3, you can easily switch between different ML algorithms</li>
<li>A common ‘language’/workflow to specify ML pipelines</li>
<li>Support for different CV strategies</li>
<li>Hyper-parameter tuning for all supported ML algorithms</li>
<li>Ensemble models</li>
</ul>
<p>Useful links:</p>
<ul>
<li><a href="https://mlr3book.mlr-org.com/">mlr3-book</a> (still in work)</li>
<li><a href="https://mlr3.mlr-org.com/">mlr3 website</a></li>
<li><a href="https://cheatsheets.mlr-org.com/mlr3.pdf">mlr3 cheatsheet</a></li>
</ul>
<div id="mlr3---the-basic-workflow" class="section level3" number="3.6.1">
<h3 number="3.6.1"><span class="header-section-number">3.6.1</span> mlr3 - the basic workflow</h3>
<p>The mlr3 actually consists of several packages for different tasks (e.g. mlr3tuning for hyper-parameter tuning, mlr3pipelines for data preparation pipes).
But let’s start with the basic workflow.</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="#cb271-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EcoData)</span>
<span id="cb271-2"><a href="#cb271-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb271-3"><a href="#cb271-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3)</span>
<span id="cb271-4"><a href="#cb271-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3learners)</span>
<span id="cb271-5"><a href="#cb271-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3pipelines)</span>
<span id="cb271-6"><a href="#cb271-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3tuning)</span>
<span id="cb271-7"><a href="#cb271-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3measures)</span>
<span id="cb271-8"><a href="#cb271-8" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(nasa)</span>
<span id="cb271-9"><a href="#cb271-9" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(nasa)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    4687 obs. of  40 variables:
##  $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...
##  $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...
##  $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...
##  $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...
##  $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...
##  $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...
##  $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...
##  $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...
##  $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...
##  $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...
##  $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...
##  $ Close.Approach.Date         : Factor w/ 777 levels &quot;1995-01-01&quot;,&quot;1995-01-08&quot;,..: 511 712 472 239 273 145 428 694 87 732 ...
##  $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...
##  $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...
##  $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...
##  $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...
##  $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...
##  $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...
##  $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...
##  $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...
##  $ Orbiting.Body               : Factor w/ 1 level &quot;Earth&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...
##  $ Orbit.Determination.Date    : Factor w/ 2680 levels &quot;2014-06-13 15:20:44&quot;,..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...
##  $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...
##  $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...
##  $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...
##  $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...
##  $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...
##  $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...
##  $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...
##  $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...
##  $ Orbital.Period              : num  419 1040 682 427 503 ...
##  $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...
##  $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...
##  $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...
##  $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...
##  $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...
##  $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...
##  $ Equinox                     : Factor w/ 1 level &quot;J2000&quot;: 1 1 NA 1 1 1 1 1 1 1 ...
##  $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...</code></pre>
<p>Let’s drop time, name, ID variable, and create a classification task:</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="#cb273-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> nasa <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Orbit.Determination.Date, <span class="sc">-</span>Close.Approach.Date, <span class="sc">-</span>Name, <span class="sc">-</span>Neo.Reference.ID)</span>
<span id="cb273-2"><a href="#cb273-2" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Hazardous <span class="ot">=</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>Hazardous)</span>
<span id="cb273-3"><a href="#cb273-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb273-4"><a href="#cb273-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb273-5"><a href="#cb273-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create a classification task</span></span>
<span id="cb273-6"><a href="#cb273-6" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> TaskClassif<span class="sc">$</span><span class="fu">new</span>(<span class="at">id =</span> <span class="st">&quot;nasa&quot;</span>, <span class="at">backend =</span> data, <span class="at">target =</span> <span class="st">&quot;Hazardous&quot;</span>, <span class="at">positive =</span> <span class="st">&quot;1&quot;</span>)</span></code></pre></div>
<p>Create a generic pipeline of data transformation (imputation -&gt; scaling -&gt; encoding of categorical variables):</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="#cb274-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s create the preprossing graph</span></span>
<span id="cb274-2"><a href="#cb274-2" aria-hidden="true" tabindex="-1"></a>preprocessing <span class="ot">=</span> <span class="fu">po</span>(<span class="st">&quot;imputeoor&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">po</span>(<span class="st">&quot;scale&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>) </span>
<span id="cb274-3"><a href="#cb274-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-4"><a href="#cb274-4" aria-hidden="true" tabindex="-1"></a><span class="co"># run the task trhough it</span></span>
<span id="cb274-5"><a href="#cb274-5" aria-hidden="true" tabindex="-1"></a>transformed_task <span class="ot">=</span> preprocessing<span class="sc">$</span><span class="fu">train</span>(task)[[<span class="dv">1</span>]]</span>
<span id="cb274-6"><a href="#cb274-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-7"><a href="#cb274-7" aria-hidden="true" tabindex="-1"></a>transformed_task<span class="sc">$</span><span class="fu">missings</span>()</span></code></pre></div>
<pre><code>##                    Hazardous           Absolute.Magnitude                Aphelion.Dist           Asc.Node.Longitude 
##                         4187                            0                            0                            0 
##                 Eccentricity    Epoch.Date.Close.Approach             Epoch.Osculation         Est.Dia.in.Feet.max. 
##                            0                            0                            0                            0 
##         Est.Dia.in.Feet.min.           Est.Dia.in.KM.max.           Est.Dia.in.KM.min.            Est.Dia.in.M.max. 
##                            0                            0                            0                            0 
##            Est.Dia.in.M.min.        Est.Dia.in.Miles.max.        Est.Dia.in.Miles.min.                  Inclination 
##                            0                            0                            0                            0 
##  Jupiter.Tisserand.Invariant                 Mean.Anomaly                  Mean.Motion               Miles.per.hour 
##                            0                            0                            0                            0 
##   Minimum.Orbit.Intersection     Miss.Dist..Astronomical.       Miss.Dist..kilometers.            Miss.Dist..lunar. 
##                            0                            0                            0                            0 
##            Miss.Dist..miles.                     Orbit.ID           Orbit.Uncertainity               Orbital.Period 
##                            0                            0                            0                            0 
##               Perihelion.Arg          Perihelion.Distance              Perihelion.Time  Relative.Velocity.km.per.hr 
##                            0                            0                            0                            0 
## Relative.Velocity.km.per.sec              Semi.Major.Axis                Equinox.J2000             Equinox..MISSING 
##                            0                            0                            0                            0 
##          Orbiting.Body.Earth       Orbiting.Body..MISSING 
##                            0                            0</code></pre>
<p>We can even visualize the pre-processing graph:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="#cb276-1" aria-hidden="true" tabindex="-1"></a>preprocessing<span class="sc">$</span><span class="fu">plot</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-129-1.png" width="672" /></p>
<p>Now, to test our model (randomForest) by 10-CV, we will:</p>
<ul>
<li>specify the missing target rows as validation so that they will be ignores</li>
<li>specify the CV, the learner (the ML model we want to use), and the measurement (AUC)</li>
<li>run (benchmark) our model</li>
</ul>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="#cb277-1" aria-hidden="true" tabindex="-1"></a>transformed_task<span class="sc">$</span><span class="fu">data</span>()</span></code></pre></div>
<pre><code>##       Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity Epoch.Date.Close.Approach
##    1:         0        -0.81322649   -0.38042005       -1.140837452 -0.315605975                -4.7929881
##    2:         0         0.02110348    0.94306517       -1.380254611  0.744287645                 1.1058704
##    3:         0         0.68365964    0.10199889        0.044905370 -0.068280074                 0.1591740
##    4:         1        -0.10159210   -0.38415066        1.606769281 -0.392030729                -0.7630231
##    5:         1        -0.15067034   -0.29632490        0.151458877 -0.516897963                -0.6305034
##   ---                                                                                                     
## 4683:      &lt;NA&gt;        -0.32244415    0.69173184       -0.171022906  1.043608082                 1.3635097
## 4684:      &lt;NA&gt;         0.46280759   -0.24203066       -0.009803808 -0.006429588                 1.3635097
## 4685:      &lt;NA&gt;         1.51798962   -0.56422744        1.514551982 -1.045386877                 1.3635097
## 4686:      &lt;NA&gt;         0.16833819    0.14193044       -1.080452287  0.017146757                 1.3635097
## 4687:      &lt;NA&gt;        -0.05251387   -0.08643345       -0.013006704 -0.579210554                 1.3635097
##       Epoch.Osculation Est.Dia.in.Feet.max. Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max.
##    1:       0.14026773          0.271417899          0.313407647        0.300713440        0.256568684       0.271095311
##    2:      -0.26325244          0.032130074         -0.029173486       -0.020055639        0.057560696       0.031844946
##    3:      -7.76281014         -0.012841645         -0.093558135       -0.080340934        0.020159164      -0.013119734
##    4:       0.24229559          0.048493723         -0.005746146        0.001880088        0.071169817       0.048206033
##    5:       0.24229559          0.056169717          0.005243343        0.012169879        0.077553695       0.055880826
##   ---                                                                                                                   
## 4683:       0.24229559          0.089353662          0.052751793        0.056653478        0.105151714       0.089059576
## 4684:       0.05711503         -0.003481174         -0.080157032       -0.067793075        0.027943967      -0.003760728
## 4685:       0.24229559         -0.027260163         -0.114200690       -0.099669182        0.008167747      -0.027535994
## 4686:       0.24229559          0.016872584         -0.051017172       -0.040508543        0.044871533       0.016589844
## 4687:       0.24229559          0.041493133         -0.015768679       -0.007504312        0.065347651       0.041206539
##       Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination Jupiter.Tisserand.Invariant Mean.Anomaly
##    1:       0.291624502          2.620443e-01           0.258651038   0.5442288                   0.3840868  -1.02876096
##    2:     -12.143577263          4.153888e-02           0.030928225  -0.5925952                  -0.7801632  -4.55056211
##    3:      -0.060269734          9.711407e-05         -10.258220292  -0.5164818                  -0.2872777  -4.55056211
##    4:       0.015659335          5.661810e-02           0.046501003   0.8225188                   0.3403535   1.02239674
##    5:       0.025161701          6.369158e-02           0.053806009  -0.6568722                  -6.2415005   1.13265516
##   ---                                                                                                                   
## 4683:       0.066241198          9.427082e-02           0.085386142   0.8222493                  -0.6412806   0.01560046
## 4684:      -0.048682099          8.722856e-03          -0.002961897   1.9818623                   0.1346891   1.08051799
## 4685:      -0.078118891         -1.318965e-02          -0.025591624  -0.5220442                   0.4810091   0.89998250
## 4686:      -0.023485512          2.747899e-02           0.016408144  -0.5912988                  -0.3061894   0.22720275
## 4687:       0.006993074          5.016700e-02           0.039838758   0.6181969                  -0.2665930   0.22740438
##       Mean.Motion Miles.per.hour Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers.
##    1:  0.31939530   -0.254130552                -5.45911858               -7.0769260             0.25122963
##    2: -0.71151122    0.009333354                 0.07077092               -0.6830928            -1.08492125
##    3: -0.34600512   -0.866997591                -0.11099960               -0.9035573            -1.40898698
##    4:  0.28551117    0.039031045                -5.45911858               -0.7188386            -4.48402327
##    5:  0.03164827   -0.995720084                -0.02962490               -0.8013948            -1.25881601
##   ---                                                                                                      
## 4683: -0.51852041    1.403775544                 0.30711241               -0.2728622            -0.48191427
## 4684:  0.17477591    0.970963141                -0.05962478               -0.7879458            -1.23904708
## 4685:  0.36895738   -1.150527134                -0.10766868               -0.9303542            -1.44837625
## 4686: -0.35895074   -0.705980518                 0.08529226               -0.7077555            -1.12117355
## 4687: -0.31462613   -0.239696213                 0.50904764                0.1075071             0.07719897
##       Miss.Dist..lunar. Miss.Dist..miles.   Orbit.ID Orbit.Uncertainity Orbital.Period Perihelion.Arg Perihelion.Distance
##    1:         0.2398625        0.23810770 -9.6514722         -1.0070872     -0.3013135   -1.170536399         -0.01831583
##    2:        -1.1742128       -1.18860632 -0.2412680          1.3770116      0.7811097    1.549452700          0.20604472
##    3:        -4.7878719       -1.53463694 -0.1803606          0.7809869      0.1566040    1.470307933          0.61816146
##    4:        -1.2298206       -1.24471124 -0.1803606         -1.0070872     -0.2866969    0.769006449          0.09005898
##    5:        -1.3582490       -1.37428752  1.0225620         -1.0070872     -0.1552813    0.006829799          0.52730977
##   ---                                                                                                                    
## 4683:        -0.5360384       -0.54472804 -0.1194531         -0.7090748      0.3873214   -0.580282684         -0.65810123
## 4684:        -1.3373272       -1.35317867 -0.3021755          1.3770116     -0.2345610    0.839430173         -0.18350549
## 4685:        -1.5588644       -1.57669598 -0.3326292          0.7809869     -0.3216884   -1.168210857          0.62646993
## 4686:        -1.2125793       -1.22731578 -0.1042262          0.7809869      0.1712806    0.824836889          0.52899080
## 4687:         0.0556823        0.05228143 -0.2717218          0.4829746      0.1224733    0.016358127          1.22720096
##       Perihelion.Time Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec Semi.Major.Axis Equinox.J2000
##    1:      0.10526107                 -0.28167821                 -0.284140684      -0.2791037             1
##    2:     -0.28203779                 -0.00604459                 -0.008343348      -7.3370940             1
##    3:      0.20313227                 -0.92285430                 -0.925697621       0.2204883             0
##    4:     -7.86832915                  0.02502487                  0.022744569      -0.2617714             1
##    5:      0.26755741                 -1.05752264                 -1.060445948      -0.1106954             1
##   ---                                                                                                       
## 4683:      0.03734532                  1.45280854                  1.451376301       0.4468886             1
## 4684:      0.09156633                  1.00000402                  0.998302826      -0.2008499             1
## 4685:      0.27629790                 -1.21948041                 -1.222499918      -0.3034586             1
## 4686:      0.37994517                 -0.75439966                 -0.757142920       0.2353030             1
## 4687:      0.37399573                 -0.26657713                 -0.269030636       0.1857979             1
##       Equinox..MISSING Orbiting.Body.Earth Orbiting.Body..MISSING
##    1:                0                   1                      0
##    2:                0                   1                      0
##    3:                1                   1                      0
##    4:                0                   1                      0
##    5:                0                   1                      0
##   ---                                                            
## 4683:                0                   1                      0
## 4684:                0                   1                      0
## 4685:                0                   1                      0
## 4686:                0                   1                      0
## 4687:                0                   1                      0</code></pre>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="#cb279-1" aria-hidden="true" tabindex="-1"></a>transformed_task<span class="sc">$</span><span class="fu">set_row_roles</span>((<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data))[<span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous)], <span class="st">&quot;validation&quot;</span>)</span>
<span id="cb279-2"><a href="#cb279-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-3"><a href="#cb279-3" aria-hidden="true" tabindex="-1"></a>cv10 <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> 10L)</span>
<span id="cb279-4"><a href="#cb279-4" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;classif.ranger&quot;</span>, <span class="at">predict_type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb279-5"><a href="#cb279-5" aria-hidden="true" tabindex="-1"></a>measurement <span class="ot">=</span>  <span class="fu">msr</span>(<span class="st">&quot;classif.auc&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="#cb280-1" aria-hidden="true" tabindex="-1"></a>result <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">resample</span>(transformed_task, rf, <span class="at">resampling =</span> cv10, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb280-2"><a href="#cb280-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb280-3"><a href="#cb280-3" aria-hidden="true" tabindex="-1"></a><span class="co"># calclate the average AUC of the holdouts</span></span>
<span id="cb280-4"><a href="#cb280-4" aria-hidden="true" tabindex="-1"></a>result<span class="sc">$</span><span class="fu">aggregate</span>( measurement )</span></code></pre></div>
<p>Very cool! Pre-processing + CV10 model evaluation in a few lines of code!</p>
<p>Let’s create the final predictions:</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="#cb281-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> </span>
<span id="cb281-2"><a href="#cb281-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="cf">function</span>(i) result<span class="sc">$</span>learners[[i]]<span class="sc">$</span><span class="fu">predict</span>(transformed_task, </span>
<span id="cb281-3"><a href="#cb281-3" aria-hidden="true" tabindex="-1"></a>                                                        <span class="at">row_ids =</span> (<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data))[<span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous)])<span class="sc">$</span>data<span class="sc">$</span>prob[,<span class="st">&quot;1&quot;</span>,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb281-4"><a href="#cb281-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(preds)</span>
<span id="cb281-5"><a href="#cb281-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">=</span> <span class="fu">apply</span>(preds, <span class="dv">1</span>, mean)</span></code></pre></div>
<p>You could now submit the predictions <a href="http://rhsbio7.uni-regensburg.de:8500">here</a></p>
<p>But we are still not happy, let’s do some hyper-parameter tuning!</p>
</div>
<div id="mlr3---hyper-parameter-tuning" class="section level3" number="3.6.2">
<h3 number="3.6.2"><span class="header-section-number">3.6.2</span> mlr3 - hyper-parameter tuning</h3>
<p>ML algorithms have a varying number of hyper-parameters which can (!!!) have a high impact on the predictive performance. To list a few hyper parameters:</p>
<p><strong>Random Forest</strong></p>
<ul>
<li>mtry</li>
<li>min node size</li>
</ul>
<p><strong>kNN</strong></p>
<ul>
<li>kernel</li>
<li>number of neighbors</li>
<li>distance metric</li>
</ul>
<p><strong>BRT</strong></p>
<ul>
<li>nrounds</li>
<li>max depth</li>
<li>alpha</li>
<li>booster</li>
<li>eta</li>
<li>gamma</li>
<li>lambda</li>
</ul>
<p>With mlr3, we can easily extend the above example to do hyper-parameter tuning within nested cross-validation (the tuning has its own inner CV)</p>
<p>Print the hyper-parameter space of our RF learner:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="#cb282-1" aria-hidden="true" tabindex="-1"></a>rf<span class="sc">$</span>param_set</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##                               id    class lower upper nlevels        default    parents value
##  1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5                 
##  2:       always.split.variables ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;                 
##  3:                class.weights ParamDbl  -Inf   Inf     Inf                                
##  4:                      holdout ParamLgl    NA    NA       2          FALSE                 
##  5:                   importance ParamFct    NA    NA       4 &lt;NoDefault[3]&gt;                 
##  6:                   keep.inbag ParamLgl    NA    NA       2          FALSE                 
##  7:                    max.depth ParamInt  -Inf   Inf     Inf                                
##  8:                min.node.size ParamInt     1   Inf     Inf              1                 
##  9:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1                 
## 10:                      minprop ParamDbl  -Inf   Inf     Inf            0.1                 
## 11:                         mtry ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;                 
## 12:            num.random.splits ParamInt     1   Inf     Inf              1  splitrule      
## 13:                  num.threads ParamInt     1   Inf     Inf              1                1
## 14:                    num.trees ParamInt     1   Inf     Inf            500                 
## 15:                    oob.error ParamLgl    NA    NA       2           TRUE                 
## 16:        regularization.factor ParamUty    NA    NA     Inf              1                 
## 17:      regularization.usedepth ParamLgl    NA    NA       2          FALSE                 
## 18:                      replace ParamLgl    NA    NA       2           TRUE                 
## 19:    respect.unordered.factors ParamFct    NA    NA       3         ignore                 
## 20:              sample.fraction ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;                 
## 21:                  save.memory ParamLgl    NA    NA       2          FALSE                 
## 22: scale.permutation.importance ParamLgl    NA    NA       2          FALSE importance      
## 23:                    se.method ParamFct    NA    NA       2        infjack                 
## 24:                         seed ParamInt  -Inf   Inf     Inf                                
## 25:         split.select.weights ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;                 
## 26:                    splitrule ParamFct    NA    NA       2           gini                 
## 27:                      verbose ParamLgl    NA    NA       2           TRUE                 
## 28:                 write.forest ParamLgl    NA    NA       2           TRUE                 
##                               id    class lower upper nlevels        default    parents value</code></pre>
<p>Define the hyper-parameter space of RF:</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(paradox)</span>
<span id="cb284-2"><a href="#cb284-2" aria-hidden="true" tabindex="-1"></a>rf_pars <span class="ot">=</span> </span>
<span id="cb284-3"><a href="#cb284-3" aria-hidden="true" tabindex="-1"></a>    paradox<span class="sc">::</span>ParamSet<span class="sc">$</span><span class="fu">new</span>(</span>
<span id="cb284-4"><a href="#cb284-4" aria-hidden="true" tabindex="-1"></a>      <span class="fu">list</span>(paradox<span class="sc">::</span>ParamInt<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;min.node.size&quot;</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> 30L),</span>
<span id="cb284-5"><a href="#cb284-5" aria-hidden="true" tabindex="-1"></a>           paradox<span class="sc">::</span>ParamInt<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;mtry&quot;</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> 30L),</span>
<span id="cb284-6"><a href="#cb284-6" aria-hidden="true" tabindex="-1"></a>           paradox<span class="sc">::</span>ParamLgl<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;regularization.usedepth&quot;</span>, <span class="at">default =</span> <span class="cn">TRUE</span>)))</span>
<span id="cb284-7"><a href="#cb284-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_pars)</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##                         id    class lower upper nlevels        default value
## 1:           min.node.size ParamInt     1    30      30 &lt;NoDefault[3]&gt;      
## 2:                    mtry ParamInt     1    30      30 &lt;NoDefault[3]&gt;      
## 3: regularization.usedepth ParamLgl    NA    NA       2           TRUE</code></pre>
<p>To setup the tuning pipeline we need:</p>
<ul>
<li>inner CV resample object</li>
<li>tuning criterion (e.g. AUC)</li>
<li>tuning method (e.g. random or block search)</li>
<li>tuning terminator (when should we stop tune? E.g. after n iterations)</li>
</ul>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="#cb286-1" aria-hidden="true" tabindex="-1"></a>inner3 <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> 3L)</span>
<span id="cb286-2"><a href="#cb286-2" aria-hidden="true" tabindex="-1"></a>measurement <span class="ot">=</span>  <span class="fu">msr</span>(<span class="st">&quot;classif.auc&quot;</span>)</span>
<span id="cb286-3"><a href="#cb286-3" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span>  mlr3tuning<span class="sc">::</span><span class="fu">tnr</span>(<span class="st">&quot;random_search&quot;</span>) </span>
<span id="cb286-4"><a href="#cb286-4" aria-hidden="true" tabindex="-1"></a>terminator <span class="ot">=</span> mlr3tuning<span class="sc">::</span><span class="fu">trm</span>(<span class="st">&quot;evals&quot;</span>, <span class="at">n_evals =</span> 5L)</span>
<span id="cb286-5"><a href="#cb286-5" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;classif.ranger&quot;</span>, <span class="at">predict_type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb286-6"><a href="#cb286-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb286-7"><a href="#cb286-7" aria-hidden="true" tabindex="-1"></a>learner_tuner <span class="ot">=</span> AutoTuner<span class="sc">$</span><span class="fu">new</span>(<span class="at">learner =</span> rf, </span>
<span id="cb286-8"><a href="#cb286-8" aria-hidden="true" tabindex="-1"></a>                              <span class="at">measure =</span> measurement, </span>
<span id="cb286-9"><a href="#cb286-9" aria-hidden="true" tabindex="-1"></a>                              <span class="at">tuner =</span> tuner, </span>
<span id="cb286-10"><a href="#cb286-10" aria-hidden="true" tabindex="-1"></a>                              <span class="at">terminator =</span> terminator,</span>
<span id="cb286-11"><a href="#cb286-11" aria-hidden="true" tabindex="-1"></a>                              <span class="at">search_space =</span> rf_pars,</span>
<span id="cb286-12"><a href="#cb286-12" aria-hidden="true" tabindex="-1"></a>                              <span class="at">resampling =</span> inner3)</span>
<span id="cb286-13"><a href="#cb286-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(learner_tuner)</span></code></pre></div>
<pre><code>## &lt;AutoTuner:classif.ranger.tuned&gt;
## * Model: -
## * Parameters: list()
## * Packages: ranger
## * Predict Type: prob
## * Feature types: logical, integer, numeric, character, factor, ordered
## * Properties: importance, multiclass, oob_error, twoclass, weights</code></pre>
<p>Now we can wrap it normally into the 10-CV setup as previously:</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="#cb288-1" aria-hidden="true" tabindex="-1"></a>outer3 <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> 3L)</span>
<span id="cb288-2"><a href="#cb288-2" aria-hidden="true" tabindex="-1"></a>result <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">resample</span>(transformed_task, learner_tuner, <span class="at">resampling =</span> outer3, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb288-3"><a href="#cb288-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb288-4"><a href="#cb288-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calclate the average AUC of the holdouts</span></span>
<span id="cb288-5"><a href="#cb288-5" aria-hidden="true" tabindex="-1"></a>result<span class="sc">$</span><span class="fu">aggregate</span>( measurement )</span></code></pre></div>
<p>Yeah, we were able to improve the performance!</p>
<p>Let’s create the final predictions:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="#cb289-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> </span>
<span id="cb289-2"><a href="#cb289-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="cf">function</span>(i) result<span class="sc">$</span>learners[[i]]<span class="sc">$</span><span class="fu">predict</span>(transformed_task, </span>
<span id="cb289-3"><a href="#cb289-3" aria-hidden="true" tabindex="-1"></a>                                                        <span class="at">row_ids =</span> (<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data))[<span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous)])<span class="sc">$</span>data<span class="sc">$</span>prob[,<span class="st">&quot;1&quot;</span>,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb289-4"><a href="#cb289-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(preds)</span>
<span id="cb289-5"><a href="#cb289-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">=</span> <span class="fu">apply</span>(preds, <span class="dv">1</span>, mean)</span></code></pre></div>
</div>
<div id="mlr3---hyper-parameter-tuning-with-oversampling" class="section level3" number="3.6.3">
<h3 number="3.6.3"><span class="header-section-number">3.6.3</span> mlr3 - hyper-parameter tuning with oversampling</h3>
<p>Let’s go one step back, maybe you have noticed that our classes are unbalanced:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(data<span class="sc">$</span>Hazardous)</span></code></pre></div>
<pre><code>## 
##   0   1 
## 412  88</code></pre>
<p>Many ML algorithms have problems with unbalanced data because if the imbalance is too strong it is cheaper for the algorithm to focus on only one class (e.g. by predicting only 0s oder 1s). You need to keep in mind that ML algorithms are greedy and their main focus is to minimize the loss function.</p>
<p>There are few techniques to correct for imbalance:</p>
<ul>
<li>oversampling (oversample the undersampled class)</li>
<li>undersampling (undersample the oversampled class)</li>
<li>SMOTE synthetic minority over-sampling technique (in short, we will use a kNN to create new samples around our undersampled class)</li>
</ul>
<p>Here, we will use oversampling which we can do by extending our rf learner:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="#cb292-1" aria-hidden="true" tabindex="-1"></a>rf_over <span class="ot">=</span> <span class="fu">po</span>(<span class="st">&quot;classbalancing&quot;</span>, <span class="at">id =</span> <span class="st">&quot;over&quot;</span>, <span class="at">adjust =</span> <span class="st">&quot;minor&quot;</span>)  <span class="sc">%&gt;&gt;%</span>  rf</span>
<span id="cb292-2"><a href="#cb292-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb292-3"><a href="#cb292-3" aria-hidden="true" tabindex="-1"></a><span class="co"># However rf_over is now a &quot;graph&quot;, but we can easily transform it back into a learner:</span></span>
<span id="cb292-4"><a href="#cb292-4" aria-hidden="true" tabindex="-1"></a>rf_over_learner <span class="ot">=</span> GraphLearner<span class="sc">$</span><span class="fu">new</span>(rf_over)</span>
<span id="cb292-5"><a href="#cb292-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_over_learner)</span></code></pre></div>
<pre><code>## &lt;GraphLearner:over.classif.ranger&gt;
## * Model: -
## * Parameters: over.ratio=1, over.reference=all, over.adjust=minor, over.shuffle=TRUE,
##   classif.ranger.num.threads=1
## * Packages: -
## * Predict Type: prob
## * Feature types: logical, integer, numeric, character, factor, ordered, POSIXct
## * Properties: featureless, importance, missings, multiclass, oob_error, selected_features, twoclass, weights</code></pre>
<p>The learner has now a new feature space:</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="#cb294-1" aria-hidden="true" tabindex="-1"></a>rf_over_learner<span class="sc">$</span>param_set</span></code></pre></div>
<pre><code>## &lt;ParamSetCollection&gt;
##                                              id    class lower upper nlevels        default                   parents
##  1:                        classif.ranger.alpha ParamDbl  -Inf   Inf     Inf            0.5                          
##  2:       classif.ranger.always.split.variables ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;                          
##  3:                classif.ranger.class.weights ParamDbl  -Inf   Inf     Inf                                         
##  4:                      classif.ranger.holdout ParamLgl    NA    NA       2          FALSE                          
##  5:                   classif.ranger.importance ParamFct    NA    NA       4 &lt;NoDefault[3]&gt;                          
##  6:                   classif.ranger.keep.inbag ParamLgl    NA    NA       2          FALSE                          
##  7:                    classif.ranger.max.depth ParamInt  -Inf   Inf     Inf                                         
##  8:                classif.ranger.min.node.size ParamInt     1   Inf     Inf              1                          
##  9:                     classif.ranger.min.prop ParamDbl  -Inf   Inf     Inf            0.1                          
## 10:                      classif.ranger.minprop ParamDbl  -Inf   Inf     Inf            0.1                          
## 11:                         classif.ranger.mtry ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;                          
## 12:            classif.ranger.num.random.splits ParamInt     1   Inf     Inf              1  classif.ranger.splitrule
## 13:                  classif.ranger.num.threads ParamInt     1   Inf     Inf              1                          
## 14:                    classif.ranger.num.trees ParamInt     1   Inf     Inf            500                          
## 15:                    classif.ranger.oob.error ParamLgl    NA    NA       2           TRUE                          
## 16:        classif.ranger.regularization.factor ParamUty    NA    NA     Inf              1                          
## 17:      classif.ranger.regularization.usedepth ParamLgl    NA    NA       2          FALSE                          
## 18:                      classif.ranger.replace ParamLgl    NA    NA       2           TRUE                          
## 19:    classif.ranger.respect.unordered.factors ParamFct    NA    NA       3         ignore                          
## 20:              classif.ranger.sample.fraction ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;                          
## 21:                  classif.ranger.save.memory ParamLgl    NA    NA       2          FALSE                          
## 22: classif.ranger.scale.permutation.importance ParamLgl    NA    NA       2          FALSE classif.ranger.importance
## 23:                    classif.ranger.se.method ParamFct    NA    NA       2        infjack                          
## 24:                         classif.ranger.seed ParamInt  -Inf   Inf     Inf                                         
## 25:         classif.ranger.split.select.weights ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;                          
## 26:                    classif.ranger.splitrule ParamFct    NA    NA       2           gini                          
## 27:                      classif.ranger.verbose ParamLgl    NA    NA       2           TRUE                          
## 28:                 classif.ranger.write.forest ParamLgl    NA    NA       2           TRUE                          
## 29:                                 over.adjust ParamFct    NA    NA       7 &lt;NoDefault[3]&gt;                          
## 30:                                  over.ratio ParamDbl     0   Inf     Inf &lt;NoDefault[3]&gt;                          
## 31:                              over.reference ParamFct    NA    NA       6 &lt;NoDefault[3]&gt;                          
## 32:                                over.shuffle ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;                          
##                                              id    class lower upper nlevels        default                   parents
##     value
##  1:      
##  2:      
##  3:      
##  4:      
##  5:      
##  6:      
##  7:      
##  8:      
##  9:      
## 10:      
## 11:      
## 12:      
## 13:     1
## 14:      
## 15:      
## 16:      
## 17:      
## 18:      
## 19:      
## 20:      
## 21:      
## 22:      
## 23:      
## 24:      
## 25:      
## 26:      
## 27:      
## 28:      
## 29: minor
## 30:     1
## 31:   all
## 32:  TRUE
##     value</code></pre>
<p>We can also tune the oversampling rate!</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="#cb296-1" aria-hidden="true" tabindex="-1"></a>rf_pars_over <span class="ot">=</span> </span>
<span id="cb296-2"><a href="#cb296-2" aria-hidden="true" tabindex="-1"></a>    paradox<span class="sc">::</span>ParamSet<span class="sc">$</span><span class="fu">new</span>(</span>
<span id="cb296-3"><a href="#cb296-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">list</span>(paradox<span class="sc">::</span>ParamInt<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;over.ratio&quot;</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> 7L),</span>
<span id="cb296-4"><a href="#cb296-4" aria-hidden="true" tabindex="-1"></a>           paradox<span class="sc">::</span>ParamInt<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;classif.ranger.min.node.size&quot;</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> 30L),</span>
<span id="cb296-5"><a href="#cb296-5" aria-hidden="true" tabindex="-1"></a>           paradox<span class="sc">::</span>ParamInt<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;classif.ranger.mtry&quot;</span>, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> 30L),</span>
<span id="cb296-6"><a href="#cb296-6" aria-hidden="true" tabindex="-1"></a>           paradox<span class="sc">::</span>ParamLgl<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;classif.ranger.regularization.usedepth&quot;</span>, <span class="at">default =</span> <span class="cn">TRUE</span>)))</span>
<span id="cb296-7"><a href="#cb296-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb296-8"><a href="#cb296-8" aria-hidden="true" tabindex="-1"></a>inner3 <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> 3L)</span>
<span id="cb296-9"><a href="#cb296-9" aria-hidden="true" tabindex="-1"></a>measurement <span class="ot">=</span>  <span class="fu">msr</span>(<span class="st">&quot;classif.auc&quot;</span>)</span>
<span id="cb296-10"><a href="#cb296-10" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span>  mlr3tuning<span class="sc">::</span><span class="fu">tnr</span>(<span class="st">&quot;random_search&quot;</span>) </span>
<span id="cb296-11"><a href="#cb296-11" aria-hidden="true" tabindex="-1"></a>terminator <span class="ot">=</span> mlr3tuning<span class="sc">::</span><span class="fu">trm</span>(<span class="st">&quot;evals&quot;</span>, <span class="at">n_evals =</span> 5L)</span>
<span id="cb296-12"><a href="#cb296-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb296-13"><a href="#cb296-13" aria-hidden="true" tabindex="-1"></a>learner_tuner_over <span class="ot">=</span> AutoTuner<span class="sc">$</span><span class="fu">new</span>(<span class="at">learner =</span> rf_over_learner, </span>
<span id="cb296-14"><a href="#cb296-14" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">measure =</span> measurement, </span>
<span id="cb296-15"><a href="#cb296-15" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">tuner =</span> tuner, </span>
<span id="cb296-16"><a href="#cb296-16" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">terminator =</span> terminator,</span>
<span id="cb296-17"><a href="#cb296-17" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">search_space =</span> rf_pars_over,</span>
<span id="cb296-18"><a href="#cb296-18" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">resampling =</span> inner3)</span>
<span id="cb296-19"><a href="#cb296-19" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(learner_tuner)</span></code></pre></div>
<pre><code>## &lt;AutoTuner:classif.ranger.tuned&gt;
## * Model: -
## * Parameters: list()
## * Packages: ranger
## * Predict Type: prob
## * Feature types: logical, integer, numeric, character, factor, ordered
## * Properties: importance, multiclass, oob_error, twoclass, weights</code></pre>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="#cb298-1" aria-hidden="true" tabindex="-1"></a>outer3 <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> 3L)</span>
<span id="cb298-2"><a href="#cb298-2" aria-hidden="true" tabindex="-1"></a>result <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">resample</span>(transformed_task, learner_tuner_over, <span class="at">resampling =</span> outer3, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb298-3"><a href="#cb298-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb298-4"><a href="#cb298-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calclate the average AUC of the holdouts</span></span>
<span id="cb298-5"><a href="#cb298-5" aria-hidden="true" tabindex="-1"></a>result<span class="sc">$</span><span class="fu">aggregate</span>( measurement )</span></code></pre></div>
<p>5 iterations in the hyper-space is not very much…</p>
<p>Let’s create the final predictions:</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="#cb299-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> </span>
<span id="cb299-2"><a href="#cb299-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="cf">function</span>(i) result<span class="sc">$</span>learners[[i]]<span class="sc">$</span><span class="fu">predict</span>(transformed_task, </span>
<span id="cb299-3"><a href="#cb299-3" aria-hidden="true" tabindex="-1"></a>                                                        <span class="at">row_ids =</span> (<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data))[<span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous)])<span class="sc">$</span>data<span class="sc">$</span>prob[,<span class="st">&quot;1&quot;</span>,<span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb299-4"><a href="#cb299-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(preds)</span>
<span id="cb299-5"><a href="#cb299-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">=</span> <span class="fu">apply</span>(preds, <span class="dv">1</span>, mean)</span></code></pre></div>
<!--chapter:end:02-fundamental.Rmd-->
</div>
</div>
</div>
<div id="Deep" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> Deep learning</h1>
<p>In this section, we will discuss both different (deep) network architectures and different means to regularize and improve those deep architectures.</p>
<div id="network-architectures" class="section level2" number="4.1">
<h2 number="4.1"><span class="header-section-number">4.1</span> Network architectures</h2>
<div id="deep-neural-networks-dnns" class="section level3" number="4.1.1">
<h3 number="4.1.1"><span class="header-section-number">4.1.1</span> Deep neural networks (DNNs)</h3>
<p>Deep neural networks are basically the same as simple ANN, only that they have more hidden layers.</p>
</div>
<div id="convolutional-neural-networks-dnns" class="section level3" number="4.1.2">
<h3 number="4.1.2"><span class="header-section-number">4.1.2</span> Convolutional neural networks (DNNs)</h3>
<p>The main purpose of CNNs is image recognition. In a CNN, we have at least one convolution layer, additional to the normal, fully connected DNN layers.</p>
<p>Neurons in a convolution layer are connected only to a small spatially contiguous area of the input layer (receptive field). We use this structure (feature map) to scan the entire picture. The weights are optimized, but the same for all nodes of the hidden layer (shared weights). Think of the feature map as a kernel or filter that is used to scan the image.</p>
<p>We use this kernel to scan the input features / neurons (e.g. picture). The kernel weights are optimized, but we use the same weights across the entire input neurons (shared weights). The resulting hidden layer is called a feature map. You can think of the feature maps as a map that shows you where the “shapes” expressed by the kernel appear in the input. One kernel / feature map will not be enough, we typically have many shapes that we want to recognize. Thus, the input layer is typically connected to several feature maps, which can be aggregated and followed by a second layer of feature maps, and so on.</p>
</div>
<div id="recurrent-neural-networks-rnns" class="section level3" number="4.1.3">
<h3 number="4.1.3"><span class="header-section-number">4.1.3</span> Recurrent neural networks (RNNs)</h3>
<p>Recurrent Neural Networks are used to model sequential data, i.e. temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/SEnXr6v2ifU" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<div id="natural-language-processing-nlp" class="section level3" number="4.1.4">
<h3 number="4.1.4"><span class="header-section-number">4.1.4</span> Natural language processing (NLP)</h3>
<p>NLP is actually more of a task than a network structure, but in the area of deep learning for NLP, particular network structures are used. This video should get you an idea about what NLP is about</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UFtXy0KRxVI" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>See also the blog post linked with the youtube video with accompanying code to the video. Moreover, here is an article that shows now NLP works with keras, however, written in Python. As a challenge, you can take the code and implement it in R <a href="https://nlpforhackers.io/keras-intro/" class="uri">https://nlpforhackers.io/keras-intro/</a></p>
</div>
</div>
<div id="case-study-dropout-and-early-stopping-in-a-deep-neural-network" class="section level2" number="4.2">
<h2 number="4.2"><span class="header-section-number">4.2</span> Case study: dropout and early stopping in a deep neural network</h2>
<p>Regularization in deep neural networks is very important because the problem of overfitting. Standard regularization from statistics like l1 and l2 regularization are often feasy and require a lot of tuning. There are more stable and robust methods:</p>
<ul>
<li>Early stopping: Early stopping allows us to stop the training when for instance the test loss does not increase anymore</li>
<li>Dropout: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Dropout is more robust than l1 and l2, and tuning of the dropout rate can be beneficial but a rate between 0.2-0.5 works often quite well</li>
</ul>
<p><strong>Data preparation</strong></p>
<p>See (mlr) for explanation about the pre-processing pipeline.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EcoData)</span>
<span id="cb300-2"><a href="#cb300-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb300-3"><a href="#cb300-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3)</span>
<span id="cb300-4"><a href="#cb300-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3pipelines)</span>
<span id="cb300-5"><a href="#cb300-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(nasa)</span>
<span id="cb300-6"><a href="#cb300-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(nasa)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    4687 obs. of  40 variables:
##  $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...
##  $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...
##  $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...
##  $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...
##  $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...
##  $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...
##  $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...
##  $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...
##  $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...
##  $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...
##  $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...
##  $ Close.Approach.Date         : Factor w/ 777 levels &quot;1995-01-01&quot;,&quot;1995-01-08&quot;,..: 511 712 472 239 273 145 428 694 87 732 ...
##  $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...
##  $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...
##  $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...
##  $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...
##  $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...
##  $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...
##  $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...
##  $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...
##  $ Orbiting.Body               : Factor w/ 1 level &quot;Earth&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...
##  $ Orbit.Determination.Date    : Factor w/ 2680 levels &quot;2014-06-13 15:20:44&quot;,..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...
##  $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...
##  $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...
##  $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...
##  $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...
##  $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...
##  $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...
##  $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...
##  $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...
##  $ Orbital.Period              : num  419 1040 682 427 503 ...
##  $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...
##  $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...
##  $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...
##  $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...
##  $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...
##  $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...
##  $ Equinox                     : Factor w/ 1 level &quot;J2000&quot;: 1 1 NA 1 1 1 1 1 1 1 ...
##  $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...</code></pre>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="#cb302-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> nasa <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Orbit.Determination.Date, <span class="sc">-</span>Close.Approach.Date, <span class="sc">-</span>Name, <span class="sc">-</span>Neo.Reference.ID)</span>
<span id="cb302-2"><a href="#cb302-2" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Hazardous <span class="ot">=</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>Hazardous)</span>
<span id="cb302-3"><a href="#cb302-3" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> TaskClassif<span class="sc">$</span><span class="fu">new</span>(<span class="at">id =</span> <span class="st">&quot;nasa&quot;</span>, <span class="at">backend =</span> data, <span class="at">target =</span> <span class="st">&quot;Hazardous&quot;</span>, <span class="at">positive =</span> <span class="st">&quot;1&quot;</span>)</span>
<span id="cb302-4"><a href="#cb302-4" aria-hidden="true" tabindex="-1"></a>preprocessing <span class="ot">=</span> <span class="fu">po</span>(<span class="st">&quot;imputeoor&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">po</span>(<span class="st">&quot;scale&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>) </span>
<span id="cb302-5"><a href="#cb302-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> preprocessing<span class="sc">$</span><span class="fu">train</span>(task)[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span>
<span id="cb302-6"><a href="#cb302-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb302-7"><a href="#cb302-7" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[<span class="sc">!</span><span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous),]</span>
<span id="cb302-8"><a href="#cb302-8" aria-hidden="true" tabindex="-1"></a>submit <span class="ot">=</span> data[<span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous),]</span>
<span id="cb302-9"><a href="#cb302-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb302-10"><a href="#cb302-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(train <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Hazardous))</span>
<span id="cb302-11"><a href="#cb302-11" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> train <span class="sc">%&gt;%</span> <span class="fu">select</span>(Hazardous)</span>
<span id="cb302-12"><a href="#cb302-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">to_categorical</span>(<span class="fu">as.matrix</span>(Y), <span class="dv">2</span>)</span></code></pre></div>
<p><strong>Early stopping</strong></p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb303-2"><a href="#cb303-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-3"><a href="#cb303-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb303-4"><a href="#cb303-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb303-5"><a href="#cb303-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X)) <span class="sc">%&gt;%</span></span>
<span id="cb303-6"><a href="#cb303-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb303-7"><a href="#cb303-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb303-8"><a href="#cb303-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>) </span>
<span id="cb303-9"><a href="#cb303-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-10"><a href="#cb303-10" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb303-11"><a href="#cb303-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb303-12"><a href="#cb303-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_5&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense_14 (Dense)                                       (None, 50)                                       1900               
## ___________________________________________________________________________________________________________________________
## dense_15 (Dense)                                       (None, 50)                                       2550               
## ___________________________________________________________________________________________________________________________
## dense_16 (Dense)                                       (None, 50)                                       2550               
## ___________________________________________________________________________________________________________________________
## dense_17 (Dense)                                       (None, 2)                                        102                
## ===========================================================================================================================
## Total params: 7,102
## Trainable params: 7,102
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="#cb305-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb305-2"><a href="#cb305-2" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb305-3"><a href="#cb305-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb305-4"><a href="#cb305-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 50L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb305-5"><a href="#cb305-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>)</span>
<span id="cb305-6"><a href="#cb305-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-143-1.png" width="672" /></p>
<p>The validation loss first decreases but then starts to increase again, can you explain this behavior?
-&gt; Overfitting!</p>
<p>Let’s try a l1+l2 regularization:</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="#cb307-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb307-2"><a href="#cb307-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-3"><a href="#cb307-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb307-4"><a href="#cb307-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb307-5"><a href="#cb307-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X), <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>( <span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb307-6"><a href="#cb307-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb307-7"><a href="#cb307-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb307-8"><a href="#cb307-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) </span>
<span id="cb307-9"><a href="#cb307-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-10"><a href="#cb307-10" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb307-11"><a href="#cb307-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb307-12"><a href="#cb307-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_6&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense_18 (Dense)                                       (None, 50)                                       1900               
## ___________________________________________________________________________________________________________________________
## dense_19 (Dense)                                       (None, 50)                                       2550               
## ___________________________________________________________________________________________________________________________
## dense_20 (Dense)                                       (None, 50)                                       2550               
## ___________________________________________________________________________________________________________________________
## dense_21 (Dense)                                       (None, 2)                                        102                
## ===========================================================================================================================
## Total params: 7,102
## Trainable params: 7,102
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="#cb309-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb309-2"><a href="#cb309-2" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb309-3"><a href="#cb309-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb309-4"><a href="#cb309-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb309-5"><a href="#cb309-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>)</span>
<span id="cb309-6"><a href="#cb309-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-144-1.png" width="672" />
Better, but the validation loss still starts to increase after 40 epochs. But we can use early stopping to end the training before the val loss starts to increase again!</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="#cb311-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb311-2"><a href="#cb311-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb311-3"><a href="#cb311-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb311-4"><a href="#cb311-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb311-5"><a href="#cb311-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X), <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>( <span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb311-6"><a href="#cb311-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb311-7"><a href="#cb311-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb311-8"><a href="#cb311-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) </span>
<span id="cb311-9"><a href="#cb311-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb311-10"><a href="#cb311-10" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb311-11"><a href="#cb311-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb311-12"><a href="#cb311-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_7&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## dense_22 (Dense)                                       (None, 50)                                       1900               
## ___________________________________________________________________________________________________________________________
## dense_23 (Dense)                                       (None, 50)                                       2550               
## ___________________________________________________________________________________________________________________________
## dense_24 (Dense)                                       (None, 50)                                       2550               
## ___________________________________________________________________________________________________________________________
## dense_25 (Dense)                                       (None, 2)                                        102                
## ===========================================================================================================================
## Total params: 7,102
## Trainable params: 7,102
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="#cb313-1" aria-hidden="true" tabindex="-1"></a>early <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">callback_early_stopping</span>(<span class="at">patience =</span> 5L)</span>
<span id="cb313-2"><a href="#cb313-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb313-3"><a href="#cb313-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb313-4"><a href="#cb313-4" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb313-5"><a href="#cb313-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb313-6"><a href="#cb313-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb313-7"><a href="#cb313-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>, <span class="at">callbacks=</span><span class="fu">c</span>(early))</span>
<span id="cb313-8"><a href="#cb313-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-145-1.png" width="672" />
Patience is the number of epochs to wait before aborting the training.</p>
<p><strong>Dropout - another type of regularization</strong></p>
<p><span class="citation"><a href="#ref-dropout" role="doc-biblioref">Srivastava et al.</a> (<a href="#ref-dropout" role="doc-biblioref">2014</a>)</span> suggests a dropout rate of 50% for internal hidden layers and 20% for the input layer. One advantage of dropout is that the training is more independent of the number of epochs i.e. the val loss usually doesn’t start to increase after several epochs.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="#cb315-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb315-2"><a href="#cb315-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb315-3"><a href="#cb315-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb315-4"><a href="#cb315-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X)) <span class="sc">%&gt;%</span></span>
<span id="cb315-5"><a href="#cb315-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb315-6"><a href="#cb315-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb315-7"><a href="#cb315-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb315-8"><a href="#cb315-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb315-9"><a href="#cb315-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb315-10"><a href="#cb315-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>) </span>
<span id="cb315-11"><a href="#cb315-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb315-12"><a href="#cb315-12" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb315-13"><a href="#cb315-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb315-14"><a href="#cb315-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb315-15"><a href="#cb315-15" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb315-16"><a href="#cb315-16" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb315-17"><a href="#cb315-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb315-18"><a href="#cb315-18" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb315-19"><a href="#cb315-19" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>)</span>
<span id="cb315-20"><a href="#cb315-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-146-1.png" width="672" />
Ofc, you can still combine early stopping and dropout, which is normally a good idea since it improves training efficiency (e.g. you could start with 1000 epochs and you know training will be aborted if it doesn’t improve anymore).</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Dropout and early stopping with torch:</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="#cb317-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb317-2"><a href="#cb317-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.2</span>),</span>
<span id="cb317-3"><a href="#cb317-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(<span class="fu">ncol</span>(X), 50L),</span>
<span id="cb317-4"><a href="#cb317-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb317-5"><a href="#cb317-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.5</span>),</span>
<span id="cb317-6"><a href="#cb317-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(50L, 50L),</span>
<span id="cb317-7"><a href="#cb317-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(), </span>
<span id="cb317-8"><a href="#cb317-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.5</span>),</span>
<span id="cb317-9"><a href="#cb317-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(50L, 50L),</span>
<span id="cb317-10"><a href="#cb317-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(), </span>
<span id="cb317-11"><a href="#cb317-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.5</span>),</span>
<span id="cb317-12"><a href="#cb317-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(50L, 2L)</span>
<span id="cb317-13"><a href="#cb317-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb317-14"><a href="#cb317-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-15"><a href="#cb317-15" aria-hidden="true" tabindex="-1"></a>YT <span class="ot">=</span> <span class="fu">apply</span>(Y, <span class="dv">1</span>,which.max)</span>
<span id="cb317-16"><a href="#cb317-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-17"><a href="#cb317-17" aria-hidden="true" tabindex="-1"></a>dataset_nasa <span class="ot">=</span> <span class="fu">dataset</span>(</span>
<span id="cb317-18"><a href="#cb317-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">name =</span> <span class="st">&quot;nasa&quot;</span>,</span>
<span id="cb317-19"><a href="#cb317-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(nasa) {</span>
<span id="cb317-20"><a href="#cb317-20" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>X <span class="ot">=</span> nasa<span class="sc">$</span>X</span>
<span id="cb317-21"><a href="#cb317-21" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>Y <span class="ot">=</span> nasa<span class="sc">$</span>Y</span>
<span id="cb317-22"><a href="#cb317-22" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb317-23"><a href="#cb317-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">.getitem =</span> <span class="cf">function</span>(i) {</span>
<span id="cb317-24"><a href="#cb317-24" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> self<span class="sc">$</span>X[i,,drop<span class="ot">=</span><span class="cn">FALSE</span>] <span class="sc">%&gt;%</span> <span class="fu">torch_tensor</span>()</span>
<span id="cb317-25"><a href="#cb317-25" aria-hidden="true" tabindex="-1"></a>    Y <span class="ot">=</span> self<span class="sc">$</span>Y[i] <span class="sc">%&gt;%</span> <span class="fu">torch_tensor</span>()</span>
<span id="cb317-26"><a href="#cb317-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(X, Y)</span>
<span id="cb317-27"><a href="#cb317-27" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb317-28"><a href="#cb317-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">.length =</span> <span class="cf">function</span>() {</span>
<span id="cb317-29"><a href="#cb317-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nrow</span>(self<span class="sc">$</span>X)</span>
<span id="cb317-30"><a href="#cb317-30" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb317-31"><a href="#cb317-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-32"><a href="#cb317-32" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(<span class="fu">dataset_nasa</span>(<span class="fu">list</span>(<span class="at">X =</span> X[<span class="dv">1</span><span class="sc">:</span><span class="dv">400</span>,], <span class="at">Y =</span> YT[<span class="dv">1</span><span class="sc">:</span><span class="dv">400</span>])), </span>
<span id="cb317-33"><a href="#cb317-33" aria-hidden="true" tabindex="-1"></a>                      <span class="at">batch_size =</span> <span class="dv">32</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb317-34"><a href="#cb317-34" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>( <span class="fu">dataset_nasa</span>(<span class="fu">list</span>(<span class="at">X =</span> X[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>,], <span class="at">Y =</span> YT[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>])), </span>
<span id="cb317-35"><a href="#cb317-35" aria-hidden="true" tabindex="-1"></a>                      <span class="at">batch_size =</span> <span class="dv">32</span>)</span>
<span id="cb317-36"><a href="#cb317-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-37"><a href="#cb317-37" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">train</span>()</span>
<span id="cb317-38"><a href="#cb317-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-39"><a href="#cb317-39" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(model_torch<span class="sc">$</span>parameters, <span class="fl">0.01</span>)</span>
<span id="cb317-40"><a href="#cb317-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-41"><a href="#cb317-41" aria-hidden="true" tabindex="-1"></a>train_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb317-42"><a href="#cb317-42" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb317-43"><a href="#cb317-43" aria-hidden="true" tabindex="-1"></a>early_epoch <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb317-44"><a href="#cb317-44" aria-hidden="true" tabindex="-1"></a>min_loss <span class="ot">=</span> <span class="cn">Inf</span></span>
<span id="cb317-45"><a href="#cb317-45" aria-hidden="true" tabindex="-1"></a>patience <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb317-46"><a href="#cb317-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb317-47"><a href="#cb317-47" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb317-48"><a href="#cb317-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(early_epoch <span class="sc">&gt;=</span> patience) <span class="cf">break</span></span>
<span id="cb317-49"><a href="#cb317-49" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb317-50"><a href="#cb317-50" aria-hidden="true" tabindex="-1"></a>  train_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb317-51"><a href="#cb317-51" aria-hidden="true" tabindex="-1"></a>  test_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb317-52"><a href="#cb317-52" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb317-53"><a href="#cb317-53" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb317-54"><a href="#cb317-54" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">squeeze</span>())</span>
<span id="cb317-55"><a href="#cb317-55" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]]<span class="sc">$</span><span class="fu">squeeze</span>(),<span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb317-56"><a href="#cb317-56" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb317-57"><a href="#cb317-57" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb317-58"><a href="#cb317-58" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="ot">=</span> <span class="fu">c</span>(train_loss, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb317-59"><a href="#cb317-59" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb317-60"><a href="#cb317-60" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb317-61"><a href="#cb317-61" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> test_dl) {</span>
<span id="cb317-62"><a href="#cb317-62" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">squeeze</span>())</span>
<span id="cb317-63"><a href="#cb317-63" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]]<span class="sc">$</span><span class="fu">squeeze</span>(),<span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb317-64"><a href="#cb317-64" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="ot">=</span> <span class="fu">c</span>(test_loss, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb317-65"><a href="#cb317-65" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb317-66"><a href="#cb317-66" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb317-67"><a href="#cb317-67" aria-hidden="true" tabindex="-1"></a>  <span class="do">### early stopping </span><span class="al">###</span></span>
<span id="cb317-68"><a href="#cb317-68" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">mean</span>(test_loss) <span class="sc">&lt;</span> min_loss) {</span>
<span id="cb317-69"><a href="#cb317-69" aria-hidden="true" tabindex="-1"></a>    min_loss <span class="ot">=</span> <span class="fu">mean</span>(test_loss)</span>
<span id="cb317-70"><a href="#cb317-70" aria-hidden="true" tabindex="-1"></a>    early_epoch <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb317-71"><a href="#cb317-71" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb317-72"><a href="#cb317-72" aria-hidden="true" tabindex="-1"></a>    early_epoch <span class="ot">=</span> early_epoch <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb317-73"><a href="#cb317-73" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb317-74"><a href="#cb317-74" aria-hidden="true" tabindex="-1"></a>  <span class="do">###</span></span>
<span id="cb317-75"><a href="#cb317-75" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb317-76"><a href="#cb317-76" aria-hidden="true" tabindex="-1"></a>  train_losses <span class="ot">=</span> <span class="fu">c</span>(train_losses, <span class="fu">mean</span>(train_loss))</span>
<span id="cb317-77"><a href="#cb317-77" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, <span class="fu">mean</span>(test_loss))</span>
<span id="cb317-78"><a href="#cb317-78" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, epoch, <span class="fu">mean</span>(train_loss)))</span>
<span id="cb317-79"><a href="#cb317-79" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 0.518420
## Loss at epoch 2: 0.455640
## Loss at epoch 3: 0.446879
## Loss at epoch 4: 0.447216
## Loss at epoch 5: 0.425368
## Loss at epoch 6: 0.425635
## Loss at epoch 7: 0.414827
## Loss at epoch 8: 0.401890
## Loss at epoch 9: 0.386621
## Loss at epoch 10: 0.391964
## Loss at epoch 11: 0.390527
## Loss at epoch 12: 0.389494
## Loss at epoch 13: 0.396623
## Loss at epoch 14: 0.396054
## Loss at epoch 15: 0.361036
## Loss at epoch 16: 0.354566</code></pre>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="fu">matplot</span>(<span class="fu">cbind</span>(train_losses, test_losses), <span class="at">type =</span> <span class="st">&quot;o&quot;</span>, <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">15</span>, <span class="dv">16</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkblue&quot;</span>, <span class="st">&quot;darkred&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">xlab =</span> <span class="st">&quot;Epoch&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Loss&quot;</span>, <span class="at">las =</span> <span class="dv">1</span>)</span>
<span id="cb319-2"><a href="#cb319-2" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkblue&quot;</span>, <span class="st">&quot;darkred&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">15</span>, <span class="dv">16</span>), <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Train loss&quot;</span>, <span class="st">&quot;Val loss&quot;</span>) )</span></code></pre></div>
<img src="_main_files/figure-html/unnamed-chunk-147-1.png" width="672" />
</details>
<p><br/></p>
</div>
<div id="case-study---fitting-a-convolutional-neural-networks-on-mnist" class="section level2" number="4.3">
<h2 number="4.3"><span class="header-section-number">4.3</span> Case study - fitting a Convolutional Neural Networks on MNIST</h2>
<p>We will show the use of convolutinal neural networks with the MNIST dataset.The MNIST dataset is maybe one of the most famous image datasets. It is a dataset of 60,000 handwritten digits from 0-9.</p>
<p>To do so, we define a few helper functions:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb320-2"><a href="#cb320-2" aria-hidden="true" tabindex="-1"></a>rotate <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, rev))</span>
<span id="cb320-3"><a href="#cb320-3" aria-hidden="true" tabindex="-1"></a>imgPlot <span class="ot">=</span> <span class="cf">function</span>(img, <span class="at">title =</span> <span class="st">&quot;&quot;</span>){</span>
<span id="cb320-4"><a href="#cb320-4" aria-hidden="true" tabindex="-1"></a> col<span class="ot">=</span><span class="fu">grey.colors</span>(<span class="dv">255</span>)</span>
<span id="cb320-5"><a href="#cb320-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">image</span>(<span class="fu">rotate</span>(img), <span class="at">col =</span> col, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">axes=</span><span class="cn">FALSE</span>, <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Label: &quot;</span>, <span class="fu">as.character</span>(title)))</span>
<span id="cb320-6"><a href="#cb320-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The dataset is so famous that there is an automatic download function in keras:</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="#cb321-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb321-2"><a href="#cb321-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data<span class="sc">$</span>train</span>
<span id="cb321-3"><a href="#cb321-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data<span class="sc">$</span>test</span></code></pre></div>
<p>Let’s visualize a few digits:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb322-2"><a href="#cb322-2" aria-hidden="true" tabindex="-1"></a>.n <span class="ot">=</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="cf">function</span>(x) <span class="fu">imgPlot</span>(train<span class="sc">$</span>x[x,,], train<span class="sc">$</span>y[x]))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-150-1.png" width="672" /></p>
<p>Similar to the normal ML workflow, we have to scale the pixels (from 0-255) to the range of [0,1] and one hot encode the response. To scale the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2d array/tensor is typically called a matrix.</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="#cb323-1" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>(train<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train<span class="sc">$</span>x), <span class="dv">1</span>))</span>
<span id="cb323-2"><a href="#cb323-2" aria-hidden="true" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">array</span>(test<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(test<span class="sc">$</span>x), <span class="dv">1</span>))</span>
<span id="cb323-3"><a href="#cb323-3" aria-hidden="true" tabindex="-1"></a>train_y <span class="ot">=</span> <span class="fu">to_categorical</span>(train<span class="sc">$</span>y, <span class="dv">10</span>)</span>
<span id="cb323-4"><a href="#cb323-4" aria-hidden="true" tabindex="-1"></a>test_y <span class="ot">=</span> <span class="fu">to_categorical</span>(test<span class="sc">$</span>y, <span class="dv">10</span>)</span></code></pre></div>
<p>The last dimension stands for the number of channels in the image. In our case we have only one channel because the images are white-black.</p>
<p>Normally we would have three channels - colors are encoded by the combination of three base colors (usually red,green,blue).</p>
<p>To build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size 2x2. These are 2D kernels because our images are 2D. For movies for example, one would use a 3D kernel (the third dimension would correspond to time and not to the color channels).</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="#cb324-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb324-2"><a href="#cb324-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb324-3"><a href="#cb324-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d</span>(<span class="at">input_shape =</span> <span class="fu">c</span>(28L, 28L,1L),<span class="at">filters =</span> 16L, <span class="at">kernel_size =</span> <span class="fu">c</span>(2L,2L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb324-4"><a href="#cb324-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb324-5"><a href="#cb324-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 16L, <span class="at">kernel_size =</span> <span class="fu">c</span>(3L,3L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb324-6"><a href="#cb324-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb324-7"><a href="#cb324-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb324-8"><a href="#cb324-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb324-9"><a href="#cb324-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(10L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb324-10"><a href="#cb324-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_9&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## conv2d (Conv2D)                                        (None, 27, 27, 16)                               80                 
## ___________________________________________________________________________________________________________________________
## max_pooling2d (MaxPooling2D)                           (None, 13, 13, 16)                               0                  
## ___________________________________________________________________________________________________________________________
## conv2d_1 (Conv2D)                                      (None, 11, 11, 16)                               2320               
## ___________________________________________________________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)                         (None, 5, 5, 16)                                 0                  
## ___________________________________________________________________________________________________________________________
## flatten (Flatten)                                      (None, 400)                                      0                  
## ___________________________________________________________________________________________________________________________
## dense_30 (Dense)                                       (None, 100)                                      40100              
## ___________________________________________________________________________________________________________________________
## dense_31 (Dense)                                       (None, 10)                                       1010               
## ===========================================================================================================================
## Total params: 43,510
## Trainable params: 43,510
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<p>We additionally used a pooling layer to downsize the resulting feature maps. After another convolutional and pooling layer we flatten the output, i.e. the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all weights from the last feature maps).Having flattened the layer, we can simply use our typical output layer.</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Prepare/download data:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb326-2"><a href="#cb326-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torchvision)</span>
<span id="cb326-3"><a href="#cb326-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-4"><a href="#cb326-4" aria-hidden="true" tabindex="-1"></a>train_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb326-5"><a href="#cb326-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;.&quot;</span>,</span>
<span id="cb326-6"><a href="#cb326-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb326-7"><a href="#cb326-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> <span class="cn">TRUE</span>,</span>
<span id="cb326-8"><a href="#cb326-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">transform =</span> transform_to_tensor</span>
<span id="cb326-9"><a href="#cb326-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb326-10"><a href="#cb326-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-11"><a href="#cb326-11" aria-hidden="true" tabindex="-1"></a>test_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb326-12"><a href="#cb326-12" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;.&quot;</span>,</span>
<span id="cb326-13"><a href="#cb326-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb326-14"><a href="#cb326-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> <span class="cn">FALSE</span>,</span>
<span id="cb326-15"><a href="#cb326-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">transform =</span> transform_to_tensor</span>
<span id="cb326-16"><a href="#cb326-16" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Build dataloader:</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="#cb327-1" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(train_ds, <span class="at">batch_size =</span> <span class="dv">32</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb327-2"><a href="#cb327-2" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> <span class="dv">32</span>)</span>
<span id="cb327-3"><a href="#cb327-3" aria-hidden="true" tabindex="-1"></a>first_batch <span class="ot">=</span> train_dl<span class="sc">$</span><span class="fu">.iter</span>()</span>
<span id="cb327-4"><a href="#cb327-4" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> first_batch<span class="sc">$</span><span class="fu">.next</span>()</span>
<span id="cb327-5"><a href="#cb327-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb327-6"><a href="#cb327-6" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>x<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32  1 28 28</code></pre>
<p>Build CNN:
We have here to calculate the shapes of our layers on our own:</p>
<p><strong>We start with our input of shape (batch_size, 1, 28, 28)</strong></p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="#cb329-1" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> df<span class="sc">$</span>x</span>
<span id="cb329-2"><a href="#cb329-2" aria-hidden="true" tabindex="-1"></a>sample<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32  1 28 28</code></pre>
<p><strong>first conv layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2)</strong></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="#cb331-1" aria-hidden="true" tabindex="-1"></a>conv1 <span class="ot">=</span> <span class="fu">nn_conv2d</span>(<span class="dv">1</span>, 16L, 2L, <span class="at">stride =</span> 1L)</span>
<span id="cb331-2"><a href="#cb331-2" aria-hidden="true" tabindex="-1"></a>(sample <span class="sc">%&gt;%</span> conv1)<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32 16 27 27</code></pre>
<p>Output: batch_size = 32, number of feature maps = 16, dimensions of each feature map = ( 27 , 27 )
Wit a kernel size of two and stride =1 we wil lose one pixel in each dimension…
Questions:</p>
<ul>
<li>what does happen if we increase the stride?</li>
<li>what does happen if we increase the kernel size?</li>
</ul>
<p><strong>pooling layer summarizes each feature map</strong></p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="#cb333-1" aria-hidden="true" tabindex="-1"></a>(sample <span class="sc">%&gt;%</span> conv1 <span class="sc">%&gt;%</span> <span class="fu">nnf_max_pool2d</span>(<span class="at">kernel_size =</span> 2L,<span class="at">stride =</span> 2L))<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32 16 13 13</code></pre>
<p>kernel_size = 2L and stride = 2L halfs the pixel dimensions of our image</p>
<p><strong>fully connected layer</strong></p>
<p>Now we have to flatten our final output of the CNN model to use a normal fully connected layer, but to do so we have to calulate the number of inputs for the fully connected layer:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="#cb335-1" aria-hidden="true" tabindex="-1"></a>dims <span class="ot">=</span> (sample <span class="sc">%&gt;%</span> conv1 <span class="sc">%&gt;%</span> <span class="fu">nnf_max_pool2d</span>(<span class="at">kernel_size =</span> 2L,<span class="at">stride =</span> 2L))<span class="sc">$</span><span class="fu">size</span>()</span>
<span id="cb335-2"><a href="#cb335-2" aria-hidden="true" tabindex="-1"></a><span class="co"># without the batch size ofc</span></span>
<span id="cb335-3"><a href="#cb335-3" aria-hidden="true" tabindex="-1"></a>final <span class="ot">=</span> <span class="fu">prod</span>(dims[<span class="sc">-</span><span class="dv">1</span>]) </span>
<span id="cb335-4"><a href="#cb335-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(final)</span></code></pre></div>
<pre><code>## [1] 2704</code></pre>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="#cb337-1" aria-hidden="true" tabindex="-1"></a>fc <span class="ot">=</span> <span class="fu">nn_linear</span>(final, 10L)</span>
<span id="cb337-2"><a href="#cb337-2" aria-hidden="true" tabindex="-1"></a>(sample <span class="sc">%&gt;%</span> conv1 <span class="sc">%&gt;%</span> <span class="fu">nnf_max_pool2d</span>(<span class="at">kernel_size =</span> 2L,<span class="at">stride =</span> 2L) <span class="sc">%&gt;%</span> <span class="fu">torch_flatten</span>(<span class="at">start_dim =</span> 2L) <span class="sc">%&gt;%</span> fc)<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32 10</code></pre>
<p>Build the network:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="#cb339-1" aria-hidden="true" tabindex="-1"></a>net <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(</span>
<span id="cb339-2"><a href="#cb339-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;mnist&quot;</span>,</span>
<span id="cb339-3"><a href="#cb339-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb339-4"><a href="#cb339-4" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>conv1 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">1</span>, 16L, 2L)</span>
<span id="cb339-5"><a href="#cb339-5" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>conv2 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(16L, 16L, <span class="dv">3</span>)</span>
<span id="cb339-6"><a href="#cb339-6" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>fc1 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(400L, 100L)</span>
<span id="cb339-7"><a href="#cb339-7" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>fc2 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(100L, 10L)</span>
<span id="cb339-8"><a href="#cb339-8" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb339-9"><a href="#cb339-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb339-10"><a href="#cb339-10" aria-hidden="true" tabindex="-1"></a>    x <span class="sc">%&gt;%</span> </span>
<span id="cb339-11"><a href="#cb339-11" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">conv1</span>() <span class="sc">%&gt;%</span></span>
<span id="cb339-12"><a href="#cb339-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb339-13"><a href="#cb339-13" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_max_pool2d</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span>         </span>
<span id="cb339-14"><a href="#cb339-14" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">conv2</span>() <span class="sc">%&gt;%</span></span>
<span id="cb339-15"><a href="#cb339-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb339-16"><a href="#cb339-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_max_pool2d</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb339-17"><a href="#cb339-17" aria-hidden="true" tabindex="-1"></a>      <span class="fu">torch_flatten</span>(<span class="at">start_dim =</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb339-18"><a href="#cb339-18" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">fc1</span>() <span class="sc">%&gt;%</span></span>
<span id="cb339-19"><a href="#cb339-19" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb339-20"><a href="#cb339-20" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">fc2</span>()</span>
<span id="cb339-21"><a href="#cb339-21" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb339-22"><a href="#cb339-22" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</details>
<p><br/></p>
<p>The rest is as usual: First we compile the model.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="#cb340-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb340-2"><a href="#cb340-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(</span>
<span id="cb340-3"><a href="#cb340-3" aria-hidden="true" tabindex="-1"></a> <span class="at">optimizer =</span> keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="fl">0.01</span>),</span>
<span id="cb340-4"><a href="#cb340-4" aria-hidden="true" tabindex="-1"></a> <span class="at">loss =</span> loss_categorical_crossentropy</span>
<span id="cb340-5"><a href="#cb340-5" aria-hidden="true" tabindex="-1"></a> )</span>
<span id="cb340-6"><a href="#cb340-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_9&quot;
## ___________________________________________________________________________________________________________________________
## Layer (type)                                           Output Shape                                     Param #            
## ===========================================================================================================================
## conv2d (Conv2D)                                        (None, 27, 27, 16)                               80                 
## ___________________________________________________________________________________________________________________________
## max_pooling2d (MaxPooling2D)                           (None, 13, 13, 16)                               0                  
## ___________________________________________________________________________________________________________________________
## conv2d_1 (Conv2D)                                      (None, 11, 11, 16)                               2320               
## ___________________________________________________________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)                         (None, 5, 5, 16)                                 0                  
## ___________________________________________________________________________________________________________________________
## flatten (Flatten)                                      (None, 400)                                      0                  
## ___________________________________________________________________________________________________________________________
## dense_30 (Dense)                                       (None, 100)                                      40100              
## ___________________________________________________________________________________________________________________________
## dense_31 (Dense)                                       (None, 10)                                       1010               
## ===========================================================================================================================
## Total params: 43,510
## Trainable params: 43,510
## Non-trainable params: 0
## ___________________________________________________________________________________________________________________________</code></pre>
<p>Then, we train the model:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="#cb342-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 5L</span>
<span id="cb342-2"><a href="#cb342-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 32L</span>
<span id="cb342-3"><a href="#cb342-3" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb342-4"><a href="#cb342-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(</span>
<span id="cb342-5"><a href="#cb342-5" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> train_x, </span>
<span id="cb342-6"><a href="#cb342-6" aria-hidden="true" tabindex="-1"></a> <span class="at">y =</span> train_y,</span>
<span id="cb342-7"><a href="#cb342-7" aria-hidden="true" tabindex="-1"></a> <span class="at">epochs =</span> epochs,</span>
<span id="cb342-8"><a href="#cb342-8" aria-hidden="true" tabindex="-1"></a> <span class="at">batch_size =</span> batch_size,</span>
<span id="cb342-9"><a href="#cb342-9" aria-hidden="true" tabindex="-1"></a> <span class="at">shuffle =</span> <span class="cn">TRUE</span>,</span>
<span id="cb342-10"><a href="#cb342-10" aria-hidden="true" tabindex="-1"></a> <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb342-11"><a href="#cb342-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Train model:</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="#cb343-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">net</span>()</span>
<span id="cb343-2"><a href="#cb343-2" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb343-3"><a href="#cb343-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb343-4"><a href="#cb343-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb343-5"><a href="#cb343-5" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb343-6"><a href="#cb343-6" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb343-7"><a href="#cb343-7" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb343-8"><a href="#cb343-8" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]])</span>
<span id="cb343-9"><a href="#cb343-9" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]], <span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb343-10"><a href="#cb343-10" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb343-11"><a href="#cb343-11" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb343-12"><a href="#cb343-12" aria-hidden="true" tabindex="-1"></a>    losses <span class="ot">=</span> <span class="fu">c</span>(losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb343-13"><a href="#cb343-13" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb343-14"><a href="#cb343-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, e, <span class="fu">mean</span>(losses)))</span>
<span id="cb343-15"><a href="#cb343-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 0.250743
## Loss at epoch 2: 0.146959
## Loss at epoch 3: 0.130747</code></pre>
<p>Evaluation:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="#cb345-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb345-2"><a href="#cb345-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-3"><a href="#cb345-3" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb345-4"><a href="#cb345-4" aria-hidden="true" tabindex="-1"></a>total <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb345-5"><a href="#cb345-5" aria-hidden="true" tabindex="-1"></a>correct <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb345-6"><a href="#cb345-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-7"><a href="#cb345-7" aria-hidden="true" tabindex="-1"></a>coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> test_dl) {</span>
<span id="cb345-8"><a href="#cb345-8" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">=</span> <span class="fu">model_torch</span>(b[[<span class="dv">1</span>]])</span>
<span id="cb345-9"><a href="#cb345-9" aria-hidden="true" tabindex="-1"></a>  labels <span class="ot">=</span> b[[<span class="dv">2</span>]]</span>
<span id="cb345-10"><a href="#cb345-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(output, labels)</span>
<span id="cb345-11"><a href="#cb345-11" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb345-12"><a href="#cb345-12" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">=</span> <span class="fu">torch_max</span>(output<span class="sc">$</span><span class="fu">data</span>(), <span class="at">dim =</span> <span class="dv">2</span>)[[<span class="dv">2</span>]]</span>
<span id="cb345-13"><a href="#cb345-13" aria-hidden="true" tabindex="-1"></a>  total <span class="ot">=</span> total <span class="sc">+</span> labels<span class="sc">$</span><span class="fu">size</span>(<span class="dv">1</span>)</span>
<span id="cb345-14"><a href="#cb345-14" aria-hidden="true" tabindex="-1"></a>  correct <span class="ot">=</span> correct <span class="sc">+</span> (predicted <span class="sc">==</span> labels)<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb345-15"><a href="#cb345-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb345-16"><a href="#cb345-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-17"><a href="#cb345-17" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(test_losses)</span></code></pre></div>
<pre><code>## [1] 0.1116432</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="#cb347-1" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="ot">&lt;-</span>  correct<span class="sc">/</span>total</span>
<span id="cb347-2"><a href="#cb347-2" aria-hidden="true" tabindex="-1"></a>test_accuracy</span></code></pre></div>
<pre><code>## [1] 0.9649</code></pre>
</details>
<p><br/></p>
</div>
<div id="advanced-training-techniques" class="section level2" number="4.4">
<h2 number="4.4"><span class="header-section-number">4.4</span> Advanced training techniques</h2>
<div id="data-augmentation" class="section level3" number="4.4.1">
<h3 number="4.4.1"><span class="header-section-number">4.4.1</span> Data Augmentation</h3>
<p>Having to train a CNN using very little data is a common problem. Data augmentation helps to artificially increase the number of images.</p>
<p>The idea is that a CNN learns specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures.</p>
<p>Luckily, it is very easy to use data augmentation in keras.</p>
<p>To show this, we will use again the MNIST dataset. We have to define a generator object (it is a specific object which infinitly draws samples from our dataset). In the generator we can turn on the data augementation. However, now we have to set the step size (steps_per_epoch) because the model does not know the first dimension of the image.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="#cb349-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> EcoData<span class="sc">::</span><span class="fu">dataset_flower</span>()</span>
<span id="cb349-2"><a href="#cb349-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data<span class="sc">$</span>train</span>
<span id="cb349-3"><a href="#cb349-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data<span class="sc">$</span>test</span>
<span id="cb349-4"><a href="#cb349-4" aria-hidden="true" tabindex="-1"></a>labels <span class="ot">=</span> data<span class="sc">$</span>labels</span>
<span id="cb349-5"><a href="#cb349-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-6"><a href="#cb349-6" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb349-7"><a href="#cb349-7" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb349-8"><a href="#cb349-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filter =</span> 16L, <span class="at">kernel_size =</span> <span class="fu">c</span>(5L, 5L), <span class="at">input_shape =</span> <span class="fu">c</span>(80L, 80L, 3L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb349-9"><a href="#cb349-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb349-10"><a href="#cb349-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filter =</span> 32L, <span class="at">kernel_size =</span> <span class="fu">c</span>(3L, 3L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb349-11"><a href="#cb349-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb349-12"><a href="#cb349-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filter =</span> 64L, <span class="at">kernel_size =</span> <span class="fu">c</span>(3L, 3L), <span class="at">strides =</span> <span class="fu">c</span>(2L, 2L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb349-13"><a href="#cb349-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb349-14"><a href="#cb349-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb349-15"><a href="#cb349-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb349-16"><a href="#cb349-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 5L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb349-17"><a href="#cb349-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-18"><a href="#cb349-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb349-19"><a href="#cb349-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Data augmentation</span></span>
<span id="cb349-20"><a href="#cb349-20" aria-hidden="true" tabindex="-1"></a>aug <span class="ot">=</span> <span class="fu">image_data_generator</span>(<span class="at">rotation_range =</span> <span class="dv">90</span>, </span>
<span id="cb349-21"><a href="#cb349-21" aria-hidden="true" tabindex="-1"></a>                           <span class="at">zoom_range =</span> <span class="fu">c</span>(<span class="fl">0.3</span>), </span>
<span id="cb349-22"><a href="#cb349-22" aria-hidden="true" tabindex="-1"></a>                           <span class="at">horizontal_flip =</span> <span class="cn">TRUE</span>, </span>
<span id="cb349-23"><a href="#cb349-23" aria-hidden="true" tabindex="-1"></a>                           <span class="at">vertical_flip =</span> <span class="cn">TRUE</span>)</span>
<span id="cb349-24"><a href="#cb349-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-25"><a href="#cb349-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preparation / splitting</span></span>
<span id="cb349-26"><a href="#cb349-26" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(train), <span class="fl">0.1</span><span class="sc">*</span><span class="fu">nrow</span>(train))</span>
<span id="cb349-27"><a href="#cb349-27" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">=</span> <span class="fu">flow_images_from_data</span>(train[<span class="sc">-</span>indices,,,]<span class="sc">/</span><span class="dv">255</span>, <span class="fu">k_one_hot</span>(labels[<span class="sc">-</span>indices], <span class="at">num_classes =</span> 5L),<span class="at">generator =</span> aug, <span class="at">batch_size =</span> 25L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb349-28"><a href="#cb349-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-29"><a href="#cb349-29" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> train[indices,,,]<span class="sc">/</span><span class="dv">255</span></span>
<span id="cb349-30"><a href="#cb349-30" aria-hidden="true" tabindex="-1"></a>test_labels <span class="ot">=</span> <span class="fu">k_one_hot</span>(labels[indices], <span class="at">num_classes =</span> 5L)</span>
<span id="cb349-31"><a href="#cb349-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-32"><a href="#cb349-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-33"><a href="#cb349-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Our own training loop with early stopping:</span></span>
<span id="cb349-34"><a href="#cb349-34" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 50L</span>
<span id="cb349-35"><a href="#cb349-35" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 25L</span>
<span id="cb349-36"><a href="#cb349-36" aria-hidden="true" tabindex="-1"></a>steps <span class="ot">=</span> <span class="fu">floor</span>(<span class="fu">dim</span>(train)[<span class="dv">1</span>]<span class="sc">/</span>batch_size)</span>
<span id="cb349-37"><a href="#cb349-37" aria-hidden="true" tabindex="-1"></a>optim <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">optimizer_rmsprop</span>()</span>
<span id="cb349-38"><a href="#cb349-38" aria-hidden="true" tabindex="-1"></a>max_patience <span class="ot">=</span> 5L</span>
<span id="cb349-39"><a href="#cb349-39" aria-hidden="true" tabindex="-1"></a>patience <span class="ot">=</span> 1L</span>
<span id="cb349-40"><a href="#cb349-40" aria-hidden="true" tabindex="-1"></a>min_val_loss <span class="ot">=</span> <span class="cn">Inf</span></span>
<span id="cb349-41"><a href="#cb349-41" aria-hidden="true" tabindex="-1"></a>val_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb349-42"><a href="#cb349-42" aria-hidden="true" tabindex="-1"></a>epoch_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb349-43"><a href="#cb349-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>epochs) {</span>
<span id="cb349-44"><a href="#cb349-44" aria-hidden="true" tabindex="-1"></a>  epoch_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb349-45"><a href="#cb349-45" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>steps) {</span>
<span id="cb349-46"><a href="#cb349-46" aria-hidden="true" tabindex="-1"></a>    batch <span class="ot">=</span> reticulate<span class="sc">::</span><span class="fu">iter_next</span>(generator)</span>
<span id="cb349-47"><a href="#cb349-47" aria-hidden="true" tabindex="-1"></a>    <span class="fu">with</span>(tf<span class="sc">$</span><span class="fu">GradientTape</span>() <span class="sc">%as%</span> tape, {</span>
<span id="cb349-48"><a href="#cb349-48" aria-hidden="true" tabindex="-1"></a>        pred <span class="ot">=</span> <span class="fu">model</span>(batch[[<span class="dv">1</span>]], <span class="at">training =</span> <span class="cn">TRUE</span>)</span>
<span id="cb349-49"><a href="#cb349-49" aria-hidden="true" tabindex="-1"></a>        loss <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">loss_categorical_crossentropy</span>(batch[[<span class="dv">2</span>]], pred)</span>
<span id="cb349-50"><a href="#cb349-50" aria-hidden="true" tabindex="-1"></a>        loss <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">reduce_mean</span>(loss)</span>
<span id="cb349-51"><a href="#cb349-51" aria-hidden="true" tabindex="-1"></a>      })</span>
<span id="cb349-52"><a href="#cb349-52" aria-hidden="true" tabindex="-1"></a>    gradients <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(<span class="at">target =</span> loss, <span class="at">sources =</span> model<span class="sc">$</span>trainable_variables)</span>
<span id="cb349-53"><a href="#cb349-53" aria-hidden="true" tabindex="-1"></a>    optim<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(gradients, model<span class="sc">$</span>trainable_variables)))</span>
<span id="cb349-54"><a href="#cb349-54" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="ot">=</span> <span class="fu">c</span>(epoch_loss, loss<span class="sc">$</span><span class="fu">numpy</span>())</span>
<span id="cb349-55"><a href="#cb349-55" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb349-56"><a href="#cb349-56" aria-hidden="true" tabindex="-1"></a>  epoch_losses <span class="ot">=</span> <span class="fu">c</span>(epoch_losses, epoch_loss)</span>
<span id="cb349-57"><a href="#cb349-57" aria-hidden="true" tabindex="-1"></a>  <span class="do">## test loss ##</span></span>
<span id="cb349-58"><a href="#cb349-58" aria-hidden="true" tabindex="-1"></a>  preds <span class="ot">=</span> model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(test)</span>
<span id="cb349-59"><a href="#cb349-59" aria-hidden="true" tabindex="-1"></a>  val_losses <span class="ot">=</span> <span class="fu">c</span>(val_losses, tf<span class="sc">$</span><span class="fu">reduce_mean</span>( keras<span class="sc">::</span><span class="fu">loss_categorical_crossentropy</span>(test_labels, preds) )<span class="sc">$</span><span class="fu">numpy</span>())</span>
<span id="cb349-60"><a href="#cb349-60" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb349-61"><a href="#cb349-61" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, e, <span class="st">&quot; Train Loss: &quot;</span>, <span class="fu">mean</span>(epoch_losses),<span class="st">&quot; Val Loss: &quot;</span>, val_losses[e],  <span class="st">&quot; </span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb349-62"><a href="#cb349-62" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb349-63"><a href="#cb349-63" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(val_losses[e] <span class="sc">&lt;</span> min_val_loss) {</span>
<span id="cb349-64"><a href="#cb349-64" aria-hidden="true" tabindex="-1"></a>    min_val_loss <span class="ot">=</span> val_losses[e]</span>
<span id="cb349-65"><a href="#cb349-65" aria-hidden="true" tabindex="-1"></a>    patience <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb349-66"><a href="#cb349-66" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> { patience <span class="ot">=</span> patience<span class="sc">+</span><span class="dv">1</span> }</span>
<span id="cb349-67"><a href="#cb349-67" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(patience <span class="sc">==</span> max_patience) <span class="cf">break</span></span>
<span id="cb349-68"><a href="#cb349-68" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb349-69"><a href="#cb349-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-70"><a href="#cb349-70" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(model, data<span class="sc">$</span>test<span class="sc">/</span><span class="dv">255</span>)</span>
<span id="cb349-71"><a href="#cb349-71" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">apply</span>(preds, <span class="dv">1</span>, which.max)<span class="sc">-</span><span class="dv">1</span></span></code></pre></div>
<p>So using data augmentation we can artificially increase the number of images.</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>In torch, we have to change the transform function (but only for the train dataloader):</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="#cb350-1" aria-hidden="true" tabindex="-1"></a>train_transforms <span class="ot">&lt;-</span> <span class="cf">function</span>(img) {</span>
<span id="cb350-2"><a href="#cb350-2" aria-hidden="true" tabindex="-1"></a>  img <span class="sc">%&gt;%</span></span>
<span id="cb350-3"><a href="#cb350-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_to_tensor</span>() <span class="sc">%&gt;%</span></span>
<span id="cb350-4"><a href="#cb350-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_random_horizontal_flip</span>(<span class="at">p =</span> <span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb350-5"><a href="#cb350-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_random_resized_crop</span>(<span class="at">size =</span> <span class="fu">c</span>(28L, 28L)) <span class="sc">%&gt;%</span></span>
<span id="cb350-6"><a href="#cb350-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_random_vertical_flip</span>(<span class="fl">0.3</span>)</span>
<span id="cb350-7"><a href="#cb350-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb350-8"><a href="#cb350-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb350-9"><a href="#cb350-9" aria-hidden="true" tabindex="-1"></a>train_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">TRUE</span>, <span class="at">transform =</span> train_transforms)</span>
<span id="cb350-10"><a href="#cb350-10" aria-hidden="true" tabindex="-1"></a>test_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">FALSE</span>,<span class="at">transform =</span> transform_to_tensor)</span>
<span id="cb350-11"><a href="#cb350-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb350-12"><a href="#cb350-12" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(train_ds, <span class="at">batch_size =</span> 100L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb350-13"><a href="#cb350-13" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> 100L)</span>
<span id="cb350-14"><a href="#cb350-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb350-15"><a href="#cb350-15" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">net</span>()</span>
<span id="cb350-16"><a href="#cb350-16" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb350-17"><a href="#cb350-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb350-18"><a href="#cb350-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb350-19"><a href="#cb350-19" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb350-20"><a href="#cb350-20" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb350-21"><a href="#cb350-21" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb350-22"><a href="#cb350-22" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]])</span>
<span id="cb350-23"><a href="#cb350-23" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]], <span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb350-24"><a href="#cb350-24" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb350-25"><a href="#cb350-25" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb350-26"><a href="#cb350-26" aria-hidden="true" tabindex="-1"></a>    losses <span class="ot">=</span> <span class="fu">c</span>(losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb350-27"><a href="#cb350-27" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb350-28"><a href="#cb350-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, e, <span class="fu">mean</span>(losses)))</span>
<span id="cb350-29"><a href="#cb350-29" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 1.440276</code></pre>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="#cb352-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb352-2"><a href="#cb352-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-3"><a href="#cb352-3" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb352-4"><a href="#cb352-4" aria-hidden="true" tabindex="-1"></a>total <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb352-5"><a href="#cb352-5" aria-hidden="true" tabindex="-1"></a>correct <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb352-6"><a href="#cb352-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-7"><a href="#cb352-7" aria-hidden="true" tabindex="-1"></a>coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> test_dl) {</span>
<span id="cb352-8"><a href="#cb352-8" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">=</span> <span class="fu">model_torch</span>(b[[<span class="dv">1</span>]])</span>
<span id="cb352-9"><a href="#cb352-9" aria-hidden="true" tabindex="-1"></a>  labels <span class="ot">=</span> b[[<span class="dv">2</span>]]</span>
<span id="cb352-10"><a href="#cb352-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(output, labels)</span>
<span id="cb352-11"><a href="#cb352-11" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb352-12"><a href="#cb352-12" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">=</span> <span class="fu">torch_max</span>(output<span class="sc">$</span><span class="fu">data</span>(), <span class="at">dim =</span> <span class="dv">2</span>)[[<span class="dv">2</span>]]</span>
<span id="cb352-13"><a href="#cb352-13" aria-hidden="true" tabindex="-1"></a>  total <span class="ot">=</span> total <span class="sc">+</span> labels<span class="sc">$</span><span class="fu">size</span>(<span class="dv">1</span>)</span>
<span id="cb352-14"><a href="#cb352-14" aria-hidden="true" tabindex="-1"></a>  correct <span class="ot">=</span> correct <span class="sc">+</span> (predicted <span class="sc">==</span> labels)<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb352-15"><a href="#cb352-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb352-16"><a href="#cb352-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-17"><a href="#cb352-17" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="ot">&lt;-</span>  correct<span class="sc">/</span>total</span>
<span id="cb352-18"><a href="#cb352-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(test_accuracy)</span></code></pre></div>
<pre><code>## [1] 0.9084</code></pre>
</details>
<p><br/></p>
</div>
<div id="transfer" class="section level3" number="4.4.2">
<h3 number="4.4.2"><span class="header-section-number">4.4.2</span> Transfer learning</h3>
<p>Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning.</p>
<p>The main idea of transfer learning is that all the convolutional layers have mainly one task - learning to identify highly correlated neighbored features and therefore these learn structures such as edges in the image and only the top layer, the dense layer is the actual classifier of the CNN. Thus, one could think that we could only train the top layer as classifier. To do so, it will be confronted by sets of different edges/structures and has to decide the label based on these.</p>
<p>Again, this sounds very complicating but is again quite easy with keras:</p>
<p>We will do this now with the CIFAR10 data set, so we have to prepare the data:</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="#cb354-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">dataset_cifar10</span>()</span>
<span id="cb354-2"><a href="#cb354-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data<span class="sc">$</span>train</span>
<span id="cb354-3"><a href="#cb354-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data<span class="sc">$</span>test</span>
<span id="cb354-4"><a href="#cb354-4" aria-hidden="true" tabindex="-1"></a>image <span class="ot">=</span> train<span class="sc">$</span>x[<span class="dv">5</span>,,,]</span>
<span id="cb354-5"><a href="#cb354-5" aria-hidden="true" tabindex="-1"></a>image <span class="sc">%&gt;%</span> </span>
<span id="cb354-6"><a href="#cb354-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">image_to_array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb354-7"><a href="#cb354-7" aria-hidden="true" tabindex="-1"></a> <span class="st">`</span><span class="at">/</span><span class="st">`</span>(., <span class="dv">255</span>) <span class="sc">%&gt;%</span></span>
<span id="cb354-8"><a href="#cb354-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">as.raster</span>() <span class="sc">%&gt;%</span></span>
<span id="cb354-9"><a href="#cb354-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-166-1.png" width="672" /></p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="#cb355-1" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>(train<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train<span class="sc">$</span>x)))</span>
<span id="cb355-2"><a href="#cb355-2" aria-hidden="true" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">array</span>(test<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(test<span class="sc">$</span>x)))</span>
<span id="cb355-3"><a href="#cb355-3" aria-hidden="true" tabindex="-1"></a>train_y <span class="ot">=</span> <span class="fu">to_categorical</span>(train<span class="sc">$</span>y, <span class="dv">10</span>)</span>
<span id="cb355-4"><a href="#cb355-4" aria-hidden="true" tabindex="-1"></a>test_y <span class="ot">=</span> <span class="fu">to_categorical</span>(test<span class="sc">$</span>y, <span class="dv">10</span>)</span></code></pre></div>
<p>Keras provides download functions for all famous architectures/CNN models which are already trained on the imagenet dataset (another famous dataset). These trained networks come already without their top layer, so we have to set include_top to false and change the input shape.</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="#cb356-1" aria-hidden="true" tabindex="-1"></a>densenet <span class="ot">=</span> <span class="fu">application_densenet201</span>(<span class="at">include_top =</span> <span class="cn">FALSE</span>, <span class="at">input_shape  =</span> <span class="fu">c</span>(32L, 32L, 3L))</span></code></pre></div>
<p>Now, we will use not a sequential model but just a “keras_model” where we can specify the inputs and outputs. Thereby, the outputs are our own top layer, but the inputs are the densenet inputs, as these are already pre-trained.</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="#cb357-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">keras_model</span>(<span class="at">inputs =</span> densenet<span class="sc">$</span>input, <span class="at">outputs =</span> </span>
<span id="cb357-2"><a href="#cb357-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_flatten</span>(<span class="fu">layer_dense</span>(densenet<span class="sc">$</span>output, <span class="at">units =</span> 10L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb357-3"><a href="#cb357-3" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<p>In the next step we want to freeze all layers except for our own last layer (with freezing I mean that these are not trained: we do not want to train the complete model, we only want to train the last layer). You can check the number of trainable weights via summary(model)</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="#cb358-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">freeze_weights</span>(<span class="at">to =</span> <span class="fu">length</span>(model<span class="sc">$</span>layers)<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb358-2"><a href="#cb358-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<p>And then the usual training:</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="#cb359-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb359-2"><a href="#cb359-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, <span class="at">optimizer =</span> <span class="fu">optimizer_adamax</span>())</span>
<span id="cb359-3"><a href="#cb359-3" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb359-4"><a href="#cb359-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(</span>
<span id="cb359-5"><a href="#cb359-5" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> train_x, </span>
<span id="cb359-6"><a href="#cb359-6" aria-hidden="true" tabindex="-1"></a> <span class="at">y =</span> train_y,</span>
<span id="cb359-7"><a href="#cb359-7" aria-hidden="true" tabindex="-1"></a> <span class="at">epochs =</span> 1L,</span>
<span id="cb359-8"><a href="#cb359-8" aria-hidden="true" tabindex="-1"></a> <span class="at">batch_size =</span> 32L,</span>
<span id="cb359-9"><a href="#cb359-9" aria-hidden="true" tabindex="-1"></a> <span class="at">shuffle =</span> T,</span>
<span id="cb359-10"><a href="#cb359-10" aria-hidden="true" tabindex="-1"></a> <span class="at">validation_split =</span> <span class="fl">0.2</span>,</span>
<span id="cb359-11"><a href="#cb359-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We have seen, that transfer-learning can easily be done using keras.</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>In torch, we have to change the transform function (but only for the train dataloader):</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="#cb360-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torchvision)</span>
<span id="cb360-2"><a href="#cb360-2" aria-hidden="true" tabindex="-1"></a>train_ds <span class="ot">=</span> <span class="fu">cifar10_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">TRUE</span>, <span class="at">transform =</span> transform_to_tensor)</span>
<span id="cb360-3"><a href="#cb360-3" aria-hidden="true" tabindex="-1"></a>test_ds <span class="ot">=</span> <span class="fu">cifar10_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">FALSE</span>,<span class="at">transform =</span> transform_to_tensor)</span>
<span id="cb360-4"><a href="#cb360-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-5"><a href="#cb360-5" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(train_ds, <span class="at">batch_size =</span> 100L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb360-6"><a href="#cb360-6" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> 100L)</span>
<span id="cb360-7"><a href="#cb360-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-8"><a href="#cb360-8" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">model_resnet18</span>(<span class="at">pretrained =</span> <span class="cn">TRUE</span>)</span>
<span id="cb360-9"><a href="#cb360-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-10"><a href="#cb360-10" aria-hidden="true" tabindex="-1"></a><span class="co"># we will set all model parameters to constant values:</span></span>
<span id="cb360-11"><a href="#cb360-11" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span>parameters <span class="sc">%&gt;%</span> purrr<span class="sc">::</span><span class="fu">walk</span>(<span class="cf">function</span>(param) param<span class="sc">$</span><span class="fu">requires_grad_</span>(<span class="cn">FALSE</span>))</span>
<span id="cb360-12"><a href="#cb360-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-13"><a href="#cb360-13" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s replace the last layer (last layer is named &#39;fc&#39;) with our own layer:</span></span>
<span id="cb360-14"><a href="#cb360-14" aria-hidden="true" tabindex="-1"></a>inFeat <span class="ot">=</span> model_torch<span class="sc">$</span>fc<span class="sc">$</span>in_features</span>
<span id="cb360-15"><a href="#cb360-15" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span>fc <span class="ot">=</span> <span class="fu">nn_linear</span>(inFeat, <span class="at">out_features =</span> 10L)</span>
<span id="cb360-16"><a href="#cb360-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-17"><a href="#cb360-17" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb360-18"><a href="#cb360-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-19"><a href="#cb360-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb360-20"><a href="#cb360-20" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb360-21"><a href="#cb360-21" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb360-22"><a href="#cb360-22" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb360-23"><a href="#cb360-23" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]])</span>
<span id="cb360-24"><a href="#cb360-24" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]], <span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb360-25"><a href="#cb360-25" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb360-26"><a href="#cb360-26" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb360-27"><a href="#cb360-27" aria-hidden="true" tabindex="-1"></a>    losses <span class="ot">=</span> <span class="fu">c</span>(losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb360-28"><a href="#cb360-28" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb360-29"><a href="#cb360-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, e, <span class="fu">mean</span>(losses)))</span>
<span id="cb360-30"><a href="#cb360-30" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 2.019492</code></pre>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="#cb362-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb362-2"><a href="#cb362-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-3"><a href="#cb362-3" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb362-4"><a href="#cb362-4" aria-hidden="true" tabindex="-1"></a>total <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb362-5"><a href="#cb362-5" aria-hidden="true" tabindex="-1"></a>correct <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb362-6"><a href="#cb362-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-7"><a href="#cb362-7" aria-hidden="true" tabindex="-1"></a>coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> test_dl) {</span>
<span id="cb362-8"><a href="#cb362-8" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">=</span> <span class="fu">model_torch</span>(b[[<span class="dv">1</span>]])</span>
<span id="cb362-9"><a href="#cb362-9" aria-hidden="true" tabindex="-1"></a>  labels <span class="ot">=</span> b[[<span class="dv">2</span>]]</span>
<span id="cb362-10"><a href="#cb362-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(output, labels)</span>
<span id="cb362-11"><a href="#cb362-11" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb362-12"><a href="#cb362-12" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">=</span> <span class="fu">torch_max</span>(output<span class="sc">$</span><span class="fu">data</span>(), <span class="at">dim =</span> <span class="dv">2</span>)[[<span class="dv">2</span>]]</span>
<span id="cb362-13"><a href="#cb362-13" aria-hidden="true" tabindex="-1"></a>  total <span class="ot">=</span> total <span class="sc">+</span> labels<span class="sc">$</span><span class="fu">size</span>(<span class="dv">1</span>)</span>
<span id="cb362-14"><a href="#cb362-14" aria-hidden="true" tabindex="-1"></a>  correct <span class="ot">=</span> correct <span class="sc">+</span> (predicted <span class="sc">==</span> labels)<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb362-15"><a href="#cb362-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb362-16"><a href="#cb362-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-17"><a href="#cb362-17" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="ot">&lt;-</span>  correct<span class="sc">/</span>total</span>
<span id="cb362-18"><a href="#cb362-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(test_accuracy)</span></code></pre></div>
<pre><code>## [1] 0.3893</code></pre>
</details>
<p><br/></p>
<p><strong>Flower dataset</strong></p>
<p>Let’s do it with our flower dataset:</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="#cb364-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> EcoData<span class="sc">::</span><span class="fu">dataset_flower</span>()</span>
<span id="cb364-2"><a href="#cb364-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data<span class="sc">$</span>train</span>
<span id="cb364-3"><a href="#cb364-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data<span class="sc">$</span>test</span>
<span id="cb364-4"><a href="#cb364-4" aria-hidden="true" tabindex="-1"></a>labels <span class="ot">=</span> data<span class="sc">$</span>labels</span>
<span id="cb364-5"><a href="#cb364-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb364-6"><a href="#cb364-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-7"><a href="#cb364-7" aria-hidden="true" tabindex="-1"></a>densenet <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">application_densenet201</span>(<span class="at">include_top =</span> <span class="cn">FALSE</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(80L, 80L, 3L))</span>
<span id="cb364-8"><a href="#cb364-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-9"><a href="#cb364-9" aria-hidden="true" tabindex="-1"></a>keras<span class="sc">::</span><span class="fu">freeze_weights</span>(inception)</span>
<span id="cb364-10"><a href="#cb364-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-11"><a href="#cb364-11" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model</span>(<span class="at">inputs =</span> densenet<span class="sc">$</span>input, </span>
<span id="cb364-12"><a href="#cb364-12" aria-hidden="true" tabindex="-1"></a>                    <span class="at">outputs =</span> densenet<span class="sc">$</span>output <span class="sc">%&gt;%</span> </span>
<span id="cb364-13"><a href="#cb364-13" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb364-14"><a href="#cb364-14" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">layer_dropout</span>(<span class="fl">0.2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb364-15"><a href="#cb364-15" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">layer_dense</span>(<span class="at">units =</span> 200L) <span class="sc">%&gt;%</span> </span>
<span id="cb364-16"><a href="#cb364-16" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">layer_dropout</span>(<span class="fl">0.2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb364-17"><a href="#cb364-17" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">layer_dense</span>(<span class="at">units =</span> 5L, <span class="at">activation=</span><span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb364-18"><a href="#cb364-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-19"><a href="#cb364-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-20"><a href="#cb364-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Data augmentation</span></span>
<span id="cb364-21"><a href="#cb364-21" aria-hidden="true" tabindex="-1"></a>aug <span class="ot">=</span> <span class="fu">image_data_generator</span>(<span class="at">rotation_range =</span> <span class="dv">180</span>,<span class="at">zoom_range =</span> <span class="fl">0.4</span>,<span class="at">width_shift_range =</span> <span class="fl">0.2</span>, <span class="at">height_shift_range =</span> <span class="fl">0.2</span>, <span class="at">vertical_flip =</span> <span class="cn">TRUE</span>, <span class="at">horizontal_flip =</span> <span class="cn">TRUE</span>,<span class="at">preprocessing_function =</span> imagenet_preprocess_input)</span>
<span id="cb364-22"><a href="#cb364-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-23"><a href="#cb364-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Data preparation / splitting</span></span>
<span id="cb364-24"><a href="#cb364-24" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(train), <span class="fl">0.1</span><span class="sc">*</span><span class="fu">nrow</span>(train))</span>
<span id="cb364-25"><a href="#cb364-25" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">=</span> <span class="fu">flow_images_from_data</span>(train[<span class="sc">-</span>indices,,,], <span class="fu">k_one_hot</span>(labels[<span class="sc">-</span>indices], <span class="at">num_classes =</span> 5L), <span class="at">batch_size =</span> 25L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb364-26"><a href="#cb364-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-27"><a href="#cb364-27" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> <span class="fu">imagenet_preprocess_input</span>(train[indices,,,])</span>
<span id="cb364-28"><a href="#cb364-28" aria-hidden="true" tabindex="-1"></a>test_labels <span class="ot">=</span> <span class="fu">k_one_hot</span>(labels[indices], <span class="at">num_classes =</span> 5L)</span>
<span id="cb364-29"><a href="#cb364-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-30"><a href="#cb364-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Our own training loop with early stopping:</span></span>
<span id="cb364-31"><a href="#cb364-31" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 1L</span>
<span id="cb364-32"><a href="#cb364-32" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 45L</span>
<span id="cb364-33"><a href="#cb364-33" aria-hidden="true" tabindex="-1"></a>steps <span class="ot">=</span> <span class="fu">floor</span>(<span class="fu">dim</span>(train)[<span class="dv">1</span>]<span class="sc">/</span>batch_size)</span>
<span id="cb364-34"><a href="#cb364-34" aria-hidden="true" tabindex="-1"></a>optim <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">optimizer_rmsprop</span>(<span class="at">lr =</span> <span class="fl">0.0005</span>)</span>
<span id="cb364-35"><a href="#cb364-35" aria-hidden="true" tabindex="-1"></a>max_patience <span class="ot">=</span> 10L</span>
<span id="cb364-36"><a href="#cb364-36" aria-hidden="true" tabindex="-1"></a>patience <span class="ot">=</span> 1L</span>
<span id="cb364-37"><a href="#cb364-37" aria-hidden="true" tabindex="-1"></a>min_val_loss <span class="ot">=</span> <span class="cn">Inf</span></span>
<span id="cb364-38"><a href="#cb364-38" aria-hidden="true" tabindex="-1"></a>val_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb364-39"><a href="#cb364-39" aria-hidden="true" tabindex="-1"></a>epoch_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb364-40"><a href="#cb364-40" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>epochs) {</span>
<span id="cb364-41"><a href="#cb364-41" aria-hidden="true" tabindex="-1"></a>  epoch_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb364-42"><a href="#cb364-42" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>steps) {</span>
<span id="cb364-43"><a href="#cb364-43" aria-hidden="true" tabindex="-1"></a>    batch <span class="ot">=</span> reticulate<span class="sc">::</span><span class="fu">iter_next</span>(generator)</span>
<span id="cb364-44"><a href="#cb364-44" aria-hidden="true" tabindex="-1"></a>    <span class="fu">with</span>(tf<span class="sc">$</span><span class="fu">GradientTape</span>() <span class="sc">%as%</span> tape, {</span>
<span id="cb364-45"><a href="#cb364-45" aria-hidden="true" tabindex="-1"></a>        pred <span class="ot">=</span> <span class="fu">model</span>(batch[[<span class="dv">1</span>]], <span class="at">training =</span> <span class="cn">TRUE</span>)</span>
<span id="cb364-46"><a href="#cb364-46" aria-hidden="true" tabindex="-1"></a>        loss <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">loss_categorical_crossentropy</span>(batch[[<span class="dv">2</span>]], pred)</span>
<span id="cb364-47"><a href="#cb364-47" aria-hidden="true" tabindex="-1"></a>        loss <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">reduce_mean</span>(loss)</span>
<span id="cb364-48"><a href="#cb364-48" aria-hidden="true" tabindex="-1"></a>      })</span>
<span id="cb364-49"><a href="#cb364-49" aria-hidden="true" tabindex="-1"></a>    gradients <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(<span class="at">target =</span> loss, <span class="at">sources =</span> model<span class="sc">$</span>trainable_variables)</span>
<span id="cb364-50"><a href="#cb364-50" aria-hidden="true" tabindex="-1"></a>    optim<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(gradients, model<span class="sc">$</span>trainable_variables)))</span>
<span id="cb364-51"><a href="#cb364-51" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="ot">=</span> <span class="fu">c</span>(epoch_loss, loss<span class="sc">$</span><span class="fu">numpy</span>())</span>
<span id="cb364-52"><a href="#cb364-52" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb364-53"><a href="#cb364-53" aria-hidden="true" tabindex="-1"></a>  epoch_losses <span class="ot">=</span> <span class="fu">c</span>(epoch_losses, epoch_loss)</span>
<span id="cb364-54"><a href="#cb364-54" aria-hidden="true" tabindex="-1"></a>  <span class="do">## test loss ##</span></span>
<span id="cb364-55"><a href="#cb364-55" aria-hidden="true" tabindex="-1"></a>  preds <span class="ot">=</span> model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(test)</span>
<span id="cb364-56"><a href="#cb364-56" aria-hidden="true" tabindex="-1"></a>  val_losses <span class="ot">=</span> <span class="fu">c</span>(val_losses, tf<span class="sc">$</span><span class="fu">reduce_mean</span>( keras<span class="sc">::</span><span class="fu">loss_categorical_crossentropy</span>(test_labels, preds) )<span class="sc">$</span><span class="fu">numpy</span>())</span>
<span id="cb364-57"><a href="#cb364-57" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb364-58"><a href="#cb364-58" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, e, <span class="st">&quot; Train Loss: &quot;</span>, <span class="fu">mean</span>(epoch_losses),<span class="st">&quot; Val Loss: &quot;</span>, val_losses[e],  <span class="st">&quot; </span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb364-59"><a href="#cb364-59" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb364-60"><a href="#cb364-60" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(val_losses[e] <span class="sc">&lt;</span> min_val_loss) {</span>
<span id="cb364-61"><a href="#cb364-61" aria-hidden="true" tabindex="-1"></a>    min_val_loss <span class="ot">=</span> val_losses[e]</span>
<span id="cb364-62"><a href="#cb364-62" aria-hidden="true" tabindex="-1"></a>    patience <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb364-63"><a href="#cb364-63" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> { patience <span class="ot">=</span> patience<span class="sc">+</span><span class="dv">1</span> }</span>
<span id="cb364-64"><a href="#cb364-64" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(patience <span class="sc">==</span> max_patience) <span class="cf">break</span></span>
<span id="cb364-65"><a href="#cb364-65" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb364-66"><a href="#cb364-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-67"><a href="#cb364-67" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(model, <span class="fu">imagenet_preprocess_input</span>(data<span class="sc">$</span>test))</span></code></pre></div>
<!--chapter:end:03-Deep.Rmd-->
</div>
</div>
</div>
<div id="interpretation-and-causality-with-machine-learning" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> Interpretation and causality with machine learning</h1>
<div id="explainable-ai" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> Explainable AI</h2>
<p>The goal of explainable AI (xAI, aka interpretable machine learning) is to explain WHY a fitted ML models makes certain predictions. A typical example is to understand how important different variables are for predictions. There incentives to do so range from a better technical understanding of the models over understanding which data is important to improve predictions to questions of fairness and discrimination (e.g. to understand if an algorithm uses skin color to make a decision).</p>
<div id="a-practical-example" class="section level3" number="5.1.1">
<h3 number="5.1.1"><span class="header-section-number">5.1.1</span> A practical example</h3>
<p>In this lecture we will work with another famous dataset, the Boston housing dataset:</p>
<p>We will fit a random forest and use the iml pkg for xAI, see <img src="https://christophm.github.io/interpretable-ml-book/" /></p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="#cb365-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb365-2"><a href="#cb365-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;iml&quot;</span>)</span>
<span id="cb365-3"><a href="#cb365-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb365-4"><a href="#cb365-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Boston&quot;</span>, <span class="at">package =</span> <span class="st">&quot;MASS&quot;</span>)</span>
<span id="cb365-5"><a href="#cb365-5" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">=</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston, <span class="at">ntree =</span> <span class="dv">50</span>)</span></code></pre></div>
<p>xAI packages are written generic, i.e. they can handle almost all ML models.
When we want to use them, we first have to create a Predictor object, that holds the model and the data. The iml package uses R6 classes, that means new objects can be created by calling Predictor$new(). (do not worry if you do not know what R6 classes are, just use the command)</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="#cb366-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> Boston[<span class="fu">which</span>(<span class="fu">names</span>(Boston) <span class="sc">!=</span> <span class="st">&quot;medv&quot;</span>)]</span>
<span id="cb366-2"><a href="#cb366-2" aria-hidden="true" tabindex="-1"></a>predictor <span class="ot">=</span> Predictor<span class="sc">$</span><span class="fu">new</span>(rf, <span class="at">data =</span> X, <span class="at">y =</span> Boston<span class="sc">$</span>medv)</span></code></pre></div>
</div>
<div id="feature-importance" class="section level3" number="5.1.2">
<h3 number="5.1.2"><span class="header-section-number">5.1.2</span> Feature Importance</h3>
<p>Feature importance, should not be mistaken with the RF variable importance. It tells us how important the individual variables are for predictions and can be calculated for all ML models and is based on a permutation approach (have a look at the book):</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="#cb367-1" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">=</span> FeatureImp<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">loss =</span> <span class="st">&quot;mae&quot;</span>)</span>
<span id="cb367-2"><a href="#cb367-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(imp)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-175-1.png" width="672" /></p>
</div>
<div id="partial-dependencies" class="section level3" number="5.1.3">
<h3 number="5.1.3"><span class="header-section-number">5.1.3</span> Partial dependencies</h3>
<p>Partial dependencies are similar to allEffects plots for normal regressions, the idea is to visualize “marginal effects” of predictors (with the feature argument we specify the variable we want to visualize):</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="#cb368-1" aria-hidden="true" tabindex="-1"></a>eff <span class="ot">=</span> FeatureEffect<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">feature =</span> <span class="st">&quot;rm&quot;</span>, <span class="at">method =</span> <span class="st">&quot;pdp&quot;</span>, <span class="at">grid.size =</span> <span class="dv">30</span>)</span>
<span id="cb368-2"><a href="#cb368-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(eff)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-176-1.png" width="672" /></p>
<p>Partial dependencies can be also plotted for single observations:</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="#cb369-1" aria-hidden="true" tabindex="-1"></a>eff <span class="ot">=</span> FeatureEffect<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">feature =</span> <span class="st">&quot;rm&quot;</span>, <span class="at">method =</span> <span class="st">&quot;pdp+ice&quot;</span>, <span class="at">grid.size =</span> <span class="dv">30</span>)</span>
<span id="cb369-2"><a href="#cb369-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(eff)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-177-1.png" width="672" /></p>
<p>One disadvantage of partial dependencies is that they are sensitive to correlated predictors. Accumulated local effects can be used to account for correlation for predictors</p>
</div>
<div id="accumulated-local-effects" class="section level3" number="5.1.4">
<h3 number="5.1.4"><span class="header-section-number">5.1.4</span> Accumulated local effects</h3>
<p>Accumulated local effects (ALE) are basically partial dependencies plots but try to correct for correlations between predictors</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="#cb370-1" aria-hidden="true" tabindex="-1"></a>ale <span class="ot">=</span> FeatureEffect<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">feature =</span> <span class="st">&quot;rm&quot;</span>, <span class="at">method =</span> <span class="st">&quot;ale&quot;</span>)</span>
<span id="cb370-2"><a href="#cb370-2" aria-hidden="true" tabindex="-1"></a>ale<span class="sc">$</span><span class="fu">plot</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-178-1.png" width="672" /></p>
<p>If there is no colinearity, you shouldn’t see much difference between partial dependencies and ALE plots.</p>
</div>
<div id="friedmans-h-statistic" class="section level3" number="5.1.5">
<h3 number="5.1.5"><span class="header-section-number">5.1.5</span> Friedmans H-statistic</h3>
<p>The H-statistic can be used to find interactions between predictors. However, again, keep in mind that the H-statistic is sensible to correlation between predictors:</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="#cb371-1" aria-hidden="true" tabindex="-1"></a>interact <span class="ot">=</span> Interaction<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="st">&quot;lstat&quot;</span>)</span>
<span id="cb371-2"><a href="#cb371-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(interact)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-179-1.png" width="672" /></p>
</div>
<div id="global-explainer---simplifying-the-ml-model" class="section level3" number="5.1.6">
<h3 number="5.1.6"><span class="header-section-number">5.1.6</span> Global explainer - Simplifying the ML model</h3>
<p>Another idea is to simplify the ML model with another simpler model such as a decision tree. We create predictions with the ML model for a lot of different input values and then we fit on these predictions a decision tree, which we can then interpret.</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="#cb372-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(partykit)</span>
<span id="cb372-2"><a href="#cb372-2" aria-hidden="true" tabindex="-1"></a>tree <span class="ot">=</span> TreeSurrogate<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">maxdepth =</span> <span class="dv">2</span>)</span>
<span id="cb372-3"><a href="#cb372-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tree)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-180-1.png" width="672" /></p>
</div>
<div id="local-explainer---lime-explaining-single-instances-observations" class="section level3" number="5.1.7">
<h3 number="5.1.7"><span class="header-section-number">5.1.7</span> Local explainer - LIME explaining single instances (observations)</h3>
<p>The global approach is to simplify the entire ML-black-box model via a simpler model, which is then interpretable.</p>
<p>However, sometimes we are only interested in understanding how single observations/predictions are generated. The lime approach explores the feature space around one observations and based on this local spare fits then a simpler model (e.g. a linear model):</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="#cb373-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb373-2"><a href="#cb373-2" aria-hidden="true" tabindex="-1"></a>lime.explain <span class="ot">=</span> LocalModel<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">x.interest =</span> X[<span class="dv">1</span>,])</span>
<span id="cb373-3"><a href="#cb373-3" aria-hidden="true" tabindex="-1"></a>lime.explain<span class="sc">$</span>results</span></code></pre></div>
<pre><code>##               beta x.recoded    effect x.original feature feature.value
## rm       4.1893817     6.575 27.545185      6.575      rm      rm=6.575
## ptratio -0.5307031    15.300 -8.119758       15.3 ptratio  ptratio=15.3
## lstat   -0.4398104     4.980 -2.190256       4.98   lstat    lstat=4.98</code></pre>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lime.explain)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-181-1.png" width="672" /></p>
</div>
<div id="local-explainer---shapley" class="section level3" number="5.1.8">
<h3 number="5.1.8"><span class="header-section-number">5.1.8</span> Local explainer - Shapley</h3>
<p>The Shapley method computes the so called Shapley value, feature contributions for single predictions, and is based on an approach from cooperative game theory. The idea is that each feature value of the instance is a “player” in a game, where the prediction is the reward. The Shapley value tells us how to fairly distribute the award among the feature.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="#cb376-1" aria-hidden="true" tabindex="-1"></a>shapley <span class="ot">=</span> Shapley<span class="sc">$</span><span class="fu">new</span>(predictor, <span class="at">x.interest =</span> X[<span class="dv">1</span>,])</span>
<span id="cb376-2"><a href="#cb376-2" aria-hidden="true" tabindex="-1"></a>shapley<span class="sc">$</span><span class="fu">plot</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-182-1.png" width="672" /></p>
</div>
</div>
<div id="causal-inference-and-machine-learning" class="section level2" number="5.2">
<h2 number="5.2"><span class="header-section-number">5.2</span> Causal inference and machine learning</h2>
<p>xAI aims at explaining how predictions are being made. In general, xAI != causality. xAI methods measure which variables are used by the algorithm for predictions, or how much variables improve predictions. The important point to note here: if a variable causes something, we could also expect that it helps to predict the very thing. The opposite, however, is not generally true - it is very often possible that a variable that doesn’t cause something can predict something.</p>
<p>In statistical courses (in particular course: advanced biostatistics), we discuss the issue of causality at length. Here, we don’t want to go into the details, but again, you should in general resist to interpret indicators of importance in xAI as causal effects. They tell you something about what’s going on in the algorithm, not about what’s going on in reality.</p>
<div id="causal-inference-on-static-data" class="section level3" number="5.2.1">
<h3 number="5.2.1"><span class="header-section-number">5.2.1</span> Causal inference on static data</h3>
<p>Methods for causal inference depend on whether we have dynamic or static data. The latter is the more common case. With static data, the problem is confounding - if you have several predictors that are correlated, you can get spurious correlations between a given predictor and the response.</p>
<p>A multiple regression, and a few other methods are able to correct for other predictors, and thus isolate the causal effect. The same is not necessarily true for ML algorithms and xAI methods. This is not a bug, but a feature - for making good predictions, it is often no problem, but rather an advantage to also use non-causal predictors.</p>
<p>Here an example for the variable importance indicators in the RF algorithm. The purpose of this script is to show that RF variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are separable and would be correctly separated by an lm / ANOVA</p>
<p>We first simulate a dataset with 2 predictors that are strongly correlated, but only one of them has an effect on the response.</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="#cb377-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulation parameters</span></span>
<span id="cb377-2"><a href="#cb377-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb377-3"><a href="#cb377-3" aria-hidden="true" tabindex="-1"></a>col <span class="ot">=</span> <span class="fl">0.7</span></span>
<span id="cb377-4"><a href="#cb377-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create collinear predictors</span></span>
<span id="cb377-5"><a href="#cb377-5" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">runif</span>(n)</span>
<span id="cb377-6"><a href="#cb377-6" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> col <span class="sc">*</span> x1 <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>col) <span class="sc">*</span> <span class="fu">runif</span>(n)</span>
<span id="cb377-7"><a href="#cb377-7" aria-hidden="true" tabindex="-1"></a><span class="co"># response is only influenced by x1</span></span>
<span id="cb377-8"><a href="#cb377-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x1 <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span></code></pre></div>
<p>lm / anova correctly identify x1 as causal variable</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="#cb378-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2))</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##            Df Sum Sq Mean Sq  F value Pr(&gt;F)    
## x1          1 106.30 106.300 110.1988 &lt;2e-16 ***
## x2          1   0.23   0.228   0.2368 0.6267    
## Residuals 997 961.73   0.965                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Fit RF and show variable importance</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="#cb380-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">importance=</span><span class="cn">TRUE</span>)</span>
<span id="cb380-2"><a href="#cb380-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-185-1.png" width="672" />
Variable importance is now split nearly evenly.</p>
<p>Task: understand why this is - remember:</p>
<ul>
<li>How the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built</li>
<li>Remember that as x1 ~ x2, we can use x2 as a replacement for x1</li>
<li>Remember that the variable importance measures the average contributions of the different variables in the trees of the forest</li>
</ul>
</div>
<div id="structural-equation-models" class="section level3" number="5.2.2">
<h3 number="5.2.2"><span class="header-section-number">5.2.2</span> Structural equation models</h3>
<p>If causal relationships get more complicated, it will not be possible to adjust correctly with a simple lm. In this case, in statistics, we will usually use structural equation models (SEMs). SEMs are are designed to estimate entire causal diagrams. There are two main SEM packages in R: for anything that is non-normal, you will currently have to estimate the DAG piece-wise with CRAN package piecewiseSEM. Example for a vegetation dataset:</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="#cb381-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(piecewiseSEM)</span>
<span id="cb381-2"><a href="#cb381-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">=</span> <span class="fu">psem</span>(</span>
<span id="cb381-3"><a href="#cb381-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">lm</span>(rich <span class="sc">~</span> distance <span class="sc">+</span> elev <span class="sc">+</span> abiotic <span class="sc">+</span> age <span class="sc">+</span> hetero <span class="sc">+</span> firesev <span class="sc">+</span> cover, <span class="at">data =</span> keeley),</span>
<span id="cb381-4"><a href="#cb381-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">lm</span>(firesev <span class="sc">~</span> elev <span class="sc">+</span> age <span class="sc">+</span> cover, <span class="at">data =</span> keeley),</span>
<span id="cb381-5"><a href="#cb381-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">lm</span>(cover <span class="sc">~</span> age <span class="sc">+</span> elev <span class="sc">+</span> hetero <span class="sc">+</span> abiotic, <span class="at">data =</span> keeley)</span>
<span id="cb381-6"><a href="#cb381-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb381-7"><a href="#cb381-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span>
<span id="cb381-8"><a href="#cb381-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod)</span></code></pre></div>
<p>For linear SEMs, we can estimate the entire DAG in one go. This also allows to have unobserved variables in the DAG. One of the most popular packages for this is lavaan</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="#cb382-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lavaan)</span>
<span id="cb382-2"><a href="#cb382-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb382-3"><a href="#cb382-3" aria-hidden="true" tabindex="-1"></a><span class="st"> rich ~ distance + elev + abiotic + age + hetero + firesev + cover</span></span>
<span id="cb382-4"><a href="#cb382-4" aria-hidden="true" tabindex="-1"></a><span class="st"> firesev ~ elev + age + cover</span></span>
<span id="cb382-5"><a href="#cb382-5" aria-hidden="true" tabindex="-1"></a><span class="st"> cover ~ age + elev + abiotic</span></span>
<span id="cb382-6"><a href="#cb382-6" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb382-7"><a href="#cb382-7" aria-hidden="true" tabindex="-1"></a>fit<span class="ot">&lt;-</span><span class="fu">sem</span>(mod,<span class="at">data=</span>keeley)</span>
<span id="cb382-8"><a href="#cb382-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<p>Plot options … not so nice as before</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="#cb383-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lavaanPlot)</span>
<span id="cb383-2"><a href="#cb383-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lavaanPlot</span>(<span class="at">model =</span> fit)</span></code></pre></div>
<p>Another plotting option</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(semPlot)</span>
<span id="cb384-2"><a href="#cb384-2" aria-hidden="true" tabindex="-1"></a><span class="fu">semPaths</span>(fit)</span></code></pre></div>
</div>
<div id="automatic-causal-discovery" class="section level3" number="5.2.3">
<h3 number="5.2.3"><span class="header-section-number">5.2.3</span> Automatic causal discovery</h3>
<p>But how to we get the causal graph? In statistics, it common to “guess” it and afterwards do residual checks, in the same way as we guess the structure of a regression. For more complicated problems, however, this is unsatisfying. Some groups therefore work on so-called causal discovery algorithsm, i.e. algorithms that automatically generate causal graphs from data. One of the most classic algorithms of this sort is the PC algorithm. Here an example using the pcalg package:</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="#cb385-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bioconductor dependencies have to installed by hand, e.g. </span></span>
<span id="cb385-2"><a href="#cb385-2" aria-hidden="true" tabindex="-1"></a><span class="co"># BiocManager::install(c(&quot;Rgraphviz&quot;, &quot;graph&quot;, &quot;RBGL&quot;)</span></span>
<span id="cb385-3"><a href="#cb385-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pcalg)</span></code></pre></div>
<p>Loading the data</p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="#cb386-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;gmG&quot;</span>, <span class="at">package =</span> <span class="st">&quot;pcalg&quot;</span>) <span class="do">## loads data sets gmG and gmG8</span></span>
<span id="cb386-2"><a href="#cb386-2" aria-hidden="true" tabindex="-1"></a>suffStat <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">C =</span> <span class="fu">cor</span>(gmG8<span class="sc">$</span>x), <span class="at">n =</span> <span class="fu">nrow</span>(gmG8<span class="sc">$</span>x))</span>
<span id="cb386-3"><a href="#cb386-3" aria-hidden="true" tabindex="-1"></a>varNames <span class="ot">&lt;-</span> gmG8<span class="sc">$</span>g<span class="sc">@</span>nodes</span></code></pre></div>
<p>First, the kkeleton algorithm creates a basic graph without connections</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="#cb387-1" aria-hidden="true" tabindex="-1"></a>skel.gmG8 <span class="ot">&lt;-</span> <span class="fu">skeleton</span>(suffStat, <span class="at">indepTest =</span> gaussCItest,</span>
<span id="cb387-2"><a href="#cb387-2" aria-hidden="true" tabindex="-1"></a><span class="at">labels =</span> varNames, <span class="at">alpha =</span> <span class="fl">0.01</span>)</span>
<span id="cb387-3"><a href="#cb387-3" aria-hidden="true" tabindex="-1"></a>Rgraphviz<span class="sc">::</span><span class="fu">plot</span>(skel.gmG8)</span></code></pre></div>
<p>What is missing here is the direction of the errors. The PC algorith now makes tests for conditional independence, which allows fixing a part (but typically not all) of the directions of the causal arrows.</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="#cb388-1" aria-hidden="true" tabindex="-1"></a>pc.gmG8 <span class="ot">&lt;-</span> <span class="fu">pc</span>(suffStat, <span class="at">indepTest =</span> gaussCItest,</span>
<span id="cb388-2"><a href="#cb388-2" aria-hidden="true" tabindex="-1"></a><span class="at">labels =</span> varNames, <span class="at">alpha =</span> <span class="fl">0.01</span>)</span>
<span id="cb388-3"><a href="#cb388-3" aria-hidden="true" tabindex="-1"></a>Rgraphviz<span class="sc">::</span><span class="fu">plot</span>(pc.gmG8 )</span></code></pre></div>
</div>
<div id="causal-inference-on-dynamic-data" class="section level3" number="5.2.4">
<h3 number="5.2.4"><span class="header-section-number">5.2.4</span> Causal inference on dynamic data</h3>
<p>When working with dynamic data, we can use an additional piece of information - the effect usually preceeds the cause, which means that we can test for a time-lag between cause and effect to determine the direction of causality. This way of testing for causality is known as Granger causality, or Granger methods. Here an example:</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="#cb389-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span></code></pre></div>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre><code>## 
## Attaching package: &#39;lmtest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:crayon&#39;:
## 
##     reset</code></pre>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="#cb395-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Which came first: the chicken or the egg?</span></span>
<span id="cb395-2"><a href="#cb395-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(ChickEgg)</span>
<span id="cb395-3"><a href="#cb395-3" aria-hidden="true" tabindex="-1"></a><span class="fu">grangertest</span>(egg <span class="sc">~</span> chicken, <span class="at">order =</span> <span class="dv">3</span>, <span class="at">data =</span> ChickEgg)</span></code></pre></div>
<pre><code>## Granger causality test
## 
## Model 1: egg ~ Lags(egg, 1:3) + Lags(chicken, 1:3)
## Model 2: egg ~ Lags(egg, 1:3)
##   Res.Df Df      F Pr(&gt;F)
## 1     44                 
## 2     47 -3 0.5916 0.6238</code></pre>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="#cb397-1" aria-hidden="true" tabindex="-1"></a><span class="fu">grangertest</span>(chicken <span class="sc">~</span> egg, <span class="at">order =</span> <span class="dv">3</span>, <span class="at">data =</span> ChickEgg)</span></code></pre></div>
<pre><code>## Granger causality test
## 
## Model 1: chicken ~ Lags(chicken, 1:3) + Lags(egg, 1:3)
## Model 2: chicken ~ Lags(chicken, 1:3)
##   Res.Df Df     F   Pr(&gt;F)   
## 1     44                     
## 2     47 -3 5.405 0.002966 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="outlook-for-machine-learning" class="section level3" number="5.2.5">
<h3 number="5.2.5"><span class="header-section-number">5.2.5</span> Outlook for machine learning</h3>
<p>As we have seen, there are already a few methods / algorithms to discover causality from large data, but the systematic transfer of these concepts to machine learning, in particular deep learning, is still at its infancy. At the moment, this field is actively researched and changes extremely fast, so we recommend to use google to see what is currently going on. Particular, in business and industry, there is a large interest in learning about causal effect from large datasets. In our opinion, a great topic for young scientists to specialize on.</p>
<!--chapter:end:04-xAI.Rmd-->
</div>
</div>
</div>
<div id="gans-vaes-and-reinforcement-learning" class="section level1" number="6">
<h1 number="6"><span class="header-section-number">6</span> GANs, VAEs, and Reinforcement learning</h1>
<div id="generative-adversarial-network-gans" class="section level2" number="6.1">
<h2 number="6.1"><span class="header-section-number">6.1</span> Generative adversarial network (GANs)</h2>
<p>The idea of generative adversarial network (GAN) is that two neural networks contest with each other in a game. On network is creating data and is trying to “trick” the other into thinking that this data is real. A possible application is to create pictures that look like real photographs. However, the application of GANs today is much wider than just the creation of data. For example, GANs can also be used to “augment” data, i.e. to create new data and thereby improve the fitted model.</p>
<div id="mnist---gan-based-on-dnns" class="section level3" number="6.1.1">
<h3 number="6.1.1"><span class="header-section-number">6.1.1</span> MNIST - GAN based on DNNs</h3>
<p>GANs - two networks are playing against each other. The generator (similar to the decoder in AEs) creates new images from noise and tries to convince the discriminator that this is a real image.</p>
<p>The discriminator is getting a mix of true images (from the dataset) and of artificially generated images from the generator.</p>
<p>Loss of the generator - when fakes are identified as fakes by the discriminator (simple binary_crossentropy loss, 0/1…)</p>
<p>Loss of the discriminator - when fakes are identified as fakes (class 1) and true images as true images (class 0), again simple binary crossentropy.</p>
<p>MNIST example:</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="#cb399-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb399-2"><a href="#cb399-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb399-3"><a href="#cb399-3" aria-hidden="true" tabindex="-1"></a>rotate <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, rev))</span>
<span id="cb399-4"><a href="#cb399-4" aria-hidden="true" tabindex="-1"></a>imgPlot <span class="ot">=</span> <span class="cf">function</span>(img, <span class="at">title =</span> <span class="st">&quot;&quot;</span>){</span>
<span id="cb399-5"><a href="#cb399-5" aria-hidden="true" tabindex="-1"></a> col<span class="ot">=</span><span class="fu">grey.colors</span>(<span class="dv">255</span>)</span>
<span id="cb399-6"><a href="#cb399-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">image</span>(<span class="fu">rotate</span>(img), <span class="at">col =</span> col, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">axes=</span><span class="cn">FALSE</span>, <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Label: &quot;</span>, <span class="fu">as.character</span>(title)))</span>
<span id="cb399-7"><a href="#cb399-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We don’t need the test set:</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="#cb400-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb400-2"><a href="#cb400-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data<span class="sc">$</span>train</span>
<span id="cb400-3"><a href="#cb400-3" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>((train<span class="sc">$</span>x<span class="fl">-127.5</span>)<span class="sc">/</span><span class="fl">127.5</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train<span class="sc">$</span>x)[<span class="dv">1</span>], 784L))</span></code></pre></div>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="#cb401-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 32L</span>
<span id="cb401-2"><a href="#cb401-2" aria-hidden="true" tabindex="-1"></a>get_batch <span class="ot">=</span> <span class="cf">function</span>(){  <span class="co"># Helper function to get batches of images</span></span>
<span id="cb401-3"><a href="#cb401-3" aria-hidden="true" tabindex="-1"></a> indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(train_x), batch_size)</span>
<span id="cb401-4"><a href="#cb401-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">return</span>(tf<span class="sc">$</span><span class="fu">constant</span>(train_x[indices,], <span class="st">&quot;float32&quot;</span>))</span>
<span id="cb401-5"><a href="#cb401-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb401-6"><a href="#cb401-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-7"><a href="#cb401-7" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">=</span> tf<span class="sc">$</span>data<span class="sc">$</span>Dataset<span class="sc">$</span><span class="fu">from_tensor_slices</span>(tf<span class="sc">$</span><span class="fu">constant</span>(train_x, <span class="st">&quot;float32&quot;</span>))</span>
<span id="cb401-8"><a href="#cb401-8" aria-hidden="true" tabindex="-1"></a>dataset<span class="sc">$</span><span class="fu">batch</span>(batch_size)</span></code></pre></div>
<pre><code>## &lt;BatchDataset shapes: (None, 784), types: tf.float32&gt;</code></pre>
<p>Define and test generator model:</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="#cb403-1" aria-hidden="true" tabindex="-1"></a>get_generator <span class="ot">=</span> <span class="cf">function</span>(){</span>
<span id="cb403-2"><a href="#cb403-2" aria-hidden="true" tabindex="-1"></a> generator <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb403-3"><a href="#cb403-3" aria-hidden="true" tabindex="-1"></a> generator <span class="sc">%&gt;%</span> </span>
<span id="cb403-4"><a href="#cb403-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 200L ,<span class="at">input_shape =</span> <span class="fu">c</span>(100L)) <span class="sc">%&gt;%</span> </span>
<span id="cb403-5"><a href="#cb403-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb403-6"><a href="#cb403-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 200L) <span class="sc">%&gt;%</span> </span>
<span id="cb403-7"><a href="#cb403-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb403-8"><a href="#cb403-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 784L, <span class="at">activation =</span> <span class="st">&quot;tanh&quot;</span>)</span>
<span id="cb403-9"><a href="#cb403-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">return</span>(generator)</span>
<span id="cb403-10"><a href="#cb403-10" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="#cb404-1" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">=</span> <span class="fu">get_generator</span>()</span>
<span id="cb404-2"><a href="#cb404-2" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(1L, 100L))</span>
<span id="cb404-3"><a href="#cb404-3" aria-hidden="true" tabindex="-1"></a><span class="fu">imgPlot</span>(<span class="fu">array</span>(<span class="fu">generator</span>(sample)<span class="sc">$</span><span class="fu">numpy</span>(), <span class="fu">c</span>(28L, 28L)))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-199-1.png" width="672" /></p>
<p>The noise of size = [100] (random vector with 100 values) is passed through the network and the output correspond to the number of pixels of one MNIST image (784)</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="#cb405-1" aria-hidden="true" tabindex="-1"></a>get_discriminator <span class="ot">=</span> <span class="cf">function</span>(){</span>
<span id="cb405-2"><a href="#cb405-2" aria-hidden="true" tabindex="-1"></a> discriminator <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb405-3"><a href="#cb405-3" aria-hidden="true" tabindex="-1"></a> discriminator <span class="sc">%&gt;%</span> </span>
<span id="cb405-4"><a href="#cb405-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 200L, <span class="at">input_shape =</span> <span class="fu">c</span>(784L)) <span class="sc">%&gt;%</span> </span>
<span id="cb405-5"><a href="#cb405-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb405-6"><a href="#cb405-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L) <span class="sc">%&gt;%</span> </span>
<span id="cb405-7"><a href="#cb405-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb405-8"><a href="#cb405-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb405-9"><a href="#cb405-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">return</span>(discriminator)</span>
<span id="cb405-10"><a href="#cb405-10" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="#cb406-1" aria-hidden="true" tabindex="-1"></a>discriminator <span class="ot">=</span> <span class="fu">get_discriminator</span>()</span>
<span id="cb406-2"><a href="#cb406-2" aria-hidden="true" tabindex="-1"></a><span class="fu">discriminator</span>(<span class="fu">generator</span>(tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(1L, 100L))))</span></code></pre></div>
<pre><code>## tf.Tensor([[0.57696146]], shape=(1, 1), dtype=float32)</code></pre>
<p>The normal architecture of a binary classifier (will get images as input)</p>
<p>Loss:</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="#cb408-1" aria-hidden="true" tabindex="-1"></a>ce <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>losses<span class="sc">$</span><span class="fu">BinaryCrossentropy</span>(<span class="at">from_logits =</span> <span class="cn">TRUE</span>)</span>
<span id="cb408-2"><a href="#cb408-2" aria-hidden="true" tabindex="-1"></a>loss_discriminator <span class="ot">=</span> <span class="cf">function</span>(real, fake){</span>
<span id="cb408-3"><a href="#cb408-3" aria-hidden="true" tabindex="-1"></a> real_loss <span class="ot">=</span> <span class="fu">ce</span>(tf<span class="sc">$</span><span class="fu">ones_like</span>(real), real)</span>
<span id="cb408-4"><a href="#cb408-4" aria-hidden="true" tabindex="-1"></a> fake_loss <span class="ot">=</span> <span class="fu">ce</span>(tf<span class="sc">$</span><span class="fu">zeros_like</span>(fake), fake)</span>
<span id="cb408-5"><a href="#cb408-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">return</span>(real_loss<span class="sc">+</span>fake_loss)</span>
<span id="cb408-6"><a href="#cb408-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb408-7"><a href="#cb408-7" aria-hidden="true" tabindex="-1"></a>loss_generator <span class="ot">=</span> <span class="cf">function</span>(fake){</span>
<span id="cb408-8"><a href="#cb408-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">return</span>(<span class="fu">ce</span>(tf<span class="sc">$</span><span class="fu">ones_like</span>(fake), fake))</span>
<span id="cb408-9"><a href="#cb408-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Binary crossentropy as loss function.</p>
<p>However, we have to encode the true and predicted values for the two networks individually.</p>
<p>The discriminator will get two losses - one for identifying fake images as fake, and one for identifying real MNIST images as real images.</p>
<p>The generator will just get one loss - was it able to deceive the discriminator?</p>
<p>Each network will get its own optimizer (while a AE will be treated as one network, in a GAN the networks will be treated independently)</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="#cb409-1" aria-hidden="true" tabindex="-1"></a>gen_opt <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>optimizers<span class="sc">$</span><span class="fu">RMSprop</span>(<span class="fl">1e-4</span>)</span>
<span id="cb409-2"><a href="#cb409-2" aria-hidden="true" tabindex="-1"></a>disc_opt <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>optimizers<span class="sc">$</span><span class="fu">RMSprop</span>(<span class="fl">1e-4</span>)</span></code></pre></div>
<p>We have to write here our own training loop (we cannot use the fit function). Let’s define a training function:</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="#cb410-1" aria-hidden="true" tabindex="-1"></a>train_step <span class="ot">=</span> <span class="cf">function</span>(images){</span>
<span id="cb410-2"><a href="#cb410-2" aria-hidden="true" tabindex="-1"></a> noise <span class="ot">=</span> tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(32L, 100L))</span>
<span id="cb410-3"><a href="#cb410-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">with</span>(tf<span class="sc">$</span><span class="fu">GradientTape</span>(<span class="at">persistent =</span> <span class="cn">TRUE</span>) <span class="sc">%as%</span> tape,{</span>
<span id="cb410-4"><a href="#cb410-4" aria-hidden="true" tabindex="-1"></a>   gen_images <span class="ot">=</span> <span class="fu">generator</span>(noise)</span>
<span id="cb410-5"><a href="#cb410-5" aria-hidden="true" tabindex="-1"></a>   fake_output <span class="ot">=</span> <span class="fu">discriminator</span>(gen_images)</span>
<span id="cb410-6"><a href="#cb410-6" aria-hidden="true" tabindex="-1"></a>   real_output <span class="ot">=</span> <span class="fu">discriminator</span>(images)</span>
<span id="cb410-7"><a href="#cb410-7" aria-hidden="true" tabindex="-1"></a>   gen_loss <span class="ot">=</span> <span class="fu">loss_generator</span>(fake_output)</span>
<span id="cb410-8"><a href="#cb410-8" aria-hidden="true" tabindex="-1"></a>   disc_loss <span class="ot">=</span> <span class="fu">loss_discriminator</span>(real_output, fake_output)</span>
<span id="cb410-9"><a href="#cb410-9" aria-hidden="true" tabindex="-1"></a> })</span>
<span id="cb410-10"><a href="#cb410-10" aria-hidden="true" tabindex="-1"></a> gen_grads <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(gen_loss, generator<span class="sc">$</span>weights)</span>
<span id="cb410-11"><a href="#cb410-11" aria-hidden="true" tabindex="-1"></a> disc_grads <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(disc_loss, discriminator<span class="sc">$</span>weights)</span>
<span id="cb410-12"><a href="#cb410-12" aria-hidden="true" tabindex="-1"></a> <span class="fu">rm</span>(tape)</span>
<span id="cb410-13"><a href="#cb410-13" aria-hidden="true" tabindex="-1"></a> gen_opt<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(gen_grads, generator<span class="sc">$</span>weights)))</span>
<span id="cb410-14"><a href="#cb410-14" aria-hidden="true" tabindex="-1"></a> disc_opt<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(disc_grads, discriminator<span class="sc">$</span>weights)))</span>
<span id="cb410-15"><a href="#cb410-15" aria-hidden="true" tabindex="-1"></a> <span class="fu">return</span>(<span class="fu">c</span>(gen_loss, disc_loss))</span>
<span id="cb410-16"><a href="#cb410-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb410-17"><a href="#cb410-17" aria-hidden="true" tabindex="-1"></a>train_step <span class="ot">=</span> tf<span class="sc">$</span><span class="st">`</span><span class="at">function</span><span class="st">`</span>(reticulate<span class="sc">::</span><span class="fu">py_func</span>(train_step))</span></code></pre></div>
<p>In each iteration (for each batch) we will do the following (the GradientTape records computations to do automatic differenation):</p>
<ol style="list-style-type: decimal">
<li>sample noise</li>
<li>Generator creates images from the noise</li>
<li>Discriminator will make predictions for fake images and real images (response is a probability between [0,1])</li>
<li>Calculate loss for generator</li>
<li>Calculate loss for discriminator</li>
<li>Calculate gradients for weights and the loss</li>
<li>Update weights of generator</li>
<li>Update weights of discriminator</li>
<li>return losses</li>
</ol>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="#cb411-1" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">=</span> <span class="fu">get_generator</span>()</span>
<span id="cb411-2"><a href="#cb411-2" aria-hidden="true" tabindex="-1"></a>discriminator <span class="ot">=</span> <span class="fu">get_discriminator</span>()</span>
<span id="cb411-3"><a href="#cb411-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 30L</span>
<span id="cb411-4"><a href="#cb411-4" aria-hidden="true" tabindex="-1"></a>steps <span class="ot">=</span> <span class="fu">as.integer</span>(<span class="fu">nrow</span>(train_x)<span class="sc">/</span>batch_size)</span>
<span id="cb411-5"><a href="#cb411-5" aria-hidden="true" tabindex="-1"></a>counter <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb411-6"><a href="#cb411-6" aria-hidden="true" tabindex="-1"></a>gen_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb411-7"><a href="#cb411-7" aria-hidden="true" tabindex="-1"></a>disc_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb411-8"><a href="#cb411-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb411-9"><a href="#cb411-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>epochs) {</span>
<span id="cb411-10"><a href="#cb411-10" aria-hidden="true" tabindex="-1"></a>  dat <span class="ot">=</span> reticulate<span class="sc">::</span><span class="fu">as_iterator</span>(dataset<span class="sc">$</span><span class="fu">batch</span>(batch_size))</span>
<span id="cb411-11"><a href="#cb411-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb411-12"><a href="#cb411-12" aria-hidden="true" tabindex="-1"></a>   coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (images <span class="cf">in</span> dat) {</span>
<span id="cb411-13"><a href="#cb411-13" aria-hidden="true" tabindex="-1"></a>      losses <span class="ot">=</span> <span class="fu">train_step</span>(images)</span>
<span id="cb411-14"><a href="#cb411-14" aria-hidden="true" tabindex="-1"></a>      gen_loss <span class="ot">=</span> <span class="fu">c</span>(gen_loss, tf<span class="sc">$</span><span class="fu">reduce_sum</span>(losses[[<span class="dv">1</span>]])<span class="sc">$</span><span class="fu">numpy</span>())</span>
<span id="cb411-15"><a href="#cb411-15" aria-hidden="true" tabindex="-1"></a>      disc_loss <span class="ot">=</span> <span class="fu">c</span>(disc_loss, tf<span class="sc">$</span><span class="fu">reduce_sum</span>(losses[[<span class="dv">2</span>]])<span class="sc">$</span><span class="fu">numpy</span>())</span>
<span id="cb411-16"><a href="#cb411-16" aria-hidden="true" tabindex="-1"></a>   })</span>
<span id="cb411-17"><a href="#cb411-17" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb411-18"><a href="#cb411-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Gen: &quot;</span>, <span class="fu">mean</span>(gen_loss), <span class="st">&quot; Disc: &quot;</span>, <span class="fu">mean</span>(disc_loss), <span class="st">&quot; </span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb411-19"><a href="#cb411-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(e <span class="sc">%%</span> <span class="dv">5</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb411-20"><a href="#cb411-20" aria-hidden="true" tabindex="-1"></a>    noise <span class="ot">=</span> tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(1L, 100L))</span>
<span id="cb411-21"><a href="#cb411-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">imgPlot</span>(<span class="fu">array</span>(<span class="fu">generator</span>(noise)<span class="sc">$</span><span class="fu">numpy</span>(), <span class="fu">c</span>(28L, 28L)), <span class="st">&quot;Gen&quot;</span>)</span>
<span id="cb411-22"><a href="#cb411-22" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb411-23"><a href="#cb411-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb411-24"><a href="#cb411-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb411-25"><a href="#cb411-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb411-26"><a href="#cb411-26" aria-hidden="true" tabindex="-1"></a><span class="co"># for(i in 1:(epochs*steps)){</span></span>
<span id="cb411-27"><a href="#cb411-27" aria-hidden="true" tabindex="-1"></a><span class="co">#  images = get_batch()</span></span>
<span id="cb411-28"><a href="#cb411-28" aria-hidden="true" tabindex="-1"></a><span class="co">#  losses = train_step(images)</span></span>
<span id="cb411-29"><a href="#cb411-29" aria-hidden="true" tabindex="-1"></a><span class="co">#  gen_loss = tf$reduce_sum(losses[[1]])$numpy()</span></span>
<span id="cb411-30"><a href="#cb411-30" aria-hidden="true" tabindex="-1"></a><span class="co">#  disc_loss = tf$reduce_sum(losses[[2]])$numpy()</span></span>
<span id="cb411-31"><a href="#cb411-31" aria-hidden="true" tabindex="-1"></a><span class="co">#  if(i %% 50*steps == 0) {</span></span>
<span id="cb411-32"><a href="#cb411-32" aria-hidden="true" tabindex="-1"></a><span class="co">#  noise = tf$random$normal(c(1L, 100L))</span></span>
<span id="cb411-33"><a href="#cb411-33" aria-hidden="true" tabindex="-1"></a><span class="co">#  imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), &quot;Gen&quot;)</span></span>
<span id="cb411-34"><a href="#cb411-34" aria-hidden="true" tabindex="-1"></a><span class="co">#  }</span></span>
<span id="cb411-35"><a href="#cb411-35" aria-hidden="true" tabindex="-1"></a><span class="co">#  if(i %% steps == 0){</span></span>
<span id="cb411-36"><a href="#cb411-36" aria-hidden="true" tabindex="-1"></a><span class="co">#  counter = 1</span></span>
<span id="cb411-37"><a href="#cb411-37" aria-hidden="true" tabindex="-1"></a><span class="co">#  cat(&quot;Gen: &quot;, mean(gen_loss), &quot; Disc: &quot;, mean(disc_loss), &quot; \n&quot;)</span></span>
<span id="cb411-38"><a href="#cb411-38" aria-hidden="true" tabindex="-1"></a><span class="co">#  }</span></span>
<span id="cb411-39"><a href="#cb411-39" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span></code></pre></div>
<p>The actual training loop:</p>
<ol style="list-style-type: decimal">
<li>Create networks</li>
<li>get batch of images</li>
<li>run train_step function</li>
<li>print losses</li>
<li>repeat step 2-4 for number of epochs</li>
</ol>
</div>
<div id="flower---gan" class="section level3" number="6.1.2">
<h3 number="6.1.2"><span class="header-section-number">6.1.2</span> Flower - GAN</h3>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="#cb412-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb412-2"><a href="#cb412-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb412-3"><a href="#cb412-3" aria-hidden="true" tabindex="-1"></a>data_files <span class="ot">=</span> <span class="fu">list.files</span>(<span class="st">&quot;flowers/&quot;</span>, <span class="at">full.names =</span> <span class="cn">TRUE</span>)</span>
<span id="cb412-4"><a href="#cb412-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data_files[<span class="fu">str_detect</span>(data_files, <span class="st">&quot;train&quot;</span>)]</span>
<span id="cb412-5"><a href="#cb412-5" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> <span class="fu">readRDS</span>(<span class="at">file =</span> <span class="st">&quot;test.RDS&quot;</span>)</span>
<span id="cb412-6"><a href="#cb412-6" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> <span class="fu">lapply</span>(train, readRDS)</span>
<span id="cb412-7"><a href="#cb412-7" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> abind<span class="sc">::</span><span class="fu">abind</span>(train, <span class="at">along =</span> 1L)</span>
<span id="cb412-8"><a href="#cb412-8" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">concat</span>(<span class="fu">list</span>(train, test), <span class="at">axis =</span> 0L)<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb412-9"><a href="#cb412-9" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>((train<span class="fl">-127.5</span>)<span class="sc">/</span><span class="fl">127.5</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train)))</span>
<span id="cb412-10"><a href="#cb412-10" aria-hidden="true" tabindex="-1"></a>get_generator <span class="ot">=</span> <span class="cf">function</span>(){</span>
<span id="cb412-11"><a href="#cb412-11" aria-hidden="true" tabindex="-1"></a>  generator <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb412-12"><a href="#cb412-12" aria-hidden="true" tabindex="-1"></a>  generator <span class="sc">%&gt;%</span> </span>
<span id="cb412-13"><a href="#cb412-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L<span class="sc">*</span>20L<span class="sc">*</span>128L, <span class="at">input_shape =</span> <span class="fu">c</span>(100L), <span class="at">use_bias =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-14"><a href="#cb412-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-15"><a href="#cb412-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_reshape</span>(<span class="fu">c</span>(20L, 20L, 128L)) <span class="sc">%&gt;%</span> </span>
<span id="cb412-16"><a href="#cb412-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dropout</span>(<span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-17"><a href="#cb412-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d_transpose</span>(<span class="at">filters =</span> 256L, <span class="at">kernel_size =</span> <span class="fu">c</span>(3L, 3L), <span class="at">padding =</span> <span class="st">&quot;same&quot;</span>, <span class="at">strides =</span> <span class="fu">c</span>(1L, 1L), <span class="at">use_bias =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-18"><a href="#cb412-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-19"><a href="#cb412-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dropout</span>(<span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-20"><a href="#cb412-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d_transpose</span>(<span class="at">filters =</span> 128L, <span class="at">kernel_size =</span> <span class="fu">c</span>(5L, 5L), <span class="at">padding =</span> <span class="st">&quot;same&quot;</span>, <span class="at">strides =</span> <span class="fu">c</span>(1L, 1L), <span class="at">use_bias =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-21"><a href="#cb412-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-22"><a href="#cb412-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dropout</span>(<span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-23"><a href="#cb412-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d_transpose</span>(<span class="at">filters =</span> 64L, <span class="at">kernel_size =</span> <span class="fu">c</span>(5L, 5L), <span class="at">padding =</span> <span class="st">&quot;same&quot;</span>, <span class="at">strides =</span> <span class="fu">c</span>(2L, 2L), <span class="at">use_bias =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb412-24"><a href="#cb412-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-25"><a href="#cb412-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dropout</span>(<span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-26"><a href="#cb412-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d_transpose</span>(<span class="at">filters =</span>3L, <span class="at">kernel_size =</span> <span class="fu">c</span>(5L, 5L), <span class="at">padding =</span> <span class="st">&quot;same&quot;</span>, <span class="at">strides =</span> <span class="fu">c</span>(2L, 2L), <span class="at">activation =</span> <span class="st">&quot;tanh&quot;</span>, <span class="at">use_bias =</span> <span class="cn">FALSE</span>)</span>
<span id="cb412-27"><a href="#cb412-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(generator)</span>
<span id="cb412-28"><a href="#cb412-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-29"><a href="#cb412-29" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">=</span> <span class="fu">get_generator</span>()</span>
<span id="cb412-30"><a href="#cb412-30" aria-hidden="true" tabindex="-1"></a>image <span class="ot">=</span> <span class="fu">generator</span>(tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(1L,100L)))<span class="sc">$</span><span class="fu">numpy</span>()[<span class="dv">1</span>,,,]</span>
<span id="cb412-31"><a href="#cb412-31" aria-hidden="true" tabindex="-1"></a>image <span class="ot">=</span> scales<span class="sc">::</span><span class="fu">rescale</span>(image, <span class="at">to =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">255</span>))</span>
<span id="cb412-32"><a href="#cb412-32" aria-hidden="true" tabindex="-1"></a>image <span class="sc">%&gt;%</span> </span>
<span id="cb412-33"><a href="#cb412-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">image_to_array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb412-34"><a href="#cb412-34" aria-hidden="true" tabindex="-1"></a>  <span class="st">`</span><span class="at">/</span><span class="st">`</span>(., <span class="dv">255</span>) <span class="sc">%&gt;%</span></span>
<span id="cb412-35"><a href="#cb412-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.raster</span>() <span class="sc">%&gt;%</span></span>
<span id="cb412-36"><a href="#cb412-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>()</span>
<span id="cb412-37"><a href="#cb412-37" aria-hidden="true" tabindex="-1"></a>get_discriminator <span class="ot">=</span> <span class="cf">function</span>(){</span>
<span id="cb412-38"><a href="#cb412-38" aria-hidden="true" tabindex="-1"></a>  discriminator <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb412-39"><a href="#cb412-39" aria-hidden="true" tabindex="-1"></a>  discriminator <span class="sc">%&gt;%</span> </span>
<span id="cb412-40"><a href="#cb412-40" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 64L, <span class="at">kernel_size =</span> <span class="fu">c</span>(5L, 5L), <span class="at">strides =</span> <span class="fu">c</span>(2L, 2L), <span class="at">padding =</span> <span class="st">&quot;same&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(80L, 80L, 3L)) <span class="sc">%&gt;%</span></span>
<span id="cb412-41"><a href="#cb412-41" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-42"><a href="#cb412-42" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dropout</span>(<span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-43"><a href="#cb412-43" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 128L, <span class="at">kernel_size =</span> <span class="fu">c</span>(5L, 5L), <span class="at">strides =</span> <span class="fu">c</span>(2L, 2L), <span class="at">padding =</span> <span class="st">&quot;same&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-44"><a href="#cb412-44" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-45"><a href="#cb412-45" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dropout</span>(<span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-46"><a href="#cb412-46" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 256L, <span class="at">kernel_size =</span> <span class="fu">c</span>(3L, 3L), <span class="at">strides =</span> <span class="fu">c</span>(2L, 2L), <span class="at">padding =</span> <span class="st">&quot;same&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-47"><a href="#cb412-47" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_activation_leaky_relu</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-48"><a href="#cb412-48" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dropout</span>(<span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb412-49"><a href="#cb412-49" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb412-50"><a href="#cb412-50" aria-hidden="true" tabindex="-1"></a>    <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb412-51"><a href="#cb412-51" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(discriminator)</span>
<span id="cb412-52"><a href="#cb412-52" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-53"><a href="#cb412-53" aria-hidden="true" tabindex="-1"></a>discriminator <span class="ot">=</span> <span class="fu">get_discriminator</span>()</span>
<span id="cb412-54"><a href="#cb412-54" aria-hidden="true" tabindex="-1"></a>discriminator</span>
<span id="cb412-55"><a href="#cb412-55" aria-hidden="true" tabindex="-1"></a><span class="fu">discriminator</span>(<span class="fu">generator</span>(tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(1L, 100L))))</span>
<span id="cb412-56"><a href="#cb412-56" aria-hidden="true" tabindex="-1"></a>ce <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>losses<span class="sc">$</span><span class="fu">BinaryCrossentropy</span>(<span class="at">from_logits =</span> <span class="cn">TRUE</span>,<span class="at">label_smoothing =</span> <span class="fl">0.1</span>)</span>
<span id="cb412-57"><a href="#cb412-57" aria-hidden="true" tabindex="-1"></a>loss_discriminator <span class="ot">=</span> <span class="cf">function</span>(real, fake){</span>
<span id="cb412-58"><a href="#cb412-58" aria-hidden="true" tabindex="-1"></a>  real_loss <span class="ot">=</span> <span class="fu">ce</span>(tf<span class="sc">$</span><span class="fu">ones_like</span>(real), real)</span>
<span id="cb412-59"><a href="#cb412-59" aria-hidden="true" tabindex="-1"></a>  fake_loss <span class="ot">=</span> <span class="fu">ce</span>(tf<span class="sc">$</span><span class="fu">zeros_like</span>(fake), fake)</span>
<span id="cb412-60"><a href="#cb412-60" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(real_loss<span class="sc">+</span>fake_loss)</span>
<span id="cb412-61"><a href="#cb412-61" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-62"><a href="#cb412-62" aria-hidden="true" tabindex="-1"></a>loss_generator <span class="ot">=</span> <span class="cf">function</span>(fake){</span>
<span id="cb412-63"><a href="#cb412-63" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">ce</span>(tf<span class="sc">$</span><span class="fu">ones_like</span>(fake), fake))</span>
<span id="cb412-64"><a href="#cb412-64" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-65"><a href="#cb412-65" aria-hidden="true" tabindex="-1"></a>gen_opt <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>optimizers<span class="sc">$</span><span class="fu">RMSprop</span>(<span class="fl">1e-4</span>)</span>
<span id="cb412-66"><a href="#cb412-66" aria-hidden="true" tabindex="-1"></a>disc_opt <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>optimizers<span class="sc">$</span><span class="fu">RMSprop</span>(<span class="fl">1e-4</span>)</span>
<span id="cb412-67"><a href="#cb412-67" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 32L</span>
<span id="cb412-68"><a href="#cb412-68" aria-hidden="true" tabindex="-1"></a>get_batch <span class="ot">=</span> <span class="cf">function</span>(){</span>
<span id="cb412-69"><a href="#cb412-69" aria-hidden="true" tabindex="-1"></a>  indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(train_x), batch_size)</span>
<span id="cb412-70"><a href="#cb412-70" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(tf<span class="sc">$</span><span class="fu">constant</span>(train_x[indices,,,,<span class="at">drop=</span><span class="cn">FALSE</span>], <span class="st">&quot;float32&quot;</span>))</span>
<span id="cb412-71"><a href="#cb412-71" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-72"><a href="#cb412-72" aria-hidden="true" tabindex="-1"></a>train_step <span class="ot">=</span> <span class="cf">function</span>(images){</span>
<span id="cb412-73"><a href="#cb412-73" aria-hidden="true" tabindex="-1"></a>  noise <span class="ot">=</span> tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(32L, 100L))</span>
<span id="cb412-74"><a href="#cb412-74" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb412-75"><a href="#cb412-75" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(tf<span class="sc">$</span><span class="fu">GradientTape</span>(<span class="at">persistent =</span> <span class="cn">TRUE</span>) <span class="sc">%as%</span> tape,{</span>
<span id="cb412-76"><a href="#cb412-76" aria-hidden="true" tabindex="-1"></a>    gen_images <span class="ot">=</span> <span class="fu">generator</span>(noise)</span>
<span id="cb412-77"><a href="#cb412-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb412-78"><a href="#cb412-78" aria-hidden="true" tabindex="-1"></a>    real_output <span class="ot">=</span> <span class="fu">discriminator</span>(images)</span>
<span id="cb412-79"><a href="#cb412-79" aria-hidden="true" tabindex="-1"></a>    fake_output <span class="ot">=</span> <span class="fu">discriminator</span>(gen_images)</span>
<span id="cb412-80"><a href="#cb412-80" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb412-81"><a href="#cb412-81" aria-hidden="true" tabindex="-1"></a>    gen_loss <span class="ot">=</span> <span class="fu">loss_generator</span>(fake_output)</span>
<span id="cb412-82"><a href="#cb412-82" aria-hidden="true" tabindex="-1"></a>    disc_loss <span class="ot">=</span> <span class="fu">loss_discriminator</span>(real_output, fake_output)</span>
<span id="cb412-83"><a href="#cb412-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb412-84"><a href="#cb412-84" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb412-85"><a href="#cb412-85" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb412-86"><a href="#cb412-86" aria-hidden="true" tabindex="-1"></a>  gen_grads <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(gen_loss, generator<span class="sc">$</span>weights)</span>
<span id="cb412-87"><a href="#cb412-87" aria-hidden="true" tabindex="-1"></a>  disc_grads <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(disc_loss, discriminator<span class="sc">$</span>weights)</span>
<span id="cb412-88"><a href="#cb412-88" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rm</span>(tape)</span>
<span id="cb412-89"><a href="#cb412-89" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb412-90"><a href="#cb412-90" aria-hidden="true" tabindex="-1"></a>  gen_opt<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(gen_grads, generator<span class="sc">$</span>weights)))</span>
<span id="cb412-91"><a href="#cb412-91" aria-hidden="true" tabindex="-1"></a>  disc_opt<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(disc_grads, discriminator<span class="sc">$</span>weights)))</span>
<span id="cb412-92"><a href="#cb412-92" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb412-93"><a href="#cb412-93" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(gen_loss, disc_loss))</span>
<span id="cb412-94"><a href="#cb412-94" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb412-95"><a href="#cb412-95" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-96"><a href="#cb412-96" aria-hidden="true" tabindex="-1"></a>train_step <span class="ot">=</span> tf<span class="sc">$</span><span class="st">`</span><span class="at">function</span><span class="st">`</span>(reticulate<span class="sc">::</span><span class="fu">py_func</span>(train_step))</span>
<span id="cb412-97"><a href="#cb412-97" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 10L</span>
<span id="cb412-98"><a href="#cb412-98" aria-hidden="true" tabindex="-1"></a>steps <span class="ot">=</span> <span class="fu">as.integer</span>(<span class="fu">nrow</span>(train_x)<span class="sc">/</span>batch_size)</span>
<span id="cb412-99"><a href="#cb412-99" aria-hidden="true" tabindex="-1"></a>counter <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb412-100"><a href="#cb412-100" aria-hidden="true" tabindex="-1"></a>gen_loss <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb412-101"><a href="#cb412-101" aria-hidden="true" tabindex="-1"></a>disc_loss <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb412-102"><a href="#cb412-102" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(epochs<span class="sc">*</span>steps)){</span>
<span id="cb412-103"><a href="#cb412-103" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb412-104"><a href="#cb412-104" aria-hidden="true" tabindex="-1"></a>  images <span class="ot">=</span> <span class="fu">get_batch</span>()</span>
<span id="cb412-105"><a href="#cb412-105" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">=</span> <span class="fu">train_step</span>(images)</span>
<span id="cb412-106"><a href="#cb412-106" aria-hidden="true" tabindex="-1"></a>  gen_loss[counter] <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">reduce_sum</span>(losses[[<span class="dv">1</span>]])<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb412-107"><a href="#cb412-107" aria-hidden="true" tabindex="-1"></a>  disc_loss[counter] <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">reduce_sum</span>(losses[[<span class="dv">2</span>]])<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb412-108"><a href="#cb412-108" aria-hidden="true" tabindex="-1"></a>  counter <span class="ot">=</span> counter<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb412-109"><a href="#cb412-109" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">%%</span> <span class="dv">10</span><span class="sc">*</span>steps <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb412-110"><a href="#cb412-110" aria-hidden="true" tabindex="-1"></a>    noise <span class="ot">=</span> tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(1L, 100L))</span>
<span id="cb412-111"><a href="#cb412-111" aria-hidden="true" tabindex="-1"></a>    image <span class="ot">=</span> <span class="fu">generator</span>(noise)<span class="sc">$</span><span class="fu">numpy</span>()[<span class="dv">1</span>,,,]</span>
<span id="cb412-112"><a href="#cb412-112" aria-hidden="true" tabindex="-1"></a>    image <span class="ot">=</span> scales<span class="sc">::</span><span class="fu">rescale</span>(image, <span class="at">to =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">255</span>))</span>
<span id="cb412-113"><a href="#cb412-113" aria-hidden="true" tabindex="-1"></a>    image <span class="sc">%&gt;%</span> </span>
<span id="cb412-114"><a href="#cb412-114" aria-hidden="true" tabindex="-1"></a>      <span class="fu">image_to_array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb412-115"><a href="#cb412-115" aria-hidden="true" tabindex="-1"></a>      <span class="st">`</span><span class="at">/</span><span class="st">`</span>(., <span class="dv">255</span>) <span class="sc">%&gt;%</span></span>
<span id="cb412-116"><a href="#cb412-116" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.raster</span>() <span class="sc">%&gt;%</span></span>
<span id="cb412-117"><a href="#cb412-117" aria-hidden="true" tabindex="-1"></a>      <span class="fu">plot</span>()</span>
<span id="cb412-118"><a href="#cb412-118" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb412-119"><a href="#cb412-119" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">%%</span> steps <span class="sc">==</span> <span class="dv">0</span>){</span>
<span id="cb412-120"><a href="#cb412-120" aria-hidden="true" tabindex="-1"></a>    counter <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb412-121"><a href="#cb412-121" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Gen: &quot;</span>, <span class="fu">mean</span>(gen_loss), <span class="st">&quot; Disc: &quot;</span>, <span class="fu">mean</span>(disc_loss), <span class="st">&quot; </span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb412-122"><a href="#cb412-122" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb412-123"><a href="#cb412-123" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-124"><a href="#cb412-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-125"><a href="#cb412-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-126"><a href="#cb412-126" aria-hidden="true" tabindex="-1"></a>results <span class="ot">=</span> <span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, 100L)</span>
<span id="cb412-127"><a href="#cb412-127" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb412-128"><a href="#cb412-128" aria-hidden="true" tabindex="-1"></a>  noise <span class="ot">=</span> tf<span class="sc">$</span>random<span class="sc">$</span><span class="fu">normal</span>(<span class="fu">c</span>(1L, 100L))</span>
<span id="cb412-129"><a href="#cb412-129" aria-hidden="true" tabindex="-1"></a>  image <span class="ot">=</span> <span class="fu">generator</span>(noise)<span class="sc">$</span><span class="fu">numpy</span>()[<span class="dv">1</span>,,,]</span>
<span id="cb412-130"><a href="#cb412-130" aria-hidden="true" tabindex="-1"></a>  image <span class="ot">=</span> scales<span class="sc">::</span><span class="fu">rescale</span>(image, <span class="at">to =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">255</span>))</span>
<span id="cb412-131"><a href="#cb412-131" aria-hidden="true" tabindex="-1"></a>  image <span class="sc">%&gt;%</span> </span>
<span id="cb412-132"><a href="#cb412-132" aria-hidden="true" tabindex="-1"></a>    <span class="fu">image_to_array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb412-133"><a href="#cb412-133" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">/</span><span class="st">`</span>(., <span class="dv">255</span>) <span class="sc">%&gt;%</span></span>
<span id="cb412-134"><a href="#cb412-134" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.raster</span>() <span class="sc">%&gt;%</span></span>
<span id="cb412-135"><a href="#cb412-135" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>()</span>
<span id="cb412-136"><a href="#cb412-136" aria-hidden="true" tabindex="-1"></a>  results[[i]] <span class="ot">=</span> image</span>
<span id="cb412-137"><a href="#cb412-137" aria-hidden="true" tabindex="-1"></a>  imager<span class="sc">::</span><span class="fu">save.image</span>(imager<span class="sc">::</span><span class="fu">as.cimg</span>(image),<span class="at">quality =</span> <span class="fl">1.0</span>,<span class="at">file =</span> <span class="fu">paste0</span>(<span class="st">&quot;images/flower&quot;</span>,i, <span class="st">&quot;.png&quot;</span>))</span>
<span id="cb412-138"><a href="#cb412-138" aria-hidden="true" tabindex="-1"></a>  imager<span class="sc">::</span><span class="fu">as.cimg</span>(image)</span>
<span id="cb412-139"><a href="#cb412-139" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb412-140"><a href="#cb412-140" aria-hidden="true" tabindex="-1"></a><span class="fu">saveRDS</span>(abind<span class="sc">::</span><span class="fu">abind</span>(results, <span class="at">along =</span> 0L), <span class="at">file =</span> <span class="st">&quot;images/result.RDS&quot;</span>)</span></code></pre></div>
<p><img src="images/flower2.png" width="300%" height="300%" /><img src="images/flower3.png" width="300%" height="300%" /><img src="images/flower4.png" width="300%" height="300%" /><img src="images/flower5.png" width="300%" height="300%" /></p>
</div>
</div>
<div id="autoencoder" class="section level2" number="6.2">
<h2 number="6.2"><span class="header-section-number">6.2</span> Autoencoder</h2>
<p>An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise.”
### Autoencoder - DNN MNIST
Autoencoders consist of Encoder and a Decoder Networks.</p>
<p>The encoder will compress the data into 2 dimensions and the decoder will reconstruct the original data:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="#cb413-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb413-2"><a href="#cb413-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb413-3"><a href="#cb413-3" aria-hidden="true" tabindex="-1"></a>rotate <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, rev))</span>
<span id="cb413-4"><a href="#cb413-4" aria-hidden="true" tabindex="-1"></a>imgPlot <span class="ot">=</span> <span class="cf">function</span>(img, <span class="at">title =</span> <span class="st">&quot;&quot;</span>){</span>
<span id="cb413-5"><a href="#cb413-5" aria-hidden="true" tabindex="-1"></a> col<span class="ot">=</span><span class="fu">grey.colors</span>(<span class="dv">255</span>)</span>
<span id="cb413-6"><a href="#cb413-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">image</span>(<span class="fu">rotate</span>(img), <span class="at">col =</span> col, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">axes=</span><span class="cn">FALSE</span>, <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Label: &quot;</span>, <span class="fu">as.character</span>(title)))</span>
<span id="cb413-7"><a href="#cb413-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb413-8"><a href="#cb413-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>datasets<span class="sc">$</span>mnist<span class="sc">$</span><span class="fu">load_data</span>()</span>
<span id="cb413-9"><a href="#cb413-9" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[[<span class="dv">1</span>]]</span>
<span id="cb413-10"><a href="#cb413-10" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>(train[[<span class="dv">1</span>]]<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train[[<span class="dv">1</span>]])[<span class="dv">1</span>], 784L))</span>
<span id="cb413-11"><a href="#cb413-11" aria-hidden="true" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">array</span>(test[[<span class="dv">1</span>]]<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(test[[<span class="dv">1</span>]])[<span class="dv">1</span>], 784L))</span>
<span id="cb413-12"><a href="#cb413-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Dense autoencoder</span></span>
<span id="cb413-13"><a href="#cb413-13" aria-hidden="true" tabindex="-1"></a><span class="do">### Inputs will be compromized to two dimensions</span></span>
<span id="cb413-14"><a href="#cb413-14" aria-hidden="true" tabindex="-1"></a>down_size_model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb413-15"><a href="#cb413-15" aria-hidden="true" tabindex="-1"></a>down_size_model <span class="sc">%&gt;%</span> </span>
<span id="cb413-16"><a href="#cb413-16" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">input_shape =</span> <span class="fu">c</span>(784L),<span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb413-17"><a href="#cb413-17" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb413-18"><a href="#cb413-18" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 2L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>)</span>
<span id="cb413-19"><a href="#cb413-19" aria-hidden="true" tabindex="-1"></a><span class="do">### Reconstruction of the images</span></span>
<span id="cb413-20"><a href="#cb413-20" aria-hidden="true" tabindex="-1"></a>up_size_model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb413-21"><a href="#cb413-21" aria-hidden="true" tabindex="-1"></a>up_size_model <span class="sc">%&gt;%</span> </span>
<span id="cb413-22"><a href="#cb413-22" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">input_shape =</span> <span class="fu">c</span>(2L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb413-23"><a href="#cb413-23" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb413-24"><a href="#cb413-24" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 784L, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb413-25"><a href="#cb413-25" aria-hidden="true" tabindex="-1"></a><span class="do">### Combine models into one</span></span>
<span id="cb413-26"><a href="#cb413-26" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>models<span class="sc">$</span><span class="fu">Model</span>(<span class="at">inputs=</span>down_size_model<span class="sc">$</span>input,  <span class="at">outputs=</span><span class="fu">up_size_model</span>(down_size_model<span class="sc">$</span>output))</span>
<span id="cb413-27"><a href="#cb413-27" aria-hidden="true" tabindex="-1"></a>autoencoder<span class="sc">$</span><span class="fu">compile</span>(<span class="at">loss =</span> loss_binary_crossentropy, <span class="at">optimizer =</span> <span class="fu">optimizer_adamax</span>(<span class="fl">0.01</span>))</span>
<span id="cb413-28"><a href="#cb413-28" aria-hidden="true" tabindex="-1"></a>image <span class="ot">=</span> <span class="fu">autoencoder</span>(train_x[<span class="dv">1</span>,,<span class="at">drop =</span> <span class="cn">FALSE</span>])<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb413-29"><a href="#cb413-29" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb413-30"><a href="#cb413-30" aria-hidden="true" tabindex="-1"></a><span class="fu">imgPlot</span>(<span class="fu">array</span>(train_x[<span class="dv">1</span>,,<span class="at">drop =</span> <span class="cn">FALSE</span>], <span class="fu">c</span>(<span class="dv">28</span>, <span class="dv">28</span>)))</span>
<span id="cb413-31"><a href="#cb413-31" aria-hidden="true" tabindex="-1"></a><span class="fu">imgPlot</span>(<span class="fu">array</span>(image, <span class="fu">c</span>(<span class="dv">28</span>, <span class="dv">28</span>)))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-208-1.png" width="672" /></p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="#cb414-1" aria-hidden="true" tabindex="-1"></a>autoencoder<span class="sc">$</span><span class="fu">fit</span>(<span class="at">x =</span> tf<span class="sc">$</span><span class="fu">constant</span>(train_x), <span class="at">y =</span> tf<span class="sc">$</span><span class="fu">constant</span>(train_x), <span class="at">epochs =</span> 5L, <span class="at">batch_size =</span> 32L)</span></code></pre></div>
<pre><code>## &lt;tensorflow.python.keras.callbacks.History&gt;</code></pre>
<p>After training:</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="#cb416-1" aria-hidden="true" tabindex="-1"></a>pred_dim <span class="ot">=</span> <span class="fu">down_size_model</span>(test_x)</span>
<span id="cb416-2"><a href="#cb416-2" aria-hidden="true" tabindex="-1"></a>reconstr_pred <span class="ot">=</span> <span class="fu">autoencoder</span>(test_x)</span>
<span id="cb416-3"><a href="#cb416-3" aria-hidden="true" tabindex="-1"></a><span class="fu">imgPlot</span>(<span class="fu">array</span>(reconstr_pred[<span class="dv">10</span>,]<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">dim =</span> <span class="fu">c</span>(28L, 28L)))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-209-1.png" width="672" /></p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="#cb417-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb417-2"><a href="#cb417-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pred_dim<span class="sc">$</span><span class="fu">numpy</span>()[,<span class="dv">1</span>], pred_dim<span class="sc">$</span><span class="fu">numpy</span>()[,<span class="dv">2</span>], <span class="at">col =</span> test[[<span class="dv">2</span>]]<span class="sc">+</span>1L)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-209-2.png" width="672" /></p>
<div id="autoencoder---mnist-cnn" class="section level3" number="6.2.1">
<h3 number="6.2.1"><span class="header-section-number">6.2.1</span> Autoencoder - MNIST CNN</h3>
<p>We can also use CNNs isntead of DNNs. There is also an inverse convolutional layer:</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="#cb418-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>datasets<span class="sc">$</span>mnist<span class="sc">$</span><span class="fu">load_data</span>()</span>
<span id="cb418-2"><a href="#cb418-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[[<span class="dv">1</span>]]</span>
<span id="cb418-3"><a href="#cb418-3" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>(train[[<span class="dv">1</span>]]<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train[[<span class="dv">1</span>]]), 1L))</span>
<span id="cb418-4"><a href="#cb418-4" aria-hidden="true" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">array</span>(data[[<span class="dv">2</span>]][[<span class="dv">1</span><span class="sc">/</span><span class="dv">255</span><span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(data[[<span class="dv">2</span>]][[<span class="dv">1</span><span class="sc">/</span><span class="dv">255</span>), 1L))</span>
<span id="cb418-5"><a href="#cb418-5" aria-hidden="true" tabindex="-1"></a>down_size_model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb418-6"><a href="#cb418-6" aria-hidden="true" tabindex="-1"></a>down_size_model <span class="sc">%&gt;%</span> </span>
<span id="cb418-7"><a href="#cb418-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 32L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_size =</span> <span class="fu">c</span>(2L,2L), </span>
<span id="cb418-8"><a href="#cb418-8" aria-hidden="true" tabindex="-1"></a>                          <span class="at">input_shape =</span> <span class="fu">c</span>(28L, 28L, 1L), <span class="at">strides =</span> <span class="fu">c</span>(4L, 4L)) <span class="sc">%&gt;%</span> </span>
<span id="cb418-9"><a href="#cb418-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 16L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, </span>
<span id="cb418-10"><a href="#cb418-10" aria-hidden="true" tabindex="-1"></a>                           <span class="at">kernel_size =</span> <span class="fu">c</span>(7L,7L), <span class="at">strides =</span> <span class="fu">c</span>(1L, 1L)) <span class="sc">%&gt;%</span> </span>
<span id="cb418-11"><a href="#cb418-11" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb418-12"><a href="#cb418-12" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 2L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>)</span>
<span id="cb418-13"><a href="#cb418-13" aria-hidden="true" tabindex="-1"></a>up_size_model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb418-14"><a href="#cb418-14" aria-hidden="true" tabindex="-1"></a>up_size_model <span class="sc">%&gt;%</span> </span>
<span id="cb418-15"><a href="#cb418-15" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 8L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(2L)) <span class="sc">%&gt;%</span> </span>
<span id="cb418-16"><a href="#cb418-16" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_reshape</span>(<span class="at">target_shape =</span> <span class="fu">c</span>(1L, 1L, 8L)) <span class="sc">%&gt;%</span> </span>
<span id="cb418-17"><a href="#cb418-17" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d_transpose</span>(<span class="at">filters =</span> 16L, <span class="at">kernel_size =</span> <span class="fu">c</span>(<span class="dv">7</span>,<span class="dv">7</span>), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">strides =</span> <span class="fu">c</span>(1L,1L)) <span class="sc">%&gt;%</span> </span>
<span id="cb418-18"><a href="#cb418-18" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d_transpose</span>(<span class="at">filters =</span> 32L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_size =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="at">strides =</span> <span class="fu">c</span>(4L,4L)) <span class="sc">%&gt;%</span> </span>
<span id="cb418-19"><a href="#cb418-19" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> <span class="dv">1</span>, <span class="at">kernel_size =</span> <span class="fu">c</span>(1L, 1L), <span class="at">strides =</span> <span class="fu">c</span>(1L, 1L), <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb418-20"><a href="#cb418-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb418-21"><a href="#cb418-21" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>models<span class="sc">$</span><span class="fu">Model</span>(<span class="at">inputs =</span> down_size_model<span class="sc">$</span>input, <span class="at">outputs =</span> <span class="fu">up_size_model</span>(down_size_model<span class="sc">$</span>output))</span>
<span id="cb418-22"><a href="#cb418-22" aria-hidden="true" tabindex="-1"></a>autoencoder<span class="sc">$</span><span class="fu">compile</span>(<span class="at">loss =</span> loss_binary_crossentropy, <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(<span class="fl">0.001</span>))</span>
<span id="cb418-23"><a href="#cb418-23" aria-hidden="true" tabindex="-1"></a>autoencoder<span class="sc">$</span><span class="fu">fit</span>(<span class="at">x =</span> tf<span class="sc">$</span><span class="fu">constant</span>(train_x), <span class="at">y =</span> tf<span class="sc">$</span><span class="fu">constant</span>(train_x), <span class="at">epochs =</span> 1L, <span class="at">batch_size =</span> 64L)</span>
<span id="cb418-24"><a href="#cb418-24" aria-hidden="true" tabindex="-1"></a>pred_dim <span class="ot">=</span> <span class="fu">down_size_model</span>(tf<span class="sc">$</span><span class="fu">constant</span>(test_x, <span class="st">&quot;float32&quot;</span>))</span>
<span id="cb418-25"><a href="#cb418-25" aria-hidden="true" tabindex="-1"></a>reconstr_pred <span class="ot">=</span> <span class="fu">autoencoder</span>(tf<span class="sc">$</span><span class="fu">constant</span>(test_x, <span class="st">&quot;float32&quot;</span>))</span>
<span id="cb418-26"><a href="#cb418-26" aria-hidden="true" tabindex="-1"></a><span class="fu">imgPlot</span>(reconstr_pred[<span class="dv">10</span>,,,]<span class="sc">$</span><span class="fu">numpy</span>()[,,<span class="dv">1</span>])</span>
<span id="cb418-27"><a href="#cb418-27" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pred_dim[,<span class="dv">1</span>]<span class="sc">$</span><span class="fu">numpy</span>(), pred_dim[,<span class="dv">2</span>]<span class="sc">$</span><span class="fu">numpy</span>(), <span class="at">col =</span> test[[<span class="dv">2</span>]]<span class="sc">+</span>1L)</span>
<span id="cb418-28"><a href="#cb418-28" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate new images!</span></span>
<span id="cb418-29"><a href="#cb418-29" aria-hidden="true" tabindex="-1"></a>new <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">10</span>), <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb418-30"><a href="#cb418-30" aria-hidden="true" tabindex="-1"></a><span class="fu">imgPlot</span>(<span class="fu">array</span>(<span class="fu">up_size_model</span>(new)<span class="sc">$</span><span class="fu">numpy</span>(), <span class="fu">c</span>(28L, 28L)))</span></code></pre></div>
</div>
</div>
<div id="varational-autoencoder" class="section level2" number="6.3">
<h2 number="6.3"><span class="header-section-number">6.3</span> Varational Autoencoder</h2>
<p>The difference to normal autoencoder is that we here try to fit latent variables which encode the images:</p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="#cb419-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow_probability)</span>
<span id="cb419-2"><a href="#cb419-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>datasets<span class="sc">$</span>mnist<span class="sc">$</span><span class="fu">load_data</span>()</span>
<span id="cb419-3"><a href="#cb419-3" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[[<span class="dv">1</span>]]</span>
<span id="cb419-4"><a href="#cb419-4" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>(train[[<span class="dv">1</span>]]<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train[[<span class="dv">1</span>]])[<span class="dv">1</span>], 784L))</span>
<span id="cb419-5"><a href="#cb419-5" aria-hidden="true" tabindex="-1"></a>tfp <span class="ot">=</span> reticulate<span class="sc">::</span><span class="fu">import</span>(<span class="st">&quot;tensorflow_probability&quot;</span>)</span>
<span id="cb419-6"><a href="#cb419-6" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> tfp<span class="sc">$</span>distributions<span class="sc">$</span><span class="fu">Independent</span>(tfp<span class="sc">$</span>distributions<span class="sc">$</span><span class="fu">Normal</span>(<span class="at">loc=</span>tf<span class="sc">$</span><span class="fu">zeros</span>(<span class="fu">shape</span>(10L,4L)), <span class="at">scale=</span><span class="fl">1.0</span>),</span>
<span id="cb419-7"><a href="#cb419-7" aria-hidden="true" tabindex="-1"></a> <span class="at">reinterpreted_batch_ndims=</span>1L)</span>
<span id="cb419-8"><a href="#cb419-8" aria-hidden="true" tabindex="-1"></a>down_size_model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb419-9"><a href="#cb419-9" aria-hidden="true" tabindex="-1"></a>down_size_model <span class="sc">%&gt;%</span> </span>
<span id="cb419-10"><a href="#cb419-10" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">input_shape =</span> <span class="fu">c</span>(784L),<span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb419-11"><a href="#cb419-11" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb419-12"><a href="#cb419-12" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 4L)</span>
<span id="cb419-13"><a href="#cb419-13" aria-hidden="true" tabindex="-1"></a><span class="do">### Reconstruction of the images</span></span>
<span id="cb419-14"><a href="#cb419-14" aria-hidden="true" tabindex="-1"></a>up_size_model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb419-15"><a href="#cb419-15" aria-hidden="true" tabindex="-1"></a>up_size_model <span class="sc">%&gt;%</span> </span>
<span id="cb419-16"><a href="#cb419-16" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">input_shape =</span> <span class="fu">c</span>(2L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb419-17"><a href="#cb419-17" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb419-18"><a href="#cb419-18" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 784L, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb419-19"><a href="#cb419-19" aria-hidden="true" tabindex="-1"></a><span class="do">### Combine models into one</span></span>
<span id="cb419-20"><a href="#cb419-20" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 32L</span>
<span id="cb419-21"><a href="#cb419-21" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 10L</span>
<span id="cb419-22"><a href="#cb419-22" aria-hidden="true" tabindex="-1"></a>steps <span class="ot">=</span> <span class="fu">as.integer</span>(<span class="fu">nrow</span>(train_x)<span class="sc">/</span>32L <span class="sc">*</span> epochs)</span>
<span id="cb419-23"><a href="#cb419-23" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> tfp<span class="sc">$</span>distributions<span class="sc">$</span><span class="fu">MultivariateNormalDiag</span>(<span class="at">loc =</span> tf<span class="sc">$</span><span class="fu">zeros</span>(<span class="fu">shape</span>(batch_size, 2L), <span class="st">&quot;float32&quot;</span>), <span class="at">scale_diag =</span> tf<span class="sc">$</span><span class="fu">ones</span>(2L, <span class="st">&quot;float32&quot;</span>))</span>
<span id="cb419-24"><a href="#cb419-24" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">=</span> tf<span class="sc">$</span>keras<span class="sc">$</span>optimizers<span class="sc">$</span><span class="fu">RMSprop</span>(<span class="fl">0.0001</span>)</span>
<span id="cb419-25"><a href="#cb419-25" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">=</span> <span class="fu">c</span>(down_size_model<span class="sc">$</span>weights, up_size_model<span class="sc">$</span>weights)</span>
<span id="cb419-26"><a href="#cb419-26" aria-hidden="true" tabindex="-1"></a>get_batch <span class="ot">=</span> <span class="cf">function</span>(){</span>
<span id="cb419-27"><a href="#cb419-27" aria-hidden="true" tabindex="-1"></a> indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(train_x), batch_size)</span>
<span id="cb419-28"><a href="#cb419-28" aria-hidden="true" tabindex="-1"></a> <span class="fu">return</span>(train_x[indices,])</span>
<span id="cb419-29"><a href="#cb419-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb419-30"><a href="#cb419-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>steps){</span>
<span id="cb419-31"><a href="#cb419-31" aria-hidden="true" tabindex="-1"></a> tmp_X <span class="ot">=</span> <span class="fu">get_batch</span>()</span>
<span id="cb419-32"><a href="#cb419-32" aria-hidden="true" tabindex="-1"></a> <span class="fu">with</span>(tf<span class="sc">$</span><span class="fu">GradientTape</span>() <span class="sc">%as%</span> tape, {</span>
<span id="cb419-33"><a href="#cb419-33" aria-hidden="true" tabindex="-1"></a> encoded <span class="ot">=</span> <span class="fu">down_size_model</span>(tmp_X)</span>
<span id="cb419-34"><a href="#cb419-34" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb419-35"><a href="#cb419-35" aria-hidden="true" tabindex="-1"></a> dd <span class="ot">=</span> tfp<span class="sc">$</span>distributions<span class="sc">$</span><span class="fu">MultivariateNormalDiag</span>(<span class="at">loc =</span> encoded[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], </span>
<span id="cb419-36"><a href="#cb419-36" aria-hidden="true" tabindex="-1"></a> <span class="at">scale_diag =</span> <span class="fl">1.0</span><span class="sc">/</span>(<span class="fl">0.01</span><span class="sc">+</span> tf<span class="sc">$</span>math<span class="sc">$</span><span class="fu">softplus</span>(encoded[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>])))</span>
<span id="cb419-37"><a href="#cb419-37" aria-hidden="true" tabindex="-1"></a> samples <span class="ot">=</span> dd<span class="sc">$</span><span class="fu">sample</span>()</span>
<span id="cb419-38"><a href="#cb419-38" aria-hidden="true" tabindex="-1"></a> reconstructed <span class="ot">=</span> <span class="fu">up_size_model</span>(samples)</span>
<span id="cb419-39"><a href="#cb419-39" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb419-40"><a href="#cb419-40" aria-hidden="true" tabindex="-1"></a> KL_loss <span class="ot">=</span> dd<span class="sc">$</span><span class="fu">kl_divergence</span>(prior) <span class="co"># constrain</span></span>
<span id="cb419-41"><a href="#cb419-41" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb419-42"><a href="#cb419-42" aria-hidden="true" tabindex="-1"></a> loss <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">reduce_mean</span>(tf<span class="sc">$</span><span class="fu">negative</span>(tfp<span class="sc">$</span>distributions<span class="sc">$</span><span class="fu">Binomial</span>(1L, <span class="at">logits =</span> reconstructed)<span class="sc">$</span><span class="fu">log_prob</span>(tmp_X)))<span class="sc">+</span>tf<span class="sc">$</span><span class="fu">reduce_mean</span>(KL_loss)</span>
<span id="cb419-43"><a href="#cb419-43" aria-hidden="true" tabindex="-1"></a> })</span>
<span id="cb419-44"><a href="#cb419-44" aria-hidden="true" tabindex="-1"></a> gradients <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(loss, weights)</span>
<span id="cb419-45"><a href="#cb419-45" aria-hidden="true" tabindex="-1"></a> optimizer<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(gradients, weights)))</span>
<span id="cb419-46"><a href="#cb419-46" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb419-47"><a href="#cb419-47" aria-hidden="true" tabindex="-1"></a> <span class="cf">if</span>(i <span class="sc">%%</span> <span class="fu">as.integer</span>(<span class="fu">nrow</span>(train_x)<span class="sc">/</span>10L) <span class="sc">==</span> <span class="dv">0</span>) <span class="fu">cat</span>(<span class="st">&quot;Loss: &quot;</span>, loss<span class="sc">$</span><span class="fu">numpy</span>(), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb419-48"><a href="#cb419-48" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<!--chapter:end:05-GAN.Rmd-->
</div>
</div>
<div id="datasets" class="section level1" number="7">
<h1 number="7"><span class="header-section-number">7</span> Datasets</h1>
<p>You can download the datasets we use in the course <a href="http://rhsbio7.uni-regensburg.de:8500">here</a> (ignore browser warnings) or by installing the EcoData package:</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="#cb420-1" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="at">repo =</span> <span class="st">&quot;florianhartig/EcoData&quot;</span>, <span class="at">subdir =</span> <span class="st">&quot;EcoData&quot;</span>, </span>
<span id="cb420-2"><a href="#cb420-2" aria-hidden="true" tabindex="-1"></a><span class="at">dependencies =</span> <span class="cn">TRUE</span>, <span class="at">build_vignettes =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div id="titanic" class="section level2" number="7.1">
<h2 number="7.1"><span class="header-section-number">7.1</span> Titanic</h2>
<p>The dataset is a collection of titanic passengers with information about their age, class, sex, and their survival status. The competition here is simple: train a ML model and predict the survival probability.</p>
<p>The titanic dataset is very well explored and serves as a stepping stone in many ML careers. For inspiration and data exploration notebooks, check out this kaggle competition: <a href="https://www.kaggle.com/c/titanic/data"></a></p>
<p><strong>Response variable:</strong> ‘survived’</p>
<p>A minimal working example:</p>
<ol style="list-style-type: decimal">
<li>Load dataset</li>
</ol>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="#cb421-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load(&quot;datasets.RData&quot;)</span></span>
<span id="cb421-2"><a href="#cb421-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EcoData)</span>
<span id="cb421-3"><a href="#cb421-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(titanic_ml)</span>
<span id="cb421-4"><a href="#cb421-4" aria-hidden="true" tabindex="-1"></a>titanic <span class="ot">=</span> titanic_ml</span>
<span id="cb421-5"><a href="#cb421-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(titanic)</span></code></pre></div>
<pre><code>##      pclass         survived          name               sex           age              sibsp            parch            ticket          fare        
##  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000  
##  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896  
##  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454  
##  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295  
##  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275  
##  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329  
##                  NA&#39;s   :655                                      NA&#39;s   :263                                        (Other) :1261   NA&#39;s   :1        
##              cabin      embarked      boat          body                      home.dest  
##                 :1014    :  2           :823   Min.   :  1.0                       :564  
##  C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0   New York, NY        : 64  
##  B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0   London              : 14  
##  G6             :   5   S:914    15     : 37   Mean   :160.8   Montreal, PQ        : 10  
##  B96 B98        :   4            14     : 33   3rd Qu.:256.0   Cornwall / Akron, OH:  9  
##  C22 C26        :   4            4      : 31   Max.   :328.0   Paris, France       :  9  
##  (Other)        : 271            (Other):308   NA&#39;s   :1188    (Other)             :639</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Impute missing values (not our response variable!)</li>
</ol>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="#cb423-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(missRanger)</span>
<span id="cb423-2"><a href="#cb423-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb423-3"><a href="#cb423-3" aria-hidden="true" tabindex="-1"></a>titanic_imputed <span class="ot">=</span> titanic <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>name, <span class="sc">-</span>ticket, <span class="sc">-</span>cabin, <span class="sc">-</span>boat, <span class="sc">-</span>home.dest)</span>
<span id="cb423-4"><a href="#cb423-4" aria-hidden="true" tabindex="-1"></a>titanic_imputed <span class="ot">=</span> missRanger<span class="sc">::</span><span class="fu">missRanger</span>(<span class="at">data =</span> titanic_imputed <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>survived))</span></code></pre></div>
<pre><code>## 
## Missing value imputation by random forests
## 
##   Variables to impute:       age, fare, body
##   Variables used to impute:  pclass, sex, age, sibsp, parch, fare, embarked, body
## iter 1:  ...
## iter 2:  ...
## iter 3:  ...</code></pre>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="#cb425-1" aria-hidden="true" tabindex="-1"></a>titanic_imputed<span class="sc">$</span>survived <span class="ot">=</span> titanic<span class="sc">$</span>survived</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Split into training and testing</li>
</ol>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="#cb426-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> titanic_imputed[<span class="sc">!</span><span class="fu">is.na</span>(titanic<span class="sc">$</span>survived), ]</span>
<span id="cb426-2"><a href="#cb426-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> titanic_imputed[<span class="fu">is.na</span>(titanic<span class="sc">$</span>survived), ]</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Train model</li>
</ol>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="#cb427-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">glm</span>(survived<span class="sc">~</span>., <span class="at">data=</span>train, <span class="at">family =</span> <span class="fu">binomial</span>())</span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>Predictions</li>
</ol>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="#cb428-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(model, <span class="at">data =</span> test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb428-2"><a href="#cb428-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(preds)</span></code></pre></div>
<pre><code>##        561        321       1177       1098       1252       1170 
## 0.79350595 0.30982388 0.01400816 0.12405376 0.14107743 0.11799810</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Create submission csv</li>
</ol>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="#cb430-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(<span class="fu">data.frame</span>(<span class="at">y=</span>preds), <span class="at">file =</span> <span class="st">&quot;glm.csv&quot;</span>)</span></code></pre></div>
<p>And submit the csv on <a href="http://rhsbio7.uni-regensburg.de:8500"></a></p>
</div>
<div id="plant-pollinator-database" class="section level2" number="7.2">
<h2 number="7.2"><span class="header-section-number">7.2</span> Plant-pollinator database</h2>
<p>The plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower).
We explored the advantage of ML algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look <img src="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13329" alt="here" />.</p>
<p><img src="images/TM.png" width="699" /></p>
<p><strong>Response variable:</strong> ‘interaction’</p>
<p>A minimal working example:</p>
<ol style="list-style-type: decimal">
<li>Load dataset</li>
</ol>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="#cb431-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;datasets.RData&quot;</span>)</span>
<span id="cb431-2"><a href="#cb431-2" aria-hidden="true" tabindex="-1"></a><span class="co"># library(EcoData)</span></span>
<span id="cb431-3"><a href="#cb431-3" aria-hidden="true" tabindex="-1"></a><span class="co"># data(plantPollinator_df)</span></span>
<span id="cb431-4"><a href="#cb431-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plant_poll = plantPollinator_df</span></span>
<span id="cb431-5"><a href="#cb431-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(plant_poll)</span></code></pre></div>
<pre><code>##                     X                           Y             type              season             diameter        corolla             colour             nectar         
##  Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480       Length:20480       Min.   :  2.00   Length:20480       Length:20480       Length:20480      
##  Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character   Class :character   1st Qu.:  5.00   Class :character   Class :character   Class :character  
##  Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character   Mode  :character   Median : 19.00   Mode  :character   Mode  :character   Mode  :character  
##  Coriandrum_sativum  :  256   Andrena_flavipes   :   80                                         Mean   : 27.03                                                           
##  Daucus_carota       :  256   Andrena_gravida    :   80                                         3rd Qu.: 25.00                                                           
##  Malus_domestica     :  256   Andrena_haemorrhoa :   80                                         Max.   :150.00                                                           
##  (Other)             :18944   (Other)            :20000                                         NA&#39;s   :9472                                                             
##    b.system         s.pollination      inflorescence       composite            guild               tongue            body        sociality           feeding         
##  Length:20480       Length:20480       Length:20480       Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00   Length:20480       Length:20480      
##  Class :character   Class :character   Class :character   Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00   Class :character   Class :character  
##  Mode  :character   Mode  :character   Mode  :character   Mode  :character   Mode  :character   Median : 6.600   Median :10.50   Mode  :character   Mode  :character  
##                                                                                                 Mean   : 8.104   Mean   :10.66                                        
##                                                                                                 3rd Qu.:10.500   3rd Qu.:13.00                                        
##                                                                                                 Max.   :26.400   Max.   :25.00                                        
##                                                                                                 NA&#39;s   :17040    NA&#39;s   :6160                                         
##  interaction 
##  0   :14095  
##  1   :  595  
##  NA&#39;s: 5790  
##              
##              
##              
## </code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Impute missing values (not our response variable!)
We will select only a few predictors here (you can work with all predictors ofc).</li>
</ol>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="#cb433-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(missRanger)</span>
<span id="cb433-2"><a href="#cb433-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb433-3"><a href="#cb433-3" aria-hidden="true" tabindex="-1"></a>plant_poll_imputed <span class="ot">=</span> plant_poll <span class="sc">%&gt;%</span> <span class="fu">select</span>(diameter, corolla, tongue, body, interaction)</span>
<span id="cb433-4"><a href="#cb433-4" aria-hidden="true" tabindex="-1"></a>plant_poll_imputed <span class="ot">=</span> missRanger<span class="sc">::</span><span class="fu">missRanger</span>(<span class="at">data =</span> plant_poll_imputed <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>interaction))</span></code></pre></div>
<pre><code>## 
## Missing value imputation by random forests
## 
##   Variables to impute:       diameter, corolla, tongue, body
##   Variables used to impute:  diameter, corolla, tongue, body
## iter 1:  ....
## iter 2:  ....
## iter 3:  ....
## iter 4:  ....</code></pre>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="#cb435-1" aria-hidden="true" tabindex="-1"></a>plant_poll_imputed<span class="sc">$</span>interaction <span class="ot">=</span> plant_poll<span class="sc">$</span>interaction</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Split into training and testing</li>
</ol>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="#cb436-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> plant_poll_imputed[<span class="sc">!</span><span class="fu">is.na</span>(plant_poll_imputed<span class="sc">$</span>interaction), ]</span>
<span id="cb436-2"><a href="#cb436-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> plant_poll_imputed[<span class="fu">is.na</span>(plant_poll_imputed<span class="sc">$</span>interaction), ]</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Train model</li>
</ol>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="#cb437-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">glm</span>(interaction<span class="sc">~</span>., <span class="at">data=</span>train, <span class="at">family =</span> <span class="fu">binomial</span>())</span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>Predictions</li>
</ol>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="#cb438-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(model, <span class="at">data =</span> test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb438-2"><a href="#cb438-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(preds)</span></code></pre></div>
<pre><code>##       3871       3872       3873       3874       3875       3876 
## 0.03850223 0.03726134 0.03917270 0.04460708 0.04456360 0.03910827</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Create submission csv</li>
</ol>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="#cb440-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(<span class="fu">data.frame</span>(<span class="at">y=</span>preds), <span class="at">file =</span> <span class="st">&quot;glm.csv&quot;</span>)</span></code></pre></div>
</div>
<div id="wine" class="section level2" number="7.3">
<h2 number="7.3"><span class="header-section-number">7.3</span> Wine</h2>
<p>The dataset is a collection of wines of different quality. The aim is to predict the quality of the wine based on physochemical predictors.</p>
<p>For inspiration and data exploration notebooks, check out this kaggle competition: <a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009"></a>
For instance, check out this very nice <a href="https://www.kaggle.com/aditimulye/red-wine-quality-assesment-starter-pack">notebook</a> which removes a few problems from the data.</p>
<p><strong>Response variable:</strong> ‘quality’</p>
<p>We could use theoretically a regression model for this task but we will stick with a classification model</p>
<p>A minimal working example:</p>
<ol style="list-style-type: decimal">
<li>Load dataset</li>
</ol>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="#cb441-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;datasets.RData&quot;</span>)</span>
<span id="cb441-2"><a href="#cb441-2" aria-hidden="true" tabindex="-1"></a><span class="co"># library(EcoData)</span></span>
<span id="cb441-3"><a href="#cb441-3" aria-hidden="true" tabindex="-1"></a><span class="co"># data(wine)</span></span>
<span id="cb441-4"><a href="#cb441-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(wine)</span></code></pre></div>
<pre><code>##  fixed.acidity    volatile.acidity  citric.acid     residual.sugar     chlorides       free.sulfur.dioxide total.sulfur.dioxide    density             pH       
##  Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900   Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901   Min.   :2.740  
##  1st Qu.: 7.100   1st Qu.:0.3900   1st Qu.:0.0900   1st Qu.: 1.900   1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956   1st Qu.:3.210  
##  Median : 7.900   Median :0.5200   Median :0.2600   Median : 2.200   Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968   Median :3.310  
##  Mean   : 8.335   Mean   :0.5284   Mean   :0.2705   Mean   : 2.533   Mean   :0.08747   Mean   :15.83       Mean   : 46.23       Mean   :0.9968   Mean   :3.311  
##  3rd Qu.: 9.300   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600   3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9979   3rd Qu.:3.400  
##  Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500   Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037   Max.   :4.010  
##  NA&#39;s   :70       NA&#39;s   :48       NA&#39;s   :41       NA&#39;s   :60       NA&#39;s   :37        NA&#39;s   :78          NA&#39;s   :78           NA&#39;s   :78       NA&#39;s   :25     
##    sulphates         alcohol         quality     
##  Min.   :0.3300   Min.   : 8.40   Min.   :3.000  
##  1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  
##  Median :0.6200   Median :10.20   Median :6.000  
##  Mean   :0.6572   Mean   :10.42   Mean   :5.596  
##  3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  
##  Max.   :2.0000   Max.   :14.90   Max.   :8.000  
##  NA&#39;s   :51                       NA&#39;s   :905</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Impute missing values (not our response variable!)</li>
</ol>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="#cb443-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(missRanger)</span>
<span id="cb443-2"><a href="#cb443-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb443-3"><a href="#cb443-3" aria-hidden="true" tabindex="-1"></a><span class="co">#wine_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest)</span></span>
<span id="cb443-4"><a href="#cb443-4" aria-hidden="true" tabindex="-1"></a>wine_imputed <span class="ot">=</span> missRanger<span class="sc">::</span><span class="fu">missRanger</span>(<span class="at">data =</span> wine <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>quality))</span></code></pre></div>
<pre><code>## 
## Missing value imputation by random forests
## 
##   Variables to impute:       fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates
##   Variables used to impute:  fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol
## iter 1:  ..........
## iter 2:  ..........
## iter 3:  ..........
## iter 4:  ..........
## iter 5:  ..........</code></pre>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="#cb445-1" aria-hidden="true" tabindex="-1"></a>wine_imputed<span class="sc">$</span>quality <span class="ot">=</span> wine<span class="sc">$</span>quality</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Split into training and testing</li>
</ol>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="#cb446-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> wine_imputed[<span class="sc">!</span><span class="fu">is.na</span>(wine<span class="sc">$</span>quality), ]</span>
<span id="cb446-2"><a href="#cb446-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> wine_imputed[<span class="fu">is.na</span>(wine<span class="sc">$</span>quality), ]</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Train model</li>
</ol>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="#cb447-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb447-2"><a href="#cb447-2" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">=</span> <span class="fu">ranger</span>(quality<span class="sc">~</span>., <span class="at">data =</span> train, <span class="at">classification =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>Predictions</li>
</ol>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="#cb448-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(rf, <span class="at">data =</span> test)<span class="sc">$</span>predictions</span>
<span id="cb448-2"><a href="#cb448-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(preds)</span></code></pre></div>
<pre><code>## [1] 6 5 5 7 6 6</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Create submission csv</li>
</ol>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="#cb450-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(<span class="fu">data.frame</span>(<span class="at">y=</span>preds), <span class="at">file =</span> <span class="st">&quot;rf.csv&quot;</span>)</span></code></pre></div>
</div>
<div id="nasa" class="section level2" number="7.4">
<h2 number="7.4"><span class="header-section-number">7.4</span> Nasa</h2>
<p>A collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not.
For inspiration and data exploration notebooks, check out the kaggle competition: <a href="https://www.kaggle.com/shrutimehta/nasa-asteroids-classification"></a></p>
<p><strong>Response variable:</strong> ‘Hazardous’
1. Load dataset</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="#cb451-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;datasets.RData&quot;</span>)</span>
<span id="cb451-2"><a href="#cb451-2" aria-hidden="true" tabindex="-1"></a><span class="co"># library(EcoData)</span></span>
<span id="cb451-3"><a href="#cb451-3" aria-hidden="true" tabindex="-1"></a><span class="co"># data(nasa)</span></span>
<span id="cb451-4"><a href="#cb451-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(nasa)</span></code></pre></div>
<pre><code>##  Neo.Reference.ID       Name         Absolute.Magnitude Est.Dia.in.KM.min. Est.Dia.in.KM.max. Est.Dia.in.M.min.   Est.Dia.in.M.max.  Est.Dia.in.Miles.min.
##  Min.   :2000433   Min.   :2000433   Min.   :11.16      Min.   : 0.00101   Min.   : 0.00226   Min.   :    1.011   Min.   :    2.26   Min.   :0.00063      
##  1st Qu.:3102682   1st Qu.:3102683   1st Qu.:20.10      1st Qu.: 0.03346   1st Qu.: 0.07482   1st Qu.:   33.462   1st Qu.:   74.82   1st Qu.:0.02079      
##  Median :3514800   Median :3514800   Median :21.90      Median : 0.11080   Median : 0.24777   Median :  110.804   Median :  247.77   Median :0.06885      
##  Mean   :3272675   Mean   :3273113   Mean   :22.27      Mean   : 0.20523   Mean   : 0.45754   Mean   :  204.649   Mean   :  458.45   Mean   :0.12734      
##  3rd Qu.:3690987   3rd Qu.:3690385   3rd Qu.:24.50      3rd Qu.: 0.25384   3rd Qu.: 0.56760   3rd Qu.:  253.837   3rd Qu.:  567.60   3rd Qu.:0.15773      
##  Max.   :3781897   Max.   :3781897   Max.   :32.10      Max.   :15.57955   Max.   :34.83694   Max.   :15579.552   Max.   :34836.94   Max.   :9.68068      
##  NA&#39;s   :53        NA&#39;s   :57        NA&#39;s   :36         NA&#39;s   :60         NA&#39;s   :23         NA&#39;s   :29          NA&#39;s   :46         NA&#39;s   :42           
##  Est.Dia.in.Miles.max. Est.Dia.in.Feet.min. Est.Dia.in.Feet.max. Close.Approach.Date Epoch.Date.Close.Approach Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr
##  Min.   : 0.00140      Min.   :    3.32     Min.   :     7.41    2016-07-22:  18     Min.   :7.889e+11         Min.   : 0.3355              Min.   :  1208             
##  1st Qu.: 0.04649      1st Qu.:  109.78     1st Qu.:   245.49    2015-01-15:  17     1st Qu.:1.016e+12         1st Qu.: 8.4497              1st Qu.: 30399             
##  Median : 0.15395      Median :  363.53     Median :   812.88    2015-02-15:  16     Median :1.203e+12         Median :12.9370              Median : 46532             
##  Mean   : 0.28486      Mean   :  670.44     Mean   :  1500.77    2007-11-08:  15     Mean   :1.180e+12         Mean   :13.9848              Mean   : 50298             
##  3rd Qu.: 0.35269      3rd Qu.:  832.80     3rd Qu.:  1862.19    2012-01-15:  15     3rd Qu.:1.356e+12         3rd Qu.:18.0774              3rd Qu.: 65068             
##  Max.   :21.64666      Max.   :51114.02     Max.   :114294.42    (Other)   :4577     Max.   :1.473e+12         Max.   :44.6337              Max.   :160681             
##  NA&#39;s   :50            NA&#39;s   :21           NA&#39;s   :46           NA&#39;s      :  29     NA&#39;s   :43                NA&#39;s   :27                   NA&#39;s   :28                 
##  Miles.per.hour    Miss.Dist..Astronomical. Miss.Dist..lunar.   Miss.Dist..kilometers. Miss.Dist..miles.  Orbiting.Body    Orbit.ID             Orbit.Determination.Date
##  Min.   :  750.5   Min.   :0.00018          Min.   :  0.06919   Min.   :   26610       Min.   :   16535   Earth:4665    Min.   :  1.00   2017-06-21 06:17:20:   9       
##  1st Qu.:18846.7   1st Qu.:0.13341          1st Qu.: 51.89874   1st Qu.:19964907       1st Qu.:12454813   NA&#39;s :  22    1st Qu.:  9.00   2017-04-06 08:57:13:   8       
##  Median :28893.7   Median :0.26497          Median :103.19415   Median :39685408       Median :24662435                 Median : 16.00   2017-04-06 09:24:24:   8       
##  Mean   :31228.0   Mean   :0.25690          Mean   : 99.91366   Mean   :38436154       Mean   :23885560                 Mean   : 28.34   2017-04-06 08:24:13:   7       
##  3rd Qu.:40436.9   3rd Qu.:0.38506          3rd Qu.:149.59244   3rd Qu.:57540318       3rd Qu.:35714721                 3rd Qu.: 31.00   2017-04-06 08:26:19:   7       
##  Max.   :99841.2   Max.   :0.49988          Max.   :194.45491   Max.   :74781600       Max.   :46467132                 Max.   :611.00   (Other)            :4622       
##  NA&#39;s   :38        NA&#39;s   :60               NA&#39;s   :30          NA&#39;s   :56             NA&#39;s   :27                       NA&#39;s   :33       NA&#39;s               :  26       
##  Orbit.Uncertainity Minimum.Orbit.Intersection Jupiter.Tisserand.Invariant Epoch.Osculation   Eccentricity     Semi.Major.Axis   Inclination       Asc.Node.Longitude
##  Min.   :0.000      Min.   :0.00000            Min.   :2.196               Min.   :2450164   Min.   :0.00752   Min.   :0.6159   Min.   : 0.01451   Min.   :  0.0019  
##  1st Qu.:0.000      1st Qu.:0.01435            1st Qu.:4.047               1st Qu.:2458000   1st Qu.:0.24086   1st Qu.:1.0012   1st Qu.: 4.93290   1st Qu.: 83.1849  
##  Median :3.000      Median :0.04653            Median :5.071               Median :2458000   Median :0.37251   Median :1.2422   Median :10.27694   Median :172.6347  
##  Mean   :3.521      Mean   :0.08191            Mean   :5.056               Mean   :2457723   Mean   :0.38267   Mean   :1.4009   Mean   :13.36159   Mean   :172.1717  
##  3rd Qu.:6.000      3rd Qu.:0.12150            3rd Qu.:6.017               3rd Qu.:2458000   3rd Qu.:0.51256   3rd Qu.:1.6782   3rd Qu.:19.47848   3rd Qu.:254.8804  
##  Max.   :9.000      Max.   :0.47789            Max.   :9.025               Max.   :2458020   Max.   :0.96026   Max.   :5.0720   Max.   :75.40667   Max.   :359.9059  
##  NA&#39;s   :49         NA&#39;s   :137                NA&#39;s   :56                  NA&#39;s   :60        NA&#39;s   :39        NA&#39;s   :53       NA&#39;s   :42         NA&#39;s   :60        
##  Orbital.Period   Perihelion.Distance Perihelion.Arg     Aphelion.Dist    Perihelion.Time    Mean.Anomaly       Mean.Motion       Equinox       Hazardous    
##  Min.   : 176.6   Min.   :0.08074     Min.   :  0.0069   Min.   :0.8038   Min.   :2450100   Min.   :  0.0032   Min.   :0.08628   J2000:4663   Min.   :0.000  
##  1st Qu.: 365.9   1st Qu.:0.63038     1st Qu.: 95.6430   1st Qu.:1.2661   1st Qu.:2457815   1st Qu.: 87.0069   1st Qu.:0.45147   NA&#39;s :  24   1st Qu.:0.000  
##  Median : 504.9   Median :0.83288     Median :189.7729   Median :1.6182   Median :2457972   Median :186.0219   Median :0.71137                Median :0.000  
##  Mean   : 635.5   Mean   :0.81316     Mean   :184.0185   Mean   :1.9864   Mean   :2457726   Mean   :181.2882   Mean   :0.73732                Mean   :0.176  
##  3rd Qu.: 793.1   3rd Qu.:0.99718     3rd Qu.:271.9535   3rd Qu.:2.4497   3rd Qu.:2458108   3rd Qu.:276.6418   3rd Qu.:0.98379                3rd Qu.:0.000  
##  Max.   :4172.2   Max.   :1.29983     Max.   :359.9931   Max.   :8.9839   Max.   :2458839   Max.   :359.9180   Max.   :2.03900                Max.   :1.000  
##  NA&#39;s   :46       NA&#39;s   :22          NA&#39;s   :48         NA&#39;s   :38       NA&#39;s   :59        NA&#39;s   :40         NA&#39;s   :48                     NA&#39;s   :4187</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Impute missing values (not our response variable!)</li>
</ol>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="#cb453-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(missRanger)</span>
<span id="cb453-2"><a href="#cb453-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb453-3"><a href="#cb453-3" aria-hidden="true" tabindex="-1"></a><span class="co">#wine_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest)</span></span>
<span id="cb453-4"><a href="#cb453-4" aria-hidden="true" tabindex="-1"></a>nasa_imputed <span class="ot">=</span> missRanger<span class="sc">::</span><span class="fu">missRanger</span>(<span class="at">data =</span> nasa <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Hazardous), <span class="at">maxiter =</span> <span class="dv">1</span>, <span class="at">num.trees=</span>5L)</span></code></pre></div>
<pre><code>## 
## Missing value imputation by random forests
## 
##   Variables to impute:       Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox
##   Variables used to impute:  Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox
## iter 1:  .....</code></pre>
<pre><code>## Warning: Dropped unused factor level(s) in dependent variable: 2017-04-06 08:35:59, 2017-04-06 09:06:29, 2017-04-06 09:10:05.</code></pre>
<pre><code>## ..................................</code></pre>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="#cb457-1" aria-hidden="true" tabindex="-1"></a>nasa_imputed<span class="sc">$</span>Hazardous <span class="ot">=</span> nasa<span class="sc">$</span>Hazardous</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Split into training and testing</li>
</ol>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="#cb458-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> nasa_imputed[<span class="sc">!</span><span class="fu">is.na</span>(nasa<span class="sc">$</span>Hazardous), ]</span>
<span id="cb458-2"><a href="#cb458-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> nasa_imputed[<span class="fu">is.na</span>(nasa<span class="sc">$</span>Hazardous), ]</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Train model</li>
</ol>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="#cb459-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb459-2"><a href="#cb459-2" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">=</span> <span class="fu">ranger</span>(Hazardous<span class="sc">~</span>., <span class="at">data =</span> train, <span class="at">classification =</span> <span class="cn">TRUE</span>, <span class="at">probability =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>Predictions</li>
</ol>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="#cb460-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">predict</span>(rf, <span class="at">data =</span> test)<span class="sc">$</span>predictions[,<span class="dv">2</span>]</span>
<span id="cb460-2"><a href="#cb460-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(preds)</span></code></pre></div>
<pre><code>## [1] 0.7266984 0.7744373 0.0015000 0.8139786 0.1496508 0.1784167</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Create submission csv</li>
</ol>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="#cb462-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(<span class="fu">data.frame</span>(<span class="at">y=</span>preds), <span class="at">file =</span> <span class="st">&quot;rf.csv&quot;</span>)</span></code></pre></div>
</div>
<div id="flower" class="section level2" number="7.5">
<h2 number="7.5"><span class="header-section-number">7.5</span> Flower</h2>
<p>A collection of over 4000 flower images of 5 plant species. The dataset is from <a href="https://www.kaggle.com/alxmamaev/flowers-recognition">kaggle</a> but we downsampled the images from <span class="math inline">\(320*240\)</span> to <span class="math inline">\(80*80\)</span> pixels.
You can download the dataset <a href="http://rhsbio7.uni-regensburg.de:8500">here</a>.</p>
<p><strong>Notes:</strong></p>
<ul>
<li>check out CNN notebooks on kaggle (they are often written in python but you can still copy the CNN architectures), e.g. <a href="https://www.kaggle.com/alirazaaliqadri/flower-recognition-tensorflow-keras-sequential">this one</a></li>
<li>Last year’s winner have used a transfer learning approach (they achieved around 70% accuracy), check out this <a href="https://www.kaggle.com/stpeteishii/flower-name-classify-densenet201">notebook</a>, see also the section about transfer learning @ref(transfer)</li>
</ul>
<p><strong>Response variable:</strong> Plant species</p>
<ol style="list-style-type: decimal">
<li>Load dataset
The dataset requires pre-processing (we have to concatenate the train and test images):</li>
</ol>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="#cb463-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb463-2"><a href="#cb463-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb463-3"><a href="#cb463-3" aria-hidden="true" tabindex="-1"></a>data_files <span class="ot">=</span> <span class="fu">list.files</span>(<span class="st">&quot;flower/&quot;</span>, <span class="at">full.names =</span> <span class="cn">TRUE</span>)</span>
<span id="cb463-4"><a href="#cb463-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data_files[<span class="fu">str_detect</span>(data_files, <span class="st">&quot;train&quot;</span>)]</span>
<span id="cb463-5"><a href="#cb463-5" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> <span class="fu">readRDS</span>(<span class="at">file =</span> <span class="st">&quot;flower/test.RDS&quot;</span>)</span>
<span id="cb463-6"><a href="#cb463-6" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> <span class="fu">lapply</span>(train, readRDS)</span>
<span id="cb463-7"><a href="#cb463-7" aria-hidden="true" tabindex="-1"></a>train_classes <span class="ot">=</span> <span class="fu">lapply</span>(train, <span class="cf">function</span>(d) <span class="fu">dim</span>(d)[<span class="dv">1</span>])</span>
<span id="cb463-8"><a href="#cb463-8" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> abind<span class="sc">::</span><span class="fu">abind</span>(train, <span class="at">along =</span> 1L)</span>
<span id="cb463-9"><a href="#cb463-9" aria-hidden="true" tabindex="-1"></a>labels_train <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>, <span class="fu">unlist</span>(train_classes))</span>
<span id="cb463-10"><a href="#cb463-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb463-11"><a href="#cb463-11" aria-hidden="true" tabindex="-1"></a>flower <span class="ot">=</span> EcoData<span class="sc">::</span><span class="fu">dataset_flower</span>()</span>
<span id="cb463-12"><a href="#cb463-12" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> flower<span class="sc">$</span>train</span>
<span id="cb463-13"><a href="#cb463-13" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> flower<span class="sc">$</span>test</span>
<span id="cb463-14"><a href="#cb463-14" aria-hidden="true" tabindex="-1"></a>train_classes <span class="ot">=</span> flower<span class="sc">$</span>labels</span></code></pre></div>
<p>Let’s visualize a flower:</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="#cb464-1" aria-hidden="true" tabindex="-1"></a>train[<span class="dv">100</span>,,,] <span class="sc">%&gt;%</span> </span>
<span id="cb464-2"><a href="#cb464-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">image_to_array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb464-3"><a href="#cb464-3" aria-hidden="true" tabindex="-1"></a> <span class="st">`</span><span class="at">/</span><span class="st">`</span>(., <span class="dv">255</span>) <span class="sc">%&gt;%</span></span>
<span id="cb464-4"><a href="#cb464-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">as.raster</span>() <span class="sc">%&gt;%</span></span>
<span id="cb464-5"><a href="#cb464-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-239-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Build &amp; train model:</li>
</ol>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="#cb465-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb465-2"><a href="#cb465-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb465-3"><a href="#cb465-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 4L, <span class="at">kernel_size =</span> 2L, <span class="at">input_shape =</span> <span class="fu">list</span>( 80L, 80L, 3L)) <span class="sc">%&gt;%</span> </span>
<span id="cb465-4"><a href="#cb465-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb465-5"><a href="#cb465-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb465-6"><a href="#cb465-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 5L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb465-7"><a href="#cb465-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-8"><a href="#cb465-8" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb465-9"><a href="#cb465-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">optimizer =</span> keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="fl">0.01</span>), <span class="at">loss =</span> keras<span class="sc">::</span>loss_categorical_crossentropy)</span>
<span id="cb465-10"><a href="#cb465-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-11"><a href="#cb465-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-12"><a href="#cb465-12" aria-hidden="true" tabindex="-1"></a><span class="do">### Model fitting </span><span class="al">###</span></span>
<span id="cb465-13"><a href="#cb465-13" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 50L</span>
<span id="cb465-14"><a href="#cb465-14" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 25L</span>
<span id="cb465-15"><a href="#cb465-15" aria-hidden="true" tabindex="-1"></a>steps <span class="ot">=</span> <span class="fu">floor</span>(<span class="fu">dim</span>(train)[<span class="dv">1</span>]<span class="sc">/</span>batch_size)</span>
<span id="cb465-16"><a href="#cb465-16" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">flow_images_from_data</span>(<span class="at">x =</span> train<span class="sc">/</span><span class="dv">255</span> , <span class="at">y =</span> keras<span class="sc">::</span><span class="fu">k_one_hot</span>(labels_train, 5L), <span class="at">batch_size =</span> batch_size)</span>
<span id="cb465-17"><a href="#cb465-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb465-18"><a href="#cb465-18" aria-hidden="true" tabindex="-1"></a>optim <span class="ot">=</span> <span class="fu">optimizer_adamax</span>(<span class="fl">0.01</span>)</span>
<span id="cb465-19"><a href="#cb465-19" aria-hidden="true" tabindex="-1"></a>epoch_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb465-20"><a href="#cb465-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>epochs) {</span>
<span id="cb465-21"><a href="#cb465-21" aria-hidden="true" tabindex="-1"></a>  epoch_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb465-22"><a href="#cb465-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>steps) {</span>
<span id="cb465-23"><a href="#cb465-23" aria-hidden="true" tabindex="-1"></a>    batch <span class="ot">=</span> reticulate<span class="sc">::</span><span class="fu">iter_next</span>(generator)</span>
<span id="cb465-24"><a href="#cb465-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">with</span>(tf<span class="sc">$</span><span class="fu">GradientTape</span>() <span class="sc">%as%</span> tape, {</span>
<span id="cb465-25"><a href="#cb465-25" aria-hidden="true" tabindex="-1"></a>        pred <span class="ot">=</span> <span class="fu">model</span>(batch[[<span class="dv">1</span>]])</span>
<span id="cb465-26"><a href="#cb465-26" aria-hidden="true" tabindex="-1"></a>        loss <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">loss_categorical_crossentropy</span>(batch[[<span class="dv">2</span>]], pred)</span>
<span id="cb465-27"><a href="#cb465-27" aria-hidden="true" tabindex="-1"></a>        loss <span class="ot">=</span> tf<span class="sc">$</span><span class="fu">reduce_mean</span>(loss)</span>
<span id="cb465-28"><a href="#cb465-28" aria-hidden="true" tabindex="-1"></a>      })</span>
<span id="cb465-29"><a href="#cb465-29" aria-hidden="true" tabindex="-1"></a>    gradients <span class="ot">=</span> tape<span class="sc">$</span><span class="fu">gradient</span>(<span class="at">target =</span> loss, <span class="at">sources =</span> model<span class="sc">$</span>weights)</span>
<span id="cb465-30"><a href="#cb465-30" aria-hidden="true" tabindex="-1"></a>    optim<span class="sc">$</span><span class="fu">apply_gradients</span>(purrr<span class="sc">::</span><span class="fu">transpose</span>(<span class="fu">list</span>(gradients, model<span class="sc">$</span>weights)))</span>
<span id="cb465-31"><a href="#cb465-31" aria-hidden="true" tabindex="-1"></a>    epoch_loss <span class="ot">=</span> <span class="fu">c</span>(epoch_loss, loss<span class="sc">$</span><span class="fu">numpy</span>())</span>
<span id="cb465-32"><a href="#cb465-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb465-33"><a href="#cb465-33" aria-hidden="true" tabindex="-1"></a>  epoch_losses <span class="ot">=</span> <span class="fu">c</span>(epoch_losses, epoch_loss)</span>
<span id="cb465-34"><a href="#cb465-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Epoch: &quot;</span>, e, <span class="st">&quot; Loss: &quot;</span>, <span class="fu">mean</span>(epoch_losses), <span class="st">&quot; </span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb465-35"><a href="#cb465-35" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Predictions</li>
</ol>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="#cb466-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(test<span class="sc">/</span><span class="dv">255</span>)</span>
<span id="cb466-2"><a href="#cb466-2" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span> <span class="fu">apply</span>(preds, <span class="dv">1</span>, which.max)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb466-3"><a href="#cb466-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(preds)</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Create submission csv</li>
</ol>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="#cb467-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(<span class="fu">data.frame</span>(<span class="at">y=</span>preds), <span class="at">file =</span> <span class="st">&quot;cnn.csv&quot;</span>)</span></code></pre></div>
<!--chapter:end:06-Datasets.Rmd-->
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<!--chapter:end:07-Refs.Rmd-->
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-dormann2018" class="csl-entry">
Dormann, Carsten F, Justin M Calabrese, Gurutzeta Guillera-Arroita, Eleni Matechou, Volker Bahn, Kamil Bartoń, Colin M Beale, et al. 2018. <span>“Model Averaging in Ecology: A Review of Bayesian, Information-Theoretic, and Tactical Approaches for Predictive Inference.”</span> <em>Ecological Monographs</em> 88 (4): 485–504.
</div>
<div id="ref-dropout" class="csl-entry">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>The Journal of Machine Learning Research</em> 15 (1): 1929–58.
</div>
<div id="ref-zou2005" class="csl-entry">
Zou, Hui, and Trevor Hastie. 2005. <span>“Regularization and Variable Selection via the Elastic Net.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.
</div>
</div>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
