<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Deep learning | Machine Learning and AI in TensorFlow and R</title>
  <meta name="description" content="4 Deep learning | Machine Learning and AI in TensorFlow and R" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Deep learning | Machine Learning and AI in TensorFlow and R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="4 Deep learning | Machine Learning and AI in TensorFlow and R" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Deep learning | Machine Learning and AI in TensorFlow and R" />
  
  <meta name="twitter:description" content="4 Deep learning | Machine Learning and AI in TensorFlow and R" />
  

<meta name="author" content="Maximilian Pichler and Florian Hartig" />


<meta name="date" content="2021-05-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fund.html"/>
<link rel="next" href="xAI.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction to Machine Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.1</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="introduction.html"><a href="introduction.html#hierarchical-clustering"><i class="fa fa-check"></i><b>2.1.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction.html"><a href="introduction.html#k-means-clustering"><i class="fa fa-check"></i><b>2.1.2</b> k-means clustering</a></li>
<li class="chapter" data-level="2.1.3" data-path="introduction.html"><a href="introduction.html#density-based-clustering"><i class="fa fa-check"></i><b>2.1.3</b> Density-based clustering</a></li>
<li class="chapter" data-level="2.1.4" data-path="introduction.html"><a href="introduction.html#model-based-clustering"><i class="fa fa-check"></i><b>2.1.4</b> Model-based clustering</a></li>
<li class="chapter" data-level="2.1.5" data-path="introduction.html"><a href="introduction.html#ordination"><i class="fa fa-check"></i><b>2.1.5</b> Ordination</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#supervised-learning-regression-and-classification"><i class="fa fa-check"></i><b>2.2</b> Supervised learning: regression and classification</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction.html"><a href="introduction.html#supervised-regression-using-random-forest"><i class="fa fa-check"></i><b>2.2.1</b> Supervised regression using Random Forest</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction.html"><a href="introduction.html#supervised-classification-using-random-forest"><i class="fa fa-check"></i><b>2.2.2</b> Supervised classification using Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#introduction-to-tensorflow"><i class="fa fa-check"></i><b>2.3</b> Introduction to Tensorflow</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction.html"><a href="introduction.html#tensorflow-data-containers"><i class="fa fa-check"></i><b>2.3.1</b> Tensorflow data containers</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction.html"><a href="introduction.html#tensorflow-data-types---good-practise-with-r-tf"><i class="fa fa-check"></i><b>2.3.2</b> Tensorflow data types - good practise with R-TF</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#introduction-to-pytorch"><i class="fa fa-check"></i><b>2.4</b> Introduction to PyTorch</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#pytorch-data-containers"><i class="fa fa-check"></i><b>2.4.1</b> PyTorch data containers</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction.html"><a href="introduction.html#torch-data-types---good-practise-with-r-tf"><i class="fa fa-check"></i><b>2.4.2</b> Torch data types - good practise with R-TF</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#first-steps-with-the-keras-framework"><i class="fa fa-check"></i><b>2.5</b> First steps with the keras framework</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introduction.html"><a href="introduction.html#example-workflow-in-keras"><i class="fa fa-check"></i><b>2.5.1</b> Example workflow in keras</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fund.html"><a href="fund.html"><i class="fa fa-check"></i><b>3</b> Fundamental principles and techniques</a>
<ul>
<li class="chapter" data-level="3.1" data-path="fund.html"><a href="fund.html#machine-learning-principles"><i class="fa fa-check"></i><b>3.1</b> Machine learning principles</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="fund.html"><a href="fund.html#optimization"><i class="fa fa-check"></i><b>3.1.1</b> Optimization</a></li>
<li class="chapter" data-level="3.1.2" data-path="fund.html"><a href="fund.html#regularization"><i class="fa fa-check"></i><b>3.1.2</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="fund.html"><a href="fund.html#tree-based-ml-algorithms"><i class="fa fa-check"></i><b>3.2</b> Tree-based ML algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="fund.html"><a href="fund.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>3.2.1</b> Classification and Regression Trees</a></li>
<li class="chapter" data-level="3.2.2" data-path="fund.html"><a href="fund.html#random-forest"><i class="fa fa-check"></i><b>3.2.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.2.3" data-path="fund.html"><a href="fund.html#boosted-regression-trees"><i class="fa fa-check"></i><b>3.2.3</b> Boosted regression trees</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="fund.html"><a href="fund.html#distance-based-algorithms"><i class="fa fa-check"></i><b>3.3</b> Distance-based algorithms</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="fund.html"><a href="fund.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>3.3.1</b> k-nearest-neighbor</a></li>
<li class="chapter" data-level="3.3.2" data-path="fund.html"><a href="fund.html#support-vector-machines-svm"><i class="fa fa-check"></i><b>3.3.2</b> Support Vector Machines (SVM)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="fund.html"><a href="fund.html#artificial-neural-networks"><i class="fa fa-check"></i><b>3.4</b> Artificial neural networks</a></li>
<li class="chapter" data-level="3.5" data-path="fund.html"><a href="fund.html#the-standard-ml-pipeline-at-the-example-of-the-titanic-dataset"><i class="fa fa-check"></i><b>3.5</b> The standard ML pipeline at the example of the titanic dataset</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="fund.html"><a href="fund.html#data-cleaning"><i class="fa fa-check"></i><b>3.5.1</b> Data cleaning</a></li>
<li class="chapter" data-level="3.5.2" data-path="fund.html"><a href="fund.html#pre-processing-and-feature-selection"><i class="fa fa-check"></i><b>3.5.2</b> Pre-processing and feature selection</a></li>
<li class="chapter" data-level="3.5.3" data-path="fund.html"><a href="fund.html#split-data-for-training-and-testing"><i class="fa fa-check"></i><b>3.5.3</b> Split data for training and testing</a></li>
<li class="chapter" data-level="3.5.4" data-path="fund.html"><a href="fund.html#model-fitting"><i class="fa fa-check"></i><b>3.5.4</b> Model fitting</a></li>
<li class="chapter" data-level="3.5.5" data-path="fund.html"><a href="fund.html#model-evaluation"><i class="fa fa-check"></i><b>3.5.5</b> Model evaluation</a></li>
<li class="chapter" data-level="3.5.6" data-path="fund.html"><a href="fund.html#predictions-and-submission"><i class="fa fa-check"></i><b>3.5.6</b> Predictions and submission</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="fund.html"><a href="fund.html#mlr"><i class="fa fa-check"></i><b>3.6</b> Bonus - ML pipelines with mlr3</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="fund.html"><a href="fund.html#mlr3---the-basic-workflow"><i class="fa fa-check"></i><b>3.6.1</b> mlr3 - the basic workflow</a></li>
<li class="chapter" data-level="3.6.2" data-path="fund.html"><a href="fund.html#mlr3---hyper-parameter-tuning"><i class="fa fa-check"></i><b>3.6.2</b> mlr3 - hyper-parameter tuning</a></li>
<li class="chapter" data-level="3.6.3" data-path="fund.html"><a href="fund.html#mlr3---hyper-parameter-tuning-with-oversampling"><i class="fa fa-check"></i><b>3.6.3</b> mlr3 - hyper-parameter tuning with oversampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Deep.html"><a href="Deep.html"><i class="fa fa-check"></i><b>4</b> Deep learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Deep.html"><a href="Deep.html#deep-neural-networks"><i class="fa fa-check"></i><b>4.1</b> Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="Deep.html"><a href="Deep.html#dropout-and-early-stopping"><i class="fa fa-check"></i><b>4.1.1</b> Dropout and Early stopping</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="Deep.html"><a href="Deep.html#convolutional-neural-networks---mnist"><i class="fa fa-check"></i><b>4.2</b> Convolutional Neural Networks - MNIST</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="Deep.html"><a href="Deep.html#data-augmentation"><i class="fa fa-check"></i><b>4.2.1</b> Data Augmentation</a></li>
<li class="chapter" data-level="4.2.2" data-path="Deep.html"><a href="Deep.html#transfer"><i class="fa fa-check"></i><b>4.2.2</b> Transfer learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Deep.html"><a href="Deep.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>4.3</b> Natural Language Processing (NLP)</a></li>
<li class="chapter" data-level="4.4" data-path="Deep.html"><a href="Deep.html#recurrent-neural-networks-rnns"><i class="fa fa-check"></i><b>4.4</b> Recurrent neural networks (RNNs)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="xAI.html"><a href="xAI.html"><i class="fa fa-check"></i><b>5</b> Explainable AI (xAI), NLP, and RNNs</a>
<ul>
<li class="chapter" data-level="5.1" data-path="xAI.html"><a href="xAI.html#xai-methods"><i class="fa fa-check"></i><b>5.1</b> xAI Methods</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="xAI.html"><a href="xAI.html#variable-importance"><i class="fa fa-check"></i><b>5.1.1</b> Variable Importance</a></li>
<li class="chapter" data-level="5.1.2" data-path="xAI.html"><a href="xAI.html#partial-dependencies"><i class="fa fa-check"></i><b>5.1.2</b> Partial dependencies</a></li>
<li class="chapter" data-level="5.1.3" data-path="xAI.html"><a href="xAI.html#accumulated-local-effects"><i class="fa fa-check"></i><b>5.1.3</b> Accumulated local effects</a></li>
<li class="chapter" data-level="5.1.4" data-path="xAI.html"><a href="xAI.html#friedmans-h-statistic"><i class="fa fa-check"></i><b>5.1.4</b> Friedmans H-statistic</a></li>
<li class="chapter" data-level="5.1.5" data-path="xAI.html"><a href="xAI.html#global-explainer---simplifying-the-ml-model"><i class="fa fa-check"></i><b>5.1.5</b> Global explainer - Simplifying the ML model</a></li>
<li class="chapter" data-level="5.1.6" data-path="xAI.html"><a href="xAI.html#local-explainer---lime-explaining-single-instances-observations"><i class="fa fa-check"></i><b>5.1.6</b> Local explainer - LIME explaining single instances (observations)</a></li>
<li class="chapter" data-level="5.1.7" data-path="xAI.html"><a href="xAI.html#local-explainer---shapley"><i class="fa fa-check"></i><b>5.1.7</b> Local explainer - Shapley</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html"><i class="fa fa-check"></i><b>6</b> GANs, VAEs, and Reinforcement learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#generative-adversarial-network-gans"><i class="fa fa-check"></i><b>6.1</b> Generative adversarial network (GANs)</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#mnist---gan-based-on-dnns"><i class="fa fa-check"></i><b>6.1.1</b> MNIST - GAN based on DNNs</a></li>
<li class="chapter" data-level="6.1.2" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#flower---gan"><i class="fa fa-check"></i><b>6.1.2</b> Flower - GAN</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#autoencoder"><i class="fa fa-check"></i><b>6.2</b> Autoencoder</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#autoencoder---mnist-cnn"><i class="fa fa-check"></i><b>6.2.1</b> Autoencoder - MNIST CNN</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#varational-autoencoder"><i class="fa fa-check"></i><b>6.3</b> Varational Autoencoder</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i><b>7</b> Datasets</a>
<ul>
<li class="chapter" data-level="7.1" data-path="datasets.html"><a href="datasets.html#titanic"><i class="fa fa-check"></i><b>7.1</b> Titanic</a></li>
<li class="chapter" data-level="7.2" data-path="datasets.html"><a href="datasets.html#plant-pollinator-database"><i class="fa fa-check"></i><b>7.2</b> Plant-pollinator database</a></li>
<li class="chapter" data-level="7.3" data-path="datasets.html"><a href="datasets.html#wine"><i class="fa fa-check"></i><b>7.3</b> Wine</a></li>
<li class="chapter" data-level="7.4" data-path="datasets.html"><a href="datasets.html#nasa"><i class="fa fa-check"></i><b>7.4</b> Nasa</a></li>
<li class="chapter" data-level="7.5" data-path="datasets.html"><a href="datasets.html#flower"><i class="fa fa-check"></i><b>7.5</b> Flower</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning and AI in TensorFlow and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Deep" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Deep learning</h1>
<div id="deep-neural-networks" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Deep Neural Networks</h2>
<p>This lecture today is about deep neural networks. We will first look at regularization in deep neural networks and then explore convolutional neural networks.</p>
<div id="dropout-and-early-stopping" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Dropout and Early stopping</h3>
<p>Regularization in deep neural networks is very important because the problem of overfitting. Standard regularization from statistics like l1 and l2 regularization are often feasy and require a lot of tuning. There are more stable and robust methods:</p>
<ul>
<li>Early stopping: Early stopping allows us to stop the training when for instance the test loss does not increase anymore</li>
<li>Dropout: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Dropout is more robust than l1 and l2, and tuning of the dropout rate can be beneficial but a rate between 0.2-0.5 works often quite well</li>
</ul>
<p><strong>Data preparation</strong></p>
<p>See (mlr) for explanation about the pre-processing pipeline.</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="Deep.html#cb299-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(EcoData)</span>
<span id="cb299-2"><a href="Deep.html#cb299-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb299-3"><a href="Deep.html#cb299-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3)</span>
<span id="cb299-4"><a href="Deep.html#cb299-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3pipelines)</span>
<span id="cb299-5"><a href="Deep.html#cb299-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(nasa)</span>
<span id="cb299-6"><a href="Deep.html#cb299-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(nasa)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    4687 obs. of  40 variables:
##  $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...
##  $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...
##  $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...
##  $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...
##  $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...
##  $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...
##  $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...
##  $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...
##  $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...
##  $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...
##  $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...
##  $ Close.Approach.Date         : Factor w/ 777 levels &quot;1995-01-01&quot;,&quot;1995-01-08&quot;,..: 511 712 472 239 273 145 428 694 87 732 ...
##  $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...
##  $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...
##  $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...
##  $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...
##  $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...
##  $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...
##  $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...
##  $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...
##  $ Orbiting.Body               : Factor w/ 1 level &quot;Earth&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...
##  $ Orbit.Determination.Date    : Factor w/ 2680 levels &quot;2014-06-13 15:20:44&quot;,..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...
##  $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...
##  $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...
##  $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...
##  $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...
##  $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...
##  $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...
##  $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...
##  $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...
##  $ Orbital.Period              : num  419 1040 682 427 503 ...
##  $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...
##  $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...
##  $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...
##  $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...
##  $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...
##  $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...
##  $ Equinox                     : Factor w/ 1 level &quot;J2000&quot;: 1 1 NA 1 1 1 1 1 1 1 ...
##  $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...</code></pre>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="Deep.html#cb301-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> nasa <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Orbit.Determination.Date, <span class="sc">-</span>Close.Approach.Date, <span class="sc">-</span>Name, <span class="sc">-</span>Neo.Reference.ID)</span>
<span id="cb301-2"><a href="Deep.html#cb301-2" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>Hazardous <span class="ot">=</span> <span class="fu">as.factor</span>(data<span class="sc">$</span>Hazardous)</span>
<span id="cb301-3"><a href="Deep.html#cb301-3" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> TaskClassif<span class="sc">$</span><span class="fu">new</span>(<span class="at">id =</span> <span class="st">&quot;nasa&quot;</span>, <span class="at">backend =</span> data, <span class="at">target =</span> <span class="st">&quot;Hazardous&quot;</span>, <span class="at">positive =</span> <span class="st">&quot;1&quot;</span>)</span>
<span id="cb301-4"><a href="Deep.html#cb301-4" aria-hidden="true" tabindex="-1"></a>preprocessing <span class="ot">=</span> <span class="fu">po</span>(<span class="st">&quot;imputeoor&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">po</span>(<span class="st">&quot;scale&quot;</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">po</span>(<span class="st">&quot;encode&quot;</span>) </span>
<span id="cb301-5"><a href="Deep.html#cb301-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> preprocessing<span class="sc">$</span><span class="fu">train</span>(task)[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span>
<span id="cb301-6"><a href="Deep.html#cb301-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-7"><a href="Deep.html#cb301-7" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[<span class="sc">!</span><span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous),]</span>
<span id="cb301-8"><a href="Deep.html#cb301-8" aria-hidden="true" tabindex="-1"></a>submit <span class="ot">=</span> data[<span class="fu">is.na</span>(data<span class="sc">$</span>Hazardous),]</span>
<span id="cb301-9"><a href="Deep.html#cb301-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-10"><a href="Deep.html#cb301-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(train <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>Hazardous))</span>
<span id="cb301-11"><a href="Deep.html#cb301-11" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> train <span class="sc">%&gt;%</span> <span class="fu">select</span>(Hazardous)</span>
<span id="cb301-12"><a href="Deep.html#cb301-12" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">to_categorical</span>(<span class="fu">as.matrix</span>(Y), <span class="dv">2</span>)</span></code></pre></div>
<p><strong>Early stopping</strong></p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="Deep.html#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb302-2"><a href="Deep.html#cb302-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb302-3"><a href="Deep.html#cb302-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb302-4"><a href="Deep.html#cb302-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb302-5"><a href="Deep.html#cb302-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X)) <span class="sc">%&gt;%</span></span>
<span id="cb302-6"><a href="Deep.html#cb302-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb302-7"><a href="Deep.html#cb302-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb302-8"><a href="Deep.html#cb302-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>) </span>
<span id="cb302-9"><a href="Deep.html#cb302-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb302-10"><a href="Deep.html#cb302-10" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb302-11"><a href="Deep.html#cb302-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb302-12"><a href="Deep.html#cb302-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_5&quot;
## _________________________________________________________________________________________________________________
## Layer (type)                                      Output Shape                                  Param #          
## =================================================================================================================
## dense_14 (Dense)                                  (None, 50)                                    1900             
## _________________________________________________________________________________________________________________
## dense_15 (Dense)                                  (None, 50)                                    2550             
## _________________________________________________________________________________________________________________
## dense_16 (Dense)                                  (None, 50)                                    2550             
## _________________________________________________________________________________________________________________
## dense_17 (Dense)                                  (None, 2)                                     102              
## =================================================================================================================
## Total params: 7,102
## Trainable params: 7,102
## Non-trainable params: 0
## _________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="Deep.html#cb304-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb304-2"><a href="Deep.html#cb304-2" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb304-3"><a href="Deep.html#cb304-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb304-4"><a href="Deep.html#cb304-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 50L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb304-5"><a href="Deep.html#cb304-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>)</span>
<span id="cb304-6"><a href="Deep.html#cb304-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-141-1.png" width="672" /></p>
<p>The validation loss first decreases but then starts to increase again, can you explain this behavior?
-&gt; Overfitting!</p>
<p>Let’s try a l1+l2 regularization:</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="Deep.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb306-2"><a href="Deep.html#cb306-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-3"><a href="Deep.html#cb306-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb306-4"><a href="Deep.html#cb306-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb306-5"><a href="Deep.html#cb306-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X), <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>( <span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb306-6"><a href="Deep.html#cb306-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb306-7"><a href="Deep.html#cb306-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb306-8"><a href="Deep.html#cb306-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) </span>
<span id="cb306-9"><a href="Deep.html#cb306-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-10"><a href="Deep.html#cb306-10" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb306-11"><a href="Deep.html#cb306-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb306-12"><a href="Deep.html#cb306-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_6&quot;
## _________________________________________________________________________________________________________________
## Layer (type)                                      Output Shape                                  Param #          
## =================================================================================================================
## dense_18 (Dense)                                  (None, 50)                                    1900             
## _________________________________________________________________________________________________________________
## dense_19 (Dense)                                  (None, 50)                                    2550             
## _________________________________________________________________________________________________________________
## dense_20 (Dense)                                  (None, 50)                                    2550             
## _________________________________________________________________________________________________________________
## dense_21 (Dense)                                  (None, 2)                                     102              
## =================================================================================================================
## Total params: 7,102
## Trainable params: 7,102
## Non-trainable params: 0
## _________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="Deep.html#cb308-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb308-2"><a href="Deep.html#cb308-2" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb308-3"><a href="Deep.html#cb308-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb308-4"><a href="Deep.html#cb308-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb308-5"><a href="Deep.html#cb308-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>)</span>
<span id="cb308-6"><a href="Deep.html#cb308-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-142-1.png" width="672" />
Better, but the validation loss still starts to increase after 40 epochs. But we can use early stopping to end the training before the val loss starts to increase again!</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="Deep.html#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb310-2"><a href="Deep.html#cb310-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb310-3"><a href="Deep.html#cb310-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb310-4"><a href="Deep.html#cb310-4" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb310-5"><a href="Deep.html#cb310-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X), <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>( <span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb310-6"><a href="Deep.html#cb310-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb310-7"><a href="Deep.html#cb310-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb310-8"><a href="Deep.html#cb310-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1_l2</span>(<span class="fl">0.001</span>, <span class="fl">0.001</span>)) </span>
<span id="cb310-9"><a href="Deep.html#cb310-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb310-10"><a href="Deep.html#cb310-10" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb310-11"><a href="Deep.html#cb310-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb310-12"><a href="Deep.html#cb310-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_7&quot;
## _________________________________________________________________________________________________________________
## Layer (type)                                      Output Shape                                  Param #          
## =================================================================================================================
## dense_22 (Dense)                                  (None, 50)                                    1900             
## _________________________________________________________________________________________________________________
## dense_23 (Dense)                                  (None, 50)                                    2550             
## _________________________________________________________________________________________________________________
## dense_24 (Dense)                                  (None, 50)                                    2550             
## _________________________________________________________________________________________________________________
## dense_25 (Dense)                                  (None, 2)                                     102              
## =================================================================================================================
## Total params: 7,102
## Trainable params: 7,102
## Non-trainable params: 0
## _________________________________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="Deep.html#cb312-1" aria-hidden="true" tabindex="-1"></a>early <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">callback_early_stopping</span>(<span class="at">patience =</span> 5L)</span>
<span id="cb312-2"><a href="Deep.html#cb312-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb312-3"><a href="Deep.html#cb312-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb312-4"><a href="Deep.html#cb312-4" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb312-5"><a href="Deep.html#cb312-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb312-6"><a href="Deep.html#cb312-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb312-7"><a href="Deep.html#cb312-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>, <span class="at">callbacks=</span><span class="fu">c</span>(early))</span>
<span id="cb312-8"><a href="Deep.html#cb312-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-143-1.png" width="672" />
Patience is the number of epochs to wait before aborting the training.</p>
<p><strong>Dropout - another type of regularization</strong></p>
<p><span class="citation"><a href="#ref-dropout" role="doc-biblioref">Srivastava et al.</a> (<a href="#ref-dropout" role="doc-biblioref">2014</a>)</span> suggests a dropout rate of 50% for internal hidden layers and 20% for the input layer. One advantage of dropout is that the training is more independent of the number of epochs i.e. the val loss usually doesn’t start to increase after several epochs.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="Deep.html#cb314-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb314-2"><a href="Deep.html#cb314-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb314-3"><a href="Deep.html#cb314-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb314-4"><a href="Deep.html#cb314-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">ncol</span>(X)) <span class="sc">%&gt;%</span></span>
<span id="cb314-5"><a href="Deep.html#cb314-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb314-6"><a href="Deep.html#cb314-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb314-7"><a href="Deep.html#cb314-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb314-8"><a href="Deep.html#cb314-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 50L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb314-9"><a href="Deep.html#cb314-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="fl">0.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb314-10"><a href="Deep.html#cb314-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="fu">ncol</span>(Y), <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>) </span>
<span id="cb314-11"><a href="Deep.html#cb314-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-12"><a href="Deep.html#cb314-12" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb314-13"><a href="Deep.html#cb314-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.001</span>))</span>
<span id="cb314-14"><a href="Deep.html#cb314-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-15"><a href="Deep.html#cb314-15" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb314-16"><a href="Deep.html#cb314-16" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb314-17"><a href="Deep.html#cb314-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, </span>
<span id="cb314-18"><a href="Deep.html#cb314-18" aria-hidden="true" tabindex="-1"></a>        <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, </span>
<span id="cb314-19"><a href="Deep.html#cb314-19" aria-hidden="true" tabindex="-1"></a>        <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split=</span><span class="fl">0.4</span>)</span>
<span id="cb314-20"><a href="Deep.html#cb314-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-144-1.png" width="672" />
Ofc, you can still combine early stopping and dropout, which is normally a good idea since it improves training efficiency (e.g. you could start with 1000 epochs and you know training will be aborted if it doesn’t improve anymore).</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Dropout and early stopping with torch:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="Deep.html#cb316-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">nn_sequential</span>(</span>
<span id="cb316-2"><a href="Deep.html#cb316-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.2</span>),</span>
<span id="cb316-3"><a href="Deep.html#cb316-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(<span class="fu">ncol</span>(X), 50L),</span>
<span id="cb316-4"><a href="Deep.html#cb316-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(),</span>
<span id="cb316-5"><a href="Deep.html#cb316-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.5</span>),</span>
<span id="cb316-6"><a href="Deep.html#cb316-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(50L, 50L),</span>
<span id="cb316-7"><a href="Deep.html#cb316-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(), </span>
<span id="cb316-8"><a href="Deep.html#cb316-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.5</span>),</span>
<span id="cb316-9"><a href="Deep.html#cb316-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(50L, 50L),</span>
<span id="cb316-10"><a href="Deep.html#cb316-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_relu</span>(), </span>
<span id="cb316-11"><a href="Deep.html#cb316-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_dropout</span>(<span class="fl">0.5</span>),</span>
<span id="cb316-12"><a href="Deep.html#cb316-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nn_linear</span>(50L, 2L)</span>
<span id="cb316-13"><a href="Deep.html#cb316-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb316-14"><a href="Deep.html#cb316-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-15"><a href="Deep.html#cb316-15" aria-hidden="true" tabindex="-1"></a>YT <span class="ot">=</span> <span class="fu">apply</span>(Y, <span class="dv">1</span>,which.max)</span>
<span id="cb316-16"><a href="Deep.html#cb316-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-17"><a href="Deep.html#cb316-17" aria-hidden="true" tabindex="-1"></a>dataset_nasa <span class="ot">=</span> <span class="fu">dataset</span>(</span>
<span id="cb316-18"><a href="Deep.html#cb316-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">name =</span> <span class="st">&quot;nasa&quot;</span>,</span>
<span id="cb316-19"><a href="Deep.html#cb316-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(nasa) {</span>
<span id="cb316-20"><a href="Deep.html#cb316-20" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>X <span class="ot">=</span> nasa<span class="sc">$</span>X</span>
<span id="cb316-21"><a href="Deep.html#cb316-21" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>Y <span class="ot">=</span> nasa<span class="sc">$</span>Y</span>
<span id="cb316-22"><a href="Deep.html#cb316-22" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb316-23"><a href="Deep.html#cb316-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">.getitem =</span> <span class="cf">function</span>(i) {</span>
<span id="cb316-24"><a href="Deep.html#cb316-24" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> self<span class="sc">$</span>X[i,,drop<span class="ot">=</span><span class="cn">FALSE</span>] <span class="sc">%&gt;%</span> <span class="fu">torch_tensor</span>()</span>
<span id="cb316-25"><a href="Deep.html#cb316-25" aria-hidden="true" tabindex="-1"></a>    Y <span class="ot">=</span> self<span class="sc">$</span>Y[i] <span class="sc">%&gt;%</span> <span class="fu">torch_tensor</span>()</span>
<span id="cb316-26"><a href="Deep.html#cb316-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(X, Y)</span>
<span id="cb316-27"><a href="Deep.html#cb316-27" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb316-28"><a href="Deep.html#cb316-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">.length =</span> <span class="cf">function</span>() {</span>
<span id="cb316-29"><a href="Deep.html#cb316-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nrow</span>(self<span class="sc">$</span>X)</span>
<span id="cb316-30"><a href="Deep.html#cb316-30" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb316-31"><a href="Deep.html#cb316-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-32"><a href="Deep.html#cb316-32" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(<span class="fu">dataset_nasa</span>(<span class="fu">list</span>(<span class="at">X =</span> X[<span class="dv">1</span><span class="sc">:</span><span class="dv">400</span>,], <span class="at">Y =</span> YT[<span class="dv">1</span><span class="sc">:</span><span class="dv">400</span>])), </span>
<span id="cb316-33"><a href="Deep.html#cb316-33" aria-hidden="true" tabindex="-1"></a>                      <span class="at">batch_size =</span> <span class="dv">32</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb316-34"><a href="Deep.html#cb316-34" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>( <span class="fu">dataset_nasa</span>(<span class="fu">list</span>(<span class="at">X =</span> X[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>,], <span class="at">Y =</span> YT[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>])), </span>
<span id="cb316-35"><a href="Deep.html#cb316-35" aria-hidden="true" tabindex="-1"></a>                      <span class="at">batch_size =</span> <span class="dv">32</span>)</span>
<span id="cb316-36"><a href="Deep.html#cb316-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-37"><a href="Deep.html#cb316-37" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">train</span>()</span>
<span id="cb316-38"><a href="Deep.html#cb316-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-39"><a href="Deep.html#cb316-39" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(model_torch<span class="sc">$</span>parameters, <span class="fl">0.01</span>)</span>
<span id="cb316-40"><a href="Deep.html#cb316-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-41"><a href="Deep.html#cb316-41" aria-hidden="true" tabindex="-1"></a>train_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb316-42"><a href="Deep.html#cb316-42" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb316-43"><a href="Deep.html#cb316-43" aria-hidden="true" tabindex="-1"></a>early_epoch <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb316-44"><a href="Deep.html#cb316-44" aria-hidden="true" tabindex="-1"></a>min_loss <span class="ot">=</span> <span class="cn">Inf</span></span>
<span id="cb316-45"><a href="Deep.html#cb316-45" aria-hidden="true" tabindex="-1"></a>patience <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb316-46"><a href="Deep.html#cb316-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb316-47"><a href="Deep.html#cb316-47" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb316-48"><a href="Deep.html#cb316-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(early_epoch <span class="sc">&gt;=</span> patience) <span class="cf">break</span></span>
<span id="cb316-49"><a href="Deep.html#cb316-49" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb316-50"><a href="Deep.html#cb316-50" aria-hidden="true" tabindex="-1"></a>  train_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb316-51"><a href="Deep.html#cb316-51" aria-hidden="true" tabindex="-1"></a>  test_loss <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb316-52"><a href="Deep.html#cb316-52" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb316-53"><a href="Deep.html#cb316-53" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb316-54"><a href="Deep.html#cb316-54" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">squeeze</span>())</span>
<span id="cb316-55"><a href="Deep.html#cb316-55" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]]<span class="sc">$</span><span class="fu">squeeze</span>(),<span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb316-56"><a href="Deep.html#cb316-56" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb316-57"><a href="Deep.html#cb316-57" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb316-58"><a href="Deep.html#cb316-58" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="ot">=</span> <span class="fu">c</span>(train_loss, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb316-59"><a href="Deep.html#cb316-59" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb316-60"><a href="Deep.html#cb316-60" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb316-61"><a href="Deep.html#cb316-61" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> test_dl) {</span>
<span id="cb316-62"><a href="Deep.html#cb316-62" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">squeeze</span>())</span>
<span id="cb316-63"><a href="Deep.html#cb316-63" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]]<span class="sc">$</span><span class="fu">squeeze</span>(),<span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb316-64"><a href="Deep.html#cb316-64" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="ot">=</span> <span class="fu">c</span>(test_loss, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb316-65"><a href="Deep.html#cb316-65" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb316-66"><a href="Deep.html#cb316-66" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb316-67"><a href="Deep.html#cb316-67" aria-hidden="true" tabindex="-1"></a>  <span class="do">### early stopping </span><span class="al">###</span></span>
<span id="cb316-68"><a href="Deep.html#cb316-68" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">mean</span>(test_loss) <span class="sc">&lt;</span> min_loss) {</span>
<span id="cb316-69"><a href="Deep.html#cb316-69" aria-hidden="true" tabindex="-1"></a>    min_loss <span class="ot">=</span> <span class="fu">mean</span>(test_loss)</span>
<span id="cb316-70"><a href="Deep.html#cb316-70" aria-hidden="true" tabindex="-1"></a>    early_epoch <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb316-71"><a href="Deep.html#cb316-71" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb316-72"><a href="Deep.html#cb316-72" aria-hidden="true" tabindex="-1"></a>    early_epoch <span class="ot">=</span> early_epoch <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb316-73"><a href="Deep.html#cb316-73" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb316-74"><a href="Deep.html#cb316-74" aria-hidden="true" tabindex="-1"></a>  <span class="do">###</span></span>
<span id="cb316-75"><a href="Deep.html#cb316-75" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb316-76"><a href="Deep.html#cb316-76" aria-hidden="true" tabindex="-1"></a>  train_losses <span class="ot">=</span> <span class="fu">c</span>(train_losses, <span class="fu">mean</span>(train_loss))</span>
<span id="cb316-77"><a href="Deep.html#cb316-77" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, <span class="fu">mean</span>(test_loss))</span>
<span id="cb316-78"><a href="Deep.html#cb316-78" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, epoch, <span class="fu">mean</span>(train_loss)))</span>
<span id="cb316-79"><a href="Deep.html#cb316-79" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 0.579769
## Loss at epoch 2: 0.502900
## Loss at epoch 3: 0.472404
## Loss at epoch 4: 0.439914
## Loss at epoch 5: 0.431646
## Loss at epoch 6: 0.401007
## Loss at epoch 7: 0.427694
## Loss at epoch 8: 0.412927
## Loss at epoch 9: 0.454472
## Loss at epoch 10: 0.446800</code></pre>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="Deep.html#cb318-1" aria-hidden="true" tabindex="-1"></a><span class="fu">matplot</span>(<span class="fu">cbind</span>(train_losses, test_losses), <span class="at">type =</span> <span class="st">&quot;o&quot;</span>, <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">15</span>, <span class="dv">16</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkblue&quot;</span>, <span class="st">&quot;darkred&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">xlab =</span> <span class="st">&quot;Epoch&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Loss&quot;</span>, <span class="at">las =</span> <span class="dv">1</span>)</span>
<span id="cb318-2"><a href="Deep.html#cb318-2" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkblue&quot;</span>, <span class="st">&quot;darkred&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">15</span>, <span class="dv">16</span>), <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Train loss&quot;</span>, <span class="st">&quot;Val loss&quot;</span>) )</span></code></pre></div>
<img src="_main_files/figure-html/unnamed-chunk-145-1.png" width="672" />
</details>
<p><br/></p>
</div>
</div>
<div id="convolutional-neural-networks---mnist" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Convolutional Neural Networks - MNIST</h2>
<p>We will show the use of convolutinal neural networks with the MNIST dataset.The MNIST dataset is maybe one of the most famous image datasets. It is a dataset of 60,000 handwritten digits from 0-9.</p>
<p>To do so, we define a few helper functions:</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="Deep.html#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb319-2"><a href="Deep.html#cb319-2" aria-hidden="true" tabindex="-1"></a>rotate <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, rev))</span>
<span id="cb319-3"><a href="Deep.html#cb319-3" aria-hidden="true" tabindex="-1"></a>imgPlot <span class="ot">=</span> <span class="cf">function</span>(img, <span class="at">title =</span> <span class="st">&quot;&quot;</span>){</span>
<span id="cb319-4"><a href="Deep.html#cb319-4" aria-hidden="true" tabindex="-1"></a> col<span class="ot">=</span><span class="fu">grey.colors</span>(<span class="dv">255</span>)</span>
<span id="cb319-5"><a href="Deep.html#cb319-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">image</span>(<span class="fu">rotate</span>(img), <span class="at">col =</span> col, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">axes=</span><span class="cn">FALSE</span>, <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Label: &quot;</span>, <span class="fu">as.character</span>(title)))</span>
<span id="cb319-6"><a href="Deep.html#cb319-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The dataset is so famous that there is an automatic download function in keras:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="Deep.html#cb320-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb320-2"><a href="Deep.html#cb320-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data<span class="sc">$</span>train</span>
<span id="cb320-3"><a href="Deep.html#cb320-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data<span class="sc">$</span>test</span></code></pre></div>
<p>Let’s visualize a few digits:</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="Deep.html#cb321-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb321-2"><a href="Deep.html#cb321-2" aria-hidden="true" tabindex="-1"></a>.n <span class="ot">=</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="cf">function</span>(x) <span class="fu">imgPlot</span>(train<span class="sc">$</span>x[x,,], train<span class="sc">$</span>y[x]))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-148-1.png" width="672" /></p>
<p>Similar to the normal ML workflow, we have to scale the pixels (from 0-255) to the range of [0,1] and one hot encode the response. To scale the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2d array/tensor is typically called a matrix.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="Deep.html#cb322-1" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>(train<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train<span class="sc">$</span>x), <span class="dv">1</span>))</span>
<span id="cb322-2"><a href="Deep.html#cb322-2" aria-hidden="true" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">array</span>(test<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(test<span class="sc">$</span>x), <span class="dv">1</span>))</span>
<span id="cb322-3"><a href="Deep.html#cb322-3" aria-hidden="true" tabindex="-1"></a>train_y <span class="ot">=</span> <span class="fu">to_categorical</span>(train<span class="sc">$</span>y, <span class="dv">10</span>)</span>
<span id="cb322-4"><a href="Deep.html#cb322-4" aria-hidden="true" tabindex="-1"></a>test_y <span class="ot">=</span> <span class="fu">to_categorical</span>(test<span class="sc">$</span>y, <span class="dv">10</span>)</span></code></pre></div>
<p>The last dimension stands for the number of channels in the image. In our case we have only one channel because the images are white-black.</p>
<p>Normally we would have three channels - colors are encoded by the combination of three base colors (usually red,green,blue).</p>
<p>To build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size 2x2. These are 2D kernels because our images are 2D. For movies for example, one would use a 3D kernel (the third dimension would correspond to time and not to the color channels).</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="Deep.html#cb323-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb323-2"><a href="Deep.html#cb323-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb323-3"><a href="Deep.html#cb323-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d</span>(<span class="at">input_shape =</span> <span class="fu">c</span>(28L, 28L,1L),<span class="at">filters =</span> 16L, <span class="at">kernel_size =</span> <span class="fu">c</span>(2L,2L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb323-4"><a href="Deep.html#cb323-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb323-5"><a href="Deep.html#cb323-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_conv_2d</span>(<span class="at">filters =</span> 16L, <span class="at">kernel_size =</span> <span class="fu">c</span>(3L,3L), <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb323-6"><a href="Deep.html#cb323-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_max_pooling_2d</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb323-7"><a href="Deep.html#cb323-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_flatten</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb323-8"><a href="Deep.html#cb323-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb323-9"><a href="Deep.html#cb323-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(10L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb323-10"><a href="Deep.html#cb323-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_9&quot;
## _________________________________________________________________________________________________________________
## Layer (type)                                      Output Shape                                  Param #          
## =================================================================================================================
## conv2d (Conv2D)                                   (None, 27, 27, 16)                            80               
## _________________________________________________________________________________________________________________
## max_pooling2d (MaxPooling2D)                      (None, 13, 13, 16)                            0                
## _________________________________________________________________________________________________________________
## conv2d_1 (Conv2D)                                 (None, 11, 11, 16)                            2320             
## _________________________________________________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)                    (None, 5, 5, 16)                              0                
## _________________________________________________________________________________________________________________
## flatten (Flatten)                                 (None, 400)                                   0                
## _________________________________________________________________________________________________________________
## dense_30 (Dense)                                  (None, 100)                                   40100            
## _________________________________________________________________________________________________________________
## dense_31 (Dense)                                  (None, 10)                                    1010             
## =================================================================================================================
## Total params: 43,510
## Trainable params: 43,510
## Non-trainable params: 0
## _________________________________________________________________________________________________________________</code></pre>
<p>We additionally used a pooling layer to downsize the resulting feature maps. After another convolutional and pooling layer we flatten the output, i.e. the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all weights from the last feature maps).Having flattened the layer, we can simply use our typical output layer.</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Prepare/download data:</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="Deep.html#cb325-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb325-2"><a href="Deep.html#cb325-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torchvision)</span>
<span id="cb325-3"><a href="Deep.html#cb325-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb325-4"><a href="Deep.html#cb325-4" aria-hidden="true" tabindex="-1"></a>train_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb325-5"><a href="Deep.html#cb325-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;.&quot;</span>,</span>
<span id="cb325-6"><a href="Deep.html#cb325-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb325-7"><a href="Deep.html#cb325-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> <span class="cn">TRUE</span>,</span>
<span id="cb325-8"><a href="Deep.html#cb325-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">transform =</span> transform_to_tensor</span>
<span id="cb325-9"><a href="Deep.html#cb325-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## Processing...</code></pre>
<pre><code>## Done!</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="Deep.html#cb328-1" aria-hidden="true" tabindex="-1"></a>test_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(</span>
<span id="cb328-2"><a href="Deep.html#cb328-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;.&quot;</span>,</span>
<span id="cb328-3"><a href="Deep.html#cb328-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">download =</span> <span class="cn">TRUE</span>,</span>
<span id="cb328-4"><a href="Deep.html#cb328-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> <span class="cn">FALSE</span>,</span>
<span id="cb328-5"><a href="Deep.html#cb328-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">transform =</span> transform_to_tensor</span>
<span id="cb328-6"><a href="Deep.html#cb328-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Build dataloader:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="Deep.html#cb329-1" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(train_ds, <span class="at">batch_size =</span> <span class="dv">32</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb329-2"><a href="Deep.html#cb329-2" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> <span class="dv">32</span>)</span>
<span id="cb329-3"><a href="Deep.html#cb329-3" aria-hidden="true" tabindex="-1"></a>first_batch <span class="ot">=</span> train_dl<span class="sc">$</span><span class="fu">.iter</span>()</span>
<span id="cb329-4"><a href="Deep.html#cb329-4" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> first_batch<span class="sc">$</span><span class="fu">.next</span>()</span>
<span id="cb329-5"><a href="Deep.html#cb329-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb329-6"><a href="Deep.html#cb329-6" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>x<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32  1 28 28</code></pre>
<p>Build CNN:
We have here to calculate the shapes of our layers on our own:</p>
<p><strong>We start with our input of shape (batch_size, 1, 28, 28)</strong></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="Deep.html#cb331-1" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> df<span class="sc">$</span>x</span>
<span id="cb331-2"><a href="Deep.html#cb331-2" aria-hidden="true" tabindex="-1"></a>sample<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32  1 28 28</code></pre>
<p><strong>first conv layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2)</strong></p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="Deep.html#cb333-1" aria-hidden="true" tabindex="-1"></a>conv1 <span class="ot">=</span> <span class="fu">nn_conv2d</span>(<span class="dv">1</span>, 16L, 2L, <span class="at">stride =</span> 1L)</span>
<span id="cb333-2"><a href="Deep.html#cb333-2" aria-hidden="true" tabindex="-1"></a>(sample <span class="sc">%&gt;%</span> conv1)<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32 16 27 27</code></pre>
<p>Output: batch_size = 32, number of feature maps = 16, dimensions of each feature map = ( 27 , 27 )
Wit a kernel size of two and stride =1 we wil lose one pixel in each dimension…
Questions:</p>
<ul>
<li>what does happen if we increase the stride?</li>
<li>what does happen if we increase the kernel size?</li>
</ul>
<p><strong>pooling layer summarizes each feature map</strong></p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="Deep.html#cb335-1" aria-hidden="true" tabindex="-1"></a>(sample <span class="sc">%&gt;%</span> conv1 <span class="sc">%&gt;%</span> <span class="fu">nnf_max_pool2d</span>(<span class="at">kernel_size =</span> 2L,<span class="at">stride =</span> 2L))<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32 16 13 13</code></pre>
<p>kernel_size = 2L and stride = 2L halfs the pixel dimensions of our image</p>
<p><strong>fully connected layer</strong></p>
<p>Now we have to flatten our final output of the CNN model to use a normal fully connected layer, but to do so we have to calulate the number of inputs for the fully connected layer:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="Deep.html#cb337-1" aria-hidden="true" tabindex="-1"></a>dims <span class="ot">=</span> (sample <span class="sc">%&gt;%</span> conv1 <span class="sc">%&gt;%</span> <span class="fu">nnf_max_pool2d</span>(<span class="at">kernel_size =</span> 2L,<span class="at">stride =</span> 2L))<span class="sc">$</span><span class="fu">size</span>()</span>
<span id="cb337-2"><a href="Deep.html#cb337-2" aria-hidden="true" tabindex="-1"></a><span class="co"># without the batch size ofc</span></span>
<span id="cb337-3"><a href="Deep.html#cb337-3" aria-hidden="true" tabindex="-1"></a>final <span class="ot">=</span> <span class="fu">prod</span>(dims[<span class="sc">-</span><span class="dv">1</span>]) </span>
<span id="cb337-4"><a href="Deep.html#cb337-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(final)</span></code></pre></div>
<pre><code>## [1] 2704</code></pre>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="Deep.html#cb339-1" aria-hidden="true" tabindex="-1"></a>fc <span class="ot">=</span> <span class="fu">nn_linear</span>(final, 10L)</span>
<span id="cb339-2"><a href="Deep.html#cb339-2" aria-hidden="true" tabindex="-1"></a>(sample <span class="sc">%&gt;%</span> conv1 <span class="sc">%&gt;%</span> <span class="fu">nnf_max_pool2d</span>(<span class="at">kernel_size =</span> 2L,<span class="at">stride =</span> 2L) <span class="sc">%&gt;%</span> <span class="fu">torch_flatten</span>(<span class="at">start_dim =</span> 2L) <span class="sc">%&gt;%</span> fc)<span class="sc">$</span><span class="fu">size</span>()</span></code></pre></div>
<pre><code>## [1] 32 10</code></pre>
<p>Build the network:</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="Deep.html#cb341-1" aria-hidden="true" tabindex="-1"></a>net <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(</span>
<span id="cb341-2"><a href="Deep.html#cb341-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;mnist&quot;</span>,</span>
<span id="cb341-3"><a href="Deep.html#cb341-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb341-4"><a href="Deep.html#cb341-4" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>conv1 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(<span class="dv">1</span>, 16L, 2L)</span>
<span id="cb341-5"><a href="Deep.html#cb341-5" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>conv2 <span class="ot">&lt;-</span> <span class="fu">nn_conv2d</span>(16L, 16L, <span class="dv">3</span>)</span>
<span id="cb341-6"><a href="Deep.html#cb341-6" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>fc1 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(400L, 100L)</span>
<span id="cb341-7"><a href="Deep.html#cb341-7" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>fc2 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(100L, 10L)</span>
<span id="cb341-8"><a href="Deep.html#cb341-8" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb341-9"><a href="Deep.html#cb341-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb341-10"><a href="Deep.html#cb341-10" aria-hidden="true" tabindex="-1"></a>    x <span class="sc">%&gt;%</span> </span>
<span id="cb341-11"><a href="Deep.html#cb341-11" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">conv1</span>() <span class="sc">%&gt;%</span></span>
<span id="cb341-12"><a href="Deep.html#cb341-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb341-13"><a href="Deep.html#cb341-13" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_max_pool2d</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span>         </span>
<span id="cb341-14"><a href="Deep.html#cb341-14" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">conv2</span>() <span class="sc">%&gt;%</span></span>
<span id="cb341-15"><a href="Deep.html#cb341-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb341-16"><a href="Deep.html#cb341-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_max_pool2d</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb341-17"><a href="Deep.html#cb341-17" aria-hidden="true" tabindex="-1"></a>      <span class="fu">torch_flatten</span>(<span class="at">start_dim =</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb341-18"><a href="Deep.html#cb341-18" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">fc1</span>() <span class="sc">%&gt;%</span></span>
<span id="cb341-19"><a href="Deep.html#cb341-19" aria-hidden="true" tabindex="-1"></a>      <span class="fu">nnf_relu</span>() <span class="sc">%&gt;%</span></span>
<span id="cb341-20"><a href="Deep.html#cb341-20" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">fc2</span>()</span>
<span id="cb341-21"><a href="Deep.html#cb341-21" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb341-22"><a href="Deep.html#cb341-22" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</details>
<p><br/></p>
<p>The rest is as usual: First we compile the model.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="Deep.html#cb342-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb342-2"><a href="Deep.html#cb342-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(</span>
<span id="cb342-3"><a href="Deep.html#cb342-3" aria-hidden="true" tabindex="-1"></a> <span class="at">optimizer =</span> keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="fl">0.01</span>),</span>
<span id="cb342-4"><a href="Deep.html#cb342-4" aria-hidden="true" tabindex="-1"></a> <span class="at">loss =</span> loss_categorical_crossentropy</span>
<span id="cb342-5"><a href="Deep.html#cb342-5" aria-hidden="true" tabindex="-1"></a> )</span>
<span id="cb342-6"><a href="Deep.html#cb342-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_9&quot;
## _________________________________________________________________________________________________________________
## Layer (type)                                      Output Shape                                  Param #          
## =================================================================================================================
## conv2d (Conv2D)                                   (None, 27, 27, 16)                            80               
## _________________________________________________________________________________________________________________
## max_pooling2d (MaxPooling2D)                      (None, 13, 13, 16)                            0                
## _________________________________________________________________________________________________________________
## conv2d_1 (Conv2D)                                 (None, 11, 11, 16)                            2320             
## _________________________________________________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)                    (None, 5, 5, 16)                              0                
## _________________________________________________________________________________________________________________
## flatten (Flatten)                                 (None, 400)                                   0                
## _________________________________________________________________________________________________________________
## dense_30 (Dense)                                  (None, 100)                                   40100            
## _________________________________________________________________________________________________________________
## dense_31 (Dense)                                  (None, 10)                                    1010             
## =================================================================================================================
## Total params: 43,510
## Trainable params: 43,510
## Non-trainable params: 0
## _________________________________________________________________________________________________________________</code></pre>
<p>Then, we train the model:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="Deep.html#cb344-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> 5L</span>
<span id="cb344-2"><a href="Deep.html#cb344-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> 32L</span>
<span id="cb344-3"><a href="Deep.html#cb344-3" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb344-4"><a href="Deep.html#cb344-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(</span>
<span id="cb344-5"><a href="Deep.html#cb344-5" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> train_x, </span>
<span id="cb344-6"><a href="Deep.html#cb344-6" aria-hidden="true" tabindex="-1"></a> <span class="at">y =</span> train_y,</span>
<span id="cb344-7"><a href="Deep.html#cb344-7" aria-hidden="true" tabindex="-1"></a> <span class="at">epochs =</span> epochs,</span>
<span id="cb344-8"><a href="Deep.html#cb344-8" aria-hidden="true" tabindex="-1"></a> <span class="at">batch_size =</span> batch_size,</span>
<span id="cb344-9"><a href="Deep.html#cb344-9" aria-hidden="true" tabindex="-1"></a> <span class="at">shuffle =</span> <span class="cn">TRUE</span>,</span>
<span id="cb344-10"><a href="Deep.html#cb344-10" aria-hidden="true" tabindex="-1"></a> <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb344-11"><a href="Deep.html#cb344-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>Train model:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="Deep.html#cb345-1" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">net</span>()</span>
<span id="cb345-2"><a href="Deep.html#cb345-2" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb345-3"><a href="Deep.html#cb345-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-4"><a href="Deep.html#cb345-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb345-5"><a href="Deep.html#cb345-5" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb345-6"><a href="Deep.html#cb345-6" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb345-7"><a href="Deep.html#cb345-7" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb345-8"><a href="Deep.html#cb345-8" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]])</span>
<span id="cb345-9"><a href="Deep.html#cb345-9" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]], <span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb345-10"><a href="Deep.html#cb345-10" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb345-11"><a href="Deep.html#cb345-11" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb345-12"><a href="Deep.html#cb345-12" aria-hidden="true" tabindex="-1"></a>    losses <span class="ot">=</span> <span class="fu">c</span>(losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb345-13"><a href="Deep.html#cb345-13" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb345-14"><a href="Deep.html#cb345-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, e, <span class="fu">mean</span>(losses)))</span>
<span id="cb345-15"><a href="Deep.html#cb345-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 0.396409
## Loss at epoch 2: 0.254314
## Loss at epoch 3: 0.226825</code></pre>
<p>Evaluation:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="Deep.html#cb347-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb347-2"><a href="Deep.html#cb347-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-3"><a href="Deep.html#cb347-3" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb347-4"><a href="Deep.html#cb347-4" aria-hidden="true" tabindex="-1"></a>total <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb347-5"><a href="Deep.html#cb347-5" aria-hidden="true" tabindex="-1"></a>correct <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb347-6"><a href="Deep.html#cb347-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-7"><a href="Deep.html#cb347-7" aria-hidden="true" tabindex="-1"></a>coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> test_dl) {</span>
<span id="cb347-8"><a href="Deep.html#cb347-8" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">=</span> <span class="fu">model_torch</span>(b[[<span class="dv">1</span>]])</span>
<span id="cb347-9"><a href="Deep.html#cb347-9" aria-hidden="true" tabindex="-1"></a>  labels <span class="ot">=</span> b[[<span class="dv">2</span>]]</span>
<span id="cb347-10"><a href="Deep.html#cb347-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(output, labels)</span>
<span id="cb347-11"><a href="Deep.html#cb347-11" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb347-12"><a href="Deep.html#cb347-12" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">=</span> <span class="fu">torch_max</span>(output<span class="sc">$</span><span class="fu">data</span>(), <span class="at">dim =</span> <span class="dv">2</span>)[[<span class="dv">2</span>]]</span>
<span id="cb347-13"><a href="Deep.html#cb347-13" aria-hidden="true" tabindex="-1"></a>  total <span class="ot">=</span> total <span class="sc">+</span> labels<span class="sc">$</span><span class="fu">size</span>(<span class="dv">1</span>)</span>
<span id="cb347-14"><a href="Deep.html#cb347-14" aria-hidden="true" tabindex="-1"></a>  correct <span class="ot">=</span> correct <span class="sc">+</span> (predicted <span class="sc">==</span> labels)<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb347-15"><a href="Deep.html#cb347-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb347-16"><a href="Deep.html#cb347-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-17"><a href="Deep.html#cb347-17" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(test_losses)</span></code></pre></div>
<pre><code>## [1] 0.2311071</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="Deep.html#cb349-1" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="ot">&lt;-</span>  correct<span class="sc">/</span>total</span>
<span id="cb349-2"><a href="Deep.html#cb349-2" aria-hidden="true" tabindex="-1"></a>test_accuracy</span></code></pre></div>
<pre><code>## [1] 0.9247</code></pre>
</details>
<p><br/></p>
<div id="data-augmentation" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Data Augmentation</h3>
<p>Having to train a CNN using very little data is a common problem. Data augmentation helps to artificially increase the number of images.</p>
<p>The idea is that a CNN learns specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures.</p>
<p>Luckily, it is very easy to use data augmentation in keras.</p>
<p>To show this, we will use again the MNIST dataset. We have to define a generator object (it is a specific object which infinitly draws samples from our dataset). In the generator we can turn on the data augementation. However, now we have to set the step size (steps_per_epoch) because the model does not know the first dimension of the image.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="Deep.html#cb351-1" aria-hidden="true" tabindex="-1"></a>aug <span class="ot">=</span> <span class="fu">image_data_generator</span>()</span>
<span id="cb351-2"><a href="Deep.html#cb351-2" aria-hidden="true" tabindex="-1"></a>generator <span class="ot">=</span> <span class="fu">flow_images_from_data</span>(train_x, train_y,<span class="at">generator =</span> aug, <span class="at">batch_size =</span> 100L)</span>
<span id="cb351-3"><a href="Deep.html#cb351-3" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb351-4"><a href="Deep.html#cb351-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(generator, <span class="at">steps_per_epoch =</span> <span class="fu">floor</span>(<span class="fu">dim</span>(train_x)[<span class="dv">1</span>]<span class="sc">/</span>100L),<span class="at">epochs =</span> 1L)</span></code></pre></div>
<p>So using data augmentation, we can artificially increase the number of images.</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>In torch, we have to change the transform function (but only for the train dataloader):</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="Deep.html#cb352-1" aria-hidden="true" tabindex="-1"></a>train_transforms <span class="ot">&lt;-</span> <span class="cf">function</span>(img) {</span>
<span id="cb352-2"><a href="Deep.html#cb352-2" aria-hidden="true" tabindex="-1"></a>  img <span class="sc">%&gt;%</span></span>
<span id="cb352-3"><a href="Deep.html#cb352-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_to_tensor</span>() <span class="sc">%&gt;%</span></span>
<span id="cb352-4"><a href="Deep.html#cb352-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_random_horizontal_flip</span>(<span class="at">p =</span> <span class="fl">0.3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb352-5"><a href="Deep.html#cb352-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_random_resized_crop</span>(<span class="at">size =</span> <span class="fu">c</span>(28L, 28L)) <span class="sc">%&gt;%</span></span>
<span id="cb352-6"><a href="Deep.html#cb352-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transform_random_vertical_flip</span>(<span class="fl">0.3</span>)</span>
<span id="cb352-7"><a href="Deep.html#cb352-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb352-8"><a href="Deep.html#cb352-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-9"><a href="Deep.html#cb352-9" aria-hidden="true" tabindex="-1"></a>train_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">TRUE</span>, <span class="at">transform =</span> train_transforms)</span>
<span id="cb352-10"><a href="Deep.html#cb352-10" aria-hidden="true" tabindex="-1"></a>test_ds <span class="ot">=</span> <span class="fu">mnist_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">FALSE</span>,<span class="at">transform =</span> transform_to_tensor)</span>
<span id="cb352-11"><a href="Deep.html#cb352-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-12"><a href="Deep.html#cb352-12" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(train_ds, <span class="at">batch_size =</span> 100L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb352-13"><a href="Deep.html#cb352-13" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> 100L)</span>
<span id="cb352-14"><a href="Deep.html#cb352-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-15"><a href="Deep.html#cb352-15" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">net</span>()</span>
<span id="cb352-16"><a href="Deep.html#cb352-16" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb352-17"><a href="Deep.html#cb352-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb352-18"><a href="Deep.html#cb352-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb352-19"><a href="Deep.html#cb352-19" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb352-20"><a href="Deep.html#cb352-20" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb352-21"><a href="Deep.html#cb352-21" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb352-22"><a href="Deep.html#cb352-22" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]])</span>
<span id="cb352-23"><a href="Deep.html#cb352-23" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]], <span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb352-24"><a href="Deep.html#cb352-24" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb352-25"><a href="Deep.html#cb352-25" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb352-26"><a href="Deep.html#cb352-26" aria-hidden="true" tabindex="-1"></a>    losses <span class="ot">=</span> <span class="fu">c</span>(losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb352-27"><a href="Deep.html#cb352-27" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb352-28"><a href="Deep.html#cb352-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, e, <span class="fu">mean</span>(losses)))</span>
<span id="cb352-29"><a href="Deep.html#cb352-29" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 1.588553</code></pre>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="Deep.html#cb354-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb354-2"><a href="Deep.html#cb354-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb354-3"><a href="Deep.html#cb354-3" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb354-4"><a href="Deep.html#cb354-4" aria-hidden="true" tabindex="-1"></a>total <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb354-5"><a href="Deep.html#cb354-5" aria-hidden="true" tabindex="-1"></a>correct <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb354-6"><a href="Deep.html#cb354-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb354-7"><a href="Deep.html#cb354-7" aria-hidden="true" tabindex="-1"></a>coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> test_dl) {</span>
<span id="cb354-8"><a href="Deep.html#cb354-8" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">=</span> <span class="fu">model_torch</span>(b[[<span class="dv">1</span>]])</span>
<span id="cb354-9"><a href="Deep.html#cb354-9" aria-hidden="true" tabindex="-1"></a>  labels <span class="ot">=</span> b[[<span class="dv">2</span>]]</span>
<span id="cb354-10"><a href="Deep.html#cb354-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(output, labels)</span>
<span id="cb354-11"><a href="Deep.html#cb354-11" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb354-12"><a href="Deep.html#cb354-12" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">=</span> <span class="fu">torch_max</span>(output<span class="sc">$</span><span class="fu">data</span>(), <span class="at">dim =</span> <span class="dv">2</span>)[[<span class="dv">2</span>]]</span>
<span id="cb354-13"><a href="Deep.html#cb354-13" aria-hidden="true" tabindex="-1"></a>  total <span class="ot">=</span> total <span class="sc">+</span> labels<span class="sc">$</span><span class="fu">size</span>(<span class="dv">1</span>)</span>
<span id="cb354-14"><a href="Deep.html#cb354-14" aria-hidden="true" tabindex="-1"></a>  correct <span class="ot">=</span> correct <span class="sc">+</span> (predicted <span class="sc">==</span> labels)<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb354-15"><a href="Deep.html#cb354-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb354-16"><a href="Deep.html#cb354-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb354-17"><a href="Deep.html#cb354-17" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="ot">&lt;-</span>  correct<span class="sc">/</span>total</span>
<span id="cb354-18"><a href="Deep.html#cb354-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(test_accuracy)</span></code></pre></div>
<pre><code>## [1] 0.8559</code></pre>
</details>
<p><br/></p>
</div>
<div id="transfer" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Transfer learning</h3>
<p>Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning.</p>
<p>The main idea of transfer learning is that all the convolutional layers have mainly one task - learning to identify highly correlated neighbored features and therefore these learn structures such as edges in the image and only the top layer, the dense layer is the actual classifier of the CNN. Thus, one could think that we could only train the top layer as classifier. To do so, it will be confronted by sets of different edges/structures and has to decide the label based on these.</p>
<p>Again, this sounds very complicating but is again quite easy with keras:</p>
<p>We will do this now with the CIFAR10 data set, so we have to prepare the data:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="Deep.html#cb356-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">dataset_cifar10</span>()</span>
<span id="cb356-2"><a href="Deep.html#cb356-2" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data<span class="sc">$</span>train</span>
<span id="cb356-3"><a href="Deep.html#cb356-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data<span class="sc">$</span>test</span>
<span id="cb356-4"><a href="Deep.html#cb356-4" aria-hidden="true" tabindex="-1"></a>image <span class="ot">=</span> train<span class="sc">$</span>x[<span class="dv">5</span>,,,]</span>
<span id="cb356-5"><a href="Deep.html#cb356-5" aria-hidden="true" tabindex="-1"></a>image <span class="sc">%&gt;%</span> </span>
<span id="cb356-6"><a href="Deep.html#cb356-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">image_to_array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb356-7"><a href="Deep.html#cb356-7" aria-hidden="true" tabindex="-1"></a> <span class="st">`</span><span class="at">/</span><span class="st">`</span>(., <span class="dv">255</span>) <span class="sc">%&gt;%</span></span>
<span id="cb356-8"><a href="Deep.html#cb356-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">as.raster</span>() <span class="sc">%&gt;%</span></span>
<span id="cb356-9"><a href="Deep.html#cb356-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-164-1.png" width="672" /></p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="Deep.html#cb357-1" aria-hidden="true" tabindex="-1"></a>train_x <span class="ot">=</span> <span class="fu">array</span>(train<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(train<span class="sc">$</span>x)))</span>
<span id="cb357-2"><a href="Deep.html#cb357-2" aria-hidden="true" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">array</span>(test<span class="sc">$</span>x<span class="sc">/</span><span class="dv">255</span>, <span class="fu">c</span>(<span class="fu">dim</span>(test<span class="sc">$</span>x)))</span>
<span id="cb357-3"><a href="Deep.html#cb357-3" aria-hidden="true" tabindex="-1"></a>train_y <span class="ot">=</span> <span class="fu">to_categorical</span>(train<span class="sc">$</span>y, <span class="dv">10</span>)</span>
<span id="cb357-4"><a href="Deep.html#cb357-4" aria-hidden="true" tabindex="-1"></a>test_y <span class="ot">=</span> <span class="fu">to_categorical</span>(test<span class="sc">$</span>y, <span class="dv">10</span>)</span></code></pre></div>
<p>Keras provides download functions for all famous architectures/CNN models which are already trained on the imagenet dataset (another famous dataset). These trained networks come already without their top layer, so we have to set include_top to false and change the input shape.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="Deep.html#cb358-1" aria-hidden="true" tabindex="-1"></a>densenet <span class="ot">=</span> <span class="fu">application_densenet201</span>(<span class="at">include_top =</span> <span class="cn">FALSE</span>, <span class="at">input_shape  =</span> <span class="fu">c</span>(32L, 32L, 3L))</span></code></pre></div>
<p>Now, we will use not a sequential model but just a “keras_model” where we can specify the inputs and outputs. Thereby, the outputs are our own top layer, but the inputs are the densenet inputs, as these are already pre-trained.</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="Deep.html#cb359-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> keras<span class="sc">::</span><span class="fu">keras_model</span>(<span class="at">inputs =</span> densenet<span class="sc">$</span>input, <span class="at">outputs =</span> </span>
<span id="cb359-2"><a href="Deep.html#cb359-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_flatten</span>(<span class="fu">layer_dense</span>(densenet<span class="sc">$</span>output, <span class="at">units =</span> 10L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb359-3"><a href="Deep.html#cb359-3" aria-hidden="true" tabindex="-1"></a> )</span></code></pre></div>
<p>In the next step we want to freeze all layers except for our own last layer (with freezing I mean that these are not trained: we do not want to train the complete model, we only want to train the last layer). You can check the number of trainable weights via summary(model)</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="Deep.html#cb360-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">freeze_weights</span>(<span class="at">to =</span> <span class="fu">length</span>(model<span class="sc">$</span>layers)<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb360-2"><a href="Deep.html#cb360-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<p>And then the usual training:</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="Deep.html#cb361-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb361-2"><a href="Deep.html#cb361-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, <span class="at">optimizer =</span> <span class="fu">optimizer_adamax</span>())</span>
<span id="cb361-3"><a href="Deep.html#cb361-3" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb361-4"><a href="Deep.html#cb361-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(</span>
<span id="cb361-5"><a href="Deep.html#cb361-5" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> train_x, </span>
<span id="cb361-6"><a href="Deep.html#cb361-6" aria-hidden="true" tabindex="-1"></a> <span class="at">y =</span> train_y,</span>
<span id="cb361-7"><a href="Deep.html#cb361-7" aria-hidden="true" tabindex="-1"></a> <span class="at">epochs =</span> 1L,</span>
<span id="cb361-8"><a href="Deep.html#cb361-8" aria-hidden="true" tabindex="-1"></a> <span class="at">batch_size =</span> 32L,</span>
<span id="cb361-9"><a href="Deep.html#cb361-9" aria-hidden="true" tabindex="-1"></a> <span class="at">shuffle =</span> T,</span>
<span id="cb361-10"><a href="Deep.html#cb361-10" aria-hidden="true" tabindex="-1"></a> <span class="at">validation_split =</span> <span class="fl">0.2</span>,</span>
<span id="cb361-11"><a href="Deep.html#cb361-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>We have seen, that transfer-learning can easily be done using keras.</p>
<details>
<summary>
<strong><span style="color: #CC2FAA">torch</span></strong>
</summary>
<p>
<p>In torch, we have to change the transform function (but only for the train dataloader):</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="Deep.html#cb362-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torchvision)</span>
<span id="cb362-2"><a href="Deep.html#cb362-2" aria-hidden="true" tabindex="-1"></a>train_ds <span class="ot">=</span> <span class="fu">cifar10_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">TRUE</span>, <span class="at">transform =</span> transform_to_tensor)</span>
<span id="cb362-3"><a href="Deep.html#cb362-3" aria-hidden="true" tabindex="-1"></a>test_ds <span class="ot">=</span> <span class="fu">cifar10_dataset</span>(<span class="st">&quot;.&quot;</span>, <span class="at">download =</span> <span class="cn">TRUE</span>, <span class="at">train =</span> <span class="cn">FALSE</span>,<span class="at">transform =</span> transform_to_tensor)</span>
<span id="cb362-4"><a href="Deep.html#cb362-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-5"><a href="Deep.html#cb362-5" aria-hidden="true" tabindex="-1"></a>train_dl <span class="ot">=</span> <span class="fu">dataloader</span>(train_ds, <span class="at">batch_size =</span> 100L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb362-6"><a href="Deep.html#cb362-6" aria-hidden="true" tabindex="-1"></a>test_dl <span class="ot">=</span> <span class="fu">dataloader</span>(test_ds, <span class="at">batch_size =</span> 100L)</span>
<span id="cb362-7"><a href="Deep.html#cb362-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-8"><a href="Deep.html#cb362-8" aria-hidden="true" tabindex="-1"></a>model_torch <span class="ot">=</span> <span class="fu">model_resnet18</span>(<span class="at">pretrained =</span> <span class="cn">TRUE</span>)</span>
<span id="cb362-9"><a href="Deep.html#cb362-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-10"><a href="Deep.html#cb362-10" aria-hidden="true" tabindex="-1"></a><span class="co"># we will set all model parameters to constant values:</span></span>
<span id="cb362-11"><a href="Deep.html#cb362-11" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span>parameters <span class="sc">%&gt;%</span> purrr<span class="sc">::</span><span class="fu">walk</span>(<span class="cf">function</span>(param) param<span class="sc">$</span><span class="fu">requires_grad_</span>(<span class="cn">FALSE</span>))</span>
<span id="cb362-12"><a href="Deep.html#cb362-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-13"><a href="Deep.html#cb362-13" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s replace the last layer (last layer is named &#39;fc&#39;) with our own layer:</span></span>
<span id="cb362-14"><a href="Deep.html#cb362-14" aria-hidden="true" tabindex="-1"></a>inFeat <span class="ot">=</span> model_torch<span class="sc">$</span>fc<span class="sc">$</span>in_features</span>
<span id="cb362-15"><a href="Deep.html#cb362-15" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span>fc <span class="ot">=</span> <span class="fu">nn_linear</span>(inFeat, <span class="at">out_features =</span> 10L)</span>
<span id="cb362-16"><a href="Deep.html#cb362-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-17"><a href="Deep.html#cb362-17" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim_adam</span>(<span class="at">params =</span> model_torch<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb362-18"><a href="Deep.html#cb362-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-19"><a href="Deep.html#cb362-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(e <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb362-20"><a href="Deep.html#cb362-20" aria-hidden="true" tabindex="-1"></a>  losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb362-21"><a href="Deep.html#cb362-21" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_dl) {</span>
<span id="cb362-22"><a href="Deep.html#cb362-22" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb362-23"><a href="Deep.html#cb362-23" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">model_torch</span>(batch[[<span class="dv">1</span>]])</span>
<span id="cb362-24"><a href="Deep.html#cb362-24" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(pred, batch[[<span class="dv">2</span>]], <span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb362-25"><a href="Deep.html#cb362-25" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb362-26"><a href="Deep.html#cb362-26" aria-hidden="true" tabindex="-1"></a>    opt<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb362-27"><a href="Deep.html#cb362-27" aria-hidden="true" tabindex="-1"></a>    losses <span class="ot">=</span> <span class="fu">c</span>(losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb362-28"><a href="Deep.html#cb362-28" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb362-29"><a href="Deep.html#cb362-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %3f</span><span class="sc">\n</span><span class="st">&quot;</span>, e, <span class="fu">mean</span>(losses)))</span>
<span id="cb362-30"><a href="Deep.html#cb362-30" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loss at epoch 1: 2.012621</code></pre>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="Deep.html#cb364-1" aria-hidden="true" tabindex="-1"></a>model_torch<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb364-2"><a href="Deep.html#cb364-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-3"><a href="Deep.html#cb364-3" aria-hidden="true" tabindex="-1"></a>test_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb364-4"><a href="Deep.html#cb364-4" aria-hidden="true" tabindex="-1"></a>total <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb364-5"><a href="Deep.html#cb364-5" aria-hidden="true" tabindex="-1"></a>correct <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb364-6"><a href="Deep.html#cb364-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-7"><a href="Deep.html#cb364-7" aria-hidden="true" tabindex="-1"></a>coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (b <span class="cf">in</span> test_dl) {</span>
<span id="cb364-8"><a href="Deep.html#cb364-8" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">=</span> <span class="fu">model_torch</span>(b[[<span class="dv">1</span>]])</span>
<span id="cb364-9"><a href="Deep.html#cb364-9" aria-hidden="true" tabindex="-1"></a>  labels <span class="ot">=</span> b[[<span class="dv">2</span>]]</span>
<span id="cb364-10"><a href="Deep.html#cb364-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">nnf_cross_entropy</span>(output, labels)</span>
<span id="cb364-11"><a href="Deep.html#cb364-11" aria-hidden="true" tabindex="-1"></a>  test_losses <span class="ot">=</span> <span class="fu">c</span>(test_losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb364-12"><a href="Deep.html#cb364-12" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">=</span> <span class="fu">torch_max</span>(output<span class="sc">$</span><span class="fu">data</span>(), <span class="at">dim =</span> <span class="dv">2</span>)[[<span class="dv">2</span>]]</span>
<span id="cb364-13"><a href="Deep.html#cb364-13" aria-hidden="true" tabindex="-1"></a>  total <span class="ot">=</span> total <span class="sc">+</span> labels<span class="sc">$</span><span class="fu">size</span>(<span class="dv">1</span>)</span>
<span id="cb364-14"><a href="Deep.html#cb364-14" aria-hidden="true" tabindex="-1"></a>  correct <span class="ot">=</span> correct <span class="sc">+</span> (predicted <span class="sc">==</span> labels)<span class="sc">$</span><span class="fu">sum</span>()<span class="sc">$</span><span class="fu">item</span>()</span>
<span id="cb364-15"><a href="Deep.html#cb364-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb364-16"><a href="Deep.html#cb364-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-17"><a href="Deep.html#cb364-17" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="ot">&lt;-</span>  correct<span class="sc">/</span>total</span>
<span id="cb364-18"><a href="Deep.html#cb364-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(test_accuracy)</span></code></pre></div>
<pre><code>## [1] 0.3977</code></pre>
</details>
<p><br/></p>
</div>
</div>
<div id="natural-language-processing-nlp" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Natural Language Processing (NLP)</h2>
<p>What this video to get an idea about what NLP is about</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UFtXy0KRxVI" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>See also the blog post linked with the youtube video with accompanying code to the video. Moreover, here is an article that shows now NLP works with keras, however, written in Python. As a challenge, you can take the code and implement it in R <img src="https://nlpforhackers.io/keras-intro/" /></p>
</div>
<div id="recurrent-neural-networks-rnns" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Recurrent neural networks (RNNs)</h2>
<p>Recurrent Neural Networks are used to model sequential data, i.e. temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/SEnXr6v2ifU" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-dropout" class="csl-entry">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>The Journal of Machine Learning Research</em> 15 (1): 1929–58.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fund.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="xAI.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
