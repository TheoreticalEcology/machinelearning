[["index.html", "Machine Learning and AI in TensorFlow and R 1 Prerequisites", " Machine Learning and AI in TensorFlow and R Maximilian Pichler and Florian Hartig 2021-04-29 1 Prerequisites R+Rstudio "],["introduction.html", "2 Introduction to Machine Learning 2.1 Supervised learning: regression and classification 2.2 Unsupervised learning 2.3 Introduction to Tensorflow 2.4 First steps with the keras framework", " 2 Introduction to Machine Learning In this lesson, we introduce the three basic ML tasks: supervised regression and classification, and unsupervised learning. In ML, we distinguish 3 basic learning paradigms: - Supervised learning - Unsupervised learning - Reinforcement learning We will speak about reinforcement learning at the end of the course. Now, we want to look at examples of supervised and unsupervised learning. Before you start with the code, here a video to remind you of what we talked about in the class: 2.1 Supervised learning: regression and classification Two two main subbranches of supervised learning are regression and classification. Here a video that explains again the difference 2.1.1 Supervised regression using Random Forest The random forest (RF) algorithm is possibly the most widely used ML algorithm and can be used for regression and classification. We will talk more about the algorithm on Day 2. Here an example of a regression: Visualization of the data: plot(iris, col = iris$Species) Fitting the model library(randomForest) m1 &lt;- randomForest(Sepal.Length ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Sepal.Length ~ ., data = iris) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 0.1388134 ## % Var explained: 79.62 Visualization of the results par(mfrow = c(1,2)) plot(predict(m1), iris$Sepal.Length, xlab = &quot;predicted&quot;, ylab = &quot;observed&quot;) abline(0,1) varImpPlot(m1) This is a nice visualization of the RF structure, but requires to load a package from GitHub # devtools::install_github(&#39;araastat/reprtree&#39;) reprtree:::plot.getTree(m1, iris) 2.1.2 Supervised classification using Random Forest Fitting the model: set.seed(123) m1 &lt;- randomForest(Species ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Species ~ ., data = iris) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 4.67% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 50 0 0 0.00 ## versicolor 0 47 3 0.06 ## virginica 0 4 46 0.08 Visualizing the fitted model: par(mfrow = c(1,2)) reprtree:::plot.getTree(m1, iris) Visualizing results ecologically: oldpar &lt;- par(mfrow = c(1,2)) plot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = &quot;observed&quot;) plot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = &quot;predicted&quot;) Confusion matrix: table(predict(m1),iris$Species) ## ## setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 4 ## virginica 0 3 46 2.2 Unsupervised learning In unsupervised learning, we basically want to identify patterns in data without having any guidance (supervision) about what the correct patterns / classes are. It is all much easier with a practical example. Consider our iris dataset. Here, we have observations of different species Together with their flower traits Imagine we didn’t know what species are. This is basically the situation in which people in the antique would have been. There is no book to look up species. You just noted that there seem to be some kind of plants that have different flowers than another, so you decide to call them by a different name. This kind of process is what unsupervised learning does. 2.2.1 k-means clustering An example for an unsupervised learning algorithm is k-means clustering, one of the simplest and popular unsupervised machine learning algorithms. A cluster refers to a collection of data points aggregated together because of certain similarities. In the algorithm, you’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid. sIris = scale(iris[,1:4]) model&lt;- kmeans(sIris,3) # aplly k-means algorithm with no. of centroids(k)=3 model ## K-means clustering with 3 clusters of sizes 47, 53, 50 ## ## Cluster means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 1.13217737 0.08812645 0.9928284 1.0141287 ## 2 -0.05005221 -0.88042696 0.3465767 0.2805873 ## 3 -1.01119138 0.85041372 -1.3006301 -1.2507035 ## ## Clustering vector: ## [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 1 ## [77] 1 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 2 1 1 1 2 1 1 2 ## ## Within cluster sum of squares by cluster: ## [1] 47.45019 44.08754 47.35062 ## (between_SS / total_SS = 76.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Visualizing the results: par(mfrow = c(1,2)) plot(Petal.Length~Petal.Width, data = sIris, col = model$cluster, main = &quot;Predicted clusters&quot;) plot(Petal.Length~Petal.Width, data = sIris, col = iris$Species, main = &quot;True species&quot;) Confusion matrix: table(model$cluster,iris$Species) ## ## setosa versicolor virginica ## 1 0 11 36 ## 2 0 39 14 ## 3 50 0 0 2.3 Introduction to Tensorflow TF is a math library which is highly optimized for neural networks If a GPU is available, computations can be easily run on the GPU but even on the CPU is TF still very fast The “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for the GPU) The interface (the part of TF that we use) is written in python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend. 2.3.1 Tensorflow data containers TF has two data containers (structures): - constant (tf\\(constant) :creates a constant (immutable) value in the computation graph - variable (tf\\)Variable): creates a mutable value in the computation graph (used as parameter/weight in models) library(tensorflow) # Don&#39;t worry about weird messages. TF supports additional optimizations exists(&quot;tf&quot;) ## [1] TRUE Don’t worry about weird messages (they will only appear once at the start of the session). a = tf$constant(5) b = tf$constant(10) print(a) ## tf.Tensor(5.0, shape=(), dtype=float32) print(b) ## tf.Tensor(10.0, shape=(), dtype=float32) c = tf$add(a, b) print(c) ## tf.Tensor(15.0, shape=(), dtype=float32) tf$print(c) Normal R methods such as print() are provided by the R package “tensorflow”. The tensorflow library (created by the RStudio team) built R methods for all common operations: `+.tensorflow.tensor` = function(a, b) return(tf$add(a,b)) tf$print(a+b) Their operators also transfrom automatically R numbers into constant tensors when attempting to add a tensor to a R number: d = c + 5 # 5 is automatically converted to a tensor print(d) ## tf.Tensor(20.0, shape=(), dtype=float32) TF container are objects, which means that they are not just simple variables of type numeric (class(5)), and they have methods For instance, there is a method to transform the tensor object back to a R object: class(d) ## [1] &quot;tensorflow.tensor&quot; &quot;tensorflow.python.framework.ops.EagerTensor&quot; &quot;tensorflow.python.framework.ops._EagerTensorBase&quot; ## [4] &quot;tensorflow.python.framework.ops.Tensor&quot; &quot;tensorflow.python.types.internal.NativeObject&quot; &quot;tensorflow.python.types.core.Tensor&quot; ## [7] &quot;python.builtin.object&quot; class(d$numpy()) ## [1] &quot;numeric&quot; 2.3.2 Tensorflow data types - good practise with R-TF R uses dynamic typing, which means you can assign to a variable a number, character, function or whatever, and R infers the type automatically. In other languages you have to state explicitly the type, e.g. in C: int a = 5; float a = 5.0; char a = “a”; While TF tries to infer dynamically the type, often you must state it explicitly. Common important types: - float32 (floating point number with 32bits, “single precision”) - float64 (“double precision”) - int8 (integer with 8bits) Why does TF support float32 numbers when most cpus today can handle easily 64bit numbers? Many GPUs (e.g. the NVIDIA geforces) can handle only up to 32bit numbers! (you do not need high precision in graphical modeling) r_matrix = matrix(runif(10*10), 10,10) m = tf$constant(r_matrix, dtype = &quot;float64&quot;) b = tf$constant(2.0, dtype = &quot;float64&quot;) c = m / b Instead of a string, you can also provide a tf$float64 object m = tf$constant(r_matrix, dtype = tf$float32) b = tf$constant(2.0, dtype = tf$float64) c = m / b # doesn&#39;t work! we try to divide float32/float64 Tensorflow arguments often require exact/explicit data types: TF expects for arguments integers, however, R needs a “L” after an integer to tell the R interpreter that it should be treated as an integer: is.integer(5) is.integer(5L) matrix(t(r_matrix), 5, 20, byrow = TRUE) tf$reshape(r_matrix, shape = c(5, 20))$numpy() tf$reshape(r_matrix, shape = c(5L, 20L))$numpy() Note: skipping the “L” is one of the most common errors when using R-TF! 2.4 First steps with the keras framework Objective of this lesson: familiarize yourself with keras. keras is a higher level API within TF and developed to build easily neural networks. Keras can be found within TF: tf.keras…however, the RStudio team built a pkg on top of tf.keras: 2.4.1 Example workflow in keras We will now build a small classifier in keras to predict the three species of the iris dataset: Load the necessary packages and datasets: library(keras) library(tensorflow) data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa It is beneficial for neural networks to scale the predictors (scaling = centering and standardization, see ?scale): X = scale(iris[,1:4]) Y = iris[,5] We also split our data into the predictors (X) and the response (Y = the three species) keras/tf cannot handle factors and we have to create contrasts (one-hot encoding): Y = to_categorical(as.integer(Y)-1L, 3) head(Y) # 3 colums, one for each level in the response ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 0 0 ## [3,] 1 0 0 ## [4,] 1 0 0 ## [5,] 1 0 0 ## [6,] 1 0 0 Do you have an idea why we subtracted 1L from the labels after we transformed them? (Tip: google “Is r 0 or 1 indexed?”) Model building: Initiliaze a sequential model in keras: model = keras_model_sequential() A sequential keras model is a higher order type of model within keras and consists of one input and one output model. Add hidden layers to the model (we will learn more about DNNs during the next days): model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(4L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 3L, activation = &quot;softmax&quot;) softmax scales to 0 1 and overall to 0 - 1, 3 output nodes for 3 response classes/labels compile the model with a cross entropy loss function and Adamax optimizer: model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(0.001)) summary(model) ## Model: &quot;sequential_6&quot; ## ______________________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================================================== ## dense_18 (Dense) (None, 20) 100 ## ______________________________________________________________________________________________________________________________________________________________ ## dense_19 (Dense) (None, 20) 420 ## ______________________________________________________________________________________________________________________________________________________________ ## dense_20 (Dense) (None, 20) 420 ## ______________________________________________________________________________________________________________________________________________________________ ## dense_21 (Dense) (None, 3) 63 ## ============================================================================================================================================================== ## Total params: 1,003 ## Trainable params: 1,003 ## Non-trainable params: 0 ## ______________________________________________________________________________________________________________________________________________________________ For now, do not worry about the “lr” argument, crossentropy or the optimizer Fit the model in 30 (epochs) iterations: model_history = model %&gt;% fit(x = X, y = apply(Y,2,as.integer), epochs = 30L, batch_size = 20L, shuffle = TRUE) Plot the training history: plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Create predictions: predictions = predict(model, X) # probabilities for each class We will get probabilites: head(predictions) # quasi-probabilities for each species ## [,1] [,2] [,3] ## [1,] 0.9732426 0.02112393 0.005633502 ## [2,] 0.9448608 0.05044184 0.004697330 ## [3,] 0.9692710 0.02755538 0.003173681 ## [4,] 0.9606974 0.03609953 0.003202966 ## [5,] 0.9794057 0.01585520 0.004739106 ## [6,] 0.9548649 0.02875621 0.016378896 For each site, we want to know for which species we got the highest probability: preds = apply(predictions, 1, which.max) print(preds) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 ## [78] 3 2 2 2 2 2 3 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Calculate Accuracy: mean(preds == as.integer(iris$Species)) ## [1] 0.92 Plot predictions: oldpar = par() par(mfrow = c(1,2)) plot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species, main = &quot;Observed&quot;) plot(iris$Sepal.Length, iris$Petal.Length, col = preds, main = &quot;Predicted&quot;) "],["fund.html", "3 Fundamental principles and techniques 3.1 Machine learning principles 3.2 Tree-based ML algorithms 3.3 Distance-based algorithms 3.4 Artificial neural networks 3.5 The standard ML pipeline", " 3 Fundamental principles and techniques 3.1 Machine learning principles 3.1.1 Optimization from wikipedia: \" an optimization problem is the problem of finding the best solution from all feasible solutions\" Why do we need this “optimization”? A loss function (e.g. we tell in each training step the algorithm how many observations were miss-classified) guides the training of ML algorithms Based on the loss, the optimizer tries to update the weights of the ML algorithms in a way that the loss function is minimized Calculating analytically the global optima of a function is a non-trivial problem and bunch of diverse optimization algorithms evolved Some optimization algorithms are inspired by biological systemse.g. Ants, Bee, or even slimve algorithms): 3.1.1.1 Small optimization example We have the following function: func = function(x) return(x^2) which we want to minimize, we could do this by hand: a = rnorm(100) plot(a, func(a)) The smallest value is at x = 0 (to be honest, we can calculate this for this simple case analytically) We can also use an optimizer with the optim(…) function opt = optim(1.0, func) ## Warning in optim(1, func): one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly print(opt$par) ## [1] -8.881784e-16 opt$par will return the best values found by the optimizer 3.1.1.2 Advanced optimization example We will now optimze the weights (slopes) for linear regression model. Basically, we will implement lm(y~x) on our own: Load the airquality dataset, remove NAs, split it into predictors and response, and scale the predictors: data = airquality[complete.cases(airquality$Ozone) &amp; complete.cases(airquality$Solar.R),] X = scale(data[,-1]) Y = data$Ozone The model we want to optimize: \\(ozone = Solar.R*X1 + Wind*X2 + Temp*X3 + Month*X4 + Day*X5 + X6\\) Our loss function: mean(predicted ozone - true ozone)^2) We found to find the parameters X1-X6 for which the loss function is the smallest: linear_regression = function(w) { pred = w[1]*X[,1] + # Solar.R w[2]*X[,2] + # Wind w[3]*X[,3] + # Temp w[4]*X[,4] + # Month w[5]*X[,5] + w[6] # or X %*% w[1:5] + w[6] # loss = MSE, we want to find the optimal weights # to minimize the sum of squared residuals loss = mean((pred - Y)^2) return(loss) } The linear_regression function takes potential solutions for the weights (X1-X6) and will return the loss for these weights: linear_regression(runif(6)) ## [1] 2847.567 Let’s try it bruteforce (which means we will try to find the optimal solution with random a set of random weights): random_search = matrix(runif(6*5000,-10,10), 5000, 6) losses = apply(random_search, 1, linear_regression) plot(losses, type = &quot;l&quot;) random_search[which.min(losses),] ## [1] 5.0567712 -7.8550341 7.4573370 0.8109562 2.0097918 9.3807506 Bruteforce isn’t a good approach, it might work well with only a few parameters. Let’s try it with the optim function: opt = optim(runif(6, -1, 1), linear_regression) opt$par ## [1] 4.150032 -15.100683 11.409332 -5.445377 -3.115452 42.502556 Compare the weights the estimated weights of the lm() function: coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 3.1.2 Regularization There are several ways to regularize models. In this section we will focus on lasso and ridge regularization for weights in neural networks. The idea of lasso and ridge regularization is to put some type of rubber band on the weights and pull them to zero, important weights are able to pull away from zero. Lasso and ridge have slightly different properties: Lasso: abs( sum(Weights)) Ridge: (sum(Weights))^2 Lasso tries to push the weights directly to zero, where as the Ridge allows small values around zero (caused by the difference of the absolute and squared function). Let’s have a look at a keras model with and without regularization (we will use here a network without hidden layers == linear regression model): library(keras) data = airquality[complete.cases(airquality),] X = scale(data[,-1]) Y = data$Ozone # l1/l2 on linear model model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2])) summary(model) ## Model: &quot;sequential_9&quot; ## ____________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================================ ## dense_18 (Dense) (None, 1) 6 ## ============================================================================================================================================ ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## ____________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 50L, batch_size = 20L, shuffle = TRUE) unconstrained = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 Now we will put a l1 (lasso) regularization on the weights: model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2]), kernel_regularizer = regularizer_l1(10), bias_regularizer = regularizer_l1(10)) summary(model) ## Model: &quot;sequential_10&quot; ## ____________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================================ ## dense_19 (Dense) (None, 1) 6 ## ============================================================================================================================================ ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## ____________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5), metrics = c(metric_mean_squared_error)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) l1 = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 cbind(unlist(l1), unlist(unconstrained)) ## [,1] [,2] ## [1,] 1.335416317 4.758992 ## [2,] -8.142886162 -12.199446 ## [3,] 13.692774773 16.987839 ## [4,] 0.015733831 -3.922871 ## [5,] -0.002944823 2.305355 ## [6,] 33.569160461 41.365257 3.2 Tree-based ML algorithms Famous ML algorithms such as random Forest and gradient boosted trees are based on classification and regression trees. 3.2.1 Classification and Regression Trees In this lecture we will explore regression and classifaction trees at the example of the airquality data set: library(rpart) library(rpart.plot) data=airquality[complete.cases(airquality),] Fit and visualize a regression tree: rt = rpart(Ozone~., data = data,control = rpart.control(minsplit = 10)) rpart.plot(rt) Visualize the predictions: pred = predict(rt, data) plot(data$Temp, data$Ozone) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) The angular form of the prediction line is typical for regression trees. There is one important hyper-parameter for regression trees: minsplit controls the depth of tree (see the help of tree for a description) controls the complexity of the tree and can be seen also as a regularization parameter 3.2.2 Random Forest Random Forest creates an ensemble of regression/classification trees. However, there are two randomization steps with the RF that are responsible for the success of RF: bootstrap sample for each tree (we will sample observations with replacement from the dataset) at each split, we will sample a subset of predictors which are then considered as potential splitting criterion Fit a RF and visualize the predictions: library(randomForest) rf = randomForest(Ozone~., data = data) pred = predict(rf, data) plot(Ozone~Temp, data = data) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) One advantage of RF is that we will get a variable importance: importance(rf) ## IncNodePurity ## Solar.R 18441.43 ## Wind 31140.89 ## Temp 33965.21 ## Month 11031.51 ## Day 15610.66 Important hyperparameters: Similar to regression and classification trees, the hyper parameter nodesize controls for complexity. -&gt; Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5). mtry - Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times. 3.2.3 Boosted regression trees RF fits hundreds of trees independent of each other. In boosted regression trees, we will start with a weak learner (weak learner == regression tree) and then fit sequentially additional weak learners. There are two different approaches to enhance the performance: AdaBoost, wrong classified observations (by the previous tree) will get a higher weight, the chain of trees will focus on difficult/missclassified observations Gradient boosting (state of the art), each sequential model will be fit on the residual errors of the previous model Fit a BRT using xgboost: library(xgboost) data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) brt = xgboost(data_xg, nrounds = 16L, nthreads = 4L) ## [19:01:14] WARNING: amalgamation/../src/learner.cc:516: ## Parameters: { nthreads } might not be used. ## ## This may not be accurate due to some parameters are only used in language bindings but ## passed down to XGBoost core. Or some parameters are not used but slip through this ## verification. Please open an issue if you find above cases. ## ## ## [1] train-rmse:39.724625 ## [2] train-rmse:30.225761 ## [3] train-rmse:23.134838 ## [4] train-rmse:17.899178 ## [5] train-rmse:14.097785 ## [6] train-rmse:11.375458 ## [7] train-rmse:9.391275 ## [8] train-rmse:7.889690 ## [9] train-rmse:6.646585 ## [10] train-rmse:5.804859 ## [11] train-rmse:5.128437 ## [12] train-rmse:4.456416 ## [13] train-rmse:4.069464 ## [14] train-rmse:3.674615 ## [15] train-rmse:3.424578 ## [16] train-rmse:3.191301 xgboost has a weird syntax, we have to transform our data into a xgb.DMatrix object to be able to fit the model. We will do 500 rounds which means we will fit 500 sequential models: Let us visualize the predictions for different number of trees: par(mfrow = c(2,2)) for(i in 1:4){ pred = predict(brt, newdata = data_xg, ntreelimit = i) plot(data$Temp, data$Ozone, main = i) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) } xgboost also provides an variable importance: xgboost::xgb.importance(model = brt) ## Feature Gain Cover Frequency ## 1: Temp 0.570071875 0.2958229 0.24836601 ## 2: Wind 0.348230710 0.3419576 0.24183007 ## 3: Solar.R 0.058795559 0.1571072 0.30718954 ## 4: Day 0.019530002 0.1779925 0.16993464 ## 5: Month 0.003371853 0.0271197 0.03267974 sqrt(mean((data$Ozone - pred)^2)) # RMSE ## [1] 17.89918 data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) xgboost has an argument to do cross-validation: brt = xgboost(data_xg, nrounds = 5L) ## [1] train-rmse:39.724625 ## [2] train-rmse:30.225760 ## [3] train-rmse:23.134840 ## [4] train-rmse:17.899178 ## [5] train-rmse:14.097784 brt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L, nrounds = 3L, nthreads = 4L) ## [1] train-rmse:39.978896+0.286917 test-rmse:40.933159+0.927615 ## [2] train-rmse:30.598419+0.425074 test-rmse:33.257562+1.932902 ## [3] train-rmse:23.706882+0.437096 test-rmse:27.806431+2.435702 print(brt_cv) ## ##### xgb.cv 3-folds ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 1 39.97890 0.2869171 40.93316 0.9276147 ## 2 30.59842 0.4250742 33.25756 1.9329018 ## 3 23.70688 0.4370956 27.80643 2.4357021 There are different ways to control for complexity: max_depth, depth of each tree shrinkage (each tree will get a weight and the weight will decrease with the number of trees) 3.3 Distance-based algorithms In this chapter, we introduce support-vector machines (SVMs) and other distance-based methods. 3.3.1 k-nearest-neighbor k-nearest-neighbor classifies new observations by calculating the nearest n neighbors. The labels of the n nearest neighbors decide the class of the new point: X = scale(iris[,1:4]) Y = iris[,5] plot(X[-100,1], X[-100,3], col = Y) points(X[100,1], X[100,3], col = &quot;blue&quot;, pch = 18, cex = 1.3) Which class would you decide for the blue point? What are the classes of the nearest points?… well this procedure is used by the kNN: Scaling is very important when dealing with distances (we also split the dataset into a training and a testing dataset): data = iris data[,1:4] = apply(data[,1:4],2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] Fit model and create predictions: library(kknn) knn = kknn(Species~., train = train, test = test) summary(knn) ## ## Call: ## kknn(formula = Species ~ ., train = train, test = test) ## ## Response: &quot;nominal&quot; ## fit prob.setosa prob.versicolor prob.virginica ## 1 setosa 1 0.0000000 0.00000000 ## 2 setosa 1 0.0000000 0.00000000 ## 3 setosa 1 0.0000000 0.00000000 ## 4 setosa 1 0.0000000 0.00000000 ## 5 setosa 1 0.0000000 0.00000000 ## 6 setosa 1 0.0000000 0.00000000 ## 7 setosa 1 0.0000000 0.00000000 ## 8 setosa 1 0.0000000 0.00000000 ## 9 setosa 1 0.0000000 0.00000000 ## 10 setosa 1 0.0000000 0.00000000 ## 11 setosa 1 0.0000000 0.00000000 ## 12 setosa 1 0.0000000 0.00000000 ## 13 setosa 1 0.0000000 0.00000000 ## 14 setosa 1 0.0000000 0.00000000 ## 15 setosa 1 0.0000000 0.00000000 ## 16 setosa 1 0.0000000 0.00000000 ## 17 versicolor 0 0.9354939 0.06450608 ## 18 versicolor 0 0.6349387 0.36506134 ## 19 versicolor 0 1.0000000 0.00000000 ## 20 versicolor 0 0.7783044 0.22169561 ## 21 versicolor 0 1.0000000 0.00000000 ## 22 versicolor 0 1.0000000 0.00000000 ## 23 versicolor 0 0.6430958 0.35690421 ## 24 versicolor 0 1.0000000 0.00000000 ## 25 virginica 0 0.2580081 0.74199187 ## 26 versicolor 0 0.9148730 0.08512700 ## 27 versicolor 0 1.0000000 0.00000000 ## 28 versicolor 0 0.7245826 0.27541743 ## 29 versicolor 0 1.0000000 0.00000000 ## 30 versicolor 0 1.0000000 0.00000000 ## 31 versicolor 0 0.7890886 0.21091135 ## 32 versicolor 0 1.0000000 0.00000000 ## 33 versicolor 0 0.9511855 0.04881448 ## 34 versicolor 0 1.0000000 0.00000000 ## 35 virginica 0 0.0000000 1.00000000 ## 36 virginica 0 0.0851270 0.91487300 ## 37 virginica 0 0.0000000 1.00000000 ## 38 virginica 0 0.0000000 1.00000000 ## 39 virginica 0 0.0000000 1.00000000 ## 40 versicolor 0 0.7013345 0.29866548 ## 41 virginica 0 0.2109114 0.78908865 ## 42 virginica 0 0.0000000 1.00000000 ## 43 virginica 0 0.0000000 1.00000000 ## 44 virginica 0 0.0000000 1.00000000 ## 45 virginica 0 0.0156916 0.98430840 table(test$Species, fitted(knn)) ## ## setosa versicolor virginica ## setosa 16 0 0 ## versicolor 0 17 1 ## virginica 0 1 10 Actually, there is no “real” learning in a kNN. 3.3.2 Support Vector Machines (SVM) Support vectors machine try to find a hyperplane in the predictor space which separates the classes in the best way. Fitting a SVM: library(e1071) data = iris data[,1:4] = apply(data[,1:4],2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] sm = svm(Species~., data = train, kernel = &quot;linear&quot;) pred = predict(sm, newdata = test) oldpar = par() par(mfrow = c(1,2)) plot(test$Sepal.Length, test$Petal.Length, col = pred, main = &quot;predicted&quot;) plot(test$Sepal.Length, test$Petal.Length, col = test$Species, main = &quot;observed&quot;) par(oldpar) ## Warning in par(oldpar): graphical parameter &quot;cin&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;cra&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;csi&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;cxy&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;din&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;page&quot; cannot be set mean(pred==test$Species) # accuracy ## [1] 0.9777778 SVM can only work on linear separable problems. ( A problem is called linearly separable if there exists at least one line in the plane with all of the points of one group on one side of the line and all the points of the others group on the other side). However, there is a trick, the so called kernel trick. The kernel trick maps the predictor space into a (higher dimensional) space in which the problem is linear separable: set.seed(42) x1 = seq(-3, 3, length.out = 100) x2 = seq(-3, 3, length.out = 100) X = expand.grid(x1, x2) y = apply(X, 1, function(x) exp(-x[1]^2 - x[2]^2)) y = ifelse(1/(1+exp(-y)) &lt; 0.62, 0, 1) image(matrix(y, 100, 100)) animation::saveGIF({ for (i in c(&quot;truth&quot;,&quot;linear&quot;, &quot;radial&quot;, &quot;sigmoid&quot;)) { if(i == &quot;truth&quot;){ image(matrix(y, 100,100),main = &quot;Ground truth&quot;,axes = FALSE, las = 2) }else{ sv = e1071::svm(x = X, y = factor(y), kernel = i) image(matrix(as.numeric(as.character(predict(sv, X))), 100,100),main = paste0(&quot;Kernel: &quot;, i),axes = FALSE, las = 2) axis(1, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1)) axis(2, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1), las = 2) } } },movie.name = &quot;svm.gif&quot;, autobrowse = FALSE) 3.4 Artificial neural networks Regularization in ANNs library(keras) data = airquality summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 Min. :5.000 Min. : 1.0 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 1st Qu.:6.000 1st Qu.: 8.0 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 Median :7.000 Median :16.0 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 Mean :6.993 Mean :15.8 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 Max. :9.000 Max. :31.0 ## NA&#39;s :37 NA&#39;s :7 data = data[complete.cases(data),] # remove NAs summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.0 Min. : 7.0 Min. : 2.30 Min. :57.00 Min. :5.000 Min. : 1.00 ## 1st Qu.: 18.0 1st Qu.:113.5 1st Qu.: 7.40 1st Qu.:71.00 1st Qu.:6.000 1st Qu.: 9.00 ## Median : 31.0 Median :207.0 Median : 9.70 Median :79.00 Median :7.000 Median :16.00 ## Mean : 42.1 Mean :184.8 Mean : 9.94 Mean :77.79 Mean :7.216 Mean :15.95 ## 3rd Qu.: 62.0 3rd Qu.:255.5 3rd Qu.:11.50 3rd Qu.:84.50 3rd Qu.:9.000 3rd Qu.:22.50 ## Max. :168.0 Max. :334.0 Max. :20.70 Max. :97.00 Max. :9.000 Max. :31.00 X = scale(data[,2:6]) Y = data[,1] model = keras_model_sequential() penalty = 0.01 model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L), kernel_regularizer = regularizer_l1(penalty)) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty) ) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty)) %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, kernel_regularizer = regularizer_l1(penalty)) # one output dimension with a linear activation function summary(model) ## Model: &quot;sequential_2&quot; ## ______________________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================================================== ## dense_8 (Dense) (None, 100) 600 ## ______________________________________________________________________________________________________________________________________________________________ ## dense_9 (Dense) (None, 100) 10100 ## ______________________________________________________________________________________________________________________________________________________________ ## dense_10 (Dense) (None, 100) 10100 ## ______________________________________________________________________________________________________________________________________________________________ ## dense_11 (Dense) (None, 1) 101 ## ============================================================================================================================================================== ## Total params: 20,901 ## Trainable params: 20,901 ## Non-trainable params: 0 ## ______________________________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(0.1)) model_history = model %&gt;% fit(x = X, y = matrix(Y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; weights = lapply(model$weights, function(w) w$numpy() ) fields::image.plot(weights[[1]]) 3.5 The standard ML pipeline The typical ML workflow consist of: Data cleaning and exploration (EDA=explorative data analysis) with tidyverse Pre-processing and feature selection Splitting dataset into train and test set for evaluation Model fitting Model evaluation New predictions Here is an (optional) video that explains the entire pipeline from a slightly different perspective In the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics here A good reference is R for data science by Hadley 3.5.1 Example of the ML workflow with the titanic dataset For this lecture you need the titanic dataset provided by us. You can find it in GRIPS (datasets.RData in the dataset and submission section) or at . We have split the dataset already into training and testing datasets (the test split has one column less than the train split, why?) 3.5.1.1 Data cleaning Load necessary libraries: library(keras) library(tensorflow) library(tidyverse) Load dataset: load(&quot;datasets.RData&quot;) train = titanic$train test = titanic$test For cleaning and exploration we will combine the datasets together (if we change a predictor in the train set, we have also to change it in the test set…). But we will create a new variable “subset” that tells us whether an observation belongs to the train or test split: test$survived = NA train$subset = &quot;train&quot; test$subset = &quot;test&quot; data = rbind(train,test) Standard summaries: str(data) ## &#39;data.frame&#39;: 111 obs. of 6 variables: ## $ Ozone : int 41 36 12 18 23 19 8 16 11 14 ... ## $ Solar.R: int 190 118 149 313 299 99 19 256 290 274 ... ## $ Wind : num 7.4 8 12.6 11.5 8.6 13.8 20.1 9.7 9.2 10.9 ... ## $ Temp : int 67 72 74 62 65 59 61 69 66 68 ... ## $ Month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : int 1 2 3 4 7 8 9 12 13 14 ... summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.0 Min. : 7.0 Min. : 2.30 Min. :57.00 Min. :5.000 Min. : 1.00 ## 1st Qu.: 18.0 1st Qu.:113.5 1st Qu.: 7.40 1st Qu.:71.00 1st Qu.:6.000 1st Qu.: 9.00 ## Median : 31.0 Median :207.0 Median : 9.70 Median :79.00 Median :7.000 Median :16.00 ## Mean : 42.1 Mean :184.8 Mean : 9.94 Mean :77.79 Mean :7.216 Mean :15.95 ## 3rd Qu.: 62.0 3rd Qu.:255.5 3rd Qu.:11.50 3rd Qu.:84.50 3rd Qu.:9.000 3rd Qu.:22.50 ## Max. :168.0 Max. :334.0 Max. :20.70 Max. :97.00 Max. :9.000 Max. :31.00 head(data) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 "]]
