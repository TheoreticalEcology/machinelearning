[["index.html", "Machine Learning and AI in TensorFlow and R 1 Prerequisites", " Machine Learning and AI in TensorFlow and R Maximilian Pichler and Florian Hartig 2021-04-30 1 Prerequisites R+Rstudio "],["introduction.html", "2 Introduction to Machine Learning 2.1 Supervised learning: regression and classification 2.2 Unsupervised learning 2.3 Introduction to Tensorflow 2.4 First steps with the keras framework", " 2 Introduction to Machine Learning In this lesson, we introduce the three basic ML tasks: supervised regression and classification, and unsupervised learning. In ML, we distinguish 3 basic learning paradigms: - Supervised learning - Unsupervised learning - Reinforcement learning We will speak about reinforcement learning at the end of the course. Now, we want to look at examples of supervised and unsupervised learning. Before you start with the code, here a video to remind you of what we talked about in the class: 2.1 Supervised learning: regression and classification Two two main subbranches of supervised learning are regression and classification. Here a video that explains again the difference 2.1.1 Supervised regression using Random Forest The random forest (RF) algorithm is possibly the most widely used ML algorithm and can be used for regression and classification. We will talk more about the algorithm on Day 2. Here an example of a regression: Visualization of the data: plot(iris, col = iris$Species) Fitting the model library(randomForest) m1 &lt;- randomForest(Sepal.Length ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Sepal.Length ~ ., data = iris) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 0.1362531 ## % Var explained: 80 Visualization of the results par(mfrow = c(1,2)) plot(predict(m1), iris$Sepal.Length, xlab = &quot;predicted&quot;, ylab = &quot;observed&quot;) abline(0,1) varImpPlot(m1) This is a nice visualization of the RF structure, but requires to load a package from GitHub # devtools::install_github(&#39;araastat/reprtree&#39;) reprtree:::plot.getTree(m1, iris) 2.1.2 Supervised classification using Random Forest Fitting the model: set.seed(123) m1 &lt;- randomForest(Species ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Species ~ ., data = iris) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 4.67% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 50 0 0 0.00 ## versicolor 0 47 3 0.06 ## virginica 0 4 46 0.08 Visualizing the fitted model: par(mfrow = c(1,2)) reprtree:::plot.getTree(m1, iris) Visualizing results ecologically: oldpar &lt;- par(mfrow = c(1,2)) plot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = &quot;observed&quot;) plot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = &quot;predicted&quot;) Confusion matrix: table(predict(m1),iris$Species) ## ## setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 4 ## virginica 0 3 46 2.2 Unsupervised learning In unsupervised learning, we basically want to identify patterns in data without having any guidance (supervision) about what the correct patterns / classes are. It is all much easier with a practical example. Consider our iris dataset. Here, we have observations of different species Together with their flower traits Imagine we didn’t know what species are. This is basically the situation in which people in the antique would have been. There is no book to look up species. You just noted that there seem to be some kind of plants that have different flowers than another, so you decide to call them by a different name. This kind of process is what unsupervised learning does. 2.2.1 k-means clustering An example for an unsupervised learning algorithm is k-means clustering, one of the simplest and popular unsupervised machine learning algorithms. A cluster refers to a collection of data points aggregated together because of certain similarities. In the algorithm, you’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid. sIris = scale(iris[,1:4]) model&lt;- kmeans(sIris,3) # aplly k-means algorithm with no. of centroids(k)=3 model ## K-means clustering with 3 clusters of sizes 47, 53, 50 ## ## Cluster means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 1.13217737 0.08812645 0.9928284 1.0141287 ## 2 -0.05005221 -0.88042696 0.3465767 0.2805873 ## 3 -1.01119138 0.85041372 -1.3006301 -1.2507035 ## ## Clustering vector: ## [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 1 ## [67] 2 2 2 2 1 2 2 2 2 1 1 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 1 1 1 1 1 ## [133] 1 2 2 1 1 1 2 1 1 1 2 1 1 1 2 1 1 2 ## ## Within cluster sum of squares by cluster: ## [1] 47.45019 44.08754 47.35062 ## (between_SS / total_SS = 76.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Visualizing the results: par(mfrow = c(1,2)) plot(Petal.Length~Petal.Width, data = sIris, col = model$cluster, main = &quot;Predicted clusters&quot;) plot(Petal.Length~Petal.Width, data = sIris, col = iris$Species, main = &quot;True species&quot;) Confusion matrix: table(model$cluster,iris$Species) ## ## setosa versicolor virginica ## 1 0 11 36 ## 2 0 39 14 ## 3 50 0 0 2.3 Introduction to Tensorflow TF is a math library which is highly optimized for neural networks If a GPU is available, computations can be easily run on the GPU but even on the CPU is TF still very fast The “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for the GPU) The interface (the part of TF that we use) is written in python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend. 2.3.1 Tensorflow data containers TF has two data containers (structures): - constant (tf\\(constant) :creates a constant (immutable) value in the computation graph - variable (tf\\)Variable): creates a mutable value in the computation graph (used as parameter/weight in models) library(tensorflow) # Don&#39;t worry about weird messages. TF supports additional optimizations exists(&quot;tf&quot;) ## [1] TRUE Don’t worry about weird messages (they will only appear once at the start of the session). a = tf$constant(5) b = tf$constant(10) print(a) ## tf.Tensor(5.0, shape=(), dtype=float32) print(b) ## tf.Tensor(10.0, shape=(), dtype=float32) c = tf$add(a, b) print(c) ## tf.Tensor(15.0, shape=(), dtype=float32) tf$print(c) Normal R methods such as print() are provided by the R package “tensorflow”. The tensorflow library (created by the RStudio team) built R methods for all common operations: `+.tensorflow.tensor` = function(a, b) return(tf$add(a,b)) tf$print(a+b) Their operators also transfrom automatically R numbers into constant tensors when attempting to add a tensor to a R number: d = c + 5 # 5 is automatically converted to a tensor print(d) ## tf.Tensor(20.0, shape=(), dtype=float32) TF container are objects, which means that they are not just simple variables of type numeric (class(5)), and they have methods For instance, there is a method to transform the tensor object back to a R object: class(d) ## [1] &quot;tensorflow.tensor&quot; &quot;tensorflow.python.framework.ops.EagerTensor&quot; ## [3] &quot;tensorflow.python.framework.ops._EagerTensorBase&quot; &quot;tensorflow.python.framework.ops.Tensor&quot; ## [5] &quot;tensorflow.python.types.internal.NativeObject&quot; &quot;tensorflow.python.types.core.Tensor&quot; ## [7] &quot;python.builtin.object&quot; class(d$numpy()) ## [1] &quot;numeric&quot; 2.3.2 Tensorflow data types - good practise with R-TF R uses dynamic typing, which means you can assign to a variable a number, character, function or whatever, and R infers the type automatically. In other languages you have to state explicitly the type, e.g. in C: int a = 5; float a = 5.0; char a = “a”; While TF tries to infer dynamically the type, often you must state it explicitly. Common important types: - float32 (floating point number with 32bits, “single precision”) - float64 (“double precision”) - int8 (integer with 8bits) Why does TF support float32 numbers when most cpus today can handle easily 64bit numbers? Many GPUs (e.g. the NVIDIA geforces) can handle only up to 32bit numbers! (you do not need high precision in graphical modeling) r_matrix = matrix(runif(10*10), 10,10) m = tf$constant(r_matrix, dtype = &quot;float64&quot;) b = tf$constant(2.0, dtype = &quot;float64&quot;) c = m / b Instead of a string, you can also provide a tf$float64 object m = tf$constant(r_matrix, dtype = tf$float32) b = tf$constant(2.0, dtype = tf$float64) c = m / b # doesn&#39;t work! we try to divide float32/float64 Tensorflow arguments often require exact/explicit data types: TF expects for arguments integers, however, R needs a “L” after an integer to tell the R interpreter that it should be treated as an integer: is.integer(5) is.integer(5L) matrix(t(r_matrix), 5, 20, byrow = TRUE) tf$reshape(r_matrix, shape = c(5, 20))$numpy() tf$reshape(r_matrix, shape = c(5L, 20L))$numpy() Note: skipping the “L” is one of the most common errors when using R-TF! 2.4 First steps with the keras framework Objective of this lesson: familiarize yourself with keras. keras is a higher level API within TF and developed to build easily neural networks. Keras can be found within TF: tf.keras…however, the RStudio team built a pkg on top of tf.keras: 2.4.1 Example workflow in keras We will now build a small classifier in keras to predict the three species of the iris dataset: Load the necessary packages and datasets: library(keras) library(tensorflow) data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa It is beneficial for neural networks to scale the predictors (scaling = centering and standardization, see ?scale): X = scale(iris[,1:4]) Y = iris[,5] We also split our data into the predictors (X) and the response (Y = the three species) keras/tf cannot handle factors and we have to create contrasts (one-hot encoding): Y = to_categorical(as.integer(Y)-1L, 3) head(Y) # 3 colums, one for each level in the response ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 0 0 ## [3,] 1 0 0 ## [4,] 1 0 0 ## [5,] 1 0 0 ## [6,] 1 0 0 Do you have an idea why we subtracted 1L from the labels after we transformed them? (Tip: google “Is r 0 or 1 indexed?”) Model building: Initiliaze a sequential model in keras: model = keras_model_sequential() A sequential keras model is a higher order type of model within keras and consists of one input and one output model. Add hidden layers to the model (we will learn more about DNNs during the next days): model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(4L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 3L, activation = &quot;softmax&quot;) softmax scales to 0 1 and overall to 0 - 1, 3 output nodes for 3 response classes/labels compile the model with a cross entropy loss function and Adamax optimizer: model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(0.001)) summary(model) ## Model: &quot;sequential_14&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## dense_37 (Dense) (None, 20) 100 ## _________________________________________________________________________________________________________________________________________ ## dense_38 (Dense) (None, 20) 420 ## _________________________________________________________________________________________________________________________________________ ## dense_39 (Dense) (None, 20) 420 ## _________________________________________________________________________________________________________________________________________ ## dense_40 (Dense) (None, 3) 63 ## ========================================================================================================================================= ## Total params: 1,003 ## Trainable params: 1,003 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ For now, do not worry about the “lr” argument, crossentropy or the optimizer Fit the model in 30 (epochs) iterations: model_history = model %&gt;% fit(x = X, y = apply(Y,2,as.integer), epochs = 30L, batch_size = 20L, shuffle = TRUE) Plot the training history: plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Create predictions: predictions = predict(model, X) # probabilities for each class We will get probabilites: head(predictions) # quasi-probabilities for each species ## [,1] [,2] [,3] ## [1,] 0.9873959 0.007063707 0.005540390 ## [2,] 0.9460187 0.044511009 0.009470323 ## [3,] 0.9804348 0.014451948 0.005113249 ## [4,] 0.9700417 0.023438154 0.006520080 ## [5,] 0.9914106 0.004160772 0.004428639 ## [6,] 0.9875814 0.004044346 0.008374327 For each site, we want to know for which species we got the highest probability: preds = apply(predictions, 1, which.max) print(preds) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 3 2 3 2 3 2 2 2 2 3 2 3 ## [67] 2 2 2 2 3 2 3 3 3 3 3 3 2 2 2 2 2 3 2 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 3 3 3 3 3 2 3 2 3 3 3 3 3 3 3 3 3 3 ## [133] 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 Calculate Accuracy: mean(preds == as.integer(iris$Species)) ## [1] 0.8333333 Plot predictions: oldpar = par() par(mfrow = c(1,2)) plot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species, main = &quot;Observed&quot;) plot(iris$Sepal.Length, iris$Petal.Length, col = preds, main = &quot;Predicted&quot;) "],["fund.html", "3 Fundamental principles and techniques 3.1 Machine learning principles 3.2 Tree-based ML algorithms 3.3 Distance-based algorithms 3.4 Artificial neural networks 3.5 The standard ML pipeline at the example of the titanic dataset", " 3 Fundamental principles and techniques 3.1 Machine learning principles 3.1.1 Optimization from wikipedia: \" an optimization problem is the problem of finding the best solution from all feasible solutions\" Why do we need this “optimization”? A loss function (e.g. we tell in each training step the algorithm how many observations were miss-classified) guides the training of ML algorithms Based on the loss, the optimizer tries to update the weights of the ML algorithms in a way that the loss function is minimized Calculating analytically the global optima of a function is a non-trivial problem and bunch of diverse optimization algorithms evolved Some optimization algorithms are inspired by biological systemse.g. Ants, Bee, or even slimve algorithms): 3.1.1.1 Small optimization example We have the following function: func = function(x) return(x^2) which we want to minimize, we could do this by hand: a = rnorm(100) plot(a, func(a)) The smallest value is at x = 0 (to be honest, we can calculate this for this simple case analytically) We can also use an optimizer with the optim(…) function opt = optim(1.0, func) ## Warning in optim(1, func): one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly print(opt$par) ## [1] -8.881784e-16 opt$par will return the best values found by the optimizer 3.1.1.2 Advanced optimization example We will now optimze the weights (slopes) for linear regression model. Basically, we will implement lm(y~x) on our own: Load the airquality dataset, remove NAs, split it into predictors and response, and scale the predictors: data = airquality[complete.cases(airquality$Ozone) &amp; complete.cases(airquality$Solar.R),] X = scale(data[,-1]) Y = data$Ozone The model we want to optimize: \\(ozone = Solar.R*X1 + Wind*X2 + Temp*X3 + Month*X4 + Day*X5 + X6\\) Our loss function: mean(predicted ozone - true ozone)^2) We found to find the parameters X1-X6 for which the loss function is the smallest: linear_regression = function(w) { pred = w[1]*X[,1] + # Solar.R w[2]*X[,2] + # Wind w[3]*X[,3] + # Temp w[4]*X[,4] + # Month w[5]*X[,5] + w[6] # or X %*% w[1:5] + w[6] # loss = MSE, we want to find the optimal weights # to minimize the sum of squared residuals loss = mean((pred - Y)^2) return(loss) } The linear_regression function takes potential solutions for the weights (X1-X6) and will return the loss for these weights: linear_regression(runif(6)) ## [1] 2847.567 Let’s try it bruteforce (which means we will try to find the optimal solution with random a set of random weights): random_search = matrix(runif(6*5000,-10,10), 5000, 6) losses = apply(random_search, 1, linear_regression) plot(losses, type = &quot;l&quot;) random_search[which.min(losses),] ## [1] 5.0567712 -7.8550341 7.4573370 0.8109562 2.0097918 9.3807506 Bruteforce isn’t a good approach, it might work well with only a few parameters. Let’s try it with the optim function: opt = optim(runif(6, -1, 1), linear_regression) opt$par ## [1] 4.150032 -15.100683 11.409332 -5.445377 -3.115452 42.502556 Compare the weights the estimated weights of the lm() function: coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 3.1.2 Regularization There are several ways to regularize models. In this section we will focus on lasso and ridge regularization for weights in neural networks. The idea of lasso and ridge regularization is to put some type of rubber band on the weights and pull them to zero, important weights are able to pull away from zero. Lasso and ridge have slightly different properties: Lasso: abs( sum(Weights)) Ridge: (sum(Weights))^2 Lasso tries to push the weights directly to zero, where as the Ridge allows small values around zero (caused by the difference of the absolute and squared function). Let’s have a look at a keras model with and without regularization (we will use here a network without hidden layers == linear regression model): library(keras) data = airquality[complete.cases(airquality),] X = scale(data[,-1]) Y = data$Ozone # l1/l2 on linear model model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2])) summary(model) ## Model: &quot;sequential_15&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## dense_41 (Dense) (None, 1) 6 ## ========================================================================================================================================= ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 50L, batch_size = 20L, shuffle = TRUE) unconstrained = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 Now we will put a l1 (lasso) regularization on the weights: model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2]), kernel_regularizer = regularizer_l1(10), bias_regularizer = regularizer_l1(10)) summary(model) ## Model: &quot;sequential_16&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## dense_42 (Dense) (None, 1) 6 ## ========================================================================================================================================= ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5), metrics = c(metric_mean_squared_error)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) l1 = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 cbind(unlist(l1), unlist(unconstrained)) ## [,1] [,2] ## [1,] 1.767281771 4.825849 ## [2,] -8.808810234 -12.174186 ## [3,] 12.964618683 17.419437 ## [4,] -0.001905646 -4.348501 ## [5,] 0.044293262 2.311703 ## [6,] 33.291839600 41.056118 3.2 Tree-based ML algorithms Famous ML algorithms such as random Forest and gradient boosted trees are based on classification and regression trees. 3.2.1 Classification and Regression Trees In this lecture we will explore regression and classifaction trees at the example of the airquality data set: library(rpart) library(rpart.plot) data=airquality[complete.cases(airquality),] Fit and visualize a regression tree: rt = rpart(Ozone~., data = data,control = rpart.control(minsplit = 10)) rpart.plot(rt) Visualize the predictions: pred = predict(rt, data) plot(data$Temp, data$Ozone) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) The angular form of the prediction line is typical for regression trees. There is one important hyper-parameter for regression trees: minsplit controls the depth of tree (see the help of tree for a description) controls the complexity of the tree and can be seen also as a regularization parameter 3.2.2 Random Forest Random Forest creates an ensemble of regression/classification trees. However, there are two randomization steps with the RF that are responsible for the success of RF: bootstrap sample for each tree (we will sample observations with replacement from the dataset) at each split, we will sample a subset of predictors which are then considered as potential splitting criterion Fit a RF and visualize the predictions: library(randomForest) rf = randomForest(Ozone~., data = data) pred = predict(rf, data) plot(Ozone~Temp, data = data) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) One advantage of RF is that we will get a variable importance: importance(rf) ## IncNodePurity ## Solar.R 18441.43 ## Wind 31140.89 ## Temp 33965.21 ## Month 11031.51 ## Day 15610.66 Important hyperparameters: Similar to regression and classification trees, the hyper parameter nodesize controls for complexity. -&gt; Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5). mtry - Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times. 3.2.3 Boosted regression trees RF fits hundreds of trees independent of each other. In boosted regression trees, we will start with a weak learner (weak learner == regression tree) and then fit sequentially additional weak learners. There are two different approaches to enhance the performance: AdaBoost, wrong classified observations (by the previous tree) will get a higher weight, the chain of trees will focus on difficult/missclassified observations Gradient boosting (state of the art), each sequential model will be fit on the residual errors of the previous model Fit a BRT using xgboost: library(xgboost) data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) brt = xgboost(data_xg, nrounds = 16L, nthreads = 4L) ## [11:39:36] WARNING: amalgamation/../src/learner.cc:516: ## Parameters: { nthreads } might not be used. ## ## This may not be accurate due to some parameters are only used in language bindings but ## passed down to XGBoost core. Or some parameters are not used but slip through this ## verification. Please open an issue if you find above cases. ## ## ## [1] train-rmse:39.724625 ## [2] train-rmse:30.225761 ## [3] train-rmse:23.134842 ## [4] train-rmse:17.899178 ## [5] train-rmse:14.097785 ## [6] train-rmse:11.375458 ## [7] train-rmse:9.391275 ## [8] train-rmse:7.889690 ## [9] train-rmse:6.646585 ## [10] train-rmse:5.804860 ## [11] train-rmse:5.128438 ## [12] train-rmse:4.456416 ## [13] train-rmse:4.069464 ## [14] train-rmse:3.674615 ## [15] train-rmse:3.424578 ## [16] train-rmse:3.191302 xgboost has a weird syntax, we have to transform our data into a xgb.DMatrix object to be able to fit the model. We will do 500 rounds which means we will fit 500 sequential models: Let us visualize the predictions for different number of trees: par(mfrow = c(2,2)) for(i in 1:4){ pred = predict(brt, newdata = data_xg, ntreelimit = i) plot(data$Temp, data$Ozone, main = i) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) } xgboost also provides an variable importance: xgboost::xgb.importance(model = brt) ## Feature Gain Cover Frequency ## 1: Temp 0.570071875 0.2958229 0.24836601 ## 2: Wind 0.348230710 0.3419576 0.24183007 ## 3: Solar.R 0.058795559 0.1571072 0.30718954 ## 4: Day 0.019530002 0.1779925 0.16993464 ## 5: Month 0.003371853 0.0271197 0.03267974 sqrt(mean((data$Ozone - pred)^2)) # RMSE ## [1] 17.89918 data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) xgboost has an argument to do cross-validation: brt = xgboost(data_xg, nrounds = 5L) ## [1] train-rmse:39.724625 ## [2] train-rmse:30.225761 ## [3] train-rmse:23.134842 ## [4] train-rmse:17.899178 ## [5] train-rmse:14.097785 brt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L, nrounds = 3L, nthreads = 4L) ## [1] train-rmse:39.978896+0.286917 test-rmse:40.933159+0.927614 ## [2] train-rmse:30.598420+0.425075 test-rmse:33.257564+1.932901 ## [3] train-rmse:23.706881+0.437094 test-rmse:27.806431+2.435701 print(brt_cv) ## ##### xgb.cv 3-folds ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 1 39.97890 0.2869171 40.93316 0.9276142 ## 2 30.59842 0.4250749 33.25756 1.9329006 ## 3 23.70688 0.4370938 27.80643 2.4357008 There are different ways to control for complexity: max_depth, depth of each tree shrinkage (each tree will get a weight and the weight will decrease with the number of trees) 3.3 Distance-based algorithms In this chapter, we introduce support-vector machines (SVMs) and other distance-based methods. 3.3.1 k-nearest-neighbor k-nearest-neighbor classifies new observations by calculating the nearest n neighbors. The labels of the n nearest neighbors decide the class of the new point: X = scale(iris[,1:4]) Y = iris[,5] plot(X[-100,1], X[-100,3], col = Y) points(X[100,1], X[100,3], col = &quot;blue&quot;, pch = 18, cex = 1.3) Which class would you decide for the blue point? What are the classes of the nearest points?… well this procedure is used by the kNN: Scaling is very important when dealing with distances (we also split the dataset into a training and a testing dataset): data = iris data[,1:4] = apply(data[,1:4],2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] Fit model and create predictions: library(kknn) knn = kknn(Species~., train = train, test = test) summary(knn) ## ## Call: ## kknn(formula = Species ~ ., train = train, test = test) ## ## Response: &quot;nominal&quot; ## fit prob.setosa prob.versicolor prob.virginica ## 1 setosa 1 0.0000000 0.00000000 ## 2 setosa 1 0.0000000 0.00000000 ## 3 setosa 1 0.0000000 0.00000000 ## 4 setosa 1 0.0000000 0.00000000 ## 5 setosa 1 0.0000000 0.00000000 ## 6 setosa 1 0.0000000 0.00000000 ## 7 setosa 1 0.0000000 0.00000000 ## 8 setosa 1 0.0000000 0.00000000 ## 9 setosa 1 0.0000000 0.00000000 ## 10 setosa 1 0.0000000 0.00000000 ## 11 setosa 1 0.0000000 0.00000000 ## 12 setosa 1 0.0000000 0.00000000 ## 13 setosa 1 0.0000000 0.00000000 ## 14 setosa 1 0.0000000 0.00000000 ## 15 setosa 1 0.0000000 0.00000000 ## 16 setosa 1 0.0000000 0.00000000 ## 17 versicolor 0 0.9354939 0.06450608 ## 18 versicolor 0 0.6349387 0.36506134 ## 19 versicolor 0 1.0000000 0.00000000 ## 20 versicolor 0 0.7783044 0.22169561 ## 21 versicolor 0 1.0000000 0.00000000 ## 22 versicolor 0 1.0000000 0.00000000 ## 23 versicolor 0 0.6430958 0.35690421 ## 24 versicolor 0 1.0000000 0.00000000 ## 25 virginica 0 0.2580081 0.74199187 ## 26 versicolor 0 0.9148730 0.08512700 ## 27 versicolor 0 1.0000000 0.00000000 ## 28 versicolor 0 0.7245826 0.27541743 ## 29 versicolor 0 1.0000000 0.00000000 ## 30 versicolor 0 1.0000000 0.00000000 ## 31 versicolor 0 0.7890886 0.21091135 ## 32 versicolor 0 1.0000000 0.00000000 ## 33 versicolor 0 0.9511855 0.04881448 ## 34 versicolor 0 1.0000000 0.00000000 ## 35 virginica 0 0.0000000 1.00000000 ## 36 virginica 0 0.0851270 0.91487300 ## 37 virginica 0 0.0000000 1.00000000 ## 38 virginica 0 0.0000000 1.00000000 ## 39 virginica 0 0.0000000 1.00000000 ## 40 versicolor 0 0.7013345 0.29866548 ## 41 virginica 0 0.2109114 0.78908865 ## 42 virginica 0 0.0000000 1.00000000 ## 43 virginica 0 0.0000000 1.00000000 ## 44 virginica 0 0.0000000 1.00000000 ## 45 virginica 0 0.0156916 0.98430840 table(test$Species, fitted(knn)) ## ## setosa versicolor virginica ## setosa 16 0 0 ## versicolor 0 17 1 ## virginica 0 1 10 Actually, there is no “real” learning in a kNN. 3.3.2 Support Vector Machines (SVM) Support vectors machine try to find a hyperplane in the predictor space which separates the classes in the best way. Fitting a SVM: library(e1071) data = iris data[,1:4] = apply(data[,1:4],2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] sm = svm(Species~., data = train, kernel = &quot;linear&quot;) pred = predict(sm, newdata = test) oldpar = par() par(mfrow = c(1,2)) plot(test$Sepal.Length, test$Petal.Length, col = pred, main = &quot;predicted&quot;) plot(test$Sepal.Length, test$Petal.Length, col = test$Species, main = &quot;observed&quot;) par(oldpar) ## Warning in par(oldpar): graphical parameter &quot;cin&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;cra&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;csi&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;cxy&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;din&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;page&quot; cannot be set mean(pred==test$Species) # accuracy ## [1] 0.9777778 SVM can only work on linear separable problems. ( A problem is called linearly separable if there exists at least one line in the plane with all of the points of one group on one side of the line and all the points of the others group on the other side). However, there is a trick, the so called kernel trick. The kernel trick maps the predictor space into a (higher dimensional) space in which the problem is linear separable: set.seed(42) x1 = seq(-3, 3, length.out = 100) x2 = seq(-3, 3, length.out = 100) X = expand.grid(x1, x2) y = apply(X, 1, function(x) exp(-x[1]^2 - x[2]^2)) y = ifelse(1/(1+exp(-y)) &lt; 0.62, 0, 1) image(matrix(y, 100, 100)) animation::saveGIF({ for (i in c(&quot;truth&quot;,&quot;linear&quot;, &quot;radial&quot;, &quot;sigmoid&quot;)) { if(i == &quot;truth&quot;){ image(matrix(y, 100,100),main = &quot;Ground truth&quot;,axes = FALSE, las = 2) }else{ sv = e1071::svm(x = X, y = factor(y), kernel = i) image(matrix(as.numeric(as.character(predict(sv, X))), 100,100),main = paste0(&quot;Kernel: &quot;, i),axes = FALSE, las = 2) axis(1, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1)) axis(2, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1), las = 2) } } },movie.name = &quot;svm.gif&quot;, autobrowse = FALSE) 3.4 Artificial neural networks Regularization in ANNs library(keras) data = airquality summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 Min. :5.000 Min. : 1.0 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 1st Qu.:6.000 1st Qu.: 8.0 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 Median :7.000 Median :16.0 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 Mean :6.993 Mean :15.8 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 Max. :9.000 Max. :31.0 ## NA&#39;s :37 NA&#39;s :7 data = data[complete.cases(data),] # remove NAs summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.0 Min. : 7.0 Min. : 2.30 Min. :57.00 Min. :5.000 Min. : 1.00 ## 1st Qu.: 18.0 1st Qu.:113.5 1st Qu.: 7.40 1st Qu.:71.00 1st Qu.:6.000 1st Qu.: 9.00 ## Median : 31.0 Median :207.0 Median : 9.70 Median :79.00 Median :7.000 Median :16.00 ## Mean : 42.1 Mean :184.8 Mean : 9.94 Mean :77.79 Mean :7.216 Mean :15.95 ## 3rd Qu.: 62.0 3rd Qu.:255.5 3rd Qu.:11.50 3rd Qu.:84.50 3rd Qu.:9.000 3rd Qu.:22.50 ## Max. :168.0 Max. :334.0 Max. :20.70 Max. :97.00 Max. :9.000 Max. :31.00 X = scale(data[,2:6]) Y = data[,1] model = keras_model_sequential() penalty = 0.01 model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L), kernel_regularizer = regularizer_l1(penalty)) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty) ) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty)) %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, kernel_regularizer = regularizer_l1(penalty)) # one output dimension with a linear activation function summary(model) ## Model: &quot;sequential_17&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## dense_43 (Dense) (None, 100) 600 ## _________________________________________________________________________________________________________________________________________ ## dense_44 (Dense) (None, 100) 10100 ## _________________________________________________________________________________________________________________________________________ ## dense_45 (Dense) (None, 100) 10100 ## _________________________________________________________________________________________________________________________________________ ## dense_46 (Dense) (None, 1) 101 ## ========================================================================================================================================= ## Total params: 20,901 ## Trainable params: 20,901 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(0.1)) model_history = model %&gt;% fit(x = X, y = matrix(Y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; weights = lapply(model$weights, function(w) w$numpy() ) fields::image.plot(weights[[1]]) 3.5 The standard ML pipeline at the example of the titanic dataset The typical ML workflow consist of: Data cleaning and exploration (EDA=explorative data analysis) with tidyverse Pre-processing and feature selection Splitting dataset into train and test set for evaluation Model fitting Model evaluation New predictions Here is an (optional) video that explains the entire pipeline from a slightly different perspective In the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics here A good reference is R for data science by Hadley For this lecture you need the titanic dataset provided by us. You can find it in GRIPS (datasets.RData in the dataset and submission section) or at . We have split the dataset already into training and testing datasets (the test split has one column less than the train split, why?) 3.5.1 Data cleaning Load necessary libraries: library(keras) library(tensorflow) library(tidyverse) Load dataset: load(&quot;datasets.RData&quot;) train = titanic$train test = titanic$test For cleaning and exploration we will combine the datasets together (if we change a predictor in the train set, we have also to change it in the test set…). But we will create a new variable “subset” that tells us whether an observation belongs to the train or test split: test$survived = NA train$subset = &quot;train&quot; test$subset = &quot;test&quot; data = rbind(train,test) Standard summaries: str(data) ## &#39;data.frame&#39;: 1309 obs. of 15 variables: ## $ pclass : int 2 1 3 3 3 3 3 1 3 1 ... ## $ survived : int 1 1 0 0 0 0 0 1 0 1 ... ## $ name : chr &quot;Sinkkonen, Miss. Anna&quot; &quot;Woolner, Mr. Hugh&quot; &quot;Sage, Mr. Douglas Bullen&quot; &quot;Palsson, Master. Paul Folke&quot; ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ... ## $ age : num 30 NA NA 6 30.5 38.5 20 53 NA 42 ... ## $ sibsp : int 0 0 8 3 0 0 0 0 0 0 ... ## $ parch : int 0 0 2 1 0 0 0 0 0 0 ... ## $ ticket : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 221 123 779 542 589 873 472 823 588 834 ... ## $ fare : num 13 35.5 69.55 21.07 8.05 ... ## $ cabin : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 1 94 1 1 1 1 1 1 1 1 ... ## $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 4 4 4 4 4 4 2 4 2 ... ## $ boat : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 3 28 1 1 1 1 1 19 1 15 ... ## $ body : int NA NA NA NA 50 32 NA NA NA NA ... ## $ home.dest: Factor w/ 370 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ... ## $ subset : chr &quot;train&quot; &quot;train&quot; &quot;train&quot; &quot;train&quot; ... summary(data) ## pclass survived name sex age sibsp parch ticket ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 Min. :0.000 CA. 2343: 11 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 1601 : 8 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 Median :0.000 CA 2144 : 8 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 Mean :0.385 3101295 : 7 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 347077 : 7 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 Max. :9.000 347082 : 7 ## NA&#39;s :655 NA&#39;s :263 (Other) :1261 ## fare cabin embarked boat body home.dest subset ## Min. : 0.000 :1014 : 2 :823 Min. : 1.0 :564 Length:1309 ## 1st Qu.: 7.896 C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 New York, NY : 64 Class :character ## Median : 14.454 B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 London : 14 Mode :character ## Mean : 33.295 G6 : 5 S:914 15 : 37 Mean :160.8 Montreal, PQ : 10 ## 3rd Qu.: 31.275 B96 B98 : 4 14 : 33 3rd Qu.:256.0 Cornwall / Akron, OH: 9 ## Max. :512.329 C22 C26 : 4 4 : 31 Max. :328.0 Paris, France : 9 ## NA&#39;s :1 (Other) : 271 (Other):308 NA&#39;s :1188 (Other) :639 head(data) ## pclass survived name sex age sibsp parch ticket fare cabin embarked boat body ## 561 2 1 Sinkkonen, Miss. Anna female 30.0 0 0 250648 13.000 S 10 NA ## 321 1 1 Woolner, Mr. Hugh male NA 0 0 19947 35.500 C52 S D NA ## 1177 3 0 Sage, Mr. Douglas Bullen male NA 8 2 CA. 2343 69.550 S NA ## 1098 3 0 Palsson, Master. Paul Folke male 6.0 3 1 349909 21.075 S NA ## 1252 3 0 Tomlin, Mr. Ernest Portage male 30.5 0 0 364499 8.050 S 50 ## 1170 3 0 Saether, Mr. Simon Sivertsen male 38.5 0 0 SOTON/O.Q. 3101262 7.250 S 32 ## home.dest subset ## 561 Finland / Washington, DC train ## 321 London, England train ## 1177 train ## 1098 train ## 1252 train ## 1170 train The name variable consists of 1309 unique factors (there are 1309 observations…): length(unique(data$name)) ## [1] 1307 However, there is a title in each name. Let’s extract the titles: we will extract all names and split each name after each comma “,” we will split the second split of the name after a point “.” and extract the titles first_split = sapply(data$name, function(x) stringr::str_split(x, pattern = &quot;,&quot;)[[1]][2]) titles = sapply(first_split, function(x) strsplit(x, &quot;.&quot;,fixed = TRUE)[[1]][1]) We get 18 unique titles: table(titles) ## titles ## Capt Col Don Dona Dr Jonkheer Lady Major Master ## 1 4 1 1 8 1 1 2 61 ## Miss Mlle Mme Mr Mrs Ms Rev Sir the Countess ## 260 2 1 757 197 2 8 1 1 With the forcats package we can easily handle and mutate factor variables. A few titles have a very low occurrence rate: titles = stringr::str_trim((titles)) titles %&gt;% fct_count() ## # A tibble: 18 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Capt 1 ## 2 Col 4 ## 3 Don 1 ## 4 Dona 1 ## 5 Dr 8 ## 6 Jonkheer 1 ## 7 Lady 1 ## 8 Major 2 ## 9 Master 61 ## 10 Miss 260 ## 11 Mlle 2 ## 12 Mme 1 ## 13 Mr 757 ## 14 Mrs 197 ## 15 Ms 2 ## 16 Rev 8 ## 17 Sir 1 ## 18 the Countess 1 We will collapse titles with low occurrences into one title: titles2 = forcats::fct_collapse(titles, officer = c(&quot;Capt&quot;, &quot;Col&quot;, &quot;Major&quot;, &quot;Dr&quot;, &quot;Rev&quot;), royal = c(&quot;Jonkheer&quot;, &quot;Don&quot;, &quot;Sir&quot;, &quot;the Countess&quot;, &quot;Dona&quot;, &quot;Lady&quot;), miss = c(&quot;Miss&quot;, &quot;Mlle&quot;), mrs = c(&quot;Mrs&quot;, &quot;Mme&quot;, &quot;Ms&quot;) ) titles = stringr::str_trim((titles)) titles %&gt;% fct_count() ## # A tibble: 18 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Capt 1 ## 2 Col 4 ## 3 Don 1 ## 4 Dona 1 ## 5 Dr 8 ## 6 Jonkheer 1 ## 7 Lady 1 ## 8 Major 2 ## 9 Master 61 ## 10 Miss 260 ## 11 Mlle 2 ## 12 Mme 1 ## 13 Mr 757 ## 14 Mrs 197 ## 15 Ms 2 ## 16 Rev 8 ## 17 Sir 1 ## 18 the Countess 1 We will collapse titles with low occurrences into one title: titles2 = forcats::fct_collapse(titles, officer = c(&quot;Capt&quot;, &quot;Col&quot;, &quot;Major&quot;, &quot;Dr&quot;, &quot;Rev&quot;), royal = c(&quot;Jonkheer&quot;, &quot;Don&quot;, &quot;Sir&quot;, &quot;the Countess&quot;, &quot;Dona&quot;, &quot;Lady&quot;), miss = c(&quot;Miss&quot;, &quot;Mlle&quot;), mrs = c(&quot;Mrs&quot;, &quot;Mme&quot;, &quot;Ms&quot;) ) Count titles again: titles2 %&gt;% fct_count() ## # A tibble: 6 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 officer 23 ## 2 royal 6 ## 3 Master 61 ## 4 miss 262 ## 5 mrs 200 ## 6 Mr 757 Add new title variable to dataset: data = data %&gt;% mutate(title = titles2) As a second example, we will explore and clean the numeric “age” variable: Explore the variable: summary(data) ## pclass survived name sex age sibsp parch ticket ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 Min. :0.000 CA. 2343: 11 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 1601 : 8 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 Median :0.000 CA 2144 : 8 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 Mean :0.385 3101295 : 7 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 347077 : 7 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 Max. :9.000 347082 : 7 ## NA&#39;s :655 NA&#39;s :263 (Other) :1261 ## fare cabin embarked boat body home.dest subset ## Min. : 0.000 :1014 : 2 :823 Min. : 1.0 :564 Length:1309 ## 1st Qu.: 7.896 C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 New York, NY : 64 Class :character ## Median : 14.454 B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 London : 14 Mode :character ## Mean : 33.295 G6 : 5 S:914 15 : 37 Mean :160.8 Montreal, PQ : 10 ## 3rd Qu.: 31.275 B96 B98 : 4 14 : 33 3rd Qu.:256.0 Cornwall / Akron, OH: 9 ## Max. :512.329 C22 C26 : 4 4 : 31 Max. :328.0 Paris, France : 9 ## NA&#39;s :1 (Other) : 271 (Other):308 NA&#39;s :1188 (Other) :639 ## title ## officer: 23 ## royal : 6 ## Master : 61 ## miss :262 ## mrs :200 ## Mr :757 ## sum(is.na(data$age))/nrow(data) ## [1] 0.2009167 20% NAs! Either we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age: data = data %&gt;% group_by(sex, pclass, title) %&gt;% mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;% ungroup() However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups. In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age….)) will be done within the specified groups. 3.5.2 Pre-processing and feature selection Keras cannot handle factors and we have to scale the data. For now we will sub-select a batch of predictors and: scale the numeric predictors change the factors with only two groups/levels into integer data_sub = data %&gt;% select(survived, sex, age2, fare, title, pclass) %&gt;% mutate(age2 = scales::rescale(age2, c(0,1)), fare = scales::rescale(fare, c(0,1))) %&gt;% mutate(sex = as.integer(sex) - 1L, title = as.integer(title) - 1L, pclass = as.integer(pclass - 1L)) One hot encoding of factors with &gt; 2 levels: one_title = k_one_hot(data_sub$title, length(unique(data$title)))$numpy() colnames(one_title) = levels(data$title) one_sex = k_one_hot(data_sub$sex, length(unique(data$sex)))$numpy() colnames(one_sex) = levels(data$sex) one_pclass = k_one_hot(data_sub$pclass, length(unique(data$pclass)))$numpy() colnames(one_pclass) = paste0(1:length(unique(data$pclass)), &quot;pclass&quot;) Add the dummy encoded variables to the dataset: data_sub = cbind(data.frame(survived= data_sub$survived, subset = data$subset), one_title, one_sex, age = data_sub$age2, fare = data_sub$fare, one_pclass) head(data_sub) ## survived subset officer royal Master miss mrs Mr female male age fare 1pclass 2pclass 3pclass ## 1 1 train 0 0 0 1 0 0 1 0 0.37369494 0.02537431 0 1 0 ## 2 1 train 0 0 0 0 0 1 0 1 0.51774510 0.06929139 1 0 0 ## 3 0 train 0 0 0 0 0 1 0 1 0.32359053 0.13575256 0 0 1 ## 4 0 train 0 0 1 0 0 0 0 1 0.07306851 0.04113566 0 0 1 ## 5 0 train 0 0 0 0 0 1 0 1 0.37995799 0.01571255 0 0 1 ## 6 0 train 0 0 0 0 0 1 0 1 0.48016680 0.01415106 0 0 1 3.5.3 Split data for training and testing The splitting consists of two splits: an outer split (the original split, remember we got a train and test split without the response “survived”) an inner split (we will split further the train dataset into another train and test split with known response) The inner split is important because to assess the model’s performance and potential overfitting Outer split: train = data_sub %&gt;% filter(subset == &quot;train&quot;) %&gt;% filter(!is.na(fare)) %&gt;% select(-subset) Inner split: indices = sample.int(nrow(train), 0.7*nrow(train)) sub_train = train[indices,] sub_test = train[-indices,] What is the difference between the two splits? (Tip: have a look at the variable survived) 3.5.4 Model fitting In the next step we will fit a keras model on the train split of the inner split: model = keras_model_sequential() model %&gt;% layer_dense(units = 20L, input_shape = ncol(sub_train) - 1L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 2L, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_18&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## dense_47 (Dense) (None, 20) 280 ## _________________________________________________________________________________________________________________________________________ ## dense_48 (Dense) (None, 20) 420 ## _________________________________________________________________________________________________________________________________________ ## dense_49 (Dense) (None, 20) 420 ## _________________________________________________________________________________________________________________________________________ ## dense_50 (Dense) (None, 2) 42 ## ========================================================================================================================================= ## Total params: 1,162 ## Trainable params: 1,162 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ model_history = model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax(0.01)) model_history = model %&gt;% fit(x = as.matrix(sub_train[,-1]), y = to_categorical(sub_train[,1],num_classes = 2L), epochs = 100L, batch_size = 32L, validation_split = 0.2, shuffle = TRUE) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.5.5 Model evaluation We will predict survived for the test set of the inner split and calculate the accuracy: preds = model %&gt;% predict(x = as.matrix(sub_test[,-1])) predicted = ifelse(preds[,2] &lt; 0.5, 0, 1) observed = sub_test[,1] (accuracy = mean(predicted == observed)) ## [1] 0.7704082 3.5.6 Predictions and submission When we are satisfied with the performance of our model in the inner split, we will create predictions for the test split of the outer split: select all observations that belong to the outer test split (the filter function) remove the subset and survived (NAs) columns submit= data_sub %&gt;% filter(subset == &quot;test&quot;) %&gt;% select(-subset, -survived) We cannot assess the performance on the test split because the true survived ratio is unknown, however, we can now submit our predictions to the submission server at http://rhsbio6.uni-regensburg.de:8500 To do so, we have to transform our survived probablities into actual 0/1 predictions (probablities are not allowed) and create a csv: pred = model %&gt;% predict(as.matrix(submit)) pred = ifelse(pred[,2] &lt; 0.5, 0, 1) All values &gt; 0.5 will be set to 1 and values &lt; 0.5 to zero. For the submission it is critical to change the predictions into a data.frame and save it with the write.csv function: write.csv(data.frame(y=pred), file = &quot;Max_1.csv&quot;) The file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself. "],["Deep.html", "4 Deep learning 4.1 Deep Neural Networks 4.2 Convolutional Neural Networks - MNIST 4.3 Flower dataset", " 4 Deep learning 4.1 Deep Neural Networks 4.1.1 Dropout and Early stopping Early stopping: you might have noticed yesterday that even with regularization the validation loss will start to increase at some point during the training. Early stopping allows us to stop the training when for instance the test loss does not increase anymore With l1/l2 regularization we have to carefully tune the regularization strength. Dropout is robuster, and tuning of the dropout rate can be beneficial but a rate between 0.2-0.5 works often quite well 4.2 Convolutional Neural Networks - MNIST The MNIST dataset is maybe one of the most famous image datasets. It is a dataset of 60,000 handwritten digits from 0-9. Let’s define a few helper functions: library(keras) rotate = function(x) t(apply(x, 2, rev)) imgPlot = function(img, title = &quot;&quot;){ col=grey.colors(255) image(rotate(img), col = col, xlab = &quot;&quot;, ylab = &quot;&quot;, axes=FALSE, main = paste0(&quot;Label: &quot;, as.character(title))) } The dataset is so famous that there is an automatic download function in keras: data = dataset_mnist() train = data$train test = data$test Let’s visualize a few digits: par(mfrow = c(3,3)) .n = sapply(1:9, function(x) imgPlot(train$x[x,,], train$y[x])) Similar to the normal ML workflow, we have to scale the pixels (from 0-255) to the range of [0,1] and one hot encode the response: train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) We also use here now arrays instead of matrices. Arrays are higher dimensional matrices (tensor of rank 3). Finally, we have a “real” tensor. The last dimension stands for the number of channels in the image. In our case we have only one channel because the images are white-black. Normally we would have three channels - colors are encoded by the combination of three channels (e.g. rgb). model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L,1L),filters = 16L, kernel_size = c(2L,2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L,3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_19&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## conv2d_8 (Conv2D) (None, 27, 27, 16) 80 ## _________________________________________________________________________________________________________________________________________ ## max_pooling2d_8 (MaxPooling2D) (None, 13, 13, 16) 0 ## _________________________________________________________________________________________________________________________________________ ## conv2d_9 (Conv2D) (None, 11, 11, 16) 2320 ## _________________________________________________________________________________________________________________________________________ ## max_pooling2d_9 (MaxPooling2D) (None, 5, 5, 16) 0 ## _________________________________________________________________________________________________________________________________________ ## flatten_5 (Flatten) (None, 400) 0 ## _________________________________________________________________________________________________________________________________________ ## dense_51 (Dense) (None, 100) 40100 ## _________________________________________________________________________________________________________________________________________ ## dense_52 (Dense) (None, 10) 1010 ## ========================================================================================================================================= ## Total params: 43,510 ## Trainable params: 43,510 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ We will start now with a 2D convolutional layer, (3D would be e.g. for movies, so the third dimension would correspond to time and not to the color channels!). We use 16 convolutional kernels (filters) with a size of 2x2. The pooling layer downsizes the resulting feature maps. After another conv and pooling layer we flatten the output, i.e. the following dense layer treats the previous layer as normal dense layer (so the dense layer is connected to all weights from the last feature maps). We end the model with our typical output layer. The rest is as usual: model %&gt;% compile( optimizer = keras::optimizer_adamax(0.01), loss = loss_categorical_crossentropy ) summary(model) ## Model: &quot;sequential_19&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## conv2d_8 (Conv2D) (None, 27, 27, 16) 80 ## _________________________________________________________________________________________________________________________________________ ## max_pooling2d_8 (MaxPooling2D) (None, 13, 13, 16) 0 ## _________________________________________________________________________________________________________________________________________ ## conv2d_9 (Conv2D) (None, 11, 11, 16) 2320 ## _________________________________________________________________________________________________________________________________________ ## max_pooling2d_9 (MaxPooling2D) (None, 5, 5, 16) 0 ## _________________________________________________________________________________________________________________________________________ ## flatten_5 (Flatten) (None, 400) 0 ## _________________________________________________________________________________________________________________________________________ ## dense_51 (Dense) (None, 100) 40100 ## _________________________________________________________________________________________________________________________________________ ## dense_52 (Dense) (None, 10) 1010 ## ========================================================================================================================================= ## Total params: 43,510 ## Trainable params: 43,510 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ epochs = 5L batch_size = 32L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) 4.2.1 Data Augmentation Having to train a CNN using very little data is a common problem. Data augmentation helps to artificially increase the number of images. The idea is that the CNN has to learn specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures. Luckily, it is very easy to use data augmentation in keras. We will use again the MNIST dataset: library(keras) data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) print(dim(train_x)) ## [1] 60000 28 28 1 print(dim(test_y)) ## [1] 10000 10 model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(NULL, 28, 28,1),filters = 16, kernel_size = c(2,2), activation = &quot;relu&quot;, use_bias = F) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = &quot;relu&quot;, use_bias = F) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100, activation = &quot;relu&quot;) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_20&quot; ## _________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================================= ## conv2d_10 (Conv2D) (None, 27, 27, 16) 64 ## _________________________________________________________________________________________________________________________________________ ## max_pooling2d_10 (MaxPooling2D) (None, 13, 13, 16) 0 ## _________________________________________________________________________________________________________________________________________ ## conv2d_11 (Conv2D) (None, 11, 11, 16) 2304 ## _________________________________________________________________________________________________________________________________________ ## max_pooling2d_11 (MaxPooling2D) (None, 5, 5, 16) 0 ## _________________________________________________________________________________________________________________________________________ ## flatten_6 (Flatten) (None, 400) 0 ## _________________________________________________________________________________________________________________________________________ ## dense_53 (Dense) (None, 100) 40100 ## _________________________________________________________________________________________________________________________________________ ## dense_54 (Dense) (None, 10) 1010 ## ========================================================================================================================================= ## Total params: 43,478 ## Trainable params: 43,478 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________________________ model %&gt;% compile( optimizer = optimizer_adamax(), loss = loss_categorical_crossentropy ) We have now to define a generator object (it is a specific object which infinitly draws samples from our dataset). In the generator we can turn on the data augementation: aug = image_data_generator() generator = flow_images_from_data(train_x, train_y,generator = aug) model %&gt;% fit_generator(generator, steps_per_epoch = dim(train_x)[1],epochs = 5L) ## Warning in fit_generator(., generator, steps_per_epoch = dim(train_x)[1], : `fit_generator` is deprecated. Use `fit` instead, it now ## accept generators. However, now we have to set the step size because the model does not know the first dimension of the image. 4.2.2 Transfer learning Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning. The idea is that all the convolutional layers have mainly one task - learning to identify highly correlated neighbored features and therefore to learn structures such as edges in the image. Also, the second idea is that only the top layer, the dense layer is the actual classifier of the CNN. The top classifier will be confronted by sets of different edges/structures and has to decide the label. Again, this sounds very complicating but is again quite easy with keras: CIFAR10 preparation: data = keras::dataset_cifar10() train = data$train test = data$test image = train$x[5,,,] image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() train_x = array(train$x/255, c(dim(train$x))) test_x = array(test$x/255, c(dim(test$x))) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) Keras provides download functions for all famous architectures/CNN models which are already trained on the imagenet dataset (anoother famous dataset) and the CNNs come already without their top layer densenet = application_densenet201(include_top = FALSE, input_shape = c(32L, 32L, 3L)) We have to specify directly here in the model our input dimension. Now, we will use not a sequential model but just a “keras_model” where we can specify the inputs and outputs: model = keras::keras_model(inputs = densenet$input, outputs = layer_flatten(layer_dense(densenet$output, units = 10L, activation = &quot;softmax&quot;)) ) The outputs are our own top layer. In the next step we want to freeze all layers except for our own last layer (with freezing I mean that we do not want to train the complete model, we only want to train the last layer): model %&gt;% freeze_weights(to = length(model$layers)-1) Btw, you can always check the number of trainable weights via summary(model) And then the usual training: model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax()) model %&gt;% fit( x = train_x, y = train_y, epochs = 1L, batch_size = 32L, shuffle = T, validation_split = 0.2, ) 4.3 Flower dataset create a CNN submit predictions see kaggle flower dataset for specific architectures! Data preparation: data_files = list.files(&quot;flower/&quot;, full.names = TRUE) train = data_files[str_detect(data_files, &quot;train&quot;)] test = readRDS(file = &quot;flower/test.RDS&quot;) train = lapply(train, readRDS) train_classes = lapply(train, function(d) dim(d)[1]) train = abind::abind(train, along = 1L) labels_train = rep(0:4, unlist(train_classes)) "],["xAI.html", "5 Explainable AI (xAI), NLP, and RNNs 5.1 xAI Methods 5.2 Natural Language Processing (NLP) 5.3 Recurrent neural networks (RNNs)", " 5 Explainable AI (xAI), NLP, and RNNs The goal of xAI is to explain WHY a fitted ML models makes certain predictions, for example how important different variables are for predictions etc. There are various important applications for this, ranging from a better technical understanding of the models over understanding which data is important to improve predictions to questions of fairness and discrimination (e.g. to understand if an algorithm uses skin color to make a decision). In general, xAI != causality Before we discuss xAI methods in more detail, we want to make one thing very clear - in general, xAI methods measure which variables are used by the algorithm for predictions, or how much variables improve predictions. The important point to note here: if a variable causes something, we could also expect that it helps to predict the very thing. The opposite, however, is not generally true - it is very often possible that a variable that doesn’t cause something can predict something. In statistics (in particular course: advanced biostatistics), we discuss the issue of causality at length. Here, we don’t want to go into the details, but again, you should in general resist to interpret indicators of importance in xAI as causal effects. They tell you something about what’s going on in the algorithm, not about what’s going on in reality. Here an example for the variable importance indicators in the RF algorithm. The purpose of this script is to show that RF variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are sepearable and would be correctly separated by an lm / ANOVA We simulate a dataset with 2 predictors that are strongly correlated, but only one of them has an effect on the response. # simulation parameters n = 1000 col = 0.7 # create collinear predictors x1 = runif(n) x2 = col * x1 + (1-col) * runif(n) # response is only influenced by x1 y = x1 + rnorm(n) lm / anova correctly identify x1 as causal variable anova(lm(y ~ x1 + x2)) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 59.74 59.741 57.7854 6.733e-14 *** ## x2 1 0.26 0.255 0.2469 0.6193 ## Residuals 997 1030.73 1.034 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Fit RF and show variable importance fit &lt;- randomForest(y ~ x1 + x2, importance=TRUE) varImpPlot(fit) Variable importance is now split nearly evenly. Task: understand why this is - remember: How the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built Remember that as x1 ~ x2, we can use x2 as a replacement for x1 Remember that the variable importance measures the average contributions of the different variables in the trees of the forest 5.1 xAI Methods In this lecture we will work with another famous dataset, the Boston housing dataset: We will fit a random forest and use the iml pkg for xAI, see set.seed(123) library(&quot;iml&quot;) library(&quot;randomForest&quot;) data(&quot;Boston&quot;, package = &quot;MASS&quot;) rf = randomForest(medv ~ ., data = Boston, ntree = 50) xAI packages are generic, i.e. they can handle almost all ML models. First, we have to create a Predictor object, that holds the model and the data. The iml package uses R6 classes: New objects can be created by calling Predictor$new(). X = Boston[which(names(Boston) != &quot;medv&quot;)] predictor = Predictor$new(rf, data = X, y = Boston$medv) 5.1.1 Variable Importance Importance - not to be mistaken for the RF importance. This importance can be calculated for all ML models and is based on a permutation approach (have a look at the book): imp = FeatureImp$new(predictor, loss = &quot;mae&quot;) plot(imp) It tells us how important the individual variables are for predictions. 5.1.2 Partial dependencies Partial dependencies are similar to allEffects plot, the idea is to visualize “marginal effects” of predictors (with the feature argument we specify the variable we want to visualize): eff = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;pdp&quot;, grid.size = 30) plot(eff) Partial dependencies can be also plotted for single observations: eff = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;pdp+ice&quot;, grid.size = 30) plot(eff) One disadvantage of partial dependencies is that they are sensitive to correlated predictors. Accumulated local effects can be used to account for correlation for predictors 5.1.3 Accumulated local effects ALE re basically partial dependencies plots but try to correct for correlations between predictors ale = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;ale&quot;) ale$plot() If there is no collinearity, you shouldn’t see much difference between partial dependencies and ALE plots. 5.1.4 Friedmans H-statistic The H-statistic can be used to find interactions between predictors. However, again, keep in mind that the H-statistic is sensible to correlation between predictors: interact = Interaction$new(predictor, &quot;lstat&quot;) plot(interact) 5.1.5 Global explainer - Simplifying the ML model Another idea is to simplify the ML model with another simpler model such as a decision tree. We create predictions for a lot of different predictors values and then we will fit a decision tree on the predictions: library(partykit) ## Loading required package: libcoin ## Loading required package: mvtnorm tree = TreeSurrogate$new(predictor, maxdepth = 2) plot(tree) 5.1.6 Local explainer - LIME explaining single instances (observations) The global approach is to simplify a black-box model via a simpler surrogate Model. However, sometimes we are only interested in understanding how single observations/predictions are generated. The lime approach explores the feature space around one observations and fits then a simpler model (e.g. a linear model) on the feature space around one observation: library(glmnet) ## Loaded glmnet 4.1-1 lime.explain = LocalModel$new(predictor, x.interest = X[1,]) ## Loading required package: gower lime.explain$results ## beta x.recoded effect x.original feature feature.value ## rm 4.1893817 6.575 27.545185 6.575 rm rm=6.575 ## ptratio -0.5307031 15.300 -8.119758 15.3 ptratio ptratio=15.3 ## lstat -0.4398104 4.980 -2.190256 4.98 lstat lstat=4.98 plot(lime.explain) 5.1.7 Local explainer - Shapley Shapley computes feature contributions for single predictions with the Shapley value, an approach from cooperative game theory. The idea is that the features values of an instance cooperate to achieve the prediction. The Shapley value fairly distributes the difference of the instance’s prediction and the datasets average prediction among the features: shapley = Shapley$new(predictor, x.interest = X[1,]) shapley$plot() 5.2 Natural Language Processing (NLP) What this video to get an idea about what NLP is about See also the blog post linked with the youtube video with accompanying code to the video. Moreover, here is an article that shows now NLP works with keras, however, written in Python. As a challenge, you can take the code and implement it in R 5.3 Recurrent neural networks (RNNs) Recurrent Neural Networks are used to model sequential data, i.e. temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic: "]]
