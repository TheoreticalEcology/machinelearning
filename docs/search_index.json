[["fund.html", "3 Fundamental principles and techniques 3.1 Machine learning principles", " 3 Fundamental principles and techniques 3.1 Machine learning principles 3.1.1 Optimization from wikipedia: \" an optimization problem is the problem of finding the best solution from all feasible solutions\" Why do we need this “optimization”? A loss function (e.g. we tell in each training step the algorithm how many observations were miss-classified) guides the training of ML algorithms Based on the loss, the optimizer tries to update the weights of the ML algorithms in a way that the loss function is minimized Calculating analytically the global optima of a function is a non-trivial problem and bunch of diverse optimization algorithms evolved Some optimization algorithms are inspired by biological systemse.g. Ants, Bee, or even slimve algorithms): 3.1.1.1 Small optimization example We have the following function: func = function(x) return(x^2) which we want to minimize, we could do this by hand: a = rnorm(100) plot(a, func(a)) The smallest value is at x = 0 (to be honest, we can calculate this for this simple case analytically) We can also use an optimizer with the optim(…) function opt = optim(1.0, func) ## Warning in optim(1, func): one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly print(opt$par) ## [1] -8.881784e-16 opt$par will return the best values found by the optimizer 3.1.1.2 Advanced optimization example We will now optimze the weights (slopes) for linear regression model. Basically, we will implement lm(y~x) on our own: Load the airquality dataset, remove NAs, split it into predictors and response, and scale the predictors: data = airquality[complete.cases(airquality$Ozone) &amp; complete.cases(airquality$Solar.R),] X = scale(data[,-1]) Y = data$Ozone The model we want to optimize: \\(ozone = Solar.R*X1 + Wind*X2 + Temp*X3 + Month*X4 + Day*X5 + X6\\) Our loss function: mean(predicted ozone - true ozone)^2) We found to find the parameters X1-X6 for which the loss function is the smallest: linear_regression = function(w) { pred = w[1]*X[,1] + # Solar.R w[2]*X[,2] + # Wind w[3]*X[,3] + # Temp w[4]*X[,4] + # Month w[5]*X[,5] + w[6] # or X %*% w[1:5] + w[6] # loss = MSE, we want to find the optimal weights # to minimize the sum of squared residuals loss = mean((pred - Y)^2) return(loss) } The linear_regression function takes potential solutions for the weights (X1-X6) and will return the loss for these weights: linear_regression(runif(6)) ## [1] 2781.875 Let’s try it bruteforce (which means we will try to find the optimal solution with random a set of random weights): random_search = matrix(runif(6*5000,-10,10), 5000, 6) losses = apply(random_search, 1, linear_regression) plot(losses, type = &quot;l&quot;) random_search[which.min(losses),] ## [1] 5.0567712 -7.8550341 7.4573370 0.8109562 2.0097918 9.3807506 Bruteforce is not a good approach, it might work well with only a few parameters but it depends completely on your luck Let’s try it with the optim function: opt = optim(runif(6, -1, 1), linear_regression) opt$par ## [1] 8.850516 -25.099056 1.680747 -10.846307 1.782977 36.493839 Compare the weights the estimated weights of the lm() function: coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 "]]
