[["index.html", "A Minimal Book Example 1 Prerequisites", " A Minimal Book Example Maximilian Pichler and Florian Hartig 2021-04-29 1 Prerequisites R+Rstudio "],["introduction.html", "2 Introduction to Machine Learning 2.1 Supervised learning: regression and classification 2.2 Unsupervised learning 2.3 Introduction to Tensorflow 2.4 First steps with the keras framework", " 2 Introduction to Machine Learning In this lesson, we introduce the three basic ML tasks: supervised regression and classification, and unsupervised learning. In ML, we distinguish 3 basic learning paradigms: - Supervised learning - Unsupervised learning - Reinforcement learning We will speak about reinforcement learning at the end of the course. Now, we want to look at examples of supervised and unsupervised learning. Before you start with the code, here a video to remind you of what we talked about in the class: 2.1 Supervised learning: regression and classification Two two main subbranches of supervised learning are regression and classification. Here a video that explains again the difference 2.1.1 Supervised regression using Random Forest The random forest (RF) algorithm is possibly the most widely used ML algorithm and can be used for regression and classification. We will talk more about the algorithm on Day 2. Here an example of a regression: Visualization of the data: plot(iris, col = iris$Species) Fitting the model library(randomForest) m1 &lt;- randomForest(Sepal.Length ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Sepal.Length ~ ., data = iris) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 0.1357869 ## % Var explained: 80.06 Visualization of the results par(mfrow = c(1,2)) plot(predict(m1), iris$Sepal.Length, xlab = &quot;predicted&quot;, ylab = &quot;observed&quot;) abline(0,1) varImpPlot(m1) This is a nice visualization of the RF structure, but requires to load a package from GitHub # devtools::install_github(&#39;araastat/reprtree&#39;) reprtree:::plot.getTree(m1, iris) 2.1.2 Supervised classification using Random Forest Fitting the model: set.seed(123) m1 &lt;- randomForest(Species ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Species ~ ., data = iris) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 4.67% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 50 0 0 0.00 ## versicolor 0 47 3 0.06 ## virginica 0 4 46 0.08 Visualizing the fitted model: par(mfrow = c(1,2)) reprtree:::plot.getTree(m1, iris) Visualizing results ecologically: oldpar &lt;- par(mfrow = c(1,2)) plot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = &quot;observed&quot;) plot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = &quot;predicted&quot;) Confusion matrix: table(predict(m1),iris$Species) ## ## setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 4 ## virginica 0 3 46 2.2 Unsupervised learning In unsupervised learning, we basically want to identify patterns in data without having any guidance (supervision) about what the correct patterns / classes are. It is all much easier with a practical example. Consider our iris dataset. Here, we have observations of different species Together with their flower traits Imagine we didn’t know what species are. This is basically the situation in which people in the antique would have been. There is no book to look up species. You just noted that there seem to be some kind of plants that have different flowers than another, so you decide to call them by a different name. This kind of process is what unsupervised learning does. 2.2.1 k-means clustering An example for an unsupervised learning algorithm is k-means clustering, one of the simplest and popular unsupervised machine learning algorithms. A cluster refers to a collection of data points aggregated together because of certain similarities. In the algorithm, you’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid. sIris = scale(iris[,1:4]) model&lt;- kmeans(sIris,3) # aplly k-means algorithm with no. of centroids(k)=3 model ## K-means clustering with 3 clusters of sizes 47, 53, 50 ## ## Cluster means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 1.13217737 0.08812645 0.9928284 1.0141287 ## 2 -0.05005221 -0.88042696 0.3465767 0.2805873 ## 3 -1.01119138 0.85041372 -1.3006301 -1.2507035 ## ## Clustering vector: ## [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 1 2 ## [68] 2 2 2 1 2 2 2 2 1 1 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 1 1 1 1 1 1 2 ## [135] 2 1 1 1 2 1 1 1 2 1 1 1 2 1 1 2 ## ## Within cluster sum of squares by cluster: ## [1] 47.45019 44.08754 47.35062 ## (between_SS / total_SS = 76.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Visualizing the results: par(mfrow = c(1,2)) plot(Petal.Length~Petal.Width, data = sIris, col = model$cluster, main = &quot;Predicted clusters&quot;) plot(Petal.Length~Petal.Width, data = sIris, col = iris$Species, main = &quot;True species&quot;) Confusion matrix: table(model$cluster,iris$Species) ## ## setosa versicolor virginica ## 1 0 11 36 ## 2 0 39 14 ## 3 50 0 0 2.3 Introduction to Tensorflow TF is a math library which is highly optimized for neural networks If a GPU is available, computations can be easily run on the GPU but even on the CPU is TF still very fast The “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for the GPU) The interface (the part of TF that we use) is written in python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend. 2.3.1 Tensorflow data containers TF has two data containers (structures): - constant (tf\\(constant) :creates a constant (immutable) value in the computation graph - variable (tf\\)Variable): creates a mutable value in the computation graph (used as parameter/weight in models) library(tensorflow) # Don&#39;t worry about weird messages. TF supports additional optimizations exists(&quot;tf&quot;) ## [1] TRUE Don’t worry about weird messages (they will only appear once at the start of the session). a = tf$constant(5) b = tf$constant(10) print(a) ## tf.Tensor(5.0, shape=(), dtype=float32) print(b) ## tf.Tensor(10.0, shape=(), dtype=float32) c = tf$add(a, b) print(c) ## tf.Tensor(15.0, shape=(), dtype=float32) tf$print(c) Normal R methods such as print() are provided by the R package “tensorflow”. The tensorflow library (created by the RStudio team) built R methods for all common operations: `+.tensorflow.tensor` = function(a, b) return(tf$add(a,b)) tf$print(a+b) Their operators also transfrom automatically R numbers into constant tensors when attempting to add a tensor to a R number: d = c + 5 # 5 is automatically converted to a tensor print(d) ## tf.Tensor(20.0, shape=(), dtype=float32) TF container are objects, which means that they are not just simple variables of type numeric (class(5)), and they have methods For instance, there is a method to transform the tensor object back to a R object: class(d) ## [1] &quot;tensorflow.tensor&quot; &quot;tensorflow.python.framework.ops.EagerTensor&quot; ## [3] &quot;tensorflow.python.framework.ops._EagerTensorBase&quot; &quot;tensorflow.python.framework.ops.Tensor&quot; ## [5] &quot;tensorflow.python.types.internal.NativeObject&quot; &quot;tensorflow.python.types.core.Tensor&quot; ## [7] &quot;python.builtin.object&quot; class(d$numpy()) ## [1] &quot;numeric&quot; 2.3.2 Tensorflow data types - good practise with R-TF R uses dynamic typing, which means you can assign to a variable a number, character, function or whatever, and R infers the type automatically. In other languages you have to state explicitly the type, e.g. in C: int a = 5; float a = 5.0; char a = “a”; While TF tries to infer dynamically the type, often you must state it explicitly. Common important types: - float32 (floating point number with 32bits, “single precision”) - float64 (“double precision”) - int8 (integer with 8bits) Why does TF support float32 numbers when most cpus today can handle easily 64bit numbers? Many GPUs (e.g. the NVIDIA geforces) can handle only up to 32bit numbers! (you do not need high precision in graphical modeling) r_matrix = matrix(runif(10*10), 10,10) m = tf$constant(r_matrix, dtype = &quot;float64&quot;) b = tf$constant(2.0, dtype = &quot;float64&quot;) c = m / b Instead of a string, you can also provide a tf$float64 object m = tf$constant(r_matrix, dtype = tf$float32) b = tf$constant(2.0, dtype = tf$float64) c = m / b # doesn&#39;t work! we try to divide float32/float64 Tensorflow arguments often require exact/explicit data types: TF expects for arguments integers, however, R needs a “L” after an integer to tell the R interpreter that it should be treated as an integer: is.integer(5) is.integer(5L) matrix(t(r_matrix), 5, 20, byrow = TRUE) tf$reshape(r_matrix, shape = c(5, 20))$numpy() tf$reshape(r_matrix, shape = c(5L, 20L))$numpy() Note: skipping the “L” is one of the most common errors when using R-TF! 2.4 First steps with the keras framework Objective of this lesson: familiarize yourself with keras. keras is a higher level API within TF and developed to build easily neural networks. Keras can be found within TF: tf.keras…however, the RStudio team built a pkg on top of tf.keras: 2.4.1 Example workflow in keras We will now build a small classifier in keras to predict the three species of the iris dataset: Load the necessary packages and datasets: library(keras) library(tensorflow) data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa It is beneficial for neural networks to scale the predictors (scaling = centering and standardization, see ?scale): X = scale(iris[,1:4]) Y = iris[,5] We also split our data into the predictors (X) and the response (Y = the three species) keras/tf cannot handle factors and we have to create contrasts (one-hot encoding): Y = to_categorical(as.integer(Y)-1L, 3) head(Y) # 3 colums, one for each level in the response ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 0 0 ## [3,] 1 0 0 ## [4,] 1 0 0 ## [5,] 1 0 0 ## [6,] 1 0 0 Do you have an idea why we subtracted 1L from the labels after we transformed them? (Tip: google “Is r 0 or 1 indexed?”) Model building: Initiliaze a sequential model in keras: model = keras_model_sequential() A sequential keras model is a higher order type of model within keras and consists of one input and one output model. Add hidden layers to the model (we will learn more about DNNs during the next days): model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(4L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 3L, activation = &quot;softmax&quot;) softmax scales to 0 1 and overall to 0 - 1, 3 output nodes for 3 response classes/labels compile the model with a cross entropy loss function and Adamax optimizer: model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(0.001)) summary(model) ## Model: &quot;sequential_2&quot; ## ________________________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================================================================================================ ## dense_8 (Dense) (None, 20) 100 ## ________________________________________________________________________________________________________________________________________________________________ ## dense_9 (Dense) (None, 20) 420 ## ________________________________________________________________________________________________________________________________________________________________ ## dense_10 (Dense) (None, 20) 420 ## ________________________________________________________________________________________________________________________________________________________________ ## dense_11 (Dense) (None, 3) 63 ## ================================================================================================================================================================ ## Total params: 1,003 ## Trainable params: 1,003 ## Non-trainable params: 0 ## ________________________________________________________________________________________________________________________________________________________________ For now, do not worry about the “lr” argument, crossentropy or the optimizer Fit the model in 30 (epochs) iterations: model_history = model %&gt;% fit(x = X, y = apply(Y,2,as.integer), epochs = 30L, batch_size = 20L, shuffle = TRUE) Plot the training history: plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Create predictions: predictions = predict(model, X) # probabilities for each class We will get probabilites: head(predictions) # quasi-probabilities for each species ## [,1] [,2] [,3] ## [1,] 0.9732426 0.02112393 0.005633502 ## [2,] 0.9448608 0.05044184 0.004697330 ## [3,] 0.9692710 0.02755538 0.003173681 ## [4,] 0.9606974 0.03609953 0.003202966 ## [5,] 0.9794057 0.01585520 0.004739106 ## [6,] 0.9548649 0.02875621 0.016378896 For each site, we want to know for which species we got the highest probability: preds = apply(predictions, 1, which.max) print(preds) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 ## [78] 3 2 2 2 2 2 3 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Calculate Accuracy: mean(preds == as.integer(iris$Species)) ## [1] 0.92 Plot predictions: oldpar = par() par(mfrow = c(1,2)) plot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species, main = &quot;Observed&quot;) plot(iris$Sepal.Length, iris$Petal.Length, col = preds, main = &quot;Predicted&quot;) "],["fund.html", "3 Fundamental principles and techniques", " 3 Fundamental principles and techniques "]]
