[["index.html", "Machine Learning and AI in TensorFlow and R 1 Prerequisites", " Machine Learning and AI in TensorFlow and R Maximilian Pichler and Florian Hartig 2021-05-01 1 Prerequisites R+Rstudio "],["introduction.html", "2 Introduction to Machine Learning 2.1 Supervised learning: regression and classification 2.2 Unsupervised learning 2.3 Introduction to Tensorflow 2.4 First steps with the keras framework", " 2 Introduction to Machine Learning In this introductory chapter, we introduce the three basic ML tasks Supervised learning Unsupervised learning Reinforcement learning In supervised learning, you train an algorithm using labeled data, which means that you already know the correct answer for a part of the data (the so called tracings data). Unsupervised learning is a technique, where one does not need to supervise the model. Instead, you allow the model to work on its own to discover information. Reinforcement learning is a technique that emulates a game-like situation. The algorithm comes up with a solution by try and error and gets for the actions ether rewards or penalties. As in games, the goal is to maximize the rewards. We will talk on the last day more about this technique. For the moment, we will focus on the first two tasks, supervised and unsupervised learning. To do so, we will first start with a small example, but before you start with the code, here a video to remind you of what we talked about in the class: 2.1 Supervised learning: regression and classification The two most prominent branches of supervised learning are regression and classification. Fundamentally, classification is about predicting a label and regression is about predicting a quantity. The following video explains that in more depth: 2.1.1 Supervised regression using Random Forest The random forest (RF) algorithm is possibly the most widely used ML algorithm and can be used for regression and classification. We will talk more about the algorithm on Day 2. In the following we see a typical workflow for a regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. Visualization of the data: plot(iris, col = iris$Species) Fitting the model library(randomForest) m1 &lt;- randomForest(Sepal.Length ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Sepal.Length ~ ., data = iris) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 0.1362531 ## % Var explained: 80 Visualization of the results par(mfrow = c(1,2)) plot(predict(m1), iris$Sepal.Length, xlab = &quot;predicted&quot;, ylab = &quot;observed&quot;) abline(0,1) varImpPlot(m1) To understand, the structure of a RF in more detail, we can use a package from GitHub # devtools::install_github(&#39;araastat/reprtree&#39;) reprtree:::plot.getTree(m1, iris) 2.1.2 Supervised classification using Random Forest With the RF, we can also do classification. The steps are the same as for regression tasks, but we can additionally, see how well it performed by looking at the so called confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represent the instances in an actual class. Thus the diagonals are the correctly predicted classes and the off-diagnoal elements are the falsly classified elements. Fitting the model: set.seed(123) m1 &lt;- randomForest(Species ~ ., data = iris) # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Species ~ ., data = iris) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 4.67% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 50 0 0 0.00 ## versicolor 0 47 3 0.06 ## virginica 0 4 46 0.08 Visualizing the fitted model: par(mfrow = c(1,2)) reprtree:::plot.getTree(m1, iris) Visualizing results ecologically: oldpar &lt;- par(mfrow = c(1,2)) plot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = &quot;observed&quot;) plot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = &quot;predicted&quot;) Confusion matrix: table(predict(m1),iris$Species) ## ## setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 4 ## virginica 0 3 46 2.2 Unsupervised learning In unsupervised learning, we basically want to identify patterns in data without having any guidance (supervision) about what the correct patterns / classes are. It is all much easier with a practical example. Consider our iris dataset. Here, we have observations of different species together with their flower traits. Imagine we didn’t know what species are, which is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does. 2.2.1 k-means clustering An example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms. A cluster refers to a collection of data points aggregated together because of certain similarities. In our example from above this similarities could be similar flowers aggregated together to a plant. To start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the imaginary or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere and then adds each new data point to the cluster which minimizes the overall in-cluster sum of squares. After the algorithm has assigned a new data point to a cluster the centroid gets updated. By iterating this procedure for all data points and then starting again, the algorithm can find the optimum centroids and the data-points belonging to this cluster. The k in K-means refers to the number of clusters and the ‘means’ refers to averaging of the data-points to find the centroids. A typical pipeline for using kmeans clustering looks the same as for the other algortihms. After having visualized the data, we fit the model, visualize the results and have a look at the performance by use of the confusion matrix. sIris = scale(iris[,1:4]) model&lt;- kmeans(sIris,3) # aplly k-means algorithm with no. of centroids(k)=3 model ## K-means clustering with 3 clusters of sizes 47, 53, 50 ## ## Cluster means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 1.13217737 0.08812645 0.9928284 1.0141287 ## 2 -0.05005221 -0.88042696 0.3465767 0.2805873 ## 3 -1.01119138 0.85041372 -1.3006301 -1.2507035 ## ## Clustering vector: ## [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 1 ## [67] 2 2 2 2 1 2 2 2 2 1 1 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 1 1 1 1 1 ## [133] 1 2 2 1 1 1 2 1 1 1 2 1 1 1 2 1 1 2 ## ## Within cluster sum of squares by cluster: ## [1] 47.45019 44.08754 47.35062 ## (between_SS / total_SS = 76.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Visualizing the results: par(mfrow = c(1,2)) plot(Petal.Length~Petal.Width, data = sIris, col = model$cluster, main = &quot;Predicted clusters&quot;) plot(Petal.Length~Petal.Width, data = sIris, col = iris$Species, main = &quot;True species&quot;) Confusion matrix: table(model$cluster,iris$Species) ## ## setosa versicolor virginica ## 1 0 11 36 ## 2 0 39 14 ## 3 50 0 0 2.3 Introduction to Tensorflow All operations in TF are written in C++ and are highly optimized. But dont worry, we don’t have to use C++ to use TF because there are several bindings for other languages. TensorFlow officialy sup- ports a Python API, but meanwhile there are several community carried APIs for other languages: • R • Go • Rust • Swift • JavaScript In this course we will use TF with the https://tensorflow.rstudio.com/ binding, that was developed and published 2017 by the RStudio Team. They developed first a R package (reticulate) to call python in R. Actually, we are using in R the python TF module (more about this later). TF offers different levels of API. We could implement a neural network completly by ourselves, or we could use Keras which is provided by TF as a submodule. Keras is a powerful module for building and training neural networks. It allows us to build and train neural networks in a few lines of codes. Since the end of 2018, Keras and TF are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras for neural networks but also how we can use the TF’s automatic differenation for using complex objective functions. One of the most commonly used frameworks for machine learning is TensorFlow. TensorFlow is a open source linear algebra library with a focus on neural networks, published by Google in 2015. TF supports several interesting features, im particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization. These advantages are nicely explained in the following video: To sum the most important points of the video up: TF is a math library which is highly optimized for neural networks If a GPU is available, computations can be easily run on the GPU but even on a CPU is TF still very fast The “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for the GPU) The interface (the part of TF that we use) is written in python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend. All operations in TF are written in C++ and are highly optimized. But dont worry, we don’t have to use C++ to use TF, because there are several bindings for other languages. Officially, TensorFlow only supports a Python API, but meanwhile there are several community carried APIs for other languages, including R, Go, Rust, Swift or JavaScript. In this book, we will use TF with the https://tensorflow.rstudio.com/ binding that was developed and published 2017 by the RStudio Team. They developed first a R package (reticulate) to call python in R. Actually, we are using in R the python TF module (more about this later). 2.3.1 Tensorflow data containers TF has two data containers (structures): - constant (tf\\(constant) :creates a constant (immutable) value in the computation graph - variable (tf\\)Variable): creates a mutable value in the computation graph (used as parameter/weight in models) To get started with tensorflow, we have to load the library and check if the installation worked. library(tensorflow) # Don&#39;t worry about weird messages. TF supports additional optimizations exists(&quot;tf&quot;) ## [1] TRUE Don’t worry about weird messages (they will only appear once at the start of the session). We now can define the variables and do some math with them: a = tf$constant(5) b = tf$constant(10) print(a) ## tf.Tensor(5.0, shape=(), dtype=float32) print(b) ## tf.Tensor(10.0, shape=(), dtype=float32) c = tf$add(a, b) print(c) ## tf.Tensor(15.0, shape=(), dtype=float32) tf$print(c) Normal R methods such as print() are provided by the R package “tensorflow”. The tensorflow library (created by the RStudio team) built R methods for all common operations: `+.tensorflow.tensor` = function(a, b) return(tf$add(a,b)) tf$print(a+b) Their operators also transfrom automatically R numbers into constant tensors when attempting to add a tensor to a R number: d = c + 5 # 5 is automatically converted to a tensor print(d) ## tf.Tensor(20.0, shape=(), dtype=float32) TF container are objects, which means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object) For instance, there is a method to transform the tensor object back to a R object: class(d) ## [1] &quot;tensorflow.tensor&quot; &quot;tensorflow.python.framework.ops.EagerTensor&quot; ## [3] &quot;tensorflow.python.framework.ops._EagerTensorBase&quot; &quot;tensorflow.python.framework.ops.Tensor&quot; ## [5] &quot;tensorflow.python.types.internal.NativeObject&quot; &quot;tensorflow.python.types.core.Tensor&quot; ## [7] &quot;python.builtin.object&quot; class(d$numpy()) ## [1] &quot;numeric&quot; 2.3.2 Tensorflow data types - good practise with R-TF R uses dynamic typing, which means you can assign to a variable a number, character, function or whatever, and the the type is automatically infered. In other languages you have to state explicitly the type, e.g. in C: int a = 5; float a = 5.0; char a = “a”; While TF tries to infer dynamically the type, often you must state it explicitly. Common important types: - float32 (floating point number with 32bits, “single precision”) - float64 (floating point number with 64bits,“double precision”) - int8 (integer with 8bits) The reason why TF is so explicit about the types is that many GPUs (e.g. the NVIDIA geforces) can handle only up to 32bit numbers! (you do not need high precision in graphical modeling) But let us see in practice, what we have to do with these types and how to specifcy them: r_matrix = matrix(runif(10*10), 10,10) m = tf$constant(r_matrix, dtype = &quot;float32&quot;) b = tf$constant(2.0, dtype = &quot;float64&quot;) c = m / b # doesn&#39;t work! we try to divide float32/float64 So what went wrong here: we tried to divide a float32 to a float64 number, but, we can only divide numbers of the same type! m = tf$constant(r_matrix, dtype = &quot;float64&quot;) b = tf$constant(2.0, dtype = &quot;float64&quot;) c = m / b # now it works We can also specify the type of the object by providing an object e.g. tf$float64. m = tf$constant(r_matrix, dtype = tf$float64) Tensorflow arguments often require exact/explicit data types: TF often expects for arguments integers. In R however an integer is normally saved as float. Thus, we have to use a “L” after an integer to tell the R interpreter that it should be treated as an integer: is.integer(5) is.integer(5L) matrix(t(r_matrix), 5, 20, byrow = TRUE) tf$reshape(r_matrix, shape = c(5, 20))$numpy() tf$reshape(r_matrix, shape = c(5L, 20L))$numpy() Skipping the “L” is one of the most common errors when using R-TF! 2.4 First steps with the keras framework We have seen that we can use TF directly from R, and we could use this knowledge to implement a neural network in TF directly from R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us with implementing the machine learning models in TF. The most common of those is Keras. Keras is a powerful framework for building and training neural networks with a few lines of codes. Since the end of 2018, Keras and TF are completely interoperable, allowing us to utilize the best of both. The objective of this lesson is to familiarize yourself with keras. If you have TF installed, Keras can be found within TF: tf.keras. However, the RStudio team has built an R package on top of tf.keras, and it is more convenient to use this. To load the keras package, type library(keras) 2.4.1 Example workflow in keras To show how keras works, we will now build a small classifier in keras to predict the three species of the iris dataset. Load the necessary packages and datasets: library(keras) library(tensorflow) data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa It is beneficial for neural networks to scale the predictors (scaling = centering and standardization, see ?scale) We also split our data into the predictors (X) and the response (Y = the three species). X = scale(iris[,1:4]) Y = iris[,5] Additionally, keras/tf cannot handle factors and we have to create contrasts (one-hot encoding): To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like python and C++ on which TF is built, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Therefore, we have to substract it. Y = to_categorical(as.integer(Y)-1L, 3) head(Y) # 3 colums, one for each level in the response ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 0 0 ## [3,] 1 0 0 ## [4,] 1 0 0 ## [5,] 1 0 0 ## [6,] 1 0 0 After having prepared the data, we will now see a typical workflow to specify a model in keras. Initiliaze a sequential model in keras: model = keras_model_sequential() A sequential keras model is a higher order type of model within keras and consists of one input and one output model. Add hidden layers to the model (we will learn more about hidden layers during the next days). When specifiying the hidden layers, we also have to specify a so called activation function and their shape. You can think of the activation function as decisive for what is forwarded to the next neuron (but we will learn more about it later). The shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3). model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(4L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 3L, activation = &quot;softmax&quot;) softmax scales a potential multidimensional vector to the interval (0,1] compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax). We will leaern about other options later, so for now, do not worry about the “lr” argument, crossentropy or the optimizer. model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(0.001)) summary(model) ## Model: &quot;sequential_31&quot; ## _________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ========================================================================================================================= ## dense_81 (Dense) (None, 20) 100 ## _________________________________________________________________________________________________________________________ ## dense_82 (Dense) (None, 20) 420 ## _________________________________________________________________________________________________________________________ ## dense_83 (Dense) (None, 20) 420 ## _________________________________________________________________________________________________________________________ ## dense_84 (Dense) (None, 3) 63 ## ========================================================================================================================= ## Total params: 1,003 ## Trainable params: 1,003 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________________ Fit the model in 30 iterations(epochs) : model_history = model %&gt;% fit(x = X, y = apply(Y,2,as.integer), epochs = 30L, batch_size = 20L, shuffle = TRUE) Plot the training history: plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Create predictions: predictions = predict(model, X) # probabilities for each class We will get probabilites: head(predictions) # quasi-probabilities for each species ## [,1] [,2] [,3] ## [1,] 0.9839143 0.012512908 0.003572818 ## [2,] 0.9320162 0.056433704 0.011550196 ## [3,] 0.9595397 0.029874496 0.010585703 ## [4,] 0.9406353 0.043385815 0.015978908 ## [5,] 0.9860346 0.011050211 0.002915121 ## [6,] 0.9889506 0.009252257 0.001797136 For each plant, we want to know for which species we got the highest probability: preds = apply(predictions, 1, which.max) print(preds) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 3 3 2 2 2 3 2 ## [59] 2 2 2 3 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 3 3 2 2 2 2 3 3 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [117] 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Calculate Accuracy (how often we have been correct): mean(preds == as.integer(iris$Species)) ## [1] 0.8933333 Plot predictions, to see if we have been done a good job: oldpar = par() par(mfrow = c(1,2)) plot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species, main = &quot;Observed&quot;) plot(iris$Sepal.Length, iris$Petal.Length, col = preds, main = &quot;Predicted&quot;) So you see, building a neural network is with keras very easy and you can already do it on your own. "],["fund.html", "3 Fundamental principles and techniques 3.1 Machine learning principles 3.2 Distance-based algorithms 3.3 Artificial neural networks 3.4 The standard ML pipeline at the example of the titanic dataset", " 3 Fundamental principles and techniques 3.1 Machine learning principles 3.1.1 Optimization from wikipedia: \" an optimization problem is the problem of finding the best solution from all feasible solutions\" Why do we need this “optimization”? We need to somehow tell the algorithm what it should learn. To do so we have the so called loss-function, which expresses what our goal is. But we also need to somewhow find the configurations for which the loss function is minimized. This is the job of the optimizer. Thus, an optimization consists of: A loss function (e.g. we tell in each training step the algorithm how many observations were miss-classified) guides the training of ML algorithms The optimizer, which tries to update the weights of the ML algorithms in a way that the loss function is minimized Calculating analytically the global optima is a non-trivial problem and thus a bunch of diverse optimization algorithms evolved Some optimization algorithms are inspired by biological systems e.g. Ants, Bee, or even slime algorithms. These optimizers are explained int the following video, have a look: 3.1.1.1 Small optimization example As an easy example for optimization we can think of a quadratic function: func = function(x) return(x^2) This function is so easy, we can randomly prob it and identify the optimum by plotting a = rnorm(100) plot(a, func(a)) The smallest value is at x = 0 (to be honest, we can calculate this for this simple case analytically) We can also use an optimizer with the optim-function (the first argument is the starting value) opt = optim(1.0, func) ## Warning in optim(1, func): one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly print(opt$par) ## [1] -8.881784e-16 opt$par will return the best values found by the optimizer, which is really close to zeor :) 3.1.1.2 Advanced optimization example Optimization is also done when fitting a linear regression model. Thereby, we optimize the weights (intercept and slope). But using lm (y~x) is too simple, we would like to do this by hand to also better understand what optimization is and how it works. As an example we take the airquality data set. First, we have to be sure to have no NAs in there. Then we split into response (Ozone) and predictors (Month, Day, Solar.R, Wind, Temp).Additionally it is beneficial for the optimizer, when the different predictors have the same support, and thus we scale them. data = airquality[complete.cases(airquality$Ozone) &amp; complete.cases(airquality$Solar.R),] X = scale(data[,-1]) Y = data$Ozone The model we want to optimize: \\(ozone = Solar.R*X1 + Wind*X2 + Temp*X3 + Month*X4 + Day*X5 + X6\\) As the we assume that the residuals are normally distributed, our loss function is the mean squared errors: mean(predicted ozone - true ozone)^2) Our task is now to find the parameters X1-X6 for which this loss function is the smallest. Therefore, we implement a function, that takes parameters and returns the loss. linear_regression = function(w) { pred = w[1]*X[,1] + # Solar.R w[2]*X[,2] + # Wind w[3]*X[,3] + # Temp w[4]*X[,4] + # Month w[5]*X[,5] + w[6] # or X %*% w[1:5] + w[6] # loss = MSE, we want to find the optimal weights # to minimize the sum of squared residuals loss = mean((pred - Y)^2) return(loss) } For example we can sample some weights and see what the loss with this weights is. linear_regression(runif(6)) ## [1] 2794.085 We can try to find the optimum bruteforce (which means we will use a random set of weights and see for which the loss function is smallest): random_search = matrix(runif(6*5000,-10,10), 5000, 6) losses = apply(random_search, 1, linear_regression) plot(losses, type = &quot;l&quot;) random_search[which.min(losses),] ## [1] 5.0567712 -7.8550341 7.4573370 0.8109562 2.0097918 9.3807506 Bruteforce isn’t a good approach, it might work well with only a few parameters, but with increasing complexity and more parameters it will take a long time. In R the optim function helps to get faster to the optimum. opt = optim(runif(6, -1, 1), linear_regression) opt$par ## [1] 4.417031 -24.987382 5.039549 -1.045033 -10.195378 28.878098 By comparing the weights from the optimizer to the estimated weights of the lm() function, we see that our self-written code obtains the same weights as the lm. coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 3.1.2 Regularization Regularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a ML model. The most important distinction is between shrinkage estimators and estimators based on model averaging. Shrikage estimators are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are “shrunk” to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penality is proportional to the absolute deviation (L1 penalty), and the Tikhonov regularization aka ridge regression, where the penalty is proportional to the squared distance from the reference (L2 penalty). Thus, the loss function that we optimize is thus given by \\[ loss = fit - \\lambda \\cdot d \\] where fit refers to the standard loss function, \\(\\lambda\\) is the strength of the regularization, and \\(d\\) is the chosen metrics, e.g. L1 or L2. \\(\\lambda\\) and possibly d are typically optimized under cross-validation Model averaging refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it often more efficient to have many simpler models and average them, than to have one “super model”. The reasons are complicated, and explained in more detail in Dormann et al., 2018. A particular important application of averaging is boosting, where the principle is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap the data, and average the boot- strapped predictions. To see how these techniques work in practice, let’s first focus on lasso and ridge regularization for weights in neural networks. We can imagine that the lasso and ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunk) to zero. Lasso (penalty ~ abs(sum(Weights))) and ridge (penalty ~ (sum(Weights))^2) have slightly different properties, which are best understood if we express those as the effective prior preference that they create on the parameters: As you can see, the Lasso creates a very strong preferencet towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the ridge. For this reason, Lasso is often interpreted more as a model selection method. The Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero. We can implement the linear regression also in keras, when we do not specify any hidden layers library(keras) data = airquality[complete.cases(airquality),] X = scale(data[,-1]) Y = data$Ozone # l1/l2 on linear model model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2])) summary(model) ## Model: &quot;sequential_42&quot; ## __________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================================================================================== ## dense_111 (Dense) (None, 1) 6 ## ================================================================================================================================================== ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## __________________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 50L, batch_size = 20L, shuffle = TRUE) unconstrained = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 But keras also allows use to use lasso and ridge on the weights. Lets see what happens when we put a l1 (lasso) regularization on the weights: model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2]), kernel_regularizer = regularizer_l1(10), bias_regularizer = regularizer_l1(10)) summary(model) ## Model: &quot;sequential_43&quot; ## __________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================================================================================== ## dense_112 (Dense) (None, 1) 6 ## ================================================================================================================================================== ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## __________________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(lr = 0.5), metrics = c(metric_mean_squared_error)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) l1 = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 cbind(unlist(l1), unlist(unconstrained)) ## [,1] [,2] ## [1,] 1.63864541 5.004925 ## [2,] -8.61910915 -12.627240 ## [3,] 12.97266769 16.611521 ## [4,] -0.06296927 -3.887562 ## [5,] 0.04676224 2.272530 ## [6,] 34.33927155 41.110775 One can clearly see that parameters are pulled towards zero because of the regularization. ## Tree-based ML algorithms Famous ML algorithms such as random Forest and gradient boosted trees are based on classification and regression trees. 3.1.3 Classification and Regression Trees Tree-based models in general use a series of if-then rules to generate predictions from one or more decision trees. In this lecture we will explore regression and classifaction trees at the example of the airquality data set. We first prepare and visualize the data and afterwards fit a decision tree. There is one important hyper-parameter for regression trees: minsplit controls the depth of tree (see the help of tree for a description) controls the complexity of the tree and can be seen also as a regularization parameter library(rpart) library(rpart.plot) data=airquality[complete.cases(airquality),] Fit and visualize a regression tree: rt = rpart(Ozone~., data = data,control = rpart.control(minsplit = 10)) rpart.plot(rt) Visualize the predictions: pred = predict(rt, data) plot(data$Temp, data$Ozone) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) The angular form of the prediction line is typical for regression trees. 3.1.4 Random Forest Random Forest creates an ensemble of regression/classification trees. However, there are two randomization steps with the RF that are responsible for the success of RF: bootstrap sample for each tree (we will sample observations with replacement from the dataset) at each split, we will sample a subset of predictors which are then considered as potential splitting criterion Fit a RF and visualize the predictions: library(randomForest) rf = randomForest(Ozone~., data = data) pred = predict(rf, data) plot(Ozone~Temp, data = data) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) One advantage of RF is that we will get a variable importance: importance(rf) ## IncNodePurity ## Solar.R 18081.51 ## Wind 29637.69 ## Temp 36636.86 ## Month 10490.42 ## Day 15000.40 Important hyperparameters: Similar to regression and classification trees, the hyper parameter nodesize controls for complexity. -&gt; Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5). mtry - Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times. 3.1.5 Boosted regression trees RF fits hundreds of trees independent of each other. In boosted regression trees, we will start with a weak learner (weak learner == regression tree) and then fit sequentially additional weak learners. There are two different approaches to enhance the performance: AdaBoost, wrong classified observations (by the previous tree) will get a higher weight, the chain of trees will focus on difficult/missclassified observations Gradient boosting (state of the art), each sequential model will be fit on the residual errors of the previous model Fit a BRT using xgboost: library(xgboost) data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) brt = xgboost(data_xg, nrounds = 16L, nthreads = 4L) ## [11:39:36] WARNING: amalgamation/../src/learner.cc:516: ## Parameters: { nthreads } might not be used. ## ## This may not be accurate due to some parameters are only used in language bindings but ## passed down to XGBoost core. Or some parameters are not used but slip through this ## verification. Please open an issue if you find above cases. ## ## ## [1] train-rmse:39.724625 ## [2] train-rmse:30.225761 ## [3] train-rmse:23.134842 ## [4] train-rmse:17.899178 ## [5] train-rmse:14.097785 ## [6] train-rmse:11.375458 ## [7] train-rmse:9.391275 ## [8] train-rmse:7.889690 ## [9] train-rmse:6.646585 ## [10] train-rmse:5.804860 ## [11] train-rmse:5.128438 ## [12] train-rmse:4.456416 ## [13] train-rmse:4.069464 ## [14] train-rmse:3.674615 ## [15] train-rmse:3.424578 ## [16] train-rmse:3.191302 xgboost has a weird syntax, we have to transform our data into a xgb.DMatrix object to be able to fit the model. We will do 500 rounds which means we will fit 500 sequential models: Let us visualize the predictions for different number of trees: par(mfrow = c(2,2)) for(i in 1:4){ pred = predict(brt, newdata = data_xg, ntreelimit = i) plot(data$Temp, data$Ozone, main = i) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) } xgboost also provides an variable importance: xgboost::xgb.importance(model = brt) ## Feature Gain Cover Frequency ## 1: Temp 0.570071875 0.2958229 0.24836601 ## 2: Wind 0.348230710 0.3419576 0.24183007 ## 3: Solar.R 0.058795559 0.1571072 0.30718954 ## 4: Day 0.019530002 0.1779925 0.16993464 ## 5: Month 0.003371853 0.0271197 0.03267974 sqrt(mean((data$Ozone - pred)^2)) # RMSE ## [1] 17.89918 data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) xgboost has an argument to do cross-validation: brt = xgboost(data_xg, nrounds = 5L) ## [1] train-rmse:39.724625 ## [2] train-rmse:30.225761 ## [3] train-rmse:23.134842 ## [4] train-rmse:17.899178 ## [5] train-rmse:14.097785 brt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L, nrounds = 3L, nthreads = 4L) ## [1] train-rmse:39.978896+0.286917 test-rmse:40.933159+0.927614 ## [2] train-rmse:30.598420+0.425075 test-rmse:33.257564+1.932901 ## [3] train-rmse:23.706881+0.437094 test-rmse:27.806431+2.435701 print(brt_cv) ## ##### xgb.cv 3-folds ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 1 39.97890 0.2869171 40.93316 0.9276142 ## 2 30.59842 0.4250749 33.25756 1.9329006 ## 3 23.70688 0.4370938 27.80643 2.4357008 There are different ways to control for complexity: max_depth, depth of each tree shrinkage (each tree will get a weight and the weight will decrease with the number of trees) 3.2 Distance-based algorithms In this chapter, we introduce support-vector machines (SVMs) and other distance-based methods. 3.2.1 k-nearest-neighbor k-nearest-neighbor classifies new observations by calculating the nearest n neighbors. The labels of the n nearest neighbors decide the class of the new point: X = scale(iris[,1:4]) Y = iris[,5] plot(X[-100,1], X[-100,3], col = Y) points(X[100,1], X[100,3], col = &quot;blue&quot;, pch = 18, cex = 1.3) Which class would you decide for the blue point? What are the classes of the nearest points?… well this procedure is used by the kNN: Scaling is very important when dealing with distances (we also split the dataset into a training and a testing dataset): data = iris data[,1:4] = apply(data[,1:4],2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] Fit model and create predictions: library(kknn) knn = kknn(Species~., train = train, test = test) summary(knn) ## ## Call: ## kknn(formula = Species ~ ., train = train, test = test) ## ## Response: &quot;nominal&quot; ## fit prob.setosa prob.versicolor prob.virginica ## 1 setosa 1 0.0000000 0.00000000 ## 2 setosa 1 0.0000000 0.00000000 ## 3 setosa 1 0.0000000 0.00000000 ## 4 setosa 1 0.0000000 0.00000000 ## 5 setosa 1 0.0000000 0.00000000 ## 6 setosa 1 0.0000000 0.00000000 ## 7 setosa 1 0.0000000 0.00000000 ## 8 setosa 1 0.0000000 0.00000000 ## 9 setosa 1 0.0000000 0.00000000 ## 10 setosa 1 0.0000000 0.00000000 ## 11 setosa 1 0.0000000 0.00000000 ## 12 setosa 1 0.0000000 0.00000000 ## 13 setosa 1 0.0000000 0.00000000 ## 14 setosa 1 0.0000000 0.00000000 ## 15 setosa 1 0.0000000 0.00000000 ## 16 setosa 1 0.0000000 0.00000000 ## 17 versicolor 0 0.9354939 0.06450608 ## 18 versicolor 0 0.6349387 0.36506134 ## 19 versicolor 0 1.0000000 0.00000000 ## 20 versicolor 0 0.7783044 0.22169561 ## 21 versicolor 0 1.0000000 0.00000000 ## 22 versicolor 0 1.0000000 0.00000000 ## 23 versicolor 0 0.6430958 0.35690421 ## 24 versicolor 0 1.0000000 0.00000000 ## 25 virginica 0 0.2580081 0.74199187 ## 26 versicolor 0 0.9148730 0.08512700 ## 27 versicolor 0 1.0000000 0.00000000 ## 28 versicolor 0 0.7245826 0.27541743 ## 29 versicolor 0 1.0000000 0.00000000 ## 30 versicolor 0 1.0000000 0.00000000 ## 31 versicolor 0 0.7890886 0.21091135 ## 32 versicolor 0 1.0000000 0.00000000 ## 33 versicolor 0 0.9511855 0.04881448 ## 34 versicolor 0 1.0000000 0.00000000 ## 35 virginica 0 0.0000000 1.00000000 ## 36 virginica 0 0.0851270 0.91487300 ## 37 virginica 0 0.0000000 1.00000000 ## 38 virginica 0 0.0000000 1.00000000 ## 39 virginica 0 0.0000000 1.00000000 ## 40 versicolor 0 0.7013345 0.29866548 ## 41 virginica 0 0.2109114 0.78908865 ## 42 virginica 0 0.0000000 1.00000000 ## 43 virginica 0 0.0000000 1.00000000 ## 44 virginica 0 0.0000000 1.00000000 ## 45 virginica 0 0.0156916 0.98430840 table(test$Species, fitted(knn)) ## ## setosa versicolor virginica ## setosa 16 0 0 ## versicolor 0 17 1 ## virginica 0 1 10 Actually, there is no “real” learning in a kNN. 3.2.2 Support Vector Machines (SVM) Support vectors machine try to find a hyperplane in the predictor space which separates the classes in the best way. Fitting a SVM: library(e1071) data = iris data[,1:4] = apply(data[,1:4],2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] sm = svm(Species~., data = train, kernel = &quot;linear&quot;) pred = predict(sm, newdata = test) oldpar = par() par(mfrow = c(1,2)) plot(test$Sepal.Length, test$Petal.Length, col = pred, main = &quot;predicted&quot;) plot(test$Sepal.Length, test$Petal.Length, col = test$Species, main = &quot;observed&quot;) par(oldpar) ## Warning in par(oldpar): graphical parameter &quot;cin&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;cra&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;csi&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;cxy&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;din&quot; cannot be set ## Warning in par(oldpar): graphical parameter &quot;page&quot; cannot be set mean(pred==test$Species) # accuracy ## [1] 0.9777778 SVM can only work on linear separable problems. ( A problem is called linearly separable if there exists at least one line in the plane with all of the points of one group on one side of the line and all the points of the others group on the other side). However, there is a trick, the so called kernel trick. The kernel trick maps the predictor space into a (higher dimensional) space in which the problem is linear separable: set.seed(42) x1 = seq(-3, 3, length.out = 100) x2 = seq(-3, 3, length.out = 100) X = expand.grid(x1, x2) y = apply(X, 1, function(x) exp(-x[1]^2 - x[2]^2)) y = ifelse(1/(1+exp(-y)) &lt; 0.62, 0, 1) image(matrix(y, 100, 100)) animation::saveGIF({ for (i in c(&quot;truth&quot;,&quot;linear&quot;, &quot;radial&quot;, &quot;sigmoid&quot;)) { if(i == &quot;truth&quot;){ image(matrix(y, 100,100),main = &quot;Ground truth&quot;,axes = FALSE, las = 2) }else{ sv = e1071::svm(x = X, y = factor(y), kernel = i) image(matrix(as.numeric(as.character(predict(sv, X))), 100,100),main = paste0(&quot;Kernel: &quot;, i),axes = FALSE, las = 2) axis(1, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1)) axis(2, at = seq(0,1, length.out = 10), labels = round(seq(-3,3, length.out = 10), 1), las = 2) } } },movie.name = &quot;svm.gif&quot;, autobrowse = FALSE) 3.3 Artificial neural networks Regularization in ANNs library(keras) data = airquality summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 Min. :5.000 Min. : 1.0 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 1st Qu.:6.000 1st Qu.: 8.0 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 Median :7.000 Median :16.0 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 Mean :6.993 Mean :15.8 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 Max. :9.000 Max. :31.0 ## NA&#39;s :37 NA&#39;s :7 data = data[complete.cases(data),] # remove NAs summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.0 Min. : 7.0 Min. : 2.30 Min. :57.00 Min. :5.000 Min. : 1.00 ## 1st Qu.: 18.0 1st Qu.:113.5 1st Qu.: 7.40 1st Qu.:71.00 1st Qu.:6.000 1st Qu.: 9.00 ## Median : 31.0 Median :207.0 Median : 9.70 Median :79.00 Median :7.000 Median :16.00 ## Mean : 42.1 Mean :184.8 Mean : 9.94 Mean :77.79 Mean :7.216 Mean :15.95 ## 3rd Qu.: 62.0 3rd Qu.:255.5 3rd Qu.:11.50 3rd Qu.:84.50 3rd Qu.:9.000 3rd Qu.:22.50 ## Max. :168.0 Max. :334.0 Max. :20.70 Max. :97.00 Max. :9.000 Max. :31.00 X = scale(data[,2:6]) Y = data[,1] model = keras_model_sequential() penalty = 0.01 model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L), kernel_regularizer = regularizer_l1(penalty)) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty) ) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty)) %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, kernel_regularizer = regularizer_l1(penalty)) # one output dimension with a linear activation function summary(model) ## Model: &quot;sequential_44&quot; ## __________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================================================================================== ## dense_113 (Dense) (None, 100) 600 ## __________________________________________________________________________________________________________________________________________________ ## dense_114 (Dense) (None, 100) 10100 ## __________________________________________________________________________________________________________________________________________________ ## dense_115 (Dense) (None, 100) 10100 ## __________________________________________________________________________________________________________________________________________________ ## dense_116 (Dense) (None, 1) 101 ## ================================================================================================================================================== ## Total params: 20,901 ## Trainable params: 20,901 ## Non-trainable params: 0 ## __________________________________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(0.1)) model_history = model %&gt;% fit(x = X, y = matrix(Y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; weights = lapply(model$weights, function(w) w$numpy() ) fields::image.plot(weights[[1]]) 3.4 The standard ML pipeline at the example of the titanic dataset The typical ML workflow consist of: Data cleaning and exploration (EDA=explorative data analysis) with tidyverse Pre-processing and feature selection Splitting dataset into train and test set for evaluation Model fitting Model evaluation New predictions Here is an (optional) video that explains the entire pipeline from a slightly different perspective In the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics here A good reference is R for data science by Hadley For this lecture you need the titanic dataset provided by us. You can find it in GRIPS (datasets.RData in the dataset and submission section) or at . We have split the dataset already into training and testing datasets (the test split has one column less than the train split, why?) 3.4.1 Data cleaning Load necessary libraries: library(keras) library(tensorflow) library(tidyverse) Load dataset: load(&quot;datasets.RData&quot;) train = titanic$train test = titanic$test For cleaning and exploration we will combine the datasets together (if we change a predictor in the train set, we have also to change it in the test set…). But we will create a new variable “subset” that tells us whether an observation belongs to the train or test split: test$survived = NA train$subset = &quot;train&quot; test$subset = &quot;test&quot; data = rbind(train,test) Standard summaries: str(data) ## &#39;data.frame&#39;: 1309 obs. of 15 variables: ## $ pclass : int 2 1 3 3 3 3 3 1 3 1 ... ## $ survived : int 1 1 0 0 0 0 0 1 0 1 ... ## $ name : chr &quot;Sinkkonen, Miss. Anna&quot; &quot;Woolner, Mr. Hugh&quot; &quot;Sage, Mr. Douglas Bullen&quot; &quot;Palsson, Master. Paul Folke&quot; ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ... ## $ age : num 30 NA NA 6 30.5 38.5 20 53 NA 42 ... ## $ sibsp : int 0 0 8 3 0 0 0 0 0 0 ... ## $ parch : int 0 0 2 1 0 0 0 0 0 0 ... ## $ ticket : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 221 123 779 542 589 873 472 823 588 834 ... ## $ fare : num 13 35.5 69.55 21.07 8.05 ... ## $ cabin : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 1 94 1 1 1 1 1 1 1 1 ... ## $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 4 4 4 4 4 4 2 4 2 ... ## $ boat : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 3 28 1 1 1 1 1 19 1 15 ... ## $ body : int NA NA NA NA 50 32 NA NA NA NA ... ## $ home.dest: Factor w/ 370 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ... ## $ subset : chr &quot;train&quot; &quot;train&quot; &quot;train&quot; &quot;train&quot; ... summary(data) ## pclass survived name sex age sibsp parch ticket fare ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 Min. :0.000 CA. 2343: 11 Min. : 0.000 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 1601 : 8 1st Qu.: 7.896 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 Median :0.000 CA 2144 : 8 Median : 14.454 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 Mean :0.385 3101295 : 7 Mean : 33.295 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 347077 : 7 3rd Qu.: 31.275 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 Max. :9.000 347082 : 7 Max. :512.329 ## NA&#39;s :655 NA&#39;s :263 (Other) :1261 NA&#39;s :1 ## cabin embarked boat body home.dest subset ## :1014 : 2 :823 Min. : 1.0 :564 Length:1309 ## C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 New York, NY : 64 Class :character ## B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 London : 14 Mode :character ## G6 : 5 S:914 15 : 37 Mean :160.8 Montreal, PQ : 10 ## B96 B98 : 4 14 : 33 3rd Qu.:256.0 Cornwall / Akron, OH: 9 ## C22 C26 : 4 4 : 31 Max. :328.0 Paris, France : 9 ## (Other) : 271 (Other):308 NA&#39;s :1188 (Other) :639 head(data) ## pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest ## 561 2 1 Sinkkonen, Miss. Anna female 30.0 0 0 250648 13.000 S 10 NA Finland / Washington, DC ## 321 1 1 Woolner, Mr. Hugh male NA 0 0 19947 35.500 C52 S D NA London, England ## 1177 3 0 Sage, Mr. Douglas Bullen male NA 8 2 CA. 2343 69.550 S NA ## 1098 3 0 Palsson, Master. Paul Folke male 6.0 3 1 349909 21.075 S NA ## 1252 3 0 Tomlin, Mr. Ernest Portage male 30.5 0 0 364499 8.050 S 50 ## 1170 3 0 Saether, Mr. Simon Sivertsen male 38.5 0 0 SOTON/O.Q. 3101262 7.250 S 32 ## subset ## 561 train ## 321 train ## 1177 train ## 1098 train ## 1252 train ## 1170 train The name variable consists of 1309 unique factors (there are 1309 observations…): length(unique(data$name)) ## [1] 1307 However, there is a title in each name. Let’s extract the titles: we will extract all names and split each name after each comma “,” we will split the second split of the name after a point “.” and extract the titles first_split = sapply(data$name, function(x) stringr::str_split(x, pattern = &quot;,&quot;)[[1]][2]) titles = sapply(first_split, function(x) strsplit(x, &quot;.&quot;,fixed = TRUE)[[1]][1]) We get 18 unique titles: table(titles) ## titles ## Capt Col Don Dona Dr Jonkheer Lady Major Master Miss ## 1 4 1 1 8 1 1 2 61 260 ## Mlle Mme Mr Mrs Ms Rev Sir the Countess ## 2 1 757 197 2 8 1 1 With the forcats package we can easily handle and mutate factor variables. A few titles have a very low occurrence rate: titles = stringr::str_trim((titles)) titles %&gt;% fct_count() ## # A tibble: 18 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Capt 1 ## 2 Col 4 ## 3 Don 1 ## 4 Dona 1 ## 5 Dr 8 ## 6 Jonkheer 1 ## 7 Lady 1 ## 8 Major 2 ## 9 Master 61 ## 10 Miss 260 ## 11 Mlle 2 ## 12 Mme 1 ## 13 Mr 757 ## 14 Mrs 197 ## 15 Ms 2 ## 16 Rev 8 ## 17 Sir 1 ## 18 the Countess 1 We will collapse titles with low occurrences into one title: titles2 = forcats::fct_collapse(titles, officer = c(&quot;Capt&quot;, &quot;Col&quot;, &quot;Major&quot;, &quot;Dr&quot;, &quot;Rev&quot;), royal = c(&quot;Jonkheer&quot;, &quot;Don&quot;, &quot;Sir&quot;, &quot;the Countess&quot;, &quot;Dona&quot;, &quot;Lady&quot;), miss = c(&quot;Miss&quot;, &quot;Mlle&quot;), mrs = c(&quot;Mrs&quot;, &quot;Mme&quot;, &quot;Ms&quot;) ) titles = stringr::str_trim((titles)) titles %&gt;% fct_count() ## # A tibble: 18 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Capt 1 ## 2 Col 4 ## 3 Don 1 ## 4 Dona 1 ## 5 Dr 8 ## 6 Jonkheer 1 ## 7 Lady 1 ## 8 Major 2 ## 9 Master 61 ## 10 Miss 260 ## 11 Mlle 2 ## 12 Mme 1 ## 13 Mr 757 ## 14 Mrs 197 ## 15 Ms 2 ## 16 Rev 8 ## 17 Sir 1 ## 18 the Countess 1 We will collapse titles with low occurrences into one title: titles2 = forcats::fct_collapse(titles, officer = c(&quot;Capt&quot;, &quot;Col&quot;, &quot;Major&quot;, &quot;Dr&quot;, &quot;Rev&quot;), royal = c(&quot;Jonkheer&quot;, &quot;Don&quot;, &quot;Sir&quot;, &quot;the Countess&quot;, &quot;Dona&quot;, &quot;Lady&quot;), miss = c(&quot;Miss&quot;, &quot;Mlle&quot;), mrs = c(&quot;Mrs&quot;, &quot;Mme&quot;, &quot;Ms&quot;) ) Count titles again: titles2 %&gt;% fct_count() ## # A tibble: 6 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 officer 23 ## 2 royal 6 ## 3 Master 61 ## 4 miss 262 ## 5 mrs 200 ## 6 Mr 757 Add new title variable to dataset: data = data %&gt;% mutate(title = titles2) As a second example, we will explore and clean the numeric “age” variable: Explore the variable: summary(data) ## pclass survived name sex age sibsp parch ticket fare ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 Min. :0.000 CA. 2343: 11 Min. : 0.000 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 1601 : 8 1st Qu.: 7.896 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 Median :0.000 CA 2144 : 8 Median : 14.454 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 Mean :0.385 3101295 : 7 Mean : 33.295 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 347077 : 7 3rd Qu.: 31.275 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 Max. :9.000 347082 : 7 Max. :512.329 ## NA&#39;s :655 NA&#39;s :263 (Other) :1261 NA&#39;s :1 ## cabin embarked boat body home.dest subset title ## :1014 : 2 :823 Min. : 1.0 :564 Length:1309 officer: 23 ## C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 New York, NY : 64 Class :character royal : 6 ## B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 London : 14 Mode :character Master : 61 ## G6 : 5 S:914 15 : 37 Mean :160.8 Montreal, PQ : 10 miss :262 ## B96 B98 : 4 14 : 33 3rd Qu.:256.0 Cornwall / Akron, OH: 9 mrs :200 ## C22 C26 : 4 4 : 31 Max. :328.0 Paris, France : 9 Mr :757 ## (Other) : 271 (Other):308 NA&#39;s :1188 (Other) :639 sum(is.na(data$age))/nrow(data) ## [1] 0.2009167 20% NAs! Either we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age: data = data %&gt;% group_by(sex, pclass, title) %&gt;% mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;% ungroup() However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups. In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age….)) will be done within the specified groups. 3.4.2 Pre-processing and feature selection Keras cannot handle factors and we have to scale the data. For now we will sub-select a batch of predictors and: scale the numeric predictors change the factors with only two groups/levels into integer data_sub = data %&gt;% select(survived, sex, age2, fare, title, pclass) %&gt;% mutate(age2 = scales::rescale(age2, c(0,1)), fare = scales::rescale(fare, c(0,1))) %&gt;% mutate(sex = as.integer(sex) - 1L, title = as.integer(title) - 1L, pclass = as.integer(pclass - 1L)) One hot encoding of factors with &gt; 2 levels: one_title = k_one_hot(data_sub$title, length(unique(data$title)))$numpy() colnames(one_title) = levels(data$title) one_sex = k_one_hot(data_sub$sex, length(unique(data$sex)))$numpy() colnames(one_sex) = levels(data$sex) one_pclass = k_one_hot(data_sub$pclass, length(unique(data$pclass)))$numpy() colnames(one_pclass) = paste0(1:length(unique(data$pclass)), &quot;pclass&quot;) Add the dummy encoded variables to the dataset: data_sub = cbind(data.frame(survived= data_sub$survived, subset = data$subset), one_title, one_sex, age = data_sub$age2, fare = data_sub$fare, one_pclass) head(data_sub) ## survived subset officer royal Master miss mrs Mr female male age fare 1pclass 2pclass 3pclass ## 1 1 train 0 0 0 1 0 0 1 0 0.37369494 0.02537431 0 1 0 ## 2 1 train 0 0 0 0 0 1 0 1 0.51774510 0.06929139 1 0 0 ## 3 0 train 0 0 0 0 0 1 0 1 0.32359053 0.13575256 0 0 1 ## 4 0 train 0 0 1 0 0 0 0 1 0.07306851 0.04113566 0 0 1 ## 5 0 train 0 0 0 0 0 1 0 1 0.37995799 0.01571255 0 0 1 ## 6 0 train 0 0 0 0 0 1 0 1 0.48016680 0.01415106 0 0 1 3.4.3 Split data for training and testing The splitting consists of two splits: an outer split (the original split, remember we got a train and test split without the response “survived”) an inner split (we will split further the train dataset into another train and test split with known response) The inner split is important because to assess the model’s performance and potential overfitting Outer split: train = data_sub %&gt;% filter(subset == &quot;train&quot;) %&gt;% filter(!is.na(fare)) %&gt;% select(-subset) Inner split: indices = sample.int(nrow(train), 0.7*nrow(train)) sub_train = train[indices,] sub_test = train[-indices,] What is the difference between the two splits? (Tip: have a look at the variable survived) 3.4.4 Model fitting In the next step we will fit a keras model on the train split of the inner split: model = keras_model_sequential() model %&gt;% layer_dense(units = 20L, input_shape = ncol(sub_train) - 1L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 2L, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_45&quot; ## _______________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================================================================================================================= ## dense_117 (Dense) (None, 20) 280 ## _______________________________________________________________________________________________________________________________________________________ ## dense_118 (Dense) (None, 20) 420 ## _______________________________________________________________________________________________________________________________________________________ ## dense_119 (Dense) (None, 20) 420 ## _______________________________________________________________________________________________________________________________________________________ ## dense_120 (Dense) (None, 2) 42 ## ======================================================================================================================================================= ## Total params: 1,162 ## Trainable params: 1,162 ## Non-trainable params: 0 ## _______________________________________________________________________________________________________________________________________________________ model_history = model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax(0.01)) model_history = model %&gt;% fit(x = as.matrix(sub_train[,-1]), y = to_categorical(sub_train[,1],num_classes = 2L), epochs = 100L, batch_size = 32L, validation_split = 0.2, shuffle = TRUE) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.4.5 Model evaluation We will predict survived for the test set of the inner split and calculate the accuracy: preds = model %&gt;% predict(x = as.matrix(sub_test[,-1])) predicted = ifelse(preds[,2] &lt; 0.5, 0, 1) observed = sub_test[,1] (accuracy = mean(predicted == observed)) ## [1] 0.75 3.4.6 Predictions and submission When we are satisfied with the performance of our model in the inner split, we will create predictions for the test split of the outer split: select all observations that belong to the outer test split (the filter function) remove the subset and survived (NAs) columns submit= data_sub %&gt;% filter(subset == &quot;test&quot;) %&gt;% select(-subset, -survived) We cannot assess the performance on the test split because the true survived ratio is unknown, however, we can now submit our predictions to the submission server at http://rhsbio7.uni-regensburg.de:8500 To do so, we have to transform our survived probablities into actual 0/1 predictions (probablities are not allowed) and create a csv: pred = model %&gt;% predict(as.matrix(submit)) pred = ifelse(pred[,2] &lt; 0.5, 0, 1) All values &gt; 0.5 will be set to 1 and values &lt; 0.5 to zero. For the submission it is critical to change the predictions into a data.frame and save it with the write.csv function: write.csv(data.frame(y=pred), file = &quot;Max_1.csv&quot;) The file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself. "],["Deep.html", "4 Deep learning 4.1 Deep Neural Networks 4.2 Convolutional Neural Networks - MNIST 4.3 Flower dataset", " 4 Deep learning 4.1 Deep Neural Networks 4.1.1 Dropout and Early stopping Early stopping: you might have noticed yesterday that even with regularization the validation loss will start to increase at some point during the training. Early stopping allows us to stop the training when for instance the test loss does not increase anymore With l1/l2 regularization we have to carefully tune the regularization strength. Dropout is robuster, and tuning of the dropout rate can be beneficial but a rate between 0.2-0.5 works often quite well 4.2 Convolutional Neural Networks - MNIST The MNIST dataset is maybe one of the most famous image datasets. It is a dataset of 60,000 handwritten digits from 0-9. Let’s define a few helper functions: library(keras) rotate = function(x) t(apply(x, 2, rev)) imgPlot = function(img, title = &quot;&quot;){ col=grey.colors(255) image(rotate(img), col = col, xlab = &quot;&quot;, ylab = &quot;&quot;, axes=FALSE, main = paste0(&quot;Label: &quot;, as.character(title))) } The dataset is so famous that there is an automatic download function in keras: data = dataset_mnist() train = data$train test = data$test Let’s visualize a few digits: par(mfrow = c(3,3)) .n = sapply(1:9, function(x) imgPlot(train$x[x,,], train$y[x])) Similar to the normal ML workflow, we have to scale the pixels (from 0-255) to the range of [0,1] and one hot encode the response: train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) We also use here now arrays instead of matrices. Arrays are higher dimensional matrices (tensor of rank 3). Finally, we have a “real” tensor. The last dimension stands for the number of channels in the image. In our case we have only one channel because the images are white-black. Normally we would have three channels - colors are encoded by the combination of three channels (e.g. rgb). model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L,1L),filters = 16L, kernel_size = c(2L,2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L,3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_46&quot; ## _______________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================================================================================================================= ## conv2d_19 (Conv2D) (None, 27, 27, 16) 80 ## _______________________________________________________________________________________________________________________________________________________ ## max_pooling2d_16 (MaxPooling2D) (None, 13, 13, 16) 0 ## _______________________________________________________________________________________________________________________________________________________ ## conv2d_20 (Conv2D) (None, 11, 11, 16) 2320 ## _______________________________________________________________________________________________________________________________________________________ ## max_pooling2d_17 (MaxPooling2D) (None, 5, 5, 16) 0 ## _______________________________________________________________________________________________________________________________________________________ ## flatten_10 (Flatten) (None, 400) 0 ## _______________________________________________________________________________________________________________________________________________________ ## dense_121 (Dense) (None, 100) 40100 ## _______________________________________________________________________________________________________________________________________________________ ## dense_122 (Dense) (None, 10) 1010 ## ======================================================================================================================================================= ## Total params: 43,510 ## Trainable params: 43,510 ## Non-trainable params: 0 ## _______________________________________________________________________________________________________________________________________________________ We will start now with a 2D convolutional layer, (3D would be e.g. for movies, so the third dimension would correspond to time and not to the color channels!). We use 16 convolutional kernels (filters) with a size of 2x2. The pooling layer downsizes the resulting feature maps. After another conv and pooling layer we flatten the output, i.e. the following dense layer treats the previous layer as normal dense layer (so the dense layer is connected to all weights from the last feature maps). We end the model with our typical output layer. The rest is as usual: model %&gt;% compile( optimizer = keras::optimizer_adamax(0.01), loss = loss_categorical_crossentropy ) summary(model) ## Model: &quot;sequential_46&quot; ## _______________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================================================================================================================= ## conv2d_19 (Conv2D) (None, 27, 27, 16) 80 ## _______________________________________________________________________________________________________________________________________________________ ## max_pooling2d_16 (MaxPooling2D) (None, 13, 13, 16) 0 ## _______________________________________________________________________________________________________________________________________________________ ## conv2d_20 (Conv2D) (None, 11, 11, 16) 2320 ## _______________________________________________________________________________________________________________________________________________________ ## max_pooling2d_17 (MaxPooling2D) (None, 5, 5, 16) 0 ## _______________________________________________________________________________________________________________________________________________________ ## flatten_10 (Flatten) (None, 400) 0 ## _______________________________________________________________________________________________________________________________________________________ ## dense_121 (Dense) (None, 100) 40100 ## _______________________________________________________________________________________________________________________________________________________ ## dense_122 (Dense) (None, 10) 1010 ## ======================================================================================================================================================= ## Total params: 43,510 ## Trainable params: 43,510 ## Non-trainable params: 0 ## _______________________________________________________________________________________________________________________________________________________ epochs = 5L batch_size = 32L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) 4.2.1 Data Augmentation Having to train a CNN using very little data is a common problem. Data augmentation helps to artificially increase the number of images. The idea is that the CNN has to learn specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures. Luckily, it is very easy to use data augmentation in keras. We will use again the MNIST dataset: library(keras) data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) print(dim(train_x)) ## [1] 60000 28 28 1 print(dim(test_y)) ## [1] 10000 10 model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(NULL, 28, 28,1),filters = 16, kernel_size = c(2,2), activation = &quot;relu&quot;, use_bias = F) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = &quot;relu&quot;, use_bias = F) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100, activation = &quot;relu&quot;) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_47&quot; ## _______________________________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================================================================================================================= ## conv2d_21 (Conv2D) (None, 27, 27, 16) 64 ## _______________________________________________________________________________________________________________________________________________________ ## max_pooling2d_18 (MaxPooling2D) (None, 13, 13, 16) 0 ## _______________________________________________________________________________________________________________________________________________________ ## conv2d_22 (Conv2D) (None, 11, 11, 16) 2304 ## _______________________________________________________________________________________________________________________________________________________ ## max_pooling2d_19 (MaxPooling2D) (None, 5, 5, 16) 0 ## _______________________________________________________________________________________________________________________________________________________ ## flatten_11 (Flatten) (None, 400) 0 ## _______________________________________________________________________________________________________________________________________________________ ## dense_123 (Dense) (None, 100) 40100 ## _______________________________________________________________________________________________________________________________________________________ ## dense_124 (Dense) (None, 10) 1010 ## ======================================================================================================================================================= ## Total params: 43,478 ## Trainable params: 43,478 ## Non-trainable params: 0 ## _______________________________________________________________________________________________________________________________________________________ model %&gt;% compile( optimizer = optimizer_adamax(), loss = loss_categorical_crossentropy ) We have now to define a generator object (it is a specific object which infinitly draws samples from our dataset). In the generator we can turn on the data augementation: aug = image_data_generator() generator = flow_images_from_data(train_x, train_y,generator = aug) model %&gt;% fit_generator(generator, steps_per_epoch = dim(train_x)[1],epochs = 5L) ## Warning in fit_generator(., generator, steps_per_epoch = dim(train_x)[1], : `fit_generator` is deprecated. Use `fit` instead, it now accept generators. However, now we have to set the step size because the model does not know the first dimension of the image. 4.2.2 Transfer learning Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning. The idea is that all the convolutional layers have mainly one task - learning to identify highly correlated neighbored features and therefore to learn structures such as edges in the image. Also, the second idea is that only the top layer, the dense layer is the actual classifier of the CNN. The top classifier will be confronted by sets of different edges/structures and has to decide the label. Again, this sounds very complicating but is again quite easy with keras: CIFAR10 preparation: data = keras::dataset_cifar10() train = data$train test = data$test image = train$x[5,,,] image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() train_x = array(train$x/255, c(dim(train$x))) test_x = array(test$x/255, c(dim(test$x))) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) Keras provides download functions for all famous architectures/CNN models which are already trained on the imagenet dataset (anoother famous dataset) and the CNNs come already without their top layer densenet = application_densenet201(include_top = FALSE, input_shape = c(32L, 32L, 3L)) We have to specify directly here in the model our input dimension. Now, we will use not a sequential model but just a “keras_model” where we can specify the inputs and outputs: model = keras::keras_model(inputs = densenet$input, outputs = layer_flatten(layer_dense(densenet$output, units = 10L, activation = &quot;softmax&quot;)) ) The outputs are our own top layer. In the next step we want to freeze all layers except for our own last layer (with freezing I mean that we do not want to train the complete model, we only want to train the last layer): model %&gt;% freeze_weights(to = length(model$layers)-1) Btw, you can always check the number of trainable weights via summary(model) And then the usual training: model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax()) model %&gt;% fit( x = train_x, y = train_y, epochs = 1L, batch_size = 32L, shuffle = T, validation_split = 0.2, ) 4.3 Flower dataset create a CNN submit predictions see kaggle flower dataset for specific architectures! Data preparation: data_files = list.files(&quot;flower/&quot;, full.names = TRUE) train = data_files[str_detect(data_files, &quot;train&quot;)] test = readRDS(file = &quot;flower/test.RDS&quot;) train = lapply(train, readRDS) train_classes = lapply(train, function(d) dim(d)[1]) train = abind::abind(train, along = 1L) labels_train = rep(0:4, unlist(train_classes)) "],["xAI.html", "5 Explainable AI (xAI), NLP, and RNNs 5.1 xAI Methods 5.2 Natural Language Processing (NLP) 5.3 Recurrent neural networks (RNNs)", " 5 Explainable AI (xAI), NLP, and RNNs The goal of xAI is to explain WHY a fitted ML models makes certain predictions, for example how important different variables are for predictions etc. There are various important applications for this, ranging from a better technical understanding of the models over understanding which data is important to improve predictions to questions of fairness and discrimination (e.g. to understand if an algorithm uses skin color to make a decision). In general, xAI != causality Before we discuss xAI methods in more detail, we want to make one thing very clear - in general, xAI methods measure which variables are used by the algorithm for predictions, or how much variables improve predictions. The important point to note here: if a variable causes something, we could also expect that it helps to predict the very thing. The opposite, however, is not generally true - it is very often possible that a variable that doesn’t cause something can predict something. In statistics (in particular course: advanced biostatistics), we discuss the issue of causality at length. Here, we don’t want to go into the details, but again, you should in general resist to interpret indicators of importance in xAI as causal effects. They tell you something about what’s going on in the algorithm, not about what’s going on in reality. Here an example for the variable importance indicators in the RF algorithm. The purpose of this script is to show that RF variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are sepearable and would be correctly separated by an lm / ANOVA We simulate a dataset with 2 predictors that are strongly correlated, but only one of them has an effect on the response. # simulation parameters n = 1000 col = 0.7 # create collinear predictors x1 = runif(n) x2 = col * x1 + (1-col) * runif(n) # response is only influenced by x1 y = x1 + rnorm(n) lm / anova correctly identify x1 as causal variable anova(lm(y ~ x1 + x2)) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 59.74 59.741 57.7854 6.733e-14 *** ## x2 1 0.26 0.255 0.2469 0.6193 ## Residuals 997 1030.73 1.034 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Fit RF and show variable importance fit &lt;- randomForest(y ~ x1 + x2, importance=TRUE) varImpPlot(fit) Variable importance is now split nearly evenly. Task: understand why this is - remember: How the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built Remember that as x1 ~ x2, we can use x2 as a replacement for x1 Remember that the variable importance measures the average contributions of the different variables in the trees of the forest 5.1 xAI Methods In this lecture we will work with another famous dataset, the Boston housing dataset: We will fit a random forest and use the iml pkg for xAI, see set.seed(123) library(&quot;iml&quot;) library(&quot;randomForest&quot;) data(&quot;Boston&quot;, package = &quot;MASS&quot;) rf = randomForest(medv ~ ., data = Boston, ntree = 50) xAI packages are generic, i.e. they can handle almost all ML models. First, we have to create a Predictor object, that holds the model and the data. The iml package uses R6 classes: New objects can be created by calling Predictor$new(). X = Boston[which(names(Boston) != &quot;medv&quot;)] predictor = Predictor$new(rf, data = X, y = Boston$medv) 5.1.1 Variable Importance Importance - not to be mistaken for the RF importance. This importance can be calculated for all ML models and is based on a permutation approach (have a look at the book): imp = FeatureImp$new(predictor, loss = &quot;mae&quot;) plot(imp) It tells us how important the individual variables are for predictions. 5.1.2 Partial dependencies Partial dependencies are similar to allEffects plot, the idea is to visualize “marginal effects” of predictors (with the feature argument we specify the variable we want to visualize): eff = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;pdp&quot;, grid.size = 30) plot(eff) Partial dependencies can be also plotted for single observations: eff = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;pdp+ice&quot;, grid.size = 30) plot(eff) One disadvantage of partial dependencies is that they are sensitive to correlated predictors. Accumulated local effects can be used to account for correlation for predictors 5.1.3 Accumulated local effects ALE re basically partial dependencies plots but try to correct for correlations between predictors ale = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;ale&quot;) ale$plot() If there is no collinearity, you shouldn’t see much difference between partial dependencies and ALE plots. 5.1.4 Friedmans H-statistic The H-statistic can be used to find interactions between predictors. However, again, keep in mind that the H-statistic is sensible to correlation between predictors: interact = Interaction$new(predictor, &quot;lstat&quot;) plot(interact) 5.1.5 Global explainer - Simplifying the ML model Another idea is to simplify the ML model with another simpler model such as a decision tree. We create predictions for a lot of different predictors values and then we will fit a decision tree on the predictions: library(partykit) tree = TreeSurrogate$new(predictor, maxdepth = 2) plot(tree) 5.1.6 Local explainer - LIME explaining single instances (observations) The global approach is to simplify a black-box model via a simpler surrogate Model. However, sometimes we are only interested in understanding how single observations/predictions are generated. The lime approach explores the feature space around one observations and fits then a simpler model (e.g. a linear model) on the feature space around one observation: library(glmnet) lime.explain = LocalModel$new(predictor, x.interest = X[1,]) lime.explain$results ## beta x.recoded effect x.original feature feature.value ## rm 4.1893817 6.575 27.545185 6.575 rm rm=6.575 ## ptratio -0.5307031 15.300 -8.119758 15.3 ptratio ptratio=15.3 ## lstat -0.4398104 4.980 -2.190256 4.98 lstat lstat=4.98 plot(lime.explain) 5.1.7 Local explainer - Shapley Shapley computes feature contributions for single predictions with the Shapley value, an approach from cooperative game theory. The idea is that the features values of an instance cooperate to achieve the prediction. The Shapley value fairly distributes the difference of the instance’s prediction and the datasets average prediction among the features: shapley = Shapley$new(predictor, x.interest = X[1,]) shapley$plot() 5.2 Natural Language Processing (NLP) What this video to get an idea about what NLP is about See also the blog post linked with the youtube video with accompanying code to the video. Moreover, here is an article that shows now NLP works with keras, however, written in Python. As a challenge, you can take the code and implement it in R 5.3 Recurrent neural networks (RNNs) Recurrent Neural Networks are used to model sequential data, i.e. temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic: "],["gans-vaes-and-reinforcement-learning.html", "6 GANs, VAEs, and Reinforcement learning 6.1 Generative adversarial network (GANs) 6.2 Autoencoder 6.3 Varational Autoencoder", " 6 GANs, VAEs, and Reinforcement learning 6.1 Generative adversarial network (GANs) The idea of generative adversarial network (GAN) is that two neural networks contest with each other in a game. On network is creating data and is trying to “trick” the other into thinking that this data is real. A possible application is to create pictures that look like real photographs. However, the application of GANs today is much wider than just the creation of data. For example, GANs can also be used to “augment” data, i.e. to create new data and thereby improve the fitted model. ### MNIST - GAN based on DNNs GANs - two networks are playing against each other. The generator (similar to the decoder in AEs) creates new images from noise and tries to convince the discriminator that this is a real image. The discriminator is getting a mix of true images (from the dataset) and of artificially generated images from the generator. Loss of the generator - when fakes are identified as fakes by the discriminator (simple binary_crossentropy loss, 0/1…) Loss of the discriminator - when fakes are identified as fakes (class 1) and true images as true images (class 0), again simple binary crossentropy. MNIST example: library(keras) library(tensorflow) rotate = function(x) t(apply(x, 2, rev)) imgPlot = function(img, title = &quot;&quot;){ col=grey.colors(255) image(rotate(img), col = col, xlab = &quot;&quot;, ylab = &quot;&quot;, axes=FALSE, main = paste0(&quot;Label: &quot;, as.character(title))) } We don’t need the test set: data = dataset_mnist() train = data$train train_x = array((train$x-127.5)/127.5, c(dim(train$x)[1], 784L)) Define and test generator model: get_generator = function(){ generator = keras_model_sequential() generator %&gt;% layer_dense(units = 200L ,input_shape = c(100L)) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 200L) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 784L, activation = &quot;tanh&quot;) return(generator) } generator = get_generator() sample = tf$random$normal(c(1L, 100L)) imgPlot(array(generator(sample)$numpy(), c(28L, 28L))) The noise of size = [100] (random vector with 100 values) is passed through the network and the output correspond to the number of pixels of one MNIST image (784) get_discriminator = function(){ discriminator = keras_model_sequential() discriminator %&gt;% layer_dense(units = 200L, input_shape = c(784L)) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 100L) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) return(discriminator) } discriminator = get_discriminator() discriminator(generator(tf$random$normal(c(1L, 100L)))) ## tf.Tensor([[0.43093494]], shape=(1, 1), dtype=float32) The normal architecture of a binary classifier (will get images as input) Loss: ce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE) loss_discriminator = function(real, fake){ real_loss = ce(tf$ones_like(real), real) fake_loss = ce(tf$zeros_like(fake), fake) return(real_loss+fake_loss) } loss_generator = function(fake){ return(ce(tf$ones_like(fake), fake)) } Binary crossentropy as loss function. However, we have to encode the true and predicted values for the two networks individually. The discriminator will get two losses - one for identifying fake images as fake, and one for identifying real MNIST images as real images. The generator will just get one loss - was it able to deceive the discriminator? Each network will get its own optimizer (while a AE will be treated as one network, in a GAN the networks will be treated independently) gen_opt = tf$keras$optimizers$RMSprop(1e-4) disc_opt = tf$keras$optimizers$RMSprop(1e-4) batch_size = 32L get_batch = function(){ # Helper function to get batches of images indices = sample.int(nrow(train_x), batch_size) return(tf$constant(train_x[indices,], &quot;float32&quot;)) } We have to write here our own training loop (we cannot use the fit function). Let’s define a training function: train_step = function(images){ noise = tf$random$normal(c(32L, 100L)) with(tf$GradientTape(persistent = TRUE) %as% tape,{ gen_images = generator(noise) fake_output = discriminator(gen_images) real_output = discriminator(images) gen_loss = loss_generator(fake_output) disc_loss = loss_discriminator(real_output, fake_output) }) gen_grads = tape$gradient(gen_loss, generator$weights) disc_grads = tape$gradient(disc_loss, discriminator$weights) rm(tape) gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights))) disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights))) return(c(gen_loss, disc_loss)) } train_step = tf$`function`(reticulate::py_func(train_step)) In each iteration (for each batch) we will do the following (the GradientTape records computations to do automatic differenation): sample noise Generator creates images from the noise Discriminator will make predictions for fake images and real images (response is a probability between [0,1]) Calculate loss for generator Calculate loss for discriminator Calculate gradients for weights and the loss Update weights of generator Update weights of discriminator return losses steps = as.integer(nrow(train_x)/batch_size) generator = get_generator() discriminator = get_discriminator() epochs = 30L steps = as.integer(nrow(train_x)/batch_size) counter = 1 gen_loss = NULL disc_loss = NULL for(i in 1:(epochs*steps)){ images = get_batch() losses = train_step(images) gen_loss = tf$reduce_sum(losses[[1]])$numpy() disc_loss = tf$reduce_sum(losses[[2]])$numpy() if(i %% 50*steps == 0) { noise = tf$random$normal(c(1L, 100L)) imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), &quot;Gen&quot;) } if(i %% steps == 0){ counter = 1 cat(&quot;Gen: &quot;, mean(gen_loss), &quot; Disc: &quot;, mean(disc_loss), &quot; \\n&quot;) } } The actual training loop: Create networks get batch of images run train_step function print losses repeat step 2-4 for number of epochs 6.1.1 Flower - GAN library(keras) library(tidyverse) data_files = list.files(&quot;flowers/&quot;, full.names = TRUE) train = data_files[str_detect(data_files, &quot;train&quot;)] test = readRDS(file = &quot;test.RDS&quot;) train = lapply(train, readRDS) train = abind::abind(train, along = 1L) train = tf$concat(list(train, test), axis = 0L)$numpy() train_x = array((train-127.5)/127.5, c(dim(train))) get_generator = function(){ generator = keras_model_sequential() generator %&gt;% layer_dense(units = 20L*20L*128L, input_shape = c(100L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_reshape(c(20L, 20L, 128L)) %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters = 256L, kernel_size = c(3L, 3L), padding = &quot;same&quot;, strides = c(1L, 1L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters = 128L, kernel_size = c(5L, 5L), padding = &quot;same&quot;, strides = c(1L, 1L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters = 64L, kernel_size = c(5L, 5L), padding = &quot;same&quot;, strides = c(2L, 2L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters =3L, kernel_size = c(5L, 5L), padding = &quot;same&quot;, strides = c(2L, 2L), activation = &quot;tanh&quot;, use_bias = FALSE) return(generator) } generator = get_generator() image = generator(tf$random$normal(c(1L,100L)))$numpy()[1,,,] image = scales::rescale(image, to = c(0, 255)) image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() get_discriminator = function(){ discriminator = keras_model_sequential() discriminator %&gt;% layer_conv_2d(filters = 64L, kernel_size = c(5L, 5L), strides = c(2L, 2L), padding = &quot;same&quot;, input_shape = c(80L, 80L, 3L)) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d(filters = 128L, kernel_size = c(5L, 5L), strides = c(2L, 2L), padding = &quot;same&quot;) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d(filters = 256L, kernel_size = c(3L, 3L), strides = c(2L, 2L), padding = &quot;same&quot;) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_flatten() %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) return(discriminator) } discriminator = get_discriminator() discriminator discriminator(generator(tf$random$normal(c(1L, 100L)))) ce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE,label_smoothing = 0.1) loss_discriminator = function(real, fake){ real_loss = ce(tf$ones_like(real), real) fake_loss = ce(tf$zeros_like(fake), fake) return(real_loss+fake_loss) } loss_generator = function(fake){ return(ce(tf$ones_like(fake), fake)) } gen_opt = tf$keras$optimizers$RMSprop(1e-4) disc_opt = tf$keras$optimizers$RMSprop(1e-4) batch_size = 32L get_batch = function(){ indices = sample.int(nrow(train_x), batch_size) return(tf$constant(train_x[indices,,,,drop=FALSE], &quot;float32&quot;)) } train_step = function(images){ noise = tf$random$normal(c(32L, 100L)) with(tf$GradientTape(persistent = TRUE) %as% tape,{ gen_images = generator(noise) real_output = discriminator(images) fake_output = discriminator(gen_images) gen_loss = loss_generator(fake_output) disc_loss = loss_discriminator(real_output, fake_output) }) gen_grads = tape$gradient(gen_loss, generator$weights) disc_grads = tape$gradient(disc_loss, discriminator$weights) rm(tape) gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights))) disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights))) return(c(gen_loss, disc_loss)) } train_step = tf$`function`(reticulate::py_func(train_step)) epochs = 10L steps = as.integer(nrow(train_x)/batch_size) counter = 1 gen_loss = NULL disc_loss = NULL for(i in 1:(epochs*steps)){ images = get_batch() losses = train_step(images) gen_loss[counter] = tf$reduce_sum(losses[[1]])$numpy() disc_loss[counter] = tf$reduce_sum(losses[[2]])$numpy() counter = counter+1 if(i %% 10*steps == 0) { noise = tf$random$normal(c(1L, 100L)) image = generator(noise)$numpy()[1,,,] image = scales::rescale(image, to = c(0, 255)) image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() } if(i %% steps == 0){ counter = 1 cat(&quot;Gen: &quot;, mean(gen_loss), &quot; Disc: &quot;, mean(disc_loss), &quot; \\n&quot;) } } results = vector(&quot;list&quot;, 100L) for(i in 1:100) { noise = tf$random$normal(c(1L, 100L)) image = generator(noise)$numpy()[1,,,] image = scales::rescale(image, to = c(0, 255)) image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() results[[i]] = image imager::save.image(imager::as.cimg(image),quality = 1.0,file = paste0(&quot;images/flower&quot;,i, &quot;.png&quot;)) imager::as.cimg(image) } saveRDS(abind::abind(results, along = 0L), file = &quot;images/result.RDS&quot;) 6.2 Autoencoder An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. ### Autoencoder - DNN MNIST Autoencoders consist of Encoder and a Decoder Networks. The encoder will compress the data into 2 dimensions and the decoder will reconstruct the original data: library(keras) library(tensorflow) rotate = function(x) t(apply(x, 2, rev)) imgPlot = function(img, title = &quot;&quot;){ col=grey.colors(255) image(rotate(img), col = col, xlab = &quot;&quot;, ylab = &quot;&quot;, axes=FALSE, main = paste0(&quot;Label: &quot;, as.character(title))) } data = tf$keras$datasets$mnist$load_data() train = data[[1]] train_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L)) test_x = array(test[[1]]/255, c(dim(test[[1]])[1], 784L)) ## Dense autoencoder ### Inputs will be compromized to two dimensions down_size_model = keras_model_sequential() down_size_model %&gt;% layer_dense(units = 100L, input_shape = c(784L),activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 2L, activation = &quot;linear&quot;) ### Reconstruction of the images up_size_model = keras_model_sequential() up_size_model %&gt;% layer_dense(units = 20L, input_shape = c(2L), activation = &quot;relu&quot;) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 784L, activation = &quot;sigmoid&quot;) ### Combine models into one autoencoder = keras_model(inputs = down_size_model$input, outputs = up_size_model(down_size_model$output)) autoencoder %&gt;% compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.01)) image = autoencoder(train_x[1,,drop = FALSE])$numpy() par(mfrow = c(1,2)) imgPlot(array(train_x[1,,drop = FALSE], c(28, 28))) imgPlot(array(image, c(28, 28))) autoencoder$fit(x = tf$constant(train_x), y = tf$constant(train_x), epochs = 10L, batch_size = 32L) ## &lt;tensorflow.python.keras.callbacks.History&gt; pred_dim = down_size_model(test_x) reconstr_pred = autoencoder(test_x) imgPlot(array(reconstr_pred[10,]$numpy(), dim = c(28L, 28L))) par(mfrow = c(1,1)) plot(pred_dim$numpy()[,1], pred_dim$numpy()[,2], col = test[[2]]+1L) 6.2.1 Autoencoder - MNIST CNN We can also use CNNs isntead of DNNs. There is also an inverse convolutional layer: data = tf$keras$datasets$mnist$load_data() train = data[[1]] train_x = array(train[[1]]/255, c(dim(train[[1]]), 1L)) down_size_model = keras_model_sequential() down_size_model %&gt;% layer_conv_2d(filters = 32L, activation = &quot;relu&quot;, kernel_size = c(2L,2L), input_shape = c(28L, 28L, 1L), strides = c(4L, 4L)) %&gt;% layer_conv_2d(filters = 16L, activation = &quot;relu&quot;, kernel_size = c(7L,7L), strides = c(1L, 1L)) %&gt;% layer_flatten() %&gt;% layer_dense(units = 2L, activation = &quot;linear&quot;) up_size_model = keras_model_sequential() up_size_model %&gt;% layer_dense(units = 8L, activation = &quot;relu&quot;, input_shape = c(2L)) %&gt;% layer_reshape(target_shape = c(1L, 1L, 8L)) %&gt;% layer_conv_2d_transpose(filters = 16L, kernel_size = c(7,7), activation = &quot;relu&quot;, strides = c(1L,1L)) %&gt;% layer_conv_2d_transpose(filters = 32L, activation = &quot;relu&quot;, kernel_size = c(2,2), strides = c(4L,4L)) %&gt;% layer_conv_2d(filters = 1, kernel_size = c(1L, 1L), strides = c(1L, 1L), activation = &quot;sigmoid&quot;) autoencoder = keras_model(inputs = down_size_model$input, outputs = up_size_model(down_size_model$output)) autoencoder %&gt;% compile(loss = loss_binary_crossentropy, optimizer = optimizer_rmsprop(0.001)) autoencoder$fit(x = tf$constant(train_x), y = tf$constant(train_x), epochs = 10L, batch_size = 64L) pred_dim = down_size_model(tf$constant(test_x, &quot;float32&quot;)) reconstr_pred = autoencoder(tf$constant(test_x, &quot;float32&quot;)) imgPlot(reconstr_pred[10,,,]$numpy()[,,1]) plot(pred_dim[,1]$numpy(), pred_dim[,2]$numpy(), col = test[[2]]+1L) ## Generate new images! new = matrix(c(10,10), 1, 2) imgPlot(array(up_size_model(new)$numpy(), c(28L, 28L))) 6.3 Varational Autoencoder The difference to normal autoencoder is that we here try to fit latent variables which encode the images: library(tensorflow_probability) data = tf$keras$datasets$mnist$load_data() train = data[[1]] train_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L)) tfp = reticulate::import(&quot;tensorflow_probability&quot;) prior = tfp$distributions$Independent(tfp$distributions$Normal(loc=tf$zeros(shape(10L,4L)), scale=1.0), reinterpreted_batch_ndims=1L) down_size_model = keras_model_sequential() down_size_model %&gt;% layer_dense(units = 100L, input_shape = c(784L),activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 4L) ### Reconstruction of the images up_size_model = keras_model_sequential() up_size_model %&gt;% layer_dense(units = 20L, input_shape = c(2L), activation = &quot;relu&quot;) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 784L, activation = &quot;sigmoid&quot;) ### Combine models into one batch_size = 32L epochs = 10L steps = as.integer(nrow(train_x)/32L * epochs) prior = tfp$distributions$MultivariateNormalDiag(loc = tf$zeros(shape(batch_size, 2L), &quot;float32&quot;), scale_diag = tf$ones(2L, &quot;float32&quot;)) optimizer = tf$keras$optimizers$RMSprop(0.0001) weights = c(down_size_model$weights, up_size_model$weights) get_batch = function(){ indices = sample.int(nrow(train_x), batch_size) return(train_x[indices,]) } for(i in 1:steps){ tmp_X = get_batch() with(tf$GradientTape() %as% tape, { encoded = down_size_model(tmp_X) dd = tfp$distributions$MultivariateNormalDiag(loc = encoded[,1:2], scale_diag = 1.0/(0.01+ tf$math$softplus(encoded[,3:4]))) samples = dd$sample() reconstructed = up_size_model(samples) KL_loss = dd$kl_divergence(prior) # constrain loss = tf$reduce_mean(tf$negative(tfp$distributions$Binomial(1L, logits = reconstructed)$log_prob(tmp_X)))+tf$reduce_mean(KL_loss) }) gradients = tape$gradient(loss, weights) optimizer$apply_gradients(purrr::transpose(list(gradients, weights))) if(i %% as.integer(nrow(train_x)/10L) == 0) cat(&quot;Loss: &quot;, loss$numpy(), &quot;\\n&quot;) } "],["datasets.html", "7 Datasets 7.1 Titanic 7.2 Plant-pollinator database 7.3 Wine 7.4 Nasa 7.5 Flower", " 7 Datasets You can download the datasets we use in the course (ignore browser warnings) 7.1 Titanic The dataset is a collection of titanic passengers with information about their age, class, sex, and their survival status. The competition here is simple: train a ML model and predict the survival probability. The titanic dataset is very well explored and serves as a stepping stone in many ML careers. For inspiration and data exploration notebooks, check out this kaggle competition: Response variable: ‘survived’ A minimal working example: Load dataset load(&quot;datasets.RData&quot;) summary(titanic) ## pclass survived name sex age sibsp parch ticket ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 Min. :0.000 CA. 2343: 11 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 1601 : 8 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 Median :0.000 CA 2144 : 8 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 Mean :0.385 3101295 : 7 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 347077 : 7 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 Max. :9.000 347082 : 7 ## NA&#39;s :655 NA&#39;s :263 (Other) :1261 ## fare cabin embarked boat body home.dest ## Min. : 0.000 :1014 : 2 :823 Min. : 1.0 :564 ## 1st Qu.: 7.896 C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 New York, NY : 64 ## Median : 14.454 B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 London : 14 ## Mean : 33.295 G6 : 5 S:914 15 : 37 Mean :160.8 Montreal, PQ : 10 ## 3rd Qu.: 31.275 B96 B98 : 4 14 : 33 3rd Qu.:256.0 Cornwall / Akron, OH: 9 ## Max. :512.329 C22 C26 : 4 4 : 31 Max. :328.0 Paris, France : 9 ## NA&#39;s :1 (Other) : 271 (Other):308 NA&#39;s :1188 (Other) :639 Impute missing values (not our response variable!) library(missRanger) library(dplyr) titanic_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest) titanic_imputed = missRanger::missRanger(data = titanic_imputed %&gt;% select(-survived)) ## ## Missing value imputation by random forests ## ## Variables to impute: age, fare, body ## Variables used to impute: pclass, sex, age, sibsp, parch, fare, embarked, body ## iter 1: ... ## iter 2: ... ## iter 3: ... titanic_imputed$survived = titanic$survived Split into training and testing train = titanic_imputed[!is.na(titanic$survived), ] test = titanic_imputed[is.na(titanic$survived), ] Train model model = glm(survived~., data=train, family = binomial()) Predictions preds = predict(model, data = test, type = &quot;response&quot;) head(preds) ## 561 321 1177 1098 1252 1170 ## 0.79272156 0.31122837 0.01379281 0.12119304 0.13886752 0.11602403 Create submission csv write.csv(data.frame(y=preds), file = &quot;glm.csv&quot;) And submit the csv on 7.2 Plant-pollinator database The plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of ML algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look . Response variable: ‘interaction’ A minimal working example: Load dataset load(&quot;datasets.RData&quot;) summary(plant_poll) ## X Y type season diameter corolla ## Vaccinium_corymbosum: 256 Andrena_wilkella : 80 Length:20480 Length:20480 Min. : 2.00 Length:20480 ## Brassica_napus : 256 Andrena_barbilabris: 80 Class :character Class :character 1st Qu.: 5.00 Class :character ## Carum_carvi : 256 Andrena_cineraria : 80 Mode :character Mode :character Median : 19.00 Mode :character ## Coriandrum_sativum : 256 Andrena_flavipes : 80 Mean : 27.03 ## Daucus_carota : 256 Andrena_gravida : 80 3rd Qu.: 25.00 ## Malus_domestica : 256 Andrena_haemorrhoa : 80 Max. :150.00 ## (Other) :18944 (Other) :20000 NA&#39;s :9472 ## colour nectar b.system s.pollination inflorescence composite guild ## Length:20480 Length:20480 Length:20480 Length:20480 Length:20480 Length:20480 Length:20480 ## Class :character Class :character Class :character Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## tongue body sociality feeding interaction ## Min. : 2.000 Min. : 2.00 Length:20480 Length:20480 0 :14095 ## 1st Qu.: 4.800 1st Qu.: 8.00 Class :character Class :character 1 : 595 ## Median : 6.600 Median :10.50 Mode :character Mode :character NA&#39;s: 5790 ## Mean : 8.104 Mean :10.66 ## 3rd Qu.:10.500 3rd Qu.:13.00 ## Max. :26.400 Max. :25.00 ## NA&#39;s :17040 NA&#39;s :6160 Impute missing values (not our response variable!) We will select only a few predictors here (you can work with all predictors ofc). library(missRanger) library(dplyr) plant_poll_imputed = plant_poll %&gt;% select(diameter, corolla, tongue, body, interaction) plant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %&gt;% select(-interaction)) ## ## Missing value imputation by random forests ## ## Variables to impute: diameter, corolla, tongue, body ## Variables used to impute: diameter, corolla, tongue, body ## iter 1: .... ## iter 2: .... ## iter 3: .... plant_poll_imputed$interaction = plant_poll$interaction Split into training and testing train = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ] test = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ] Train model model = glm(interaction~., data=train, family = binomial()) Predictions preds = predict(model, data = test, type = &quot;response&quot;) head(preds) ## 3871 3872 3873 3874 3875 3876 ## 0.03882583 0.03757548 0.03772374 0.04455096 0.04511771 0.03965961 Create submission csv write.csv(data.frame(y=preds), file = &quot;glm.csv&quot;) 7.3 Wine The dataset is a collection of wines of different quality. The aim is to predict the quality of the wine based on physochemical predictors. For inspiration and data exploration notebooks, check out this kaggle competition: For instance, check out this very nice which removes a few problems from the data. Response variable: ‘quality’ We could use theoretically a regression model for this task but we will stick with a classification model A minimal working example: Load dataset load(&quot;datasets.RData&quot;) summary(wine) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide ## Min. : 4.600 Min. :0.1200 Min. :0.0000 Min. : 0.900 Min. :0.01200 Min. : 1.00 Min. : 6.00 ## 1st Qu.: 7.100 1st Qu.:0.3900 1st Qu.:0.0900 1st Qu.: 1.900 1st Qu.:0.07000 1st Qu.: 7.00 1st Qu.: 22.00 ## Median : 7.900 Median :0.5200 Median :0.2600 Median : 2.200 Median :0.07900 Median :14.00 Median : 38.00 ## Mean : 8.335 Mean :0.5284 Mean :0.2705 Mean : 2.533 Mean :0.08747 Mean :15.83 Mean : 46.23 ## 3rd Qu.: 9.300 3rd Qu.:0.6400 3rd Qu.:0.4200 3rd Qu.: 2.600 3rd Qu.:0.09000 3rd Qu.:21.00 3rd Qu.: 62.00 ## Max. :15.900 Max. :1.5800 Max. :1.0000 Max. :15.500 Max. :0.61100 Max. :72.00 Max. :289.00 ## NA&#39;s :70 NA&#39;s :48 NA&#39;s :41 NA&#39;s :60 NA&#39;s :37 NA&#39;s :78 NA&#39;s :78 ## density pH sulphates alcohol quality ## Min. :0.9901 Min. :2.740 Min. :0.3300 Min. : 8.40 Min. :3.000 ## 1st Qu.:0.9956 1st Qu.:3.210 1st Qu.:0.5500 1st Qu.: 9.50 1st Qu.:5.000 ## Median :0.9968 Median :3.310 Median :0.6200 Median :10.20 Median :6.000 ## Mean :0.9968 Mean :3.311 Mean :0.6572 Mean :10.42 Mean :5.596 ## 3rd Qu.:0.9979 3rd Qu.:3.400 3rd Qu.:0.7300 3rd Qu.:11.10 3rd Qu.:6.000 ## Max. :1.0037 Max. :4.010 Max. :2.0000 Max. :14.90 Max. :8.000 ## NA&#39;s :78 NA&#39;s :25 NA&#39;s :51 NA&#39;s :905 Impute missing values (not our response variable!) library(missRanger) library(dplyr) #wine_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest) wine_imputed = missRanger::missRanger(data = wine %&gt;% select(-quality)) ## ## Missing value imputation by random forests ## ## Variables to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates ## Variables used to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol ## iter 1: .......... ## iter 2: .......... ## iter 3: .......... wine_imputed$quality = wine$quality Split into training and testing train = wine_imputed[!is.na(wine$quality), ] test = wine_imputed[is.na(wine$quality), ] Train model library(ranger) rf = ranger(quality~., data = train, classification = TRUE) Predictions preds = predict(rf, data = test)$predictions head(preds) ## [1] 6 5 5 7 6 6 Create submission csv write.csv(data.frame(y=preds), file = &quot;rf.csv&quot;) 7.4 Nasa A collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. For inspiration and data exploration notebooks, check out the kaggle competition: Response variable: ‘Hazardous’ 1. Load dataset load(&quot;datasets.RData&quot;) summary(nasa) ## Neo.Reference.ID Name Absolute.Magnitude Est.Dia.in.KM.min. Est.Dia.in.KM.max. Est.Dia.in.M.min. Est.Dia.in.M.max. ## Min. :2000433 Min. :2000433 Min. :11.16 Min. : 0.00101 Min. : 0.00226 Min. : 1.011 Min. : 2.26 ## 1st Qu.:3102682 1st Qu.:3102683 1st Qu.:20.10 1st Qu.: 0.03346 1st Qu.: 0.07482 1st Qu.: 33.462 1st Qu.: 74.82 ## Median :3514800 Median :3514800 Median :21.90 Median : 0.11080 Median : 0.24777 Median : 110.804 Median : 247.77 ## Mean :3272675 Mean :3273113 Mean :22.27 Mean : 0.20523 Mean : 0.45754 Mean : 204.649 Mean : 458.45 ## 3rd Qu.:3690987 3rd Qu.:3690385 3rd Qu.:24.50 3rd Qu.: 0.25384 3rd Qu.: 0.56760 3rd Qu.: 253.837 3rd Qu.: 567.60 ## Max. :3781897 Max. :3781897 Max. :32.10 Max. :15.57955 Max. :34.83694 Max. :15579.552 Max. :34836.94 ## NA&#39;s :53 NA&#39;s :57 NA&#39;s :36 NA&#39;s :60 NA&#39;s :23 NA&#39;s :29 NA&#39;s :46 ## Est.Dia.in.Miles.min. Est.Dia.in.Miles.max. Est.Dia.in.Feet.min. Est.Dia.in.Feet.max. Close.Approach.Date Epoch.Date.Close.Approach ## Min. :0.00063 Min. : 0.00140 Min. : 3.32 Min. : 7.41 2016-07-22: 18 Min. :7.889e+11 ## 1st Qu.:0.02079 1st Qu.: 0.04649 1st Qu.: 109.78 1st Qu.: 245.49 2015-01-15: 17 1st Qu.:1.016e+12 ## Median :0.06885 Median : 0.15395 Median : 363.53 Median : 812.88 2015-02-15: 16 Median :1.203e+12 ## Mean :0.12734 Mean : 0.28486 Mean : 670.44 Mean : 1500.77 2007-11-08: 15 Mean :1.180e+12 ## 3rd Qu.:0.15773 3rd Qu.: 0.35269 3rd Qu.: 832.80 3rd Qu.: 1862.19 2012-01-15: 15 3rd Qu.:1.356e+12 ## Max. :9.68068 Max. :21.64666 Max. :51114.02 Max. :114294.42 (Other) :4577 Max. :1.473e+12 ## NA&#39;s :42 NA&#39;s :50 NA&#39;s :21 NA&#39;s :46 NA&#39;s : 29 NA&#39;s :43 ## Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr Miles.per.hour Miss.Dist..Astronomical. Miss.Dist..lunar. ## Min. : 0.3355 Min. : 1208 Min. : 750.5 Min. :0.00018 Min. : 0.06919 ## 1st Qu.: 8.4497 1st Qu.: 30399 1st Qu.:18846.7 1st Qu.:0.13341 1st Qu.: 51.89874 ## Median :12.9370 Median : 46532 Median :28893.7 Median :0.26497 Median :103.19415 ## Mean :13.9848 Mean : 50298 Mean :31228.0 Mean :0.25690 Mean : 99.91366 ## 3rd Qu.:18.0774 3rd Qu.: 65068 3rd Qu.:40436.9 3rd Qu.:0.38506 3rd Qu.:149.59244 ## Max. :44.6337 Max. :160681 Max. :99841.2 Max. :0.49988 Max. :194.45491 ## NA&#39;s :27 NA&#39;s :28 NA&#39;s :38 NA&#39;s :60 NA&#39;s :30 ## Miss.Dist..kilometers. Miss.Dist..miles. Orbiting.Body Orbit.ID Orbit.Determination.Date Orbit.Uncertainity ## Min. : 26610 Min. : 16535 Earth:4665 Min. : 1.00 2017-06-21 06:17:20: 9 Min. :0.000 ## 1st Qu.:19964907 1st Qu.:12454813 NA&#39;s : 22 1st Qu.: 9.00 2017-04-06 08:57:13: 8 1st Qu.:0.000 ## Median :39685408 Median :24662435 Median : 16.00 2017-04-06 09:24:24: 8 Median :3.000 ## Mean :38436154 Mean :23885560 Mean : 28.34 2017-04-06 08:24:13: 7 Mean :3.521 ## 3rd Qu.:57540318 3rd Qu.:35714721 3rd Qu.: 31.00 2017-04-06 08:26:19: 7 3rd Qu.:6.000 ## Max. :74781600 Max. :46467132 Max. :611.00 (Other) :4622 Max. :9.000 ## NA&#39;s :56 NA&#39;s :27 NA&#39;s :33 NA&#39;s : 26 NA&#39;s :49 ## Minimum.Orbit.Intersection Jupiter.Tisserand.Invariant Epoch.Osculation Eccentricity Semi.Major.Axis Inclination ## Min. :0.00000 Min. :2.196 Min. :2450164 Min. :0.00752 Min. :0.6159 Min. : 0.01451 ## 1st Qu.:0.01435 1st Qu.:4.047 1st Qu.:2458000 1st Qu.:0.24086 1st Qu.:1.0012 1st Qu.: 4.93290 ## Median :0.04653 Median :5.071 Median :2458000 Median :0.37251 Median :1.2422 Median :10.27694 ## Mean :0.08191 Mean :5.056 Mean :2457723 Mean :0.38267 Mean :1.4009 Mean :13.36159 ## 3rd Qu.:0.12150 3rd Qu.:6.017 3rd Qu.:2458000 3rd Qu.:0.51256 3rd Qu.:1.6782 3rd Qu.:19.47848 ## Max. :0.47789 Max. :9.025 Max. :2458020 Max. :0.96026 Max. :5.0720 Max. :75.40667 ## NA&#39;s :137 NA&#39;s :56 NA&#39;s :60 NA&#39;s :39 NA&#39;s :53 NA&#39;s :42 ## Asc.Node.Longitude Orbital.Period Perihelion.Distance Perihelion.Arg Aphelion.Dist Perihelion.Time Mean.Anomaly ## Min. : 0.0019 Min. : 176.6 Min. :0.08074 Min. : 0.0069 Min. :0.8038 Min. :2450100 Min. : 0.0032 ## 1st Qu.: 83.1849 1st Qu.: 365.9 1st Qu.:0.63038 1st Qu.: 95.6430 1st Qu.:1.2661 1st Qu.:2457815 1st Qu.: 87.0069 ## Median :172.6347 Median : 504.9 Median :0.83288 Median :189.7729 Median :1.6182 Median :2457972 Median :186.0219 ## Mean :172.1717 Mean : 635.5 Mean :0.81316 Mean :184.0185 Mean :1.9864 Mean :2457726 Mean :181.2882 ## 3rd Qu.:254.8804 3rd Qu.: 793.1 3rd Qu.:0.99718 3rd Qu.:271.9535 3rd Qu.:2.4497 3rd Qu.:2458108 3rd Qu.:276.6418 ## Max. :359.9059 Max. :4172.2 Max. :1.29983 Max. :359.9931 Max. :8.9839 Max. :2458839 Max. :359.9180 ## NA&#39;s :60 NA&#39;s :46 NA&#39;s :22 NA&#39;s :48 NA&#39;s :38 NA&#39;s :59 NA&#39;s :40 ## Mean.Motion Equinox Hazardous ## Min. :0.08628 J2000:4663 Min. :0.000 ## 1st Qu.:0.45147 NA&#39;s : 24 1st Qu.:0.000 ## Median :0.71137 Median :0.000 ## Mean :0.73732 Mean :0.176 ## 3rd Qu.:0.98379 3rd Qu.:0.000 ## Max. :2.03900 Max. :1.000 ## NA&#39;s :48 NA&#39;s :4187 Impute missing values (not our response variable!) library(missRanger) library(dplyr) #wine_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest) nasa_imputed = missRanger::missRanger(data = nasa %&gt;% select(-Hazardous), maxiter = 1, num.trees=5L) ## ## Missing value imputation by random forests ## ## Variables to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox ## Variables used to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox ## iter 1: ..... ## Warning: Dropped unused factor level(s) in dependent variable: 2017-04-06 08:35:59, 2017-04-06 09:06:29, 2017-04-06 09:10:05. ## .................................. nasa_imputed$Hazardous = nasa$Hazardous Split into training and testing train = nasa_imputed[!is.na(nasa$Hazardous), ] test = nasa_imputed[is.na(nasa$Hazardous), ] Train model library(ranger) rf = ranger(Hazardous~., data = train, classification = TRUE, probability = TRUE) Predictions preds = predict(rf, data = test)$predictions[,2] head(preds) ## [1] 0.7227714 0.7291341 0.0017500 0.8403690 0.1601960 0.1616310 Create submission csv write.csv(data.frame(y=preds), file = &quot;rf.csv&quot;) 7.5 Flower A collection of over 4000 flower images of 5 plant species. The dataset is from but we downsampled the images from \\(320*240\\) to \\(80*80\\) pixels. You can download the dataset . Notes: - check out CNN notebooks on kaggle (they are often written in python but you can still copy the CNN architectures), e.g.  - Last year’s winner have used a transfer learning approach (they achieved around 70% accuracy), check out this , see also the section about transfer learning 4.2.2 Response variable: Plant species Load dataset The dataset requires pre-processing (we have to concatenate the train and test images): library(keras) library(stringr) data_files = list.files(&quot;flower/&quot;, full.names = TRUE) train = data_files[str_detect(data_files, &quot;train&quot;)] test = readRDS(file = &quot;flower/test.RDS&quot;) train = lapply(train, readRDS) train_classes = lapply(train, function(d) dim(d)[1]) train = abind::abind(train, along = 1L) labels_train = rep(0:4, unlist(train_classes)) Let’s visualize a flower: train[100,,,] %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() Build &amp; train model: model = keras_model_sequential() model %&gt;% layer_conv_2d(filters = 4L, kernel_size = 2L, input_shape = c(80L, 80L, 3L)) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) model %&gt;% compile(optimizer = optimizer_adamax(0.01), loss = loss_categorical_crossentropy) model %&gt;% fit(epochs = 2L, x = train/255, y = keras::k_one_hot(labels_train, 5L)) Predictions preds = model %&gt;% predict(test/255) preds = apply(preds, 1, which.max)-1 head(preds) ## [1] 1 2 4 4 4 3 Create submission csv write.csv(data.frame(y=preds), file = &quot;cnn.csv&quot;) "]]
