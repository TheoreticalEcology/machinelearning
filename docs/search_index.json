[["index.html", "Machine Learning and AI in TensorFlow and R 1 Prerequisites 1.1 R System 1.2 TensorFlow and Keras 1.3 Torch for R 1.4 EcoData 1.5 Further Used Libraries 1.6 Linux/UNIX systems have to fulfill some durther dependencies", " Machine Learning and AI in TensorFlow and R Maximilian Pichler and Florian Hartig 2021-11-19 1 Prerequisites 1.1 R System Make sure you have a recent version of R (&gt;=3.6, ideally &gt;=4.0) and RStudio on your computers. 1.2 TensorFlow and Keras If you want to run the code on your own computers, you also will need to install TensorFlow / Keras for R. For this, the following should work for most people. Run in R: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;rstudio/tensorflow&quot;) library(tensorflow) install_tensorflow() install.packages(&quot;keras&quot;, dependencies = T) keras::install_keras() This should work on most computers, in particular if all software is recent. Sometimes, however, things don’t work well, especially the python distribution often makes problems. If the installation does not work for you, we can look at it together. Also, we will provide some virtual machines in case your computers / laptops are too old or you don’t manage to install TensorFlow. Warning: You need at least TensorFlow version 2.6, otherwise, the argument “learning_rate” must be “lr!” 1.3 Torch for R We may also use Torch for R. This is an R frontend for the popular PyTorch framework. To install Torch, type in R: install.packages(&quot;torch&quot;) library(torch) 1.4 EcoData We may sometimes use data sets from the EcoData package. To install the package, run: devtools::install_github(repo = &quot;florianhartig/EcoData&quot;, subdir = &quot;EcoData&quot;, dependencies = TRUE, build_vignettes = TRUE) 1.5 Further Used Libraries We will make huge use of different libraries. So take a coffee or two (that will take a while…) and install the following libraries. Please do this in the given order unless you know what you’re doing, because there are some dependencies between the packages. install.packages(&quot;abind&quot;) install.packages(&quot;animation&quot;) install.packages(&quot;ape&quot;) install.packages(&quot;BiocManager&quot;) BiocManager::install(c(&quot;Rgraphviz&quot;, &quot;graph&quot;, &quot;RBGL&quot;)) install.packages(&quot;coro&quot;) install.packages(&quot;dbscan&quot;) install.packages(&quot;dendextend&quot;) install.packages(&quot;devtools&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;e1071&quot;) install.packages(&quot;factoextra&quot;) install.packages(&quot;fields&quot;) install.packages(&quot;forcats&quot;) install.packages(&quot;glmnet&quot;) install.packages(&quot;gym&quot;) install.packages(&quot;kknn&quot;) install.packages(&quot;knitr&quot;) install.packages(&quot;iml&quot;) install.packages(&quot;lavaan&quot;) install.packages(&quot;lmtest&quot;) install.packages(&quot;magick&quot;) install.packages(&quot;mclust&quot;) install.packages(&quot;Metrics&quot;) install.packages(&quot;microbenchmark&quot;) install.packages(&quot;missRanger&quot;) install.packages(&quot;mlbench&quot;) install.packages(&quot;mlr3&quot;) install.packages(&quot;mlr3learners&quot;) install.packages(&quot;mlr3measures&quot;) install.packages(&quot;mlr3pipelines&quot;) install.packages(&quot;mlr3tuning&quot;) install.packages(&quot;paradox&quot;) install.packages(&quot;partykit&quot;) install.packages(&quot;pcalg&quot;) install.packages(&quot;piecewiseSEM&quot;) install.packages(&quot;purrr&quot;) install.packages(&quot;randomForest&quot;) install.packages(&quot;ranger&quot;) install.packages(&quot;reticulate&quot;) install.packages(&quot;rpart&quot;) install.packages(&quot;rpart.plot&quot;) install.packages(&quot;scales&quot;) install.packages(&quot;semPlot&quot;) install.packages(&quot;stringr&quot;) install.packages(&quot;tfprobability&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;torchvision&quot;) install.packages(&quot;xgboost&quot;) devtools::install_github(&quot;andrie/deepviz&quot;, dependencies = TRUE, upgrade = &quot;always&quot;) devtools::install_github(repo = &quot;florianhartig/EcoData&quot;, subdir = &quot;EcoData&quot;, dependencies = TRUE, build_vignettes = FALSE) devtools::install_github(&#39;skinner927/reprtree&#39;) devtools::install_version(&quot;lavaanPlot&quot;, version = &quot;0.6.0&quot;) reticulate::conda_install(&quot;r-reticulate&quot;, packages = &quot;scipy&quot;, pip = TRUE) torch::install_torch() 1.6 Linux/UNIX systems have to fulfill some durther dependencies Debian based systems For Debian based systems, we need: build-essential gfortran libmagick++-dev r-base-dev If you are new to installing packages on Debian / Ubuntu, etc., type the following: sudo apt update &amp;&amp; sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev Authors: Maximilian Pichler: @_Max_Pichler Florian Hartig: @florianhartig Contributors: Johannes Oberpriller, Matthias Meier This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License "],["reminder.html", "2 Reminders About Basic Operations in R 2.1 Your R System 2.2 Data types in R 2.3 Data selection, Slicing and Subsetting 2.4 Applying Functions and Aggregates Across a Data set 2.5 Plotting 2.6 Additional Resources", " 2 Reminders About Basic Operations in R 2.1 Your R System In this course, we work with the combination of R + RStudio. R is the calculation engine that performs the computations. RStudio is the editor that helps you sending inputs to R and collect outputs. Make sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is an introductory video. 2.2 Data types in R The following subchapters introduce / remind you of R data types. 2.2.1 Test Your Knowledge Discuss with your partner(s) - what is the meaning / structure / properties of the following data types in R: Atomic types (which atomic types exist) list vector data.frame matrix array 2.2.2 Iris Data What is the data type of the iris data set? Explore using the following commands: iris class(iris) dim(iris) str(iris) 2.2.3 Dynamic typing R is a dynamically typed language, which means that the type of variables is determined automatically depending on what values you supply. Try this: x = 1 class(x) x = &quot;dog&quot; class(x) This also works if a data set already exists, i.e. if you assign a different value, the type will automatically be changed. Look at what happens if we assign a character value to a previously numeric column in a data.frame: iris$Sepal.Length[2] = &quot;dog&quot; str(iris) Note that all numeric values are changed to characters as well. You can try to force back the values to numeric by: iris$Sepal.Length = as.numeric(iris$Sepal.Length) Have a look at what this does to the values in iris$Sepal.Length. Note: The actions above operate on a local copy of the iris data set. You don’t overwrite the base data and can use it again in a new R session or reset it with “data(iris).” 2.3 Data selection, Slicing and Subsetting In this chapter, we will discuss data selection, slicing and subsetting. 2.3.1 Subsetting and Slicing for Single Data Types We often want to select only a subset of our data. You can generally subset from data structures using indices and TRUE/FALSE (or T/F). Here for a vector: vector[1] # First element. vector[1:3] # Elements 1,2,3. vector[c(1,5,6)] # Elements 1,5,6. vector[c(T,T,F,F,T)] # Elements 1,2,5. If you use TRUE/FALSE, you must specify a truth value for every (!) position. vector = c(1,2,3,4,5) vector[c(T,F)] # Does NOT work! For a list, it’s basically the same, except the following points: Elements in lists usually have a name, so you can also access those via “list$name.” Lists accessed with [] return a list. If you want to select a single element, you have to access it via [[]], as in “list[[2]].” For data.frames and other objects with im &gt; 2, the same is true, except that you have several indices. matrix[1,2] # Element in first row, second column. matrix[1:2,] # First two rows, all columns. matrix[,c(T,F,T)] # All rows, 1st and 3rd column. The syntax “matrix[1,]” is also called slicing, for obvious reasons. Data.frames are the same as matrices, except that, like with lists of vectors, you can also access columns via names as in “data.frame$column.” 2.3.2 Logic and Slicing Slicing is very powerful if you combine it with logical operators, such as “&amp;” (logical and), “|” (logical or), “==” (equal), “!=” (not equal), “&lt;=,” “&gt;,” etc. Here are a few examples: iris[iris$Species == &quot;virginica&quot;, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 107 4.9 2.5 4.5 1.7 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 113 6.8 3.0 5.5 2.1 virginica ## 114 5.7 2.5 5.0 2.0 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 116 6.4 3.2 5.3 2.3 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 120 6.0 2.2 5.0 1.5 virginica ## 121 6.9 3.2 5.7 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 125 6.7 3.3 5.7 2.1 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 127 6.2 2.8 4.8 1.8 virginica ## 128 6.1 3.0 4.9 1.8 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 136 7.7 3.0 6.1 2.3 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 140 6.9 3.1 5.4 2.1 virginica ## 141 6.7 3.1 5.6 2.4 virginica ## 142 6.9 3.1 5.1 2.3 virginica ## 143 5.8 2.7 5.1 1.9 virginica ## 144 6.8 3.2 5.9 2.3 virginica ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica Note that this is identical to the following: subset(iris, Species == &quot;virginica&quot;) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 101 6.3 3.3 6.0 2.5 virginica ## 102 5.8 2.7 5.1 1.9 virginica ## 103 7.1 3.0 5.9 2.1 virginica ## 104 6.3 2.9 5.6 1.8 virginica ## 105 6.5 3.0 5.8 2.2 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 107 4.9 2.5 4.5 1.7 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 111 6.5 3.2 5.1 2.0 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 113 6.8 3.0 5.5 2.1 virginica ## 114 5.7 2.5 5.0 2.0 virginica ## 115 5.8 2.8 5.1 2.4 virginica ## 116 6.4 3.2 5.3 2.3 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 120 6.0 2.2 5.0 1.5 virginica ## 121 6.9 3.2 5.7 2.3 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 124 6.3 2.7 4.9 1.8 virginica ## 125 6.7 3.3 5.7 2.1 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 127 6.2 2.8 4.8 1.8 virginica ## 128 6.1 3.0 4.9 1.8 virginica ## 129 6.4 2.8 5.6 2.1 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 136 7.7 3.0 6.1 2.3 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 139 6.0 3.0 4.8 1.8 virginica ## 140 6.9 3.1 5.4 2.1 virginica ## 141 6.7 3.1 5.6 2.4 virginica ## 142 6.9 3.1 5.1 2.3 virginica ## 143 5.8 2.7 5.1 1.9 virginica ## 144 6.8 3.2 5.9 2.3 virginica ## 145 6.7 3.3 5.7 2.5 virginica ## 146 6.7 3.0 5.2 2.3 virginica ## 147 6.3 2.5 5.0 1.9 virginica ## 148 6.5 3.0 5.2 2.0 virginica ## 149 6.2 3.4 5.4 2.3 virginica ## 150 5.9 3.0 5.1 1.8 virginica You can also combine several logical commands: iris[iris$Species == &quot;virginica&quot; &amp; iris$Sepal.Length &gt; 7, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 103 7.1 3.0 5.9 2.1 virginica ## 106 7.6 3.0 6.6 2.1 virginica ## 108 7.3 2.9 6.3 1.8 virginica ## 110 7.2 3.6 6.1 2.5 virginica ## 118 7.7 3.8 6.7 2.2 virginica ## 119 7.7 2.6 6.9 2.3 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 126 7.2 3.2 6.0 1.8 virginica ## 130 7.2 3.0 5.8 1.6 virginica ## 131 7.4 2.8 6.1 1.9 virginica ## 132 7.9 3.8 6.4 2.0 virginica ## 136 7.7 3.0 6.1 2.3 virginica Note that this works element-wise! 2.4 Applying Functions and Aggregates Across a Data set In this chapter, we discuss the most basic functions in R for calculating means, averages or apply other functions across a data set. 2.4.1 Functions Maybe this is a good time to remind you about functions. The two basic options we use in R are: Variables / data structures. Functions. We have already used variables / data structures. Variables have a name and if you type this name in R, you get the values that are inside the respective data structure. Functions are algorithms that are called like: function(variable) For example, you can do: summary(iris) If you want to know what the summary function does, type “?summary,” or put your mouse on the function and press “F1.” To be able to work properly with data, you have to know how to define your own functions. This works like the following: squareValue = function(x){ temp = x * x return(temp) } Tasks Try what happens if you type in &quot;squareValue(2)&quot;. Write a function for multiplying 2 values. Hint - this should start with &quot;function(x1, x2)&quot;. Change the first line of the &quot;squareValue&quot; function to &quot;function(x = 3)&quot; and try out the following commands: &quot;squareValue(2)&quot;, &quot;squareValue()&quot; - What is the sense of this syntax? Solution 1 multiply = function(x1, x2){ return(x1 * x2) } 2 squareValue(2) ## [1] 4 3 squareValue = function(x = 3){ temp = x * x return(temp) } squareValue(2) ## [1] 4 squareValue() ## [1] 9 The given value (3 in the example above) is the default value. This value is used automatically, if no value is supplied for the respective variable. Default values can be specified for all variables, but you should put them to the end of the function definition. Hint: in R, it is always useful to name the parameters when using functions. Look at the following example: testFunction = function(a = 1, b, c = 3){ return(a * b + c) } testFunction() ## Error in testFunction(): argument &quot;b&quot; is missing, with no default testFunction(10) ## Error in testFunction(10): argument &quot;b&quot; is missing, with no default testFunction(10, 20) ## [1] 203 testFunction(10, 20, 30) ## [1] 230 testFunction(b = 10, c = 20, a = 30) ## [1] 320 2.4.2 The apply() Function Now that we know functions, we can introduce functions that use functions. One of the most important is the apply function. The apply function applies a function of a data structure, typically a matrix or data.frame. Try the following: apply(iris[,1:4], 2, mean) Tasks Check the help of apply to understand what this does. Why is the first result of &quot;apply(iris[,1:4], 2, mean)&quot; NA? Check the help of mean to understand this. Try &quot;apply(iris[,1:4], 1, mean)&quot;. Think about what changed here. What would happen if you use &quot;iris&quot; instead of &quot;iris[,1:4]&quot;?. Solution 1 ?apply 2 Remember, what we have done above (if you run this part separately, execute the following lines again): iris$Sepal.Length[2] = &quot;Hund&quot; iris$Sepal.Length = as.numeric(iris$Sepal.Length) ## Warning: NAs introduced by coercion apply(iris[,1:4], 2, mean) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## NA 3.057333 3.758000 1.199333 Taking the mean of a character sequence is not possible, so the result is NA (Not Available, missing value(s)). But you can skip missing values with the option “na.rm = TRUE” of the “mean” function. To use it with the “apply” function, pass the argument(s) after. apply(iris[,1:4], 2, mean, na.rm = T) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.849664 3.057333 3.758000 1.199333 3 apply(iris[,1:4], 1, mean) ## [1] 2.550 NA 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 ## [23] 2.350 2.650 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 ## [45] 2.800 2.375 2.675 2.350 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300 2.875 3.650 3.300 3.775 3.350 3.900 ## [67] 3.650 3.400 3.600 3.275 3.925 3.550 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850 3.600 3.875 4.000 3.575 ## [89] 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575 4.200 4.850 ## [111] 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025 ## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550 4.550 4.300 3.925 4.175 4.325 3.950 Arrays (and thus matrices, data.frame(s), etc.) have several dimensions. For a simple \\(2D\\) array (or matrix), the first dimension is the rows and the second dimension is the columns. The second parameter of the “apply” function specifies the dimension of which the mean should be computed. If you use \\(1\\), you demand the row means (150), if you use \\(2\\), you request the column means (5, resp. 4). 4 apply(iris, 2, mean) ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA ## Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## NA NA NA NA NA The 5th column is “Species.” These values are not numeric. So the whole data.frame is taken as a data.frame full of characters. apply(iris[,1:4], 2, str) ## num [1:150] 5.1 NA 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## NULL apply(iris, 2, str) ## chr [1:150] &quot;5.1&quot; NA &quot;4.7&quot; &quot;4.6&quot; &quot;5.0&quot; &quot;5.4&quot; &quot;4.6&quot; &quot;5.0&quot; &quot;4.4&quot; &quot;4.9&quot; &quot;5.4&quot; &quot;4.8&quot; &quot;4.8&quot; &quot;4.3&quot; &quot;5.8&quot; &quot;5.7&quot; &quot;5.4&quot; &quot;5.1&quot; &quot;5.7&quot; &quot;5.1&quot; &quot;5.4&quot; ... ## chr [1:150] &quot;3.5&quot; &quot;3.0&quot; &quot;3.2&quot; &quot;3.1&quot; &quot;3.6&quot; &quot;3.9&quot; &quot;3.4&quot; &quot;3.4&quot; &quot;2.9&quot; &quot;3.1&quot; &quot;3.7&quot; &quot;3.4&quot; &quot;3.0&quot; &quot;3.0&quot; &quot;4.0&quot; &quot;4.4&quot; &quot;3.9&quot; &quot;3.5&quot; &quot;3.8&quot; &quot;3.8&quot; ... ## chr [1:150] &quot;1.4&quot; &quot;1.4&quot; &quot;1.3&quot; &quot;1.5&quot; &quot;1.4&quot; &quot;1.7&quot; &quot;1.4&quot; &quot;1.5&quot; &quot;1.4&quot; &quot;1.5&quot; &quot;1.5&quot; &quot;1.6&quot; &quot;1.4&quot; &quot;1.1&quot; &quot;1.2&quot; &quot;1.5&quot; &quot;1.3&quot; &quot;1.4&quot; &quot;1.7&quot; &quot;1.5&quot; ... ## chr [1:150] &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.4&quot; &quot;0.3&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.1&quot; &quot;0.2&quot; &quot;0.2&quot; &quot;0.1&quot; &quot;0.1&quot; &quot;0.2&quot; &quot;0.4&quot; &quot;0.4&quot; &quot;0.3&quot; &quot;0.3&quot; &quot;0.3&quot; ... ## chr [1:150] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... ## NULL Remark: the “NULL” statement is the return value of apply. “str” returns nothing (but prints something out), so the returned vector (or array, list, …) is empty, just like: c() ## NULL 2.4.3 The aggregate() Function aggregate() calculates a function per grouping variable. Try out this example: aggregate(. ~ Species, data = iris, FUN = max) Note that max is the function to get the maximum value, and has nothing to do with your lecturer, who should be spelled Max. The dot is general R syntax and usually refers to “use all columns in the data set.” 2.5 Plotting The following two commands are identical: plot(iris$Sepal.Length, iris$Sepal.Width) plot(Sepal.Width ~ Sepal.Length, data = iris) The second option is preferable, because it allows you to subset data easier. plot(Sepal.Width ~ Sepal.Length, data = iris[iris$Species == &quot;versicolor&quot;, ]) The plot command will use the standard plot depending on the type of variable supplied. For example, if the x axis is a factor, a boxplot will be produced. plot(Sepal.Width ~ Species, data = iris) You can change color, size, shape etc. and this is often useful for visualization. plot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species, cex = iris$Petal.Length) More on plots in R in this short video: 2.6 Additional Resources As additional R resources for self-study, we recommend: 2.6.1 Books The PDF An introduction to R. An Introduction to Statistical Learning - A simplified version version of a classic machine learning textbook, free PDF for download. Quick R - Good site as a reference with code examples for most standard tasks. Ebook Hands on Programming with R. 2.6.2 Instructional videos YouTube - R Programming Tutorial - Learn the Basics of Statistical Computing (approx 2h, goes through most basics). YouTube - Statistics with R, Tutorials by MarinStatsLectures - Lots of smaller videos for particular topics. "],["introduction.html", "3 Introduction to Machine Learning 3.1 Unsupervised Learning 3.2 Supervised Learning: Regression and Classification 3.3 Small Introduction Into the Underlying Mathematical Concepts of all Following Lessons - Optional 3.4 Introduction to TensorFlow 3.5 Introduction to PyTorch 3.6 First Steps With the Keras Framework", " 3 Introduction to Machine Learning There are three basic machine learning tasks: Supervised learning Unsupervised learning Reinforcement learning In supervised learning, you train algorithms using labeled data, what means that you already know the correct answer for a part of the data (the so called training data). Unsupervised learning in contrast is a technique, where one does not need to monitor the model or apply labels. Instead, you allow the model to work on its own to discover information. Reinforcement learning is a technique that emulates a game-like situation. The algorithm finds a solution by trial and error and gets either rewards or penalties for every action. As in games, the goal is to maximize the rewards. We will talk more about this technique on the last day of the course. For the moment, we will focus on the first two tasks, supervised and unsupervised learning. To do so, we will begin with a small example. But before you start with the code, here is a video to prepare you for what we will do in the class: 3.1 Unsupervised Learning In unsupervised learning, we want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider the iris data set. Here, we have 150 observations of 4 floral traits: iris = datasets::iris colors = hcl.colors(3) traits = as.matrix(iris[,1:4]) species = iris$Species image(y = 1:4, x = 1:length(species) , z = traits, ylab = &quot;Floral trait&quot;, xlab = &quot;Individual&quot;) segments(50.5, 0, 50.5, 5, col = &quot;black&quot;, lwd = 2) segments(100.5, 0, 100.5, 5, col = &quot;black&quot;, lwd = 2) The observations are from 3 species and indeed those species tend to have different traits, meaning that the observations form 3 clusters. pairs(traits, pch = as.integer(species), col = colors[as.integer(species)]) However, imagine we don’t know what species are, what is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does. 3.1.1 Hierarchical Clustering A cluster refers to a collection of data points aggregated together because of certain similarities. In hierarchical clustering, a hierarchy (tree) between data points is built. Agglomerative: Start with each data point in their own cluster, merge them up hierarchically. Divisive: Start with all data points in one cluster, and split hierarchically. Merges / splits are done according to linkage criterion, which measures distance between (potential) clusters. Cut the tree at a certain height to get clusters. Here an example set.seed(123) #Reminder: traits = as.matrix(iris[,1:4]). d = dist(traits) hc = hclust(d, method = &quot;complete&quot;) plot(hc) rect.hclust(hc, k = 3) # Draw rectangles around the branches. Same plot, but with colors for true species identity library(ape) plot(as.phylo(hc), tip.color = colors[as.integer(species)], direction = &quot;downwards&quot;) hcRes3 = cutree(hc, k = 3) #Cut a dendrogram tree into groups. Calculate confusion matrix. Note we are switching labels here so that it fits to the species. tmp = hcRes3 tmp[hcRes3 == 2] = 3 tmp[hcRes3 == 3] = 2 hcRes3 = tmp table(hcRes3, species) ## species ## hcRes3 setosa versicolor virginica ## 1 50 0 0 ## 2 0 27 1 ## 3 0 23 49 Note that results might change if you choose a different agglomeration method, distance metric or scale of your variables. Compare, e.g. to this example: hc = hclust(d, method = &quot;ward.D2&quot;) plot(as.phylo(hc), tip.color = colors[as.integer(species)], direction = &quot;downwards&quot;) hcRes3 = cutree(hc, k = 3) #Cut a dendrogram tree into groups. table(hcRes3, species) ## species ## hcRes3 setosa versicolor virginica ## 1 50 0 0 ## 2 0 49 15 ## 3 0 1 35 Which method is best? library(dendextend) set.seed(123) methods = c(&quot;ward.D&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot;, &quot;centroid&quot;, &quot;ward.D2&quot;) out = dendlist() # Create a dendlist object from several dendrograms. for(method in methods){ res = hclust(d, method = method) out = dendlist(out, as.dendrogram(res)) } names(out) = methods print(out) ## $ward.D ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 200.0981 ## ## $single ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 1.640122 ## ## $complete ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 7.085196 ## ## $average ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 4.069377 ## ## $mcquitty ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 4.520039 ## ## $median ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 2.883062 ## ## $centroid ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 3.001472 ## ## $ward.D2 ## &#39;dendrogram&#39; with 2 branches and 150 members total, at height 32.50651 ## ## attr(,&quot;class&quot;) ## [1] &quot;dendlist&quot; get_ordered_3_clusters = function(dend){ # order.dendrogram function returns the order (index) # or the &quot;label&quot; attribute for the leaves. # cutree: Cut the tree (dendrogram) into groups of data. cutree(dend, k = 3)[order.dendrogram(dend)] } dend_3_clusters = lapply(out, get_ordered_3_clusters) # Calculate Fowlkes-Mallows Index (determine the similarity between clusterings) compare_clusters_to_iris = function(clus){ FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE) } clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris) dotchart(sort(clusters_performance), xlim = c(0.3, 1), xlab = &quot;Fowlkes-Mallows index&quot;, main = &quot;Performance of linkage methods in detecting the 3 species \\n in our example&quot;, pch = 19) We might conclude that ward.D2 works best here. However, as we will learn later, optimizing the method without a hold-out for testing implies that our model may be overfitting. We should check this using cross-validation. 3.1.2 K-means Clustering Another example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms. To start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the assumed or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere. Afterwards each data point is assigned to the respective cluster that raises the overall in-cluster sum of squares (variance) related to the distance to the centroid least of all. After the algorithm has placed all data points into a cluster the centroids get updated. By iterating this procedure until the assignment doesn’t change any longer, the algorithm can find the (locally) optimal centroids and the data points belonging to this cluster. Note that results might differ according to the initial positions of the centroids. Thus several (locally) optimal solutions might be found. The “k” in K-means refers to the number of clusters and the ‘means’ refers to averaging the data-points to find the centroids. A typical pipeline for using k-means clustering looks the same as for other algorithms. After having visualized the data, we fit a model, visualize the results and have a look at the performance by use of the confusion matrix. By setting a fixed seed, we can ensure that results are reproducible. set.seed(123) #Reminder: traits = as.matrix(iris[,1:4]). kc = kmeans(traits, 3) print(kc) ## K-means clustering with 3 clusters of sizes 50, 62, 38 ## ## Cluster means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.006000 3.428000 1.462000 0.246000 ## 2 5.901613 2.748387 4.393548 1.433871 ## 3 6.850000 3.073684 5.742105 2.071053 ## ## Clustering vector: ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 ## [61] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 2 3 3 3 3 2 ## [121] 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2 ## ## Within cluster sum of squares by cluster: ## [1] 15.15100 39.82097 23.87947 ## (between_SS / total_SS = 88.4 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Visualizing the results. Color codes true species identity, symbol shows cluster result. plot(iris[c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;)], col = colors[as.integer(species)], pch = kc$cluster) points(kc$centers[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;)], col = colors, pch = 1:3, cex = 3) We see that there are are some discrepancies. Confusion matrix: table(iris$Species, kc$cluster) ## ## 1 2 3 ## setosa 50 0 0 ## versicolor 0 48 2 ## virginica 0 14 36 If you want to animate the clustering process, you could run library(animation) saveGIF(kmeans.ani(x = traits[,1:2], col = colors), interval = 1, ani.width = 800, ani.height = 800) Elbow technique to determine the probably best suited number of clusters: set.seed(123) getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss } #Perform algorithm for different cluster sizes and retrieve variance. iris.kmeans1to10 = sapply(1:10, getSumSq) plot(1:10, iris.kmeans1to10, type = &quot;b&quot;, pch = 19, frame = FALSE, xlab = &quot;Number of clusters K&quot;, ylab = &quot;Total within-clusters sum of squares&quot;, col = c(&quot;black&quot;, &quot;red&quot;, rep(&quot;black&quot;, 8))) Often, one is interested in sparse models. Furthermore, higher k than necessary tends to overfitting. At the kink in the picture, the sum of squares dropped enough and k is still low enough. But keep in mind, this is only a rule of thumb and might be wrong in some special cases. 3.1.3 Density-based Clustering Determine the affinity of a data point according to the affinity of its k nearest neighbors. This is a very general description as there are many ways to do so. #Reminder: traits = as.matrix(iris[,1:4]). library(dbscan) set.seed(123) kNNdistplot(traits, k = 4) # Calculate and plot k-nearest-neighbor distances. abline(h = 0.4, lty = 2) dc = dbscan(traits, eps = 0.4, minPts = 6) print(dc) ## DBSCAN clustering for 150 objects. ## Parameters: eps = 0.4, minPts = 6 ## The clustering contains 4 cluster(s) and 32 noise points. ## ## 0 1 2 3 4 ## 32 46 36 14 22 ## ## Available fields: cluster, eps, minPts library(factoextra) fviz_cluster(dc, traits, geom = &quot;point&quot;, ggtheme = theme_light()) 3.1.4 Model-based Clustering The last class of methods for unsupervised clustering are so-called model-based clustering methods. library(mclust) ## __ ___________ __ _____________ ## / |/ / ____/ / / / / / ___/_ __/ ## / /|_/ / / / / / / / /\\__ \\ / / ## / / / / /___/ /___/ /_/ /___/ // / ## /_/ /_/\\____/_____/\\____//____//_/ version 5.4.8 ## Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications. mb = Mclust(traits) Mclust automatically compares a number of candidate models (clusters, shape) according to BIC (The BIC is a criterion for classifying algorithms depending their prediction quality and their usage of parameters). We can look at the selected model via: mb$G # Two clusters. ## [1] 2 mb$modelName # &gt; Ellipsoidal, equal shape. ## [1] &quot;VEV&quot; We see that the algorithm prefers having 2 clusters. For better comparability to the other 2 methods, we will override this by setting: mb3 = Mclust(traits, 3) ## fitting ... ## | | | 0% | |======== | 7% | |=============== | 13% | |======================= | 20% | |=============================== | 27% | |======================================= | 33% | |============================================== | 40% | |====================================================== | 47% | |============================================================== | 53% | |====================================================================== | 60% | |============================================================================= | 67% | |===================================================================================== | 73% | |============================================================================================= | 80% | |===================================================================================================== | 87% | |============================================================================================================ | 93% | |====================================================================================================================| 100% Result in terms of the predicted densities for 3 clusters plot(mb3, &quot;density&quot;) Predicted clusters: plot(mb3, what=c(&quot;classification&quot;), add = T) Confusion matrix: table(iris$Species, mb3$classification) ## ## 1 2 3 ## setosa 50 0 0 ## versicolor 0 45 5 ## virginica 0 0 50 3.1.5 Ordination Ordination is used in explorative analysis and compared to clustering, similar objects are ordered together. So there is a relationship between clustering and ordination. Here a PCA ordination on on the iris data set. pcTraits = prcomp(traits, center = TRUE, scale. = TRUE) biplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25)) You can cluster the results of this ordination, ordinate before clustering, or superimpose one on the other. Tasks Go through the 4(5) algorithms above, and check if they are sensitive (i.e. if results change) if you scale the input features (= predictors), instead of using the raw data. Discuss in your group: Which is more appropriate for this analysis and/or in general: Scaling or not scaling? Solution Hierarchical Clustering library(dendextend) methods = c(&quot;ward.D&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;average&quot;, &quot;mcquitty&quot;, &quot;median&quot;, &quot;centroid&quot;, &quot;ward.D2&quot;) cluster_all_methods = function(distances){ out = dendlist() for(method in methods){ res = hclust(distances, method = method) out = dendlist(out, as.dendrogram(res)) } names(out) = methods return(out) } get_ordered_3_clusters = function(dend){ return(cutree(dend, k = 3)[order.dendrogram(dend)]) } compare_clusters_to_iris = function(clus){ return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)) } do_clustering = function(traits, scale = FALSE){ set.seed(123) headline = &quot;Performance of linkage methods\\nin detecting the 3 species\\n&quot; if(scale){ traits = scale(traits) # Do scaling on copy of traits. headline = paste0(headline, &quot;Scaled&quot;) }else{ headline = paste0(headline, &quot;Not scaled&quot;) } distances = dist(traits) out = cluster_all_methods(distances) dend_3_clusters = lapply(out, get_ordered_3_clusters) clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris) dotchart(sort(clusters_performance), xlim = c(0.3,1), xlab = &quot;Fowlkes-Mallows index&quot;, main = headline, pch = 19) } traits = as.matrix(iris[,1:4]) # Do clustering on unscaled data. do_clustering(traits, FALSE) # Do clustering on scaled data. do_clustering(traits, TRUE) It seems that scaling is harmful for hierarchical clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence. K-means Clustering do_clustering = function(traits, scale = FALSE){ set.seed(123) if(scale){ traits = scale(traits) # Do scaling on copy of traits. headline = &quot;K-means Clustering\\nScaled\\nSum of all tries: &quot; }else{ headline = &quot;K-means Clustering\\nNot scaled\\nSum of all tries: &quot; } getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss } iris.kmeans1to10 = sapply(1:10, getSumSq) headline = paste0(headline, round(sum(iris.kmeans1to10), 2)) plot(1:10, iris.kmeans1to10, type = &quot;b&quot;, pch = 19, frame = FALSE, main = headline, xlab = &quot;Number of clusters K&quot;, ylab = &quot;Total within-clusters sum of squares&quot;, col = c(&quot;black&quot;, &quot;red&quot;, rep(&quot;black&quot;, 8)) ) } traits = as.matrix(iris[,1:4]) # Do clustering on unscaled data. do_clustering(traits, FALSE) # Do clustering on scaled data. do_clustering(traits, TRUE) It seems that scaling is harmful for K-means clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence. Density-based Clustering library(dbscan) correct = as.factor(iris[,5]) # Start at 1. Noise points will get 0 later. levels(correct) = 1:length(levels(correct)) correct ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 ## [61] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [121] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## Levels: 1 2 3 do_clustering = function(traits, scale = FALSE){ set.seed(123) if(scale){ traits = scale(traits) } # Do scaling on copy of traits. ##### # Play around with the parameters &quot;eps&quot; and &quot;minPts&quot; on your own! ##### dc = dbscan(traits, eps = 0.41, minPts = 4) labels = as.factor(dc$cluster) noise = sum(dc$cluster == 0) levels(labels) = c(&quot;noise&quot;, 1:( length(levels(labels)) - 1)) tbl = table(correct, labels) correct_classified = 0 for(i in 1:length(levels(correct))){ correct_classified = correct_classified + tbl[i, i + 1] } cat( if(scale){ &quot;Scaled&quot; }else{ &quot;Not scaled&quot; }, &quot;\\n\\n&quot; ) cat(&quot;Confusion matrix:\\n&quot;) print(tbl) cat(&quot;\\nCorrect classified points: &quot;, correct_classified, &quot; / &quot;, length(iris[,5])) cat(&quot;\\nSum of noise points: &quot;, noise, &quot;\\n&quot;) } traits = as.matrix(iris[,1:4]) # Do clustering on unscaled data. do_clustering(traits, FALSE) ## Not scaled ## ## Confusion matrix: ## labels ## correct noise 1 2 3 4 ## 1 3 47 0 0 0 ## 2 5 0 38 3 4 ## 3 17 0 0 33 0 ## ## Correct classified points: 118 / 150 ## Sum of noise points: 25 # Do clustering on scaled data. do_clustering(traits, TRUE) ## Scaled ## ## Confusion matrix: ## labels ## correct noise 1 2 3 4 ## 1 9 41 0 0 0 ## 2 14 0 36 0 0 ## 3 36 0 1 4 9 ## ## Correct classified points: 81 / 150 ## Sum of noise points: 59 It seems that scaling is harmful for density based clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence. Model-based Clustering library(mclust) do_clustering = function(traits, scale = FALSE){ set.seed(123) if(scale){ traits = scale(traits) } # Do scaling on copy of traits. mb3 = Mclust(traits, 3) tbl = table(iris$Species, mb3$classification) cat( if(scale){ &quot;Scaled&quot; }else{ &quot;Not scaled&quot; }, &quot;\\n\\n&quot; ) cat(&quot;Confusion matrix:\\n&quot;) print(tbl) cat(&quot;\\nCorrect classified points: &quot;, sum(diag(tbl)), &quot; / &quot;, length(iris[,5])) } traits = as.matrix(iris[,1:4]) # Do clustering on unscaled data. do_clustering(traits, FALSE) ## fitting ... ## | | | 0% | |======== | 7% | |=============== | 13% | |======================= | 20% | |=============================== | 27% | |======================================= | 33% | |============================================== | 40% | |====================================================== | 47% | |============================================================== | 53% | |====================================================================== | 60% | |============================================================================= | 67% | |===================================================================================== | 73% | |============================================================================================= | 80% | |===================================================================================================== | 87% | |============================================================================================================ | 93% | |====================================================================================================================| 100% ## Not scaled ## ## Confusion matrix: ## ## 1 2 3 ## setosa 50 0 0 ## versicolor 0 45 5 ## virginica 0 0 50 ## ## Correct classified points: 145 / 150 # Do clustering on scaled data. do_clustering(traits, TRUE) ## fitting ... ## | | | 0% | |======== | 7% | |=============== | 13% | |======================= | 20% | |=============================== | 27% | |======================================= | 33% | |============================================== | 40% | |====================================================== | 47% | |============================================================== | 53% | |====================================================================== | 60% | |============================================================================= | 67% | |===================================================================================== | 73% | |============================================================================================= | 80% | |===================================================================================================== | 87% | |============================================================================================================ | 93% | |====================================================================================================================| 100% ## Scaled ## ## Confusion matrix: ## ## 1 2 3 ## setosa 50 0 0 ## versicolor 0 45 5 ## virginica 0 0 50 ## ## Correct classified points: 145 / 150 For model based clustering, scaling does not matter. Ordination traits = as.matrix(iris[,1:4]) biplot(prcomp(traits, center = TRUE, scale. = TRUE), main = &quot;Use integrated scaling&quot;) biplot(prcomp(scale(traits), center = FALSE, scale. = FALSE), main = &quot;Scale explicitly&quot;) biplot(prcomp(traits, center = FALSE, scale. = FALSE), main = &quot;No scaling at all&quot;) For PCA ordination, scaling matters. Because we are interested in directions of maximal variance, all parameters should be scaled, or the one with the highest values might dominate all others. 3.2 Supervised Learning: Regression and Classification The two most prominent branches of supervised learning are regression and classification. Fundamentally, classification is about predicting a label and regression is about predicting a quantity. The following video explains that in more depth: 3.2.1 Supervised Regression Using Random Forest The random forest (RF) algorithm is possibly the most widely used machine learning algorithm and can be used for regression and classification. We will talk more about the algorithm tomorrow. For the moment, we want to go through a typical workflow for a supervised regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. We will again use the iris data set that we used before. The goal is now to predict Sepal.Length based on the information about the other variables (including species). Fitting the model: library(randomForest) set.seed(123) m1 = randomForest(Sepal.Length ~ ., data = iris) # ~.: Against all others. # str(m1) # m1$type # predict(m1) print(m1) ## ## Call: ## randomForest(formula = Sepal.Length ~ ., data = iris) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 0.1364625 ## % Var explained: 79.97 Visualization of the results: oldpar = par(mfrow = c(1, 2)) plot(predict(m1), iris$Sepal.Length, xlab = &quot;Predicted&quot;, ylab = &quot;Observed&quot;) abline(0, 1) varImpPlot(m1) par(oldpar) To understand the structure of a random forest in more detail, we can use a package from GitHub. reprtree:::plot.getTree(m1, iris) Here, one of the regression trees is shown. 3.2.2 Supervised Classification Using Random Forest With the random forest, we can also do classification. The steps are the same as for regression tasks, but we can additionally see how well it performed by looking at the confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represents the instances in the actual class. Thus the diagonals are the correctly predicted classes and the off-diagonal elements are the falsely classified elements. Fitting the model: set.seed(123) m1 = randomForest(Species ~ ., data = iris) Visualizing one of the fitted models: oldpar = par(mfrow = c(1, 2)) reprtree:::plot.getTree(m1, iris) Visualizing results ecologically: par(mfrow = c(1, 2)) plot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = &quot;Observed&quot;) plot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = &quot;Predicted&quot;) par(oldpar) #Reset par. Confusion matrix: table(predict(m1), iris$Species) ## ## setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 4 ## virginica 0 3 46 Questions 3.3 Small Introduction Into the Underlying Mathematical Concepts of all Following Lessons - Optional If are not yet familiar with the underlying concepts of neural networks and want to know more about that, it is suggested to read / view the following videos / sites. Consider the Links and videos with descriptions in parentheses as optional bonus. This might be useful to understand the further concepts in more depth. (https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially the animated graphic is interesting).) https://en.wikipedia.org/wiki/Gradient_descent#Description Neural networks (Backpropagation, etc.). Activation functions in detail (requires the above as prerequisite). Videos about the topic: Gradient descent explained (Stochastic gradient descent explained) (Entropy explained) Short explanation of entropy, cross entropy and Kullback–Leibler divergence Deep Learning (chapter 1) How neural networks learn - Deep Learning (chapter 2) Backpropagation - Deep Learning (chapter 3) Another video about backpropagation (extends the previous one) - Deep Learning (chapter 4) 3.3.1 Caveat About Learning Rates and Activation Functions Depending on activation functions, it might occur that the network won’t get updated, even with high learning rates (called vanishing gradient, especially for “sigmoid” functions). Furthermore, updates might overshoot (called exploding gradients) or activation functions will result in many zeros (especially for “relu,” dying relu). In general, the first layers of a network tend to learn (much) more slowly than subsequent ones. 3.4 Introduction to TensorFlow One of the most commonly used frameworks for machine learning is TensorFlow. TensorFlow is an open source linear algebra library with focus on neural networks, published by Google in 2015. TensorFlow supports several interesting features, in particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization. These advantages are nicely explained in the following video: To sum up the most important points of the video: TensorFlow is a math library which is highly optimized for neural networks. If a GPU is available, computations can be easily run on the GPU but even on a CPU TensorFlow is still very fast. The “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for NVIDIA GPUs). The interface (the part of TensorFlow we use) is written in Python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend. All operations in TensorFlow are written in C++ and are highly optimized. But don’t worry, we don’t have to use C++ to use TensorFlow because there are several bindings for other languages. TensorFlow officially supports a Python API, but meanwhile there are several community carried APIs for other languages: R Go Rust Swift JavaScript In this course we will use TensorFlow with the https://tensorflow.rstudio.com/ binding, that was developed and published 2017 by the RStudio team. First, they developed an R package (reticulate) for calling Python in R. Actually, we are using the Python TensorFlow module in R (more about this later). TensorFlow offers different levels of API. We could implement a neural network completely by ourselves or we could use Keras which is provided as a submodule by TensorFlow. Keras is a powerful module for building and training neural networks. It allows us building and training neural networks in a few lines of codes. Since the end of 2018, Keras and TensorFlow are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras for neural networks but also how we can use the TensorFlow’s automatic differenation for using complex objective functions. Useful links: TensorFlow documentation (This is for the Python API, but just replace the “.” with “$.”) Rstudio TensorFlow website 3.4.1 TensorFlow Data Containers TensorFlow has two data containers (structures): constant (tf$constant): Creates a constant (immutable) value in the computation graph. variable (tf$Variable): Creates a mutable value in the computation graph (used as parameter/weight in models). To get started with TensorFlow, we have to load the library and check if the installation worked. library(tensorflow) library(keras) # Don&#39;t worry about weird messages. TensorFlow supports additional optimizations. exists(&quot;tf&quot;) ## [1] TRUE immutable = tf$constant(5.0) ## Loaded Tensorflow version 2.7.0 mutable = tf$constant(5.0) Don’t worry about weird messages (they will only appear once at the start of the session). 3.4.2 Basic Operations We now can define the variables and do some math with them: a = tf$constant(5) b = tf$constant(10) print(a) ## tf.Tensor(5.0, shape=(), dtype=float32) print(b) ## tf.Tensor(10.0, shape=(), dtype=float32) c = tf$add(a, b) print(c) ## tf.Tensor(15.0, shape=(), dtype=float32) tf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message). k_print_tensor(c) # Comes out of Keras! ## tf.Tensor(15.0, shape=(), dtype=float32) Normal R methods such as print() are provided by the R package “tensorflow.” The TensorFlow library (created by the RStudio team) built R methods for all common operations: `+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) } # Mind the backticks. k_print_tensor(a+b) ## tf.Tensor(15.0, shape=(), dtype=float32) Their operators also automatically transform R numbers into constant tensors when attempting to add a tensor to an R number: d = c + 5 # 5 is automatically converted to a tensor. print(d) ## tf.Tensor(20.0, shape=(), dtype=float32) TensorFlow containers are objects, what means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object). For instance, there is a method to transform the tensor object back to an R object: class(d) ## [1] &quot;tensorflow.tensor&quot; &quot;tensorflow.python.framework.ops.EagerTensor&quot; ## [3] &quot;tensorflow.python.framework.ops._EagerTensorBase&quot; &quot;tensorflow.python.framework.ops.Tensor&quot; ## [5] &quot;tensorflow.python.types.internal.NativeObject&quot; &quot;tensorflow.python.types.core.Tensor&quot; ## [7] &quot;python.builtin.object&quot; class(d$numpy()) ## [1] &quot;numeric&quot; 3.4.3 TensorFlow Data Types - Good Practice With R-TensorFlow R uses dynamic typing, what means you can assign a number, character, function or whatever to a variable and the the type is automatically inferred. In other languages you have to state the type explicitly, e.g. in C: int a = 5; float a = 5.0; char a = &quot;a&quot;; While TensorFlow tries to infer the type dynamically, you must often state it explicitly. Common important types: float32 (floating point number with 32 bits, “single precision”) float64 (floating point number with 64 bits, “double precision”) int8 (integer with 8 bits) The reason why TensorFlow is so explicit about types is that many GPUs (e.g. the NVIDIA GeForces) can handle only up to 32 bit numbers! (you do not need high precision in graphical modeling) But let us see in practice what we have to do with these types and how to specifcy them: r_matrix = matrix(runif(10*10), 10, 10) m = tf$constant(r_matrix, dtype = &quot;float32&quot;) b = tf$constant(2.0, dtype = &quot;float64&quot;) c = m / b # Doesn&#39;t work! We try to divide float32/float64. So what went wrong here? We tried to divide a float32 by a float64 number, but we can only divide numbers of the same type! r_matrix = matrix(runif(10*10), 10, 10) m = tf$constant(r_matrix, dtype = &quot;float64&quot;) b = tf$constant(2.0, dtype = &quot;float64&quot;) c = m / b # Now it works. We can also specify the type of the object by providing an object e.g. tf$float64. r_matrix = matrix(runif(10*10), 10, 10) m = tf$constant(r_matrix, dtype = tf$float64) In TensorFlow, arguments often require exact/explicit data types: TensorFlow often expects integers as arguments. In R however an integer is normally saved as float. Thus, we have to use an “L” after an integer to tell the R interpreter that it should be treated as an integer: is.integer(5) is.integer(5L) matrix(t(r_matrix), 5, 20, byrow = TRUE) tf$reshape(r_matrix, shape = c(5, 20))$numpy() tf$reshape(r_matrix, shape = c(5L, 20L))$numpy() Skipping the “L” is one of the most common errors when using R-TensorFlow! Tasks To run TensorFlow from R, note that you can access the different mathematical operations in TensorFlow via tf$…, e.g. there is a tf$math$… for all common math operations or the tf$linalg$… for different linear algebra operations. Tip: type tf$ and then hit the tab key to list all available options (sometimes you have to do this directly in the console). An example: How to get the maximum value of a vector? library(tensorflow) library(keras) x = 100:1 y = as.double(100:1) max(x) # R solution. Integer! tf$math$reduce_max(x) # TensorFlow solution. Integer! max(y) # Float! tf$math$reduce_max(y) # Float! Rewrite the following expressions (a to g) in TensorFlow: x = 100:1 y = as.double(100:1) # a) min(x) ## [1] 1 # b) mean(x) ## [1] 50.5 # c) Tip: Use Google! which.max(x) ## [1] 1 # d) which.min(x) ## [1] 100 # e) Tip: Use Google! order(x) ## [1] 100 99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76 75 74 73 72 71 ## [31] 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 ## [61] 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 ## [91] 10 9 8 7 6 5 4 3 2 1 # f) Tip: See tf$reshape. m = matrix(y, 10, 10) # Mind: We use y here! (Float) m_2 = abs(m %*% t(m)) # m %*% m is the normal matrix multiplication. m_2_log = log(m_2) print(m_2_log) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305 10.43705 10.42079 ## [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910 10.42317 10.40699 ## [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496 10.40910 10.39299 ## [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061 10.39482 10.37879 ## [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605 10.38034 10.36439 ## [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127 10.36565 10.34977 ## [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628 10.35073 10.33495 ## [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105 10.33559 10.31989 ## [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559 10.32022 10.30461 ## [10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989 10.30461 10.28909 # g) Custom mean function i.e. rewrite the function using TensorFlow. mean_R = function(y){ result = sum(y) / length(y) return(result) } mean_R(y) == mean(y) # Test for equality. ## [1] TRUE Solution library(tensorflow) library(keras) x = 100:1 y = as.double(100:1) # a) min(x) tf$math$reduce_min(x) # Integer! ## tf.Tensor(1, shape=(), dtype=int32) tf$math$reduce_min(y) # Float! ## tf.Tensor(1.0, shape=(), dtype=float32) # b) mean(x) # Check out the difference here: mean(x) ## [1] 50.5 mean(y) ## [1] 50.5 tf$math$reduce_mean(x) # Integer! ## tf.Tensor(50, shape=(), dtype=int32) tf$math$reduce_mean(y) # Float! ## tf.Tensor(50.5, shape=(), dtype=float32) # c) which.max(x) tf$argmax(x) ## tf.Tensor(0, shape=(), dtype=int64) tf$argmax(y) ## tf.Tensor(0, shape=(), dtype=int64) # d) which.min(x) tf$argmin(x) ## tf.Tensor(99, shape=(), dtype=int64) # e) order(x) tf$argsort(x) ## tf.Tensor( ## [99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76 ## 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 ## 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 ## 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 ## 3 2 1 0], shape=(100,), dtype=int32) # f) # m = matrix(y, 10, 10) # m_2 = abs(m %*% m) # m_2_log = log(m_2) # Mind: We use y here! TensorFlow just accepts floats in the following lines! mTF = tf$reshape(y, list(10L, 10L)) m_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) ) m_2_logTF = tf$math$log(m_2TF) print(m_2_logTF) ## tf.Tensor( ## [[11.4217415 11.311237 11.186988 11.045079 10.87965 10.68132 ## 10.433674 10.103771 9.608109 8.582045 ] ## [11.311237 11.200746 11.076511 10.934624 10.769221 10.570931 ## 10.323348 9.993557 9.498147 8.473242 ] ## [11.186988 11.076511 10.952296 10.810434 10.645067 10.446828 ## 10.199324 9.869672 9.374583 8.351139 ] ## [11.045079 10.934624 10.810434 10.668606 10.503284 10.305112 ## 10.057709 9.728241 9.233569 8.212026 ] ## [10.87965 10.769221 10.645067 10.503284 10.338025 10.139941 ## 9.892679 9.563459 9.069353 8.0503845] ## [10.68132 10.570931 10.446828 10.305112 10.139941 9.941987 ## 9.694924 9.366061 8.872768 7.857481 ] ## [10.433674 10.323348 10.199324 10.057709 9.892679 9.694924 ## 9.448175 9.119869 8.62784 7.6182513] ## [10.103771 9.993557 9.869672 9.728241 9.563459 9.366061 ## 9.119869 8.79255 8.302762 7.30317 ] ## [ 9.608109 9.498147 9.374583 9.233569 9.069353 8.872768 ## 8.62784 8.302762 7.818028 6.8405466] ## [ 8.582045 8.473242 8.351139 8.212026 8.0503845 7.857481 ## 7.6182513 7.30317 6.8405466 5.9532433]], shape=(10, 10), dtype=float32) # g) # Custom mean function mean_TF = function(y){ result = tf$math$reduce_sum(y) return( result / length(y) ) # If y is an R object. } mean_TF(y) == mean(y) ## tf.Tensor(True, shape=(), dtype=bool) Task This exercise compares the speed of R to TensorFlow. The first exercise is to rewrite the following function in TensorFlow: do_something_R = function(x = matrix(0.0, 10L, 10L)){ mean_per_row = apply(x, 1, mean) result = x - mean_per_row return(result) } Here, we provide a skeleton for a TensorFlow function: do_something_TF = function(x = matrix(0.0, 10L, 10L)){ ... } We can compare the speed using the Microbenchmark package: test = matrix(0.0, 100L, 100L) microbenchmark::microbenchmark(do_something_R(test), do_something_TF(test)) Try different matrix sizes for the test matrix and compare the speed. Tip: Have a look at the the tf.reduce_mean documentation and the “axis” argument. Compare the following with different matrix sizes: test = matrix(0.0, 1000L, 500L) testTF = tf$constant(test) Also try the following: microbenchmark::microbenchmark( tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style. test %*% t(test) # R style. ) Solution do_something_TF = function(x = matrix(0.0, 10L, 10L)){ x = tf$constant(x) # Remember, this is a local copy! mean_per_row = tf$reduce_mean(x, axis = 0L) result = x - mean_per_row return(result) } test = matrix(0.0, 100L, 100L) microbenchmark::microbenchmark(do_something_R(test), do_something_TF(test)) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## do_something_R(test) 457.297 493.101 1003.893 509.8455 528.595 47142.537 100 a ## do_something_TF(test) 2384.436 2448.121 2563.876 2510.2245 2551.953 6028.575 100 b test = matrix(0.0, 1000L, 500L) microbenchmark::microbenchmark(do_something_R(test), do_something_TF(test)) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## do_something_R(test) 7.048826 7.350961 10.291932 7.530190 15.173997 21.306708 100 b ## do_something_TF(test) 3.471403 3.593884 4.028746 3.689198 4.108474 9.197866 100 a Why is R faster (the first time)? The R functions we used (apply, mean, “-”) are also implemented in C. The problem is not large enough and TensorFlow has an overhead. test = matrix(0.0, 1000L, 500L) testTF = tf$constant(test) microbenchmark::microbenchmark( tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style. test %*% t(test) # R style. ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## tf$matmul(testTF, tf$transpose(testTF)) 5.279501 6.674446 8.450876 7.431472 8.791987 20.62599 100 a ## test %*% t(test) 65.143662 69.645367 76.857925 71.003062 77.398229 415.56301 100 b Task Google to find out how to write the following tasks in TensorFlow: A = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3) # i) solve(A) # Solve equation AX = B. If just A is given, invert it. ## [,1] [,2] [,3] ## [1,] 1 0.0 -0.6666667 ## [2,] -1 0.5 -0.1666667 ## [3,] 0 0.0 0.3333333 # j) diag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix. ## [1] 1 2 3 # k) diag(diag(A)) # Diagonal matrix with entries diag(A). ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 2 0 ## [3,] 0 0 3 # l) eigen(A) ## eigen() decomposition ## $values ## [1] 3 2 1 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.1400280 0 0.4472136 ## [2,] 0.9801961 1 -0.8944272 ## [3,] 0.1400280 0 0.0000000 # m) det(A) ## [1] 6 Solution library(tensorflow) library(keras) A = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3) # Do not use the &quot;L&quot; form here! # i) solve(A) tf$linalg$inv(A) ## tf.Tensor( ## [[ 1. 0. -0.66666667] ## [-1. 0.5 -0.16666667] ## [ 0. 0. 0.33333333]], shape=(3, 3), dtype=float64) # j) diag(A) tf$linalg$diag_part(A) ## tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64) # k) diag(diag(A)) tf$linalg$diag(tf$linalg$diag_part(A)) ## tf.Tensor( ## [[1. 0. 0.] ## [0. 2. 0.] ## [0. 0. 3.]], shape=(3, 3), dtype=float64) # l) eigen(A) tf$linalg$eigh(A) ## [[1]] ## tf.Tensor([-0.56155281 3. 3.56155281], shape=(3,), dtype=float64) ## ## [[2]] ## tf.Tensor( ## [[-0.78820544 0. -0.61541221] ## [ 0.61541221 0. -0.78820544] ## [ 0. 1. -0. ]], shape=(3, 3), dtype=float64) # m) det(A) tf$linalg$det(A) ## tf.Tensor(6.0, shape=(), dtype=float64) Task TensorFlow supports automatic differentiation (analytical and not numerical!). Let’s have a look at the function \\(f(x) = 5 x^2 + 3\\) with derivative \\(f&#39;(x) = 10x\\). So for \\(f&#39;(5)\\) we will get \\(10\\). Let’s do this in TensorFlow. Define the function: f = function(x){ return(5.0 * tf$square(x) + 3.0) } We want to calculate the derivative for \\(x = 2.0\\): x = tf$constant(2.0) To do automatic differentiation, we have to forward \\(x\\) through the function within the tf$GradientTape() environment. We have also have to tell TensorFlow which value to “watch”: with(tf$GradientTape() %as% tape, { tape$watch(x) y = f(x) } ) To print the gradient: (tape$gradient(y, x)) ## tf.Tensor(20.0, shape=(), dtype=float32) We can also calculate the second order derivative \\(f&#39;&#39;(x) = 10\\): with(tf$GradientTape() %as% first, { first$watch(x) with(tf$GradientTape() %as% second, { second$watch(x) y = f(x) g = first$gradient(y, x) } ) } ) (second$gradient(g, x)) ## tf.Tensor(10.0, shape=(), dtype=float32) What is happening here? Think about and discuss it. A more advanced example: Linear regression In this case we first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows a normal distribution == error). set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. x = matrix(round(runif(500, -2, 2), 3), 100, 5) w = round(rnorm(5, 2, 1), 3) y = x %*% w + round(rnorm(100, 0, 0.25), 4) In R we would do the following to fit a linear regression model: summary(lm(y~x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.67893 -0.16399 0.00968 0.15058 0.51099 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.004865 0.027447 0.177 0.86 ## x1 2.191511 0.023243 94.287 &lt;2e-16 *** ## x2 2.741690 0.025328 108.249 &lt;2e-16 *** ## x3 1.179181 0.023644 49.872 &lt;2e-16 *** ## x4 0.591873 0.025154 23.530 &lt;2e-16 *** ## x5 2.302417 0.022575 101.991 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2645 on 94 degrees of freedom ## Multiple R-squared: 0.9974, Adjusted R-squared: 0.9972 ## F-statistic: 7171 on 5 and 94 DF, p-value: &lt; 2.2e-16 Let’s build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights (\\(\\boldsymbol{w}\\)) of the regression model). We want our model to learn these weights. The input (predictors, independent variables or features, \\(\\boldsymbol{X}\\)) and the observed (response, \\(\\boldsymbol{y}\\)) are constant and will not be learned/optimized. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. x = matrix(round(runif(500, -2, 2), 3), 100, 5) w = round(rnorm(5, 2, 1), 3) y = x %*% w + round(rnorm(100, 0, 0.25), 4) # Weights we want to learn. # We know the real weights but in reality we wouldn&#39;t know them. # So use guessed ones. wTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1)) xTF = tf$constant(x) yTF = tf$constant(y) # We need an optimizer which updates the weights (wTF). optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1) for(i in 1:100){ with(tf$GradientTape() %as% tape, { pred = tf$matmul(xTF, wTF) loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred))) } ) if(!i%%10){ k_print_tensor(loss, message = &quot;Loss: &quot;) } # Every 10 times. grads = tape$gradient(loss, wTF) optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF)))) } k_print_tensor(wTF, message = &quot;Resulting weights:\\n&quot;) ## &lt;tf.Variable &#39;Variable:0&#39; shape=(5, 1) dtype=float64, numpy= ## array([[2.19290567], ## [2.74534135], ## [1.1714656 ], ## [0.58811305], ## [2.30174942]])&gt; cat(&quot;Original weights: &quot;, w, &quot;\\n&quot;) ## Original weights: 2.217 2.719 1.165 0.593 2.303 Discuss the code, go through the code line by line and try to understand it. Additional exercise: Play around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model). Solution library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. numberOfWeights = 3 numberOfFeatures = 10000 x = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3), numberOfFeatures, numberOfWeights) w = round(rnorm(numberOfWeights, 2, 1), 3) intercept = round(rnorm(1, 3, .5), 3) y = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4) # Guessed weights and intercept. wTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1)) interceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32. xTF = tf$constant(x) yTF = tf$constant(y) optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05) for(i in 1:100){ with(tf$GradientTape(persistent = TRUE) %as% tape, { pred = tf$add(interceptTF, tf$matmul(xTF, wTF)) loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred))) } ) if(!i%%10){ k_print_tensor(loss, message = &quot;Loss: &quot;) } # Every 10 times. grads = tape$gradient(loss, wTF) optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF)))) grads = tape$gradient(loss, interceptTF) optimizer$apply_gradients(purrr::transpose(list(list(grads), list(interceptTF)))) } k_print_tensor(wTF, message = &quot;Resulting weights:\\n&quot;) ## &lt;tf.Variable &#39;Variable:0&#39; shape=(3, 1) dtype=float64, numpy= ## array([[2.46391571], ## [2.45852885], ## [1.00566707]])&gt; cat(&quot;Original weights: &quot;, w, &quot;\\n&quot;) ## Original weights: 2.47 2.465 1.003 k_print_tensor(interceptTF, message = &quot;Resulting intercept:\\n&quot;) ## &lt;tf.Variable &#39;Variable:0&#39; shape=(1, 1) dtype=float64, numpy=array([[4.22135068]])&gt; cat(&quot;Original intercept: &quot;, intercept, &quot;\\n&quot;) ## Original intercept: 4.09 3.5 Introduction to PyTorch PyTorch is another famous library for deep learning. Like TensorFlow, Torch itself is written in C++ with an API for Python. In 2020, the RStudio team released R-Torch, and while R-TensorFlow calls the Python API in the background, the R-Torch API is built directly on the C++ Torch library! Useful links: PyTorch documentation (This is for the Python API, bust just replace the “.” with “$.”) R-Torch website To get started with Torch, we have to load the library and check if the installation worked. library(torch) 3.5.1 PyTorch Data Containers Unlike TensorFlow, Torch doesn’t have two data containers for mutable and immutable variables. All variables are initialized via the torch_tensor function: a = torch_tensor(1.) To mark variables as mutable (and to track their operations for automatic differentiation) we have to set the argument ‘requires_grad’ to true in the torch_tensor function: mutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...) immutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...) 3.5.2 Basic Operations We now can define the variables and do some math with them: a = torch_tensor(5.) b = torch_tensor(10.) print(a) ## torch_tensor ## 5 ## [ CPUFloatType{1} ] print(b) ## torch_tensor ## 10 ## [ CPUFloatType{1} ] c = a$add(b) print(c) ## torch_tensor ## 15 ## [ CPUFloatType{1} ] The R-Torch package provides all common methods (an advantage over TensorFlow). a = torch_tensor(5.) b = torch_tensor(10.) print(a+b) ## torch_tensor ## 15 ## [ CPUFloatType{1} ] print(a/b) ## torch_tensor ## 0.5000 ## [ CPUFloatType{1} ] print(a*b) ## torch_tensor ## 50 ## [ CPUFloatType{1} ] Their operators also automatically transform R numbers into tensors when attempting to add a tensor to a R number: d = a + 5 # 5 is automatically converted to a tensor. print(d) ## torch_tensor ## 10 ## [ CPUFloatType{1} ] As for TensorFlow, we have to explicitly transform the tensors back to R: class(d) ## [1] &quot;torch_tensor&quot; &quot;R7&quot; class(as.numeric(d)) ## [1] &quot;numeric&quot; 3.5.3 Torch Data Types - Good Practice With R-Torch Similar to TensorFlow: r_matrix = matrix(runif(10*10), 10, 10) m = torch_tensor(r_matrix, dtype = torch_float32()) b = torch_tensor(2.0, dtype = torch_float64()) c = m / b But here’s a difference! With TensorFlow we would get an error, but with R-Torch, m is automatically casted to a double (float64). However, this is still bad practice! During the course we will try to provide the corresponding PyTorch code snippets for all Keras/TensorFlow examples. 3.6 First Steps With the Keras Framework We have seen that we can use TensorFlow directly out of R, and we could use this knowledge to implement a neural network in TensorFlow directly in R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us with implementing the machine learning models in TensorFlow. The most common of those is Keras. Keras is a powerful framework for building and training neural networks with a few lines of codes. Since the end of 2018, Keras and TensorFlow are completely interoperable, allowing us to utilize the best of both. The objective of this lesson is to familiarize yourself with Keras. If you have installed TensorFlow, Keras can be found within TensorFlow: tf.keras. However, the RStudio team has built an R package on top of tf.keras, and it is more convenient to use this. To load the Keras package, type library(keras) 3.6.1 Example Workflow in Keras To show how Keras works, we will now build a small classifier to predict the three species of the iris data set. Load the necessary packages and data sets: library(keras) library(tensorflow) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa For neural networks, it is beneficial to scale the predictors (scaling = centering and standardization, see ?scale). We also split our data into predictors (X) and response (Y = the three species). X = scale(iris[,1:4]) Y = iris[,5] Additionally, Keras/TensorFlow cannot handle factors and we have to create contrasts (one-hot encoding). To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like Python and C++, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Keep this in mind. Y = to_categorical(as.integer(Y) - 1L, 3) head(Y) # 3 columns, one for each level of the response. ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 0 0 ## [3,] 1 0 0 ## [4,] 1 0 0 ## [5,] 1 0 0 ## [6,] 1 0 0 After having prepared the data, we will now see a typical workflow to specify a model in Keras. 1. Initialize a sequential model in Keras: model = keras_model_sequential() A sequential Keras model is a higher order type of model within Keras and consists of one input and one output model. 2. Add hidden layers to the model (we will learn more about hidden layers during the next days). When specifying the hidden layers, we also have to specify the shape and a so called activation function. You can think of the activation function as decision for what is forwarded to the next neuron (but we will learn more about it later). If you want to know this topic in even more depth, consider watching the videos presented in section 3.3. The shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3). model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(4L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 3L, activation = &quot;softmax&quot;) softmax scales a potential multidimensional vector to the interval \\((0, 1]\\) for each component. The sum of all components equals 1. This might be very useful for example for handling probabilities. Ensure ther the labels start at 0! Otherwise the softmax function does not work well! Torch The Torch syntax is very similar, we will give a list of layers to the “nn_sequential” function. Here, we have to specify the softmax activation function as an extra layer: library(torch) torch_manual_seed(321L) set.seed(123) model_torch = nn_sequential( nn_linear(4L, 20L), nn_linear(20L, 20L), nn_linear(20L, 20L), nn_linear(20L, 3L), nn_softmax(2) ) 3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax). We will learn about other options later, so for now, do not worry about the “learning_rate” (“lr” in Torch or earlier in TensorFlow) argument, cross entropy or the optimizer. model %&gt;% compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(learning_rate = 0.001)) summary(model) ## Model: &quot;sequential&quot; ## ______________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================== ## dense_3 (Dense) (None, 20) 100 ## ## dense_2 (Dense) (None, 20) 420 ## ## dense_1 (Dense) (None, 20) 420 ## ## dense (Dense) (None, 3) 63 ## ## ============================================================================================================================== ## Total params: 1,003 ## Trainable params: 1,003 ## Non-trainable params: 0 ## ______________________________________________________________________________________________________________________________ Torch Specify optimizer and the parameters which will be trained (in our case the parameters of the network): optimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001) 4. Fit model in 30 iterations (epochs) library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model_history = model %&gt;% fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L, batch_size = 20L, shuffle = TRUE) Torch In Torch, we jump directly to the training loop, however, here we have to write our own training loop: Get a batch of data. Predict on batch. Ccalculate loss between predictions and true labels. Backpropagate error. Update weights. Go to step 1 and repeat. library(torch) torch_manual_seed(321L) set.seed(123) # Calculate number of training steps. epochs = 30 batch_size = 20 steps = round(nrow(X)/batch_size * epochs) X_torch = torch_tensor(X) Y_torch = torch_tensor(apply(Y, 1, which.max)) # Set model into training status. model_torch$train() log_losses = NULL # Training loop. for(i in 1:steps){ # Get batch. indices = sample.int(nrow(X), batch_size) # Reset backpropagation. optimizer_torch$zero_grad() # Predict and calculate loss. pred = model_torch(X_torch[indices, ]) loss = nnf_cross_entropy(pred, Y_torch[indices]) # Backpropagation and weight update. loss$backward() optimizer_torch$step() log_losses[i] = as.numeric(loss) } 5. Plot training history: plot(model_history) Torch plot(log_losses, xlab = &quot;steps&quot;, ylab = &quot;loss&quot;, las = 1) 6. Create predictions: predictions = predict(model, X) # Probabilities for each class. Get probabilities: head(predictions) # Quasi-probabilities for each species. ## [,1] [,2] [,3] ## [1,] 0.9819264 0.01476339 0.003310233 ## [2,] 0.9563531 0.03986334 0.003783490 ## [3,] 0.9830713 0.01501246 0.001916326 ## [4,] 0.9789234 0.01915257 0.001923954 ## [5,] 0.9871404 0.01057777 0.002281805 ## [6,] 0.9808626 0.01525487 0.003882427 For each plant, we want to know for which species we got the highest probability: preds = apply(predictions, 1, which.max) print(preds) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 3 2 3 2 2 2 ## [61] 2 3 2 3 2 3 3 2 2 2 3 2 2 2 2 3 3 3 3 2 2 2 2 3 2 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 ## [121] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Torch model_torch$eval() preds_torch = model_torch(torch_tensor(X)) preds_torch = apply(preds_torch, 1, which.max) print(preds_torch) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 3 2 2 2 3 2 2 2 ## [61] 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 ## [121] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 mean(preds_torch == as.integer(iris$Species)) ## [1] 0.9333333 7. Calculate Accuracy (how often we have been correct): mean(preds == as.integer(iris$Species)) ## [1] 0.8666667 8. Plot predictions, to see if we have done a good job: oldpar = par(mfrow = c(1, 2)) plot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species, main = &quot;Observed&quot;) plot(iris$Sepal.Length, iris$Petal.Length, col = preds, main = &quot;Predicted&quot;) par(oldpar) # Reset par. So you see, building a neural network is very easy with Keras and you can already do it on your own. Questions Task We will now build a regression for the airquality data set with Keras. We want to predict the variable “Ozone.” Load and prepare the data set: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality Explore the data with summary() and plot(): summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 Min. :5.000 Min. : 1.0 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 1st Qu.:6.000 1st Qu.: 8.0 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 Median :7.000 Median :16.0 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 Mean :6.993 Mean :15.8 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 Max. :9.000 Max. :31.0 ## NA&#39;s :37 NA&#39;s :7 plot(data) There are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don’t know how to remove NAs from a data.frame, use Google (e.g. with the query: “remove-rows-with-all-or-some-nas-missing-values-in-data-frame”). Split the data in predictors (\\(\\boldsymbol{X}\\)) and response (\\(\\boldsymbol{y}\\), Ozone) and scale the \\(\\boldsymbol{X}\\) matrix. Build a sequential Keras model. Add hidden layers (input and output layer are already specified, you have to add hidden layers between them): model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% .... layer_dense(units = 1L, activation = &quot;linear&quot;) Why do we use 5L as input shape? Why only one output node and “linear” activation layer? Compile model. model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05)) What is the “mean_squared_error” loss? Fit model: Tip: Only matrices are accepted for \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{y}\\) by Keras. R often drops a one column matrix into a vector (change it back to a matrix!) Plot training history. Create predictions. Compare your Keras model with a linear model: fit = lm(Ozone ~ ., data = data) pred_lm = predict(fit, data) rmse_lm = mean(sqrt((y - pred_lm)^2)) rmse_keras = mean(sqrt((y - pred_keras)^2)) print(rmse_lm) print(rmse_keras) Solution data = airquality 1. There are NAs in the data, which we have to remove because Keras cannot handle NAs. data = data[complete.cases(data),] # Remove NAs. summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.0 Min. : 7.0 Min. : 2.30 Min. :57.00 Min. :5.000 Min. : 1.00 ## 1st Qu.: 18.0 1st Qu.:113.5 1st Qu.: 7.40 1st Qu.:71.00 1st Qu.:6.000 1st Qu.: 9.00 ## Median : 31.0 Median :207.0 Median : 9.70 Median :79.00 Median :7.000 Median :16.00 ## Mean : 42.1 Mean :184.8 Mean : 9.94 Mean :77.79 Mean :7.216 Mean :15.95 ## 3rd Qu.: 62.0 3rd Qu.:255.5 3rd Qu.:11.50 3rd Qu.:84.50 3rd Qu.:9.000 3rd Qu.:22.50 ## Max. :168.0 Max. :334.0 Max. :20.70 Max. :97.00 Max. :9.000 Max. :31.00 2. Split the data in predictors and response and scale the matrix. x = scale(data[,2:6]) y = data[,1] 3. Build sequential Keras model. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() 4. Add hidden layers (input and output layer are already specified, you have to add hidden layers between them). model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;) We use 5L as input shape, because we have 5 predictors. Analogously, we use 1L for our 1d response. Because we do not want any compression, dilation or other nonlinear effects, we use the simple linear layer (equal to no activation function at all). For more about activation functions, look for example here. Or wait for the next days. You may also have seen the previously shown link about activation functions in more detail. 5. Compile model. model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05)) The mean_squared_error is the ordinary least squares approach in regression analysis. 6. Fit model. model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE) 7. Plot training history. plot(model_history) model %&gt;% evaluate(x, y) ## loss ## 173.4729 8. Create predictions. pred_keras = predict(model, x) 9. Compare Keras model with a linear model. fit = lm(Ozone ~ ., data = data) pred_lm = predict(fit, data) rmse_lm = mean(sqrt((y - pred_lm)^2)) rmse_keras = mean(sqrt((y - pred_keras)^2)) print(rmse_lm) ## [1] 14.78897 print(rmse_keras) ## [1] 9.621961 Look at this slightly more complex model and compare the loss plot and the accuracy in contrast to the former. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 30L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE) plot(model_history) model %&gt;% evaluate(x, y) ## loss ## 105.1682 pred_keras = predict(model, x) fit = lm(Ozone ~ ., data = data) pred_lm = predict(fit, data) rmse_lm = mean(sqrt((y - pred_lm)^2)) rmse_keras = mean(sqrt((y - pred_keras)^2)) print(rmse_lm) ## [1] 14.78897 print(rmse_keras) ## [1] 7.798208 You see, the more complex model works better, because it can learn the coherences better. But keep the overfitting problem in mind! Look at the little change in learning rates for the next 2 models and compare the loss plot and the accuracy in contrast to the former. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 30L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE) plot(model_history) model %&gt;% evaluate(x, y) ## loss ## 116.6115 pred_keras = predict(model, x) fit = lm(Ozone ~ ., data = data) pred_lm = predict(fit, data) rmse_lm = mean(sqrt((y - pred_lm)^2)) rmse_keras = mean(sqrt((y - pred_keras)^2)) print(rmse_lm) ## [1] 14.78897 print(rmse_keras) ## [1] 8.247861 You can see, the higher learning rate yields a little bit worse results. The optimum is jumped over. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 30L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.01)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE) plot(model_history) model %&gt;% evaluate(x, y) ## loss ## 238.9949 pred_keras = predict(model, x) fit = lm(Ozone ~ ., data = data) pred_lm = predict(fit, data) rmse_lm = mean(sqrt((y - pred_lm)^2)) rmse_keras = mean(sqrt((y - pred_keras)^2)) print(rmse_lm) ## [1] 14.78897 print(rmse_keras) ## [1] 11.58784 You can see, that for the lower learning rate, the optimum (compared to the run with learning rate 0.05) is not yet reached (to few epochs have gone by). But also here, mind the overfitting problem. For too many epochs, things might get worse! Task This task is about the airquality example, go through the code line by line and try to understand it. Note, this is TensorFlow intermingled with Keras. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] # Remove NAs. x = scale(data[,2:6]) y = data[,1] layers = tf$keras$layers model = tf$keras$models$Sequential( c( layers$InputLayer(input_shape = list(5L)), layers$Dense(units = 20L, activation = tf$nn$relu), layers$Dense(units = 20L, activation = tf$nn$relu), layers$Dense(units = 20L, activation = tf$nn$relu), layers$Dense(units = 1L, activation = NULL) # No activation == &quot;linear&quot;. ) ) epochs = 200L optimizer = tf$keras$optimizers$Adamax(0.01) # Stochastic gradient optimization is more efficient # in each optimization step, we use a random subset of the data. get_batch = function(batch_size = 32L){ indices = sample.int(nrow(x), size = batch_size) return(list(bX = x[indices,], bY = y[indices])) } get_batch() # Try out this function. ## $bX ## Solar.R Wind Temp Month Day ## 87 -1.13877323 -0.37654514 0.44147123 -0.1467431 1.1546835 ## 117 0.58361881 -1.83815816 0.33653910 0.5319436 1.0398360 ## 129 -1.01809608 1.56290291 0.65133550 1.2106304 -1.1422676 ## 121 0.44100036 -2.14734553 1.70065685 0.5319436 1.4992262 ## 91 0.74817856 -0.71384046 0.54640337 -0.1467431 1.6140738 ## 137 -1.76410028 0.26993754 -0.71278225 1.2106304 -0.2234871 ## 21 -1.93963068 -0.06735777 -1.97196786 -1.5041165 0.5804458 ## 141 -1.73118833 0.10128988 -0.18812157 1.2106304 0.2359031 ## 78 0.97856221 0.10128988 0.44147123 -0.1467431 0.1210555 ## 15 -1.31430363 0.91642022 -2.07689999 -1.5041165 -0.1086396 ## 38 -0.63412333 -0.06735777 0.44147123 -0.8254298 -1.0274200 ## 49 -1.62148183 -0.20789749 -1.34237505 -0.8254298 0.2359031 ## 123 0.03508631 -1.02302783 1.70065685 0.5319436 1.7289213 ## 136 0.58361881 -1.02302783 -0.08318944 1.2106304 -0.3383347 ## 120 0.19964606 -0.06735777 2.01545325 0.5319436 1.3843787 ## 114 -1.63245248 1.22560759 -0.60785011 0.5319436 0.6952933 ## 145 -1.87380678 -0.20789749 -0.71278225 1.2106304 0.6952933 ## 140 0.43002971 1.08506788 -1.13251078 1.2106304 0.1210555 ## 64 0.56167751 -0.20789749 0.33653910 -0.1467431 -1.4868103 ## 118 0.33129386 -0.54519280 0.86119977 0.5319436 1.1546835 ## 128 -0.98518413 -0.71384046 0.96613190 1.2106304 -1.2571152 ## 62 0.92370896 -1.64140257 0.65133550 -0.1467431 -1.7165054 ## 125 0.13382216 -1.36032314 1.49079258 1.2106304 -1.6016578 ## 4 1.40641756 0.43858520 -1.65717146 -1.5041165 -1.3719627 ## 79 1.09923936 -1.02302783 0.65133550 -0.1467431 0.2359031 ## 82 -1.95060133 -0.85438017 -0.39798584 -0.1467431 0.5804458 ## 149 0.08993956 -0.85438017 -0.81771438 1.2106304 1.1546835 ## 17 1.34059366 0.57912491 -1.23744292 -1.5041165 0.1210555 ## 48 1.08826871 3.02451593 -0.60785011 -0.8254298 0.1210555 ## 130 0.73720791 0.26993754 0.23160696 1.2106304 -1.0274200 ## 132 0.49585361 0.26993754 -0.29305371 1.2106304 -0.7977249 ## 30 0.41905906 -1.19167548 0.12667483 -1.5041165 1.6140738 ## ## $bY ## [1] 20 168 32 118 64 9 1 13 35 18 29 20 85 28 76 9 23 18 32 73 47 135 78 18 61 16 30 34 37 20 ## [31] 21 115 steps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch. for(i in 1:steps){ # Get data. batch = get_batch() # Transform it into tensors. bX = tf$constant(batch$bX) bY = tf$constant(matrix(batch$bY, ncol = 1L)) # Automatic differentiation: # Record computations with respect to our model variables. with(tf$GradientTape() %as% tape, { pred = model(bX) # We record the operation for our model weights. loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred)) } ) # Calculate the gradients for our model$weights at the loss / backpropagation. gradients = tape$gradient(loss, model$weights) # Update our model weights with the learning rate specified above. optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) if(! i%%30){ cat(&quot;Loss: &quot;, loss$numpy(), &quot;\\n&quot;) # Print loss every 30 steps (not epochs!). } } ## Loss: 645.1247 ## Loss: 257.9622 ## Loss: 257.9248 ## Loss: 424.3474 ## Loss: 132.1914 ## Loss: 201.8619 ## Loss: 225.3891 ## Loss: 111.7508 ## Loss: 343.3166 ## Loss: 255.3797 ## Loss: 245.1779 ## Loss: 227.4516 ## Loss: 222.4553 ## Loss: 348.0878 ## Loss: 365.9766 ## Loss: 178.8896 ## Loss: 220.2557 ## Loss: 344.3786 ## Loss: 238.2619 ## Loss: 324.3968 Now change the code from above for the iris data set. Tip: In tf$keras$losses$… you can find various loss functions. Solution library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. x = scale(iris[,1:4]) y = iris[,5] y = keras::to_categorical(as.integer(Y)-1L, 3) layers = tf$keras$layers model = tf$keras$models$Sequential( c( layers$InputLayer(input_shape = list(4L)), layers$Dense(units = 20L, activation = tf$nn$relu), layers$Dense(units = 20L, activation = tf$nn$relu), layers$Dense(units = 20L, activation = tf$nn$relu), layers$Dense(units = 3L, activation = tf$nn$softmax) ) ) epochs = 200L optimizer = tf$keras$optimizers$Adamax(0.01) # Stochastic gradient optimization is more efficient. get_batch = function(batch_size = 32L){ indices = sample.int(nrow(x), size = batch_size) return(list(bX = x[indices,], bY = y[indices,])) } steps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch. for(i in 1:steps){ batch = get_batch() bX = tf$constant(batch$bX) bY = tf$constant(batch$bY) # Automatic differentiation. with(tf$GradientTape() %as% tape, { pred = model(bX) # we record the operation for our model weights loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred)) } ) # Calculate the gradients for the loss at our model$weights / backpropagation. gradients = tape$gradient(loss, model$weights) # Update our model weights with the learning rate specified above. optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) if(! i%%30){ cat(&quot;Loss: &quot;, loss$numpy(), &quot;\\n&quot;) # Print loss every 30 steps (not epochs!). } } ## Loss: 0.002592293 ## Loss: 0.0004166076 ## Loss: 0.0005878588 ## Loss: 0.0001814875 ## Loss: 0.0003771908 ## Loss: 0.0006105488 ## Loss: 0.0003895137 ## Loss: 0.0001911997 ## Loss: 0.0002297242 ## Loss: 0.0001141025 ## Loss: 0.0002618307 ## Loss: 0.0001288063 ## Loss: 5.751499e-05 ## Loss: 0.0002563585 ## Loss: 0.0002148754 ## Loss: 0.0001434369 ## Loss: 0.0001919963 ## Loss: 0.0001954481 ## Loss: 7.472201e-05 ## Loss: 2.273075e-05 ## Loss: 0.0001157298 ## Loss: 2.059802e-05 ## Loss: 7.065437e-05 ## Loss: 1.294948e-05 ## Loss: 6.738135e-05 ## Loss: 2.542896e-05 Remarks: Mind the different input and output layer numbers. The loss function increases randomly, because different subsets of the data were drawn. This is a downside of stochastic gradient descent. A positive thing about stochastic gradient descent is, that local valleys or hills may be left and global ones can be found instead. "],["fundamental.html", "4 Fundamental Principles and Techniques 4.1 Machine Learning Principles 4.2 Artificial Neural Networks 4.3 Tree-based Machine Learning Algorithms 4.4 Distance-based Algorithms 4.5 The Standard Machine Learning Pipeline at the Eexample of the Titanic Data set 4.6 Bonus - Machine Learning Pipelines with mlr3", " 4 Fundamental Principles and Techniques 4.1 Machine Learning Principles 4.1.1 Optimization Out of Wikipedia: “An optimization problem is the problem of finding the best solution from all feasible solutions.” Why do we need this “optimization?” Somehow, we need to tell the algorithm what it should learn. To do so we have the so called loss function, which expresses what our goal is. But we also need to find the configurations where the loss function attains its minimum. This is the job of the optimizer. Thus, an optimization consists of: A loss function (e.g. we tell the algorithm in each training step how many observations were misclassified) guides the training of machine learning algorithms. The optimizer, which tries to update the weights of the machine learning algorithms in a way that the loss function is minimized. Calculating the global optimum analytically is a non-trivial problem and thus a bunch of diverse optimization algorithms evolved. Some optimization algorithms are inspired by biological systems e.g. ants, bees or even slimes. These optimizers are explained in the following video, have a look: Questions 4.1.1.1 Small Optimization Example As an easy example for an optimization we can think of a quadratic function: func = function(x){ return(x^2) } This function is so easy, we can randomly probe it and identify the optimum by plotting. set.seed(123) a = rnorm(100) plot(a, func(a)) The minimal value is at \\(x = 0\\) (to be honest, we can calculate this analytically in this simple case). We can also use an optimizer with the optim-function (the first argument is the starting value): set.seed(123) opt = optim(1.0, func, method = &quot;Brent&quot;, lower = -100, upper = 100) print(opt$par) ## [1] -3.552714e-15 opt$par will return the best values found by the optimizer, which is really close to zero :) 4.1.2 Advanced Optimization Example Optimization is also done when fitting a linear regression model. Thereby, we optimize the weights (intercept and slope). Just using lm(y~x) is too simple. We want to do this by hand to also better understand what optimization is and how it works. As an example we take the airquality data set. First, we have to be sure to have no NAs in there. Then we split into response (Ozone) and predictors (Month, Day, Solar.R, Wind, Temp). Additionally it is beneficial for the optimizer, when the different predictors have the same support, and thus we scale them. data = airquality[complete.cases(airquality$Ozone) &amp; complete.cases(airquality$Solar.R),] X = scale(data[,-1]) Y = data$Ozone The model we want to optimize: \\(Ozone = Solar.R \\cdot X1 + Wind \\cdot X2 + Temp \\cdot X3 + Month \\cdot X4 + Day \\cdot X5 + X6\\) We assume that the residuals are normally distributed, and our loss function to be the mean squared error: \\(\\mathrm{mean}(predicted~ozone - true~ozone)^{2}\\) Our task is now to find the parameters \\(X1,\\dots,X6\\) for which this loss function is minimal. Therefore, we implement a function, that takes parameters and returns the loss. linear_regression = function(w){ pred = w[1]*X[,1] + # Solar.R w[2]*X[,2] + # Wind w[3]*X[,3] + # Temp w[4]*X[,4] + # Month w[5]*X[,5] + w[6] # or X * w[1:5]^T + w[6] # loss = MSE, we want to find the optimal weights # to minimize the sum of squared residuals. loss = mean((pred - Y)^2) return(loss) } For example we can sample some weights and see how the loss changes with this weights. set.seed(123) linear_regression(runif(6)) ## [1] 2866.355 We can try to find the optimum by bruteforce (what means we will use a random set of weights and see for which the loss function is minimal): set.seed(123) random_search = matrix(runif(6*5000, -10, 10), 5000, 6) losses = apply(random_search, 1, linear_regression) plot(losses, type = &quot;l&quot;) random_search[which.min(losses),] ## [1] 7.411631 -7.018960 9.376949 6.490659 5.432706 9.460573 In most cases, bruteforce isn’t a good approach. It might work well with only a few parameters, but with increasing complexity and more parameters it will take a long time. Furthermore it is not guaranteed that it finds a stable solution on continuous data. In R the optim function helps computing the optimum faster. opt = optim(runif(6, -1, 1), linear_regression) opt$par ## [1] 1.631666 -17.272902 11.645608 -7.371417 1.754860 42.577956 In the background, mostly some gradient descent methods are used. By comparing the weights from the optimizer to the estimated weights of the lm() function, we see that our self-written code obtains the same weights as the lm. Keep in mind, that our simple method uses random numbers thus the results might differ each run (without setting the seed). coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 4.1.3 Regularization Regularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a machine learning model. The most important distinction is between shrinkage estimators and estimators based on model averaging. Shrikage estimators are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are “shrunk” to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penalty is proportional to the sum of absolute deviations (\\(L1\\) penalty), and the Tikhonov regularization aka Ridge regression, where the penalty is proportional to the sum of squared distances from the reference (\\(L2\\) penalty). Thus, the loss function that we optimize is given by \\[ loss = fit - \\lambda \\cdot d \\] where fit refers to the standard loss function, \\(\\lambda\\) is the strength of the regularization, and \\(d\\) is the chosen metric, e.g. \\(L1\\) or\\(L2\\): \\[ loss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1 \\] \\[ loss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2 \\] \\(\\lambda\\) and possibly d are typically optimized under cross-validation. \\(L1\\) and \\(L2\\) can be also combined what is then called elastic net (see Zou and Hastie (2005)). Model averaging refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it is often more efficient having many simpler models and average them, than one “super model.” The reasons are complicated, and explained in more detail in Dormann et al. (2018). A particular important application of averaging is boosting, where the idea is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap (use random sampling with replacement ) the data, and average the bootstrapped predictions. To see how these techniques work in practice, let’s first focus on LASSO and Ridge regularization for weights in neural networks. We can imagine that the LASSO and Ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunken) to zero. LASSO \\(\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)\\) and Ridge \\(\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)\\) have slightly different properties. They are best understood if we express those as the effective prior preference they create on the parameters: As you can see, the LASSO creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the Ridge. For this reason, LASSO is often more interpreted as a model selection method. The Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero. We can implement the linear regression also in Keras, when we do not specify any hidden layers: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] X = scale(data[,-1]) Y = data$Ozone # L1/L2 on linear model. model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2])) summary(model) ## Model: &quot;sequential_7&quot; ## ______________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================== ## dense_31 (Dense) (None, 1) 6 ## ## ============================================================================================================================== ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## ______________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5), metrics = c(metric_mean_squared_error)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE) unconstrained = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 Torch library(torch) torch_manual_seed(321L) set.seed(123) model_torch = nn_sequential( nn_linear(in_features = dim(X)[2], out_features = 1L) ) opt = optim_adam(params = model_torch$parameters, lr = 0.5) X_torch = torch_tensor(X) Y_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32()) for(i in 1:500){ indices = sample.int(nrow(X), 20L) opt$zero_grad() pred = model_torch(X_torch[indices, ]) loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE]) loss$sum()$backward() opt$step() } coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 model_torch$parameters ## $`0.weight` ## torch_tensor ## 4.1083 -10.1831 18.2815 -4.3478 1.2937 ## [ CPUFloatType{1,5} ][ requires_grad = TRUE ] ## ## $`0.bias` ## torch_tensor ## 42.0958 ## [ CPUFloatType{1} ][ requires_grad = TRUE ] TensorFlow and thus Keras also allow use using LASSO and Ridge on the weights. Lets see what happens when we put an \\(L1\\) (LASSO) regularization on the weights: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% # Remind the penalty lambda that is set to 10 here. layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(X)[2]), kernel_regularizer = regularizer_l1(10), bias_regularizer = regularizer_l1(10)) summary(model) ## Model: &quot;sequential_8&quot; ## ______________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================== ## dense_32 (Dense) (None, 1) 6 ## ## ============================================================================================================================== ## Total params: 6 ## Trainable params: 6 ## Non-trainable params: 0 ## ______________________________________________________________________________________________________________________________ model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5), metrics = c(metric_mean_squared_error)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) l1 = model$get_weights() summary(lm(Y~X)) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## XSolar.R 4.583 2.135 2.147 0.0341 * ## XWind -11.806 2.293 -5.149 1.23e-06 *** ## XTemp 18.067 2.610 6.922 3.66e-10 *** ## XMonth -4.479 2.230 -2.009 0.0471 * ## XDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 cbind(unlist(l1), unlist(unconstrained)) ## [,1] [,2] ## [1,] 1.69559026 4.576884 ## [2,] -8.43734646 -11.771938 ## [3,] 12.91758442 18.096060 ## [4,] 0.01775982 -4.407187 ## [5,] 0.01594419 2.432310 ## [6,] 33.05084229 42.205029 One can clearly see that parameters are pulled towards zero because of the regularization. Torch In Torch, we have to specify the regularization on our own when calculating the loss. library(torch) torch_manual_seed(321L) set.seed(123) model_torch = nn_sequential( nn_linear(in_features = dim(X)[2], out_features = 1L) ) opt = optim_adam(params = model_torch$parameters, lr = 0.5) X_torch = torch_tensor(X) Y_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32()) for(i in 1:500){ indices = sample.int(nrow(X), 20L) opt$zero_grad() pred = model_torch(X_torch[indices, ]) loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE]) # Add L1: for(i in 1:length(model_torch$parameters)){ # Remind the penalty lambda that is set to 10 here. loss = loss + model_torch$parameters[[i]]$abs()$sum()*10.0 } loss$sum()$backward() opt$step() } coef(lm(Y~X)) ## (Intercept) XSolar.R XWind XTemp XMonth XDay ## 42.099099 4.582620 -11.806072 18.066786 -4.479175 2.384705 model_torch$parameters ## $`0.weight` ## torch_tensor ## 0.7787 -6.5937 14.6648 -0.0670 0.0475 ## [ CPUFloatType{1,5} ][ requires_grad = TRUE ] ## ## $`0.bias` ## torch_tensor ## 37.2661 ## [ CPUFloatType{1} ][ requires_grad = TRUE ] Tasks In our linear regression model for the airquality dataset we only had to optimize 6 parameters/weights... ... in neural networks, we have to optimize thousands, up to billions of weights. You can surlely imagine that the surface of the loss function is very complex, with many local optima. Finding a global optimum is almost impossible, most of the times we are only searching for good local optima. The learning rate of the optimizers defines the step size in which the weights of the neural network are updated: Too high learning rates and optima might be jumped over. Too low learning rates and we might be land in local optima or it might take too long. Below, there is the code for the iris dataset. Try out different learning rates, very high and very low learning rates, can you see a difference? library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. iris = datasets::iris X = scale(iris[,1:4]) Y = iris[,5] Y = keras::k_one_hot(as.integer(Y)-1L, 3) model = keras_model_sequential() model %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;, input_shape = list(4L)) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 20L) %&gt;% layer_dense(units = 3L, activation = &quot;softmax&quot;) # Softmax scales to (0, 1]; 3 output nodes for 3 response classes/labels. # The labels MUST start at 0! model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.5), metrics = c(metric_categorical_accuracy) ) model_history = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.04896358 0.98000002 plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Solution keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.005), metrics = c(metric_categorical_accuracy) ) model_history2 = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.04000899 0.98666668 plot(model_history2) ########## -&gt; (very) Low learning rate: May take (very) long # (and may need very many epochs) and get stuck in local optima. keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.00001), metrics = c(metric_categorical_accuracy) ) model_history3 = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.03999748 0.98666668 plot(model_history3) keras::reset_states(model) # Try higher epoch number model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.00001), metrics = c(metric_categorical_accuracy) ) model_history4 = model %&gt;% fit(x = X, y = Y, epochs = 200L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.03996202 0.98666668 plot(model_history4) ########## -&gt; (very) High learning rate (may skip optimum). keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 3), metrics = c(metric_categorical_accuracy) ) model_history5 = model %&gt;% fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 3.2687612 0.9666666 plot(model_history5) ########## -&gt; Higher epoch number (possibly better fitting, maybe overfitting). keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 3), metrics = c(metric_categorical_accuracy) ) model_history6 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 5.2018671 0.9066667 plot(model_history6) ########## keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.5), metrics = c(metric_categorical_accuracy) ) model_history7 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 23.2035313 0.7733333 plot(model_history7) ########## keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.00001), metrics = c(metric_categorical_accuracy) ) model_history8 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 20.6392994 0.7733333 plot(model_history8) ########## -&gt; Lower batch size. keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 3), metrics = c(metric_categorical_accuracy) ) model_history9 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 5L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.5272142 0.6666667 plot(model_history9) ########## keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.5), metrics = c(metric_categorical_accuracy) ) model_history10 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 5L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.5486130 0.6666667 plot(model_history10) ########## keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.00001), metrics = c(metric_categorical_accuracy) ) model_history11 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 5L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.5061002 0.6666667 plot(model_history11) ########## -&gt; Higher batch size (faster but less accurate). keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 3), metrics = c(metric_categorical_accuracy) ) model_history12 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 50L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.4643030 0.6666667 plot(model_history12) ########## keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.5), metrics = c(metric_categorical_accuracy) ) model_history13 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 50L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.4621144 0.6666667 plot(model_history13) ########## keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.00001), metrics = c(metric_categorical_accuracy) ) model_history14 = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 50L, shuffle = TRUE) model %&gt;% evaluate(X, Y) ## loss categorical_accuracy ## 0.4621110 0.6666667 plot(model_history14) #################### #################### keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.05), metrics = c(metric_categorical_accuracy) ) model_history15 = model %&gt;% fit(x = X, y = Y, epochs = 150L, batch_size = 50L, shuffle = TRUE) plot(model_history15) ########## -&gt; shuffle = FALSE (some kind of overfitting) keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.05), metrics = c(metric_categorical_accuracy) ) model_history16 = model %&gt;% fit(x = X, y = Y, epochs = 150L, batch_size = 50L, shuffle = FALSE) plot(model_history16) ########## -&gt; shuffle = FALSE + lower batch size keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.05), metrics = c(metric_categorical_accuracy) ) model_history17 = model %&gt;% fit(x = X, y = Y, epochs = 150L, batch_size = 5L, shuffle = FALSE) plot(model_history17) ########## -&gt; shuffle = FALSE + higher batch size # (Many samples are taken at once, so no &quot;hopping&quot; any longer.) keras::reset_states(model) model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer_adamax(learning_rate = 0.05), metrics = c(metric_categorical_accuracy) ) model_history18 = model %&gt;% fit(x = X, y = Y, epochs = 150L, batch_size = 75L, shuffle = FALSE) plot(model_history18) Play around with the parameters on your own! 4.2 Artificial Neural Networks Now, we will come to artificial neural networks (ANNs), for which the topic of regularization is very important. We can specify the regularization in each layer via the kernel_regularization (and/or the bias_regularization) argument. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 Min. :5.000 Min. : 1.0 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 1st Qu.:6.000 1st Qu.: 8.0 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 Median :7.000 Median :16.0 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 Mean :6.993 Mean :15.8 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 Max. :9.000 Max. :31.0 ## NA&#39;s :37 NA&#39;s :7 data = data[complete.cases(data),] # Remove NAs. summary(data) ## Ozone Solar.R Wind Temp Month Day ## Min. : 1.0 Min. : 7.0 Min. : 2.30 Min. :57.00 Min. :5.000 Min. : 1.00 ## 1st Qu.: 18.0 1st Qu.:113.5 1st Qu.: 7.40 1st Qu.:71.00 1st Qu.:6.000 1st Qu.: 9.00 ## Median : 31.0 Median :207.0 Median : 9.70 Median :79.00 Median :7.000 Median :16.00 ## Mean : 42.1 Mean :184.8 Mean : 9.94 Mean :77.79 Mean :7.216 Mean :15.95 ## 3rd Qu.: 62.0 3rd Qu.:255.5 3rd Qu.:11.50 3rd Qu.:84.50 3rd Qu.:9.000 3rd Qu.:22.50 ## Max. :168.0 Max. :334.0 Max. :20.70 Max. :97.00 Max. :9.000 Max. :31.00 X = scale(data[,2:6]) Y = data[,1] model = keras_model_sequential() penalty = 0.1 model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L), kernel_regularizer = regularizer_l1(penalty)) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty) ) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1(penalty)) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;, kernel_regularizer = regularizer_l1(penalty)) model %&gt;% compile( loss = loss_mean_squared_error, keras::optimizer_adamax(learning_rate = 0.1) ) model_history = model %&gt;% fit(x = X, y = matrix(Y, ncol = 1L), epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) weights = lapply(model$weights, function(w) w$numpy() ) fields::image.plot(weights[[1]]) Torch Again, we have to do the regularization on our own: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model_torch = nn_sequential( nn_linear(in_features = dim(X)[2], out_features = 100L), nn_relu(), nn_linear(100L, 100L), nn_relu(), nn_linear(100L, 100L), nn_relu(), nn_linear(100L, 1L), ) opt = optim_adam(params = model_torch$parameters, lr = 0.1) X_torch = torch_tensor(X) Y_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32()) for(i in 1:500){ indices = sample.int(nrow(X), 20L) opt$zero_grad() pred = model_torch(X_torch[indices, ]) loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE]) # Add L1 (only on the &#39;kernel weights&#39;): for(i in seq(1, 8, by = 2)){ loss = loss + model_torch$parameters[[i]]$abs()$sum()*0.1 } loss$sum()$backward() opt$step() } Let’s visualize the first (input) layer: fields::image.plot(as.matrix(model_torch$parameters$`0.weight`)) Additionally to the usual \\(L1\\) and \\(L2\\) regularization there is another regularization: the so called dropout-layer (we will learn about this in more detail later). Task The following code is our working example for the next exercises: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] x = scale(data[,-1]) y = data$Ozone model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(x)[2]), kernel_regularizer = regularizer_l1(10), bias_regularizer = regularizer_l1(10)) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5)) model_history = model %&gt;% fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE) plot(model_history) l1 = model$get_weights() linear = lm(y~x) summary(linear) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.014 -12.284 -3.302 8.454 95.348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.980 21.264 &lt; 2e-16 *** ## xSolar.R 4.583 2.135 2.147 0.0341 * ## xWind -11.806 2.293 -5.149 1.23e-06 *** ## xTemp 18.067 2.610 6.922 3.66e-10 *** ## xMonth -4.479 2.230 -2.009 0.0471 * ## xDay 2.385 2.000 1.192 0.2358 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.86 on 105 degrees of freedom ## Multiple R-squared: 0.6249, Adjusted R-squared: 0.6071 ## F-statistic: 34.99 on 5 and 105 DF, p-value: &lt; 2.2e-16 cat(&quot;Linear: &quot;, round(coef(linear), 3)) ## Linear: 42.099 4.583 -11.806 18.067 -4.479 2.385 # The last parameter is the intercept (first parameter in lm). cat(&quot;L1: &quot;, round(c(unlist(l1)[length(unlist(l1))], unlist(l1)[1:(length(unlist(l1)) - 1 )]), 3)) ## L1: 36.766 1.193 -8.446 13.394 -0.108 0.034 What happens if you change the regularization from \\(L1\\) to \\(L2\\)? Solution library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] x = scale(data[,-1]) y = data$Ozone model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(x)[2]), kernel_regularizer = regularizer_l2(10), bias_regularizer = regularizer_l2(10)) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5)) model_history = model %&gt;% fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE) plot(model_history) l2 = model$get_weights() ## Linear: 42.099 4.583 -11.806 18.067 -4.479 2.385 ## L1: 36.766 1.193 -8.446 13.394 -0.108 0.034 ## L2: 3.756 0.976 -1.721 1.956 0.402 -0.112 Weights 4 and 5 are strongly pushed towards zero with \\(L1\\) regularization, but \\(L2\\) regularization shrinks more in general. Note: The weights are not similar to the linear model! (But often, they keep their sign.) Task Try different regularization strengths, try to push the weights to zero. What is the strategy to push the parameters to zero? Solution Now with less regularization: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] x = scale(data[,-1]) y = data$Ozone model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(x)[2]), kernel_regularizer = regularizer_l1(1), bias_regularizer = regularizer_l1(1)) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5)) model_history = model %&gt;% fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE) plot(model_history) l1_lesser = model$get_weights() ## Linear: 42.099 4.583 -11.806 18.067 -4.479 2.385 ## L1: 36.766 1.193 -8.446 13.394 -0.108 0.034 ## L1, lesser: 41.087 4.164 -11.617 17.043 -3.571 1.986 library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] x = scale(data[,-1]) y = data$Ozone model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(x)[2]), kernel_regularizer = regularizer_l2(1), bias_regularizer = regularizer_l2(1)) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5)) model_history = model %&gt;% fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE) plot(model_history) l2_lesser = model$get_weights() ## Linear: 42.099 4.583 -11.806 18.067 -4.479 2.385 ## L2: 3.756 0.976 -1.721 1.956 0.402 -0.112 ## L2, lesser: 20.999 3.726 -7.585 9.065 -0.065 0.669 And with more regularization: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] x = scale(data[,-1]) y = data$Ozone model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(x)[2]), kernel_regularizer = regularizer_l1(25), bias_regularizer = regularizer_l1(25)) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5)) model_history = model %&gt;% fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE) plot(model_history) l1_higher = model$get_weights() ## Linear: 42.099 4.583 -11.806 18.067 -4.479 2.385 ## L1: 36.766 1.193 -8.446 13.394 -0.108 0.034 ## L1, lesser: 41.087 4.164 -11.617 17.043 -3.571 1.986 ## L1, higher: 29.367 0.112 -3.354 8.732 0.058 0.03 library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] x = scale(data[,-1]) y = data$Ozone model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(x)[2]), kernel_regularizer = regularizer_l2(25), bias_regularizer = regularizer_l2(25)) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5)) model_history = model %&gt;% fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE) plot(model_history) l2_higher = model$get_weights() ## Linear: 42.099 4.583 -11.806 18.067 -4.479 2.385 ## L2: 3.756 0.976 -1.721 1.956 0.402 -0.112 ## L2, lesser: 20.999 3.726 -7.585 9.065 -0.065 0.669 ## L2, higher: 1.564 0.414 -0.797 0.83 0.165 0.048 For pushing weights towards zero, \\(L1\\) regularization is used rather than \\(L2\\). Higher regularization leads to smaller parameters (maybe in combination with smaller learning rates). Play around on your own! Ask questions if you have any. Task Use a combination of \\(L1\\) and a \\(L2\\) regularization (there is a Keras function for this). How is this kind of regularization called and what is the advantage of this approach? Solution library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality[complete.cases(airquality),] x = scale(data[,-1]) y = data$Ozone model = keras_model_sequential() model %&gt;% layer_dense(units = 1L, activation = &quot;linear&quot;, input_shape = list(dim(x)[2]), kernel_regularizer = regularizer_l1_l2(10), bias_regularizer = regularizer_l1_l2(10)) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5)) model_history = model %&gt;% fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE) plot(model_history) This kind of regularization is called Elastic net. It is a combination of LASSO (\\(L1\\)) and Ridge. It is more flexible than \\(L1\\) and less flexible than \\(L2\\) and higher computational cost. Elastic net does shrinkage as well, but does not separate highly correlated parameters out that much. Task In Keras you can tell the model to keep a specific percentage of the data as holdout (validation_split argument in the fit function): library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] x = scale(data[,2:6]) y = data[,1] model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 100L) %&gt;% layer_dense(units = 100L) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 50L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) Run the code and view the loss for the train and the validation (test) set in the viewer panel. What happens with the validation loss? Why? Solution The training loss keeps decreasing, but the validation loss increases after a time. This increase in validation loss is due to overfitting. Task Add \\(L1\\) / \\(L2\\) regularization to the neural network above and try to keep the test loss close to the training loss. Try a little higher epoch number! Explain the strategy that helps to achieve this. Solution Adding regularization (\\(L1\\) and \\(L2\\)): library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] x = scale(data[,2:6]) y = data[,1] model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5), bias_regularizer = regularizer_l1(2)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5), bias_regularizer = regularizer_l1(2)) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) Adding higher regularization (\\(L1\\) and \\(L2\\)): library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] x = scale(data[,2:6]) y = data[,1] model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(15), bias_regularizer = regularizer_l1(2)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(15), bias_regularizer = regularizer_l1(2)) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) Keep in mind: With higher regularization, both the training and the validation loss keep relatively high! Adding normal regularization (\\(L1\\) and \\(L2\\)) and use a larger learning rate: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] x = scale(data[,2:6]) y = data[,1] model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5), bias_regularizer = regularizer_l1(2)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5), bias_regularizer = regularizer_l1(2)) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.2)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) Adding normal regularization (\\(L1\\) and \\(L2\\)) and use a very low learning rate: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] x = scale(data[,2:6]) y = data[,1] model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5), bias_regularizer = regularizer_l1(2)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5), bias_regularizer = regularizer_l1(2)) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) Look at the constantly lower validation loss. Adding low regularization (\\(L1\\) and \\(L2\\)) and use a very low learning rate: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] x = scale(data[,2:6]) y = data[,1] model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5), bias_regularizer = regularizer_l1(2)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5), bias_regularizer = regularizer_l1(2)) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L, shuffle = TRUE, validation_split = 0.2) plot(model_history) This was a good example for early stopping. Taking the example from above, but without shuffling: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = airquality data = data[complete.cases(data),] x = scale(data[,2:6]) y = data[,1] model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = list(5L)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5), bias_regularizer = regularizer_l1(2)) %&gt;% layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5), bias_regularizer = regularizer_l1(2)) %&gt;% # One output dimension with a linear activation function. layer_dense(units = 1L, activation = &quot;linear&quot;) model %&gt;% compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001)) model_history = model %&gt;% fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L, shuffle = FALSE, validation_split = 0.2) plot(model_history) Play around with regularization kind (\\(L1\\), \\(L2\\), \\(L1,L2\\)) strength and the learning rate also on your own! 4.3 Tree-based Machine Learning Algorithms Famous machine learning algorithms such as Random Forest and Gradient Boosted trees are based on classification and regression trees. Hint: Tree-based algorithms are not distance based and thus do not need scaling. If you want to know a little bit more about the concepts described in the following - like decision, classification and regression trees as well as random forests - you might watch the following videos: Decision trees Regression trees Random forests Pruning trees After watching these videos, you should know what the different hyperparameters are doing and how to prevent trees / forests from doing something you don’t want. 4.3.1 Classification and Regression Trees Tree-based models in general use a series of if-then rules to generate predictions from one or more decision trees. In this lecture, we will explore regression and classification trees by the example of the airquality data set. There is one important hyperparameter for regression trees: “minsplit.” It controls the depth of tree (see the help of rpart for a description). It controls the complexity of the tree and can thus also be seen as a regularization parameter. We first prepare and visualize the data and afterwards fit a decision tree. library(rpart) library(rpart.plot) data = airquality[complete.cases(airquality),] Fit and visualize one(!) regression tree: rt = rpart(Ozone~., data = data, control = rpart.control(minsplit = 10)) rpart.plot(rt) Visualize the predictions: pred = predict(rt, data) plot(data$Temp, data$Ozone) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) The angular form of the prediction line is typical for regression trees and is a weakness of it. 4.3.2 Random Forest To overcome this weakness, a random forest uses an ensemble of regression/classification trees. Thus, the random forest is in principle nothing else than a normal regression/classification tree, but it uses the idea of the “wisdom of the crowd” : By asking many people (regression/classification trees) one can make a more informed decision (prediction/classification). When you want to buy a new phone for example you also wouldn’t go directly into the shop, but search in the internet and ask your friends and family. There are two randomization steps with the random forest that are responsible for their success: Bootstrap samples for each tree (we will sample observations with replacement from the data set. For the phone this is like not everyone has experience about each phone). At each split, we will sample a subset of predictors that is then considered as potential splitting criterion (for the phone this is like that not everyone has the same decision criteria). Annotation: While building a decision tree (random forests consist of many decision trees), one splits the data at some point according to their features. For example if you have females and males, big and small people in a crowd, you con split this crowd by gender and then by size or by size and then by gender to build a decision tree. Applying the random forest follows the same principle as for the methods before: We visualize the data (we have already done this so often for the airquality data set, thus we skip it here), fit the algorithm and then plot the outcomes. Fit a random forest and visualize the predictions: library(randomForest) set.seed(123) data = airquality[complete.cases(airquality),] rf = randomForest(Ozone~., data = data) pred = predict(rf, data) plot(Ozone~Temp, data = data) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) One advantage of random forests is that we will get an importance of variables. At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Thus the variable importance shows us how important a variable is averaged over all trees. rf$importance ## IncNodePurity ## Solar.R 17969.59 ## Wind 31978.36 ## Temp 34176.71 ## Month 10753.73 ## Day 15436.47 There are several important hyperparameters in a random forest that we can tune to get better results: Similar to the minsplit parameter in regression and classification trees, the hyperparameter “nodesize” controls for complexity \\(\\rightarrow\\) Minimum size of terminal nodes in the tree. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5). mtry: Number of features randomly sampled as candidates at each split. 4.3.3 Boosted Regression Trees Random forests fit hundreds of trees independent of each other. Here, the idea of a boosted regression tree comes in. Maybe we could learn from the errors the previous weak learners made and thus enhance the performance of the algorithm. A boosted regression tree (BRT) starts with a simple regression tree (weak learner) and then sequentially fits additional trees to improve the results. There are two different strategies to do so: AdaBoost: Wrong classified observations (by the previous tree) will get a higher weight and therefore the next trees will focus on difficult/missclassified observations. Gradient boosting (state of the art): Each sequential model will be fit on the residual errors of the previous model. We can fit a boosted regression tree using xgboost, but before we have to transform the data into a xgb.Dmatrix. library(xgboost) set.seed(123) data = airquality[complete.cases(airquality),] data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) brt = xgboost(data_xg, nrounds = 16L) ## [1] train-rmse:39.724628 ## [2] train-rmse:30.225760 ## [3] train-rmse:23.134838 ## [4] train-rmse:17.899178 ## [5] train-rmse:14.097786 ## [6] train-rmse:11.375458 ## [7] train-rmse:9.391275 ## [8] train-rmse:7.889689 ## [9] train-rmse:6.646585 ## [10] train-rmse:5.804860 ## [11] train-rmse:5.128437 ## [12] train-rmse:4.456416 ## [13] train-rmse:4.069464 ## [14] train-rmse:3.674615 ## [15] train-rmse:3.424578 ## [16] train-rmse:3.191302 The parameter “nrounds” controls how many sequential trees we fit, in our example this was 16. When we predict on new data, we can limit the number of trees used to prevent overfitting (remember: each new tree tries to improve the predictions of the previous trees). Let us visualize the predictions for different numbers of trees: oldpar = par(mfrow = c(2, 2)) for(i in 1:4){ pred = predict(brt, newdata = data_xg, ntreelimit = i) plot(data$Temp, data$Ozone, main = i) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) } ## [11:10:48] WARNING: amalgamation/../src/c_api/c_api.cc:718: `ntree_limit` is deprecated, use `iteration_range` instead. ## [11:10:48] WARNING: amalgamation/../src/c_api/c_api.cc:718: `ntree_limit` is deprecated, use `iteration_range` instead. ## [11:10:48] WARNING: amalgamation/../src/c_api/c_api.cc:718: `ntree_limit` is deprecated, use `iteration_range` instead. ## [11:10:48] WARNING: amalgamation/../src/c_api/c_api.cc:718: `ntree_limit` is deprecated, use `iteration_range` instead. par(oldpar) There are also other ways to control for complexity of the boosted regression tree algorithm: max_depth: Maximum depth of each tree. shrinkage (each tree will get a weight and the weight will decrease with the number of trees). When having specified the final model, we can obtain the importance of the variables like for random forests: xgboost::xgb.importance(model = brt) ## Feature Gain Cover Frequency ## 1: Temp 0.570071903 0.2958229 0.24836601 ## 2: Wind 0.348230710 0.3419576 0.24183007 ## 3: Solar.R 0.058795542 0.1571072 0.30718954 ## 4: Day 0.019529993 0.1779925 0.16993464 ## 5: Month 0.003371853 0.0271197 0.03267974 sqrt(mean((data$Ozone - pred)^2)) # RMSE ## [1] 17.89918 data_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone) One important strength of xgboost is that we can directly do a cross-validation (which is independent of the boosted regression tree itself!) and specify its properties with the parameter “n-fold”: set.seed(123) brt = xgboost(data_xg, nrounds = 5L) ## [1] train-rmse:39.724625 ## [2] train-rmse:30.225765 ## [3] train-rmse:23.134838 ## [4] train-rmse:17.899179 ## [5] train-rmse:14.097784 brt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L, nrounds = 3L, nthreads = 4L) ## [1] train-rmse:39.895106+2.127352 test-rmse:40.685477+5.745326 ## [2] train-rmse:30.367660+1.728788 test-rmse:32.255812+5.572961 ## [3] train-rmse:23.446236+1.366759 test-rmse:27.282436+5.746244 print(brt_cv) ## ##### xgb.cv 3-folds ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 1 39.89511 2.127352 40.68548 5.745326 ## 2 30.36766 1.728788 32.25581 5.572961 ## 3 23.44624 1.366759 27.28244 5.746244 Annotation: The original data set is randomly partitioned into \\(n\\) equal sized subsamples. Each time, the model is trained on \\(n - 1\\) subsets (training set) and tested on the left out set (test set) to judge the performance. If we do three-folded cross-validation, we actually fit three different boosted regression tree models (xgboost models) on \\(\\approx 67\\%\\) of the data points. Afterwards, we judge the performance on the respective holdout. This now tells us how well the model performed. Task We will use the following code snippet to see the influence of mincut on trees. library(tree) set.seed(123) data = airquality rt = tree(Ozone~., data = data, control = tree.control(mincut = 1L, nobs = nrow(data))) plot(rt) text(rt) pred = predict(rt, data) plot(data$Temp, data$Ozone) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) sqrt(mean((data$Ozone - pred)^2)) # RMSE Try different mincut parameters and see what happens. (Compare the root mean squared error for different mincut parameters and explain what you see. Compare predictions for different mincut parameters and explain what happens.) What was wrong in the snippet above? Solution library(tree) set.seed(123) data = airquality[complete.cases(airquality),] doTask = function(mincut){ rt = tree(Ozone~., data = data, control = tree.control(mincut = mincut, nobs = nrow(data))) pred = predict(rt, data) plot(data$Temp, data$Ozone, main = paste0( &quot;mincut: &quot;, mincut, &quot;\\nRMSE: &quot;, round(sqrt(mean((data$Ozone - pred)^2)), 2) ) ) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) } for(i in c(1, 2, 3, 5, 10, 15, 25, 50, 54, 55, 56, 57, 75, 100)){ doTask(i) } Approximately at mincut = 15, prediction is the best (mind overfitting). After mincut = 56, the prediction has no information at all and the RMSE stays constant. Mind the complete cases of the airquality data set, that was the error. Task We will use the following code snippet to explore a random forest: library(randomForest) set.seed(123) airquality[complete.cases(airquality),] ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 ## 9 8 19 20.1 61 5 9 ## 12 16 256 9.7 69 5 12 ## 13 11 290 9.2 66 5 13 ## 14 14 274 10.9 68 5 14 ## 15 18 65 13.2 58 5 15 ## 16 14 334 11.5 64 5 16 ## 17 34 307 12.0 66 5 17 ## 18 6 78 18.4 57 5 18 ## 19 30 322 11.5 68 5 19 ## 20 11 44 9.7 62 5 20 ## 21 1 8 9.7 59 5 21 ## 22 11 320 16.6 73 5 22 ## 23 4 25 9.7 61 5 23 ## 24 32 92 12.0 61 5 24 ## 28 23 13 12.0 67 5 28 ## 29 45 252 14.9 81 5 29 ## 30 115 223 5.7 79 5 30 ## 31 37 279 7.4 76 5 31 ## 38 29 127 9.7 82 6 7 ## 40 71 291 13.8 90 6 9 ## 41 39 323 11.5 87 6 10 ## 44 23 148 8.0 82 6 13 ## 47 21 191 14.9 77 6 16 ## 48 37 284 20.7 72 6 17 ## 49 20 37 9.2 65 6 18 ## 50 12 120 11.5 73 6 19 ## 51 13 137 10.3 76 6 20 ## 62 135 269 4.1 84 7 1 ## 63 49 248 9.2 85 7 2 ## 64 32 236 9.2 81 7 3 ## 66 64 175 4.6 83 7 5 ## 67 40 314 10.9 83 7 6 ## 68 77 276 5.1 88 7 7 ## 69 97 267 6.3 92 7 8 ## 70 97 272 5.7 92 7 9 ## 71 85 175 7.4 89 7 10 ## 73 10 264 14.3 73 7 12 ## 74 27 175 14.9 81 7 13 ## 76 7 48 14.3 80 7 15 ## 77 48 260 6.9 81 7 16 ## 78 35 274 10.3 82 7 17 ## 79 61 285 6.3 84 7 18 ## 80 79 187 5.1 87 7 19 ## 81 63 220 11.5 85 7 20 ## 82 16 7 6.9 74 7 21 ## 85 80 294 8.6 86 7 24 ## 86 108 223 8.0 85 7 25 ## 87 20 81 8.6 82 7 26 ## 88 52 82 12.0 86 7 27 ## 89 82 213 7.4 88 7 28 ## 90 50 275 7.4 86 7 29 ## 91 64 253 7.4 83 7 30 ## 92 59 254 9.2 81 7 31 ## 93 39 83 6.9 81 8 1 ## 94 9 24 13.8 81 8 2 ## 95 16 77 7.4 82 8 3 ## 99 122 255 4.0 89 8 7 ## 100 89 229 10.3 90 8 8 ## 101 110 207 8.0 90 8 9 ## 104 44 192 11.5 86 8 12 ## 105 28 273 11.5 82 8 13 ## 106 65 157 9.7 80 8 14 ## 108 22 71 10.3 77 8 16 ## 109 59 51 6.3 79 8 17 ## 110 23 115 7.4 76 8 18 ## 111 31 244 10.9 78 8 19 ## 112 44 190 10.3 78 8 20 ## 113 21 259 15.5 77 8 21 ## 114 9 36 14.3 72 8 22 ## 116 45 212 9.7 79 8 24 ## 117 168 238 3.4 81 8 25 ## 118 73 215 8.0 86 8 26 ## 120 76 203 9.7 97 8 28 ## 121 118 225 2.3 94 8 29 ## 122 84 237 6.3 96 8 30 ## 123 85 188 6.3 94 8 31 ## 124 96 167 6.9 91 9 1 ## 125 78 197 5.1 92 9 2 ## 126 73 183 2.8 93 9 3 ## 127 91 189 4.6 93 9 4 ## 128 47 95 7.4 87 9 5 ## 129 32 92 15.5 84 9 6 ## 130 20 252 10.9 80 9 7 ## 131 23 220 10.3 78 9 8 ## 132 21 230 10.9 75 9 9 ## 133 24 259 9.7 73 9 10 ## 134 44 236 14.9 81 9 11 ## 135 21 259 15.5 76 9 12 ## 136 28 238 6.3 77 9 13 ## 137 9 24 10.9 71 9 14 ## 138 13 112 11.5 71 9 15 ## 139 46 237 6.9 78 9 16 ## 140 18 224 13.8 67 9 17 ## 141 13 27 10.3 76 9 18 ## 142 24 238 10.3 68 9 19 ## 143 16 201 8.0 82 9 20 ## 144 13 238 12.6 64 9 21 ## 145 23 14 9.2 71 9 22 ## 146 36 139 10.3 81 9 23 ## 147 7 49 10.3 69 9 24 ## 148 14 20 16.6 63 9 25 ## 149 30 193 6.9 70 9 26 ## 151 14 191 14.3 75 9 28 ## 152 18 131 8.0 76 9 29 ## 153 20 223 11.5 68 9 30 rf = randomForest(Ozone~., data = data) pred = predict(rf, data) importance(rf) ## IncNodePurity ## Solar.R 17969.59 ## Wind 31978.36 ## Temp 34176.71 ## Month 10753.73 ## Day 15436.47 cat(&quot;RMSE: &quot;, sqrt(mean((data$Ozone - pred)^2)), &quot;\\n&quot;) ## RMSE: 9.507848 plot(data$Temp, data$Ozone) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) Try different values for the nodesize and mtry and describe how the predictions depend on these parameters. Solution library(randomForest) set.seed(123) data = airquality[complete.cases(airquality),] for(nodesize in c(1, 5, 15, 50, 100)){ for(mtry in c(1, 3, 5)){ rf = randomForest(Ozone~., data = data, mtry = mtry, nodesize = nodesize) pred = predict(rf, data) plot(data$Temp, data$Ozone, main = paste0( &quot;mtry: &quot;, mtry, &quot; nodesize: &quot;, nodesize, &quot;\\nRMSE: &quot;, round(sqrt(mean((data$Ozone - pred)^2)), 2) ) ) lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = &quot;red&quot;) } } Higher numbers for mtry smooth the prediction curve and yield less overfitting. The same holds for the nodesize. In other words: The bigger the nodesize, the smaller the trees and the more bias/less variance. Task library(xgboost) library(animation) set.seed(123) x1 = seq(-3, 3, length.out = 100) x2 = seq(-3, 3, length.out = 100) x = expand.grid(x1, x2) y = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2)) image(matrix(y, 100, 100), main = &quot;Original image&quot;, axes = FALSE, las = 2) axis(1, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1)) axis(2, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1), las = 2) model = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y), nrounds = 500L, verbose = 0L) pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)), ntreelimit = 10L) saveGIF( { for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){ pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)), ntreelimit = i) image(matrix(pred, 100, 100), main = paste0(&quot;Trees: &quot;, i), axes = FALSE, las = 2) axis(1, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1)) axis(2, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1), las = 2) } }, movie.name = &quot;boosting.gif&quot;, autobrowse = FALSE ) Run the above code and play with different parameters for xgboost (especially with parameters that control the complexity) and describe what you see! Tip: have a look at the boosting.gif. Solution library(xgboost) library(animation) set.seed(123) x1 = seq(-3, 3, length.out = 100) x2 = seq(-3, 3, length.out = 100) x = expand.grid(x1, x2) y = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2)) image(matrix(y, 100, 100), main = &quot;Original image&quot;, axes = FALSE, las = 2) axis(1, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1)) axis(2, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1), las = 2) for(eta in c(.1, .3, .5, .7, .9)){ for(max_depth in c(3, 6, 10, 20)){ model = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y), max_depth = max_depth, eta = eta, nrounds = 500, verbose = 0L) saveGIF( { for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){ pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)), ntreelimit = i) image(matrix(pred, 100, 100), main = paste0(&quot;eta: &quot;, eta, &quot; max_depth: &quot;, max_depth, &quot; Trees: &quot;, i), axes = FALSE, las = 2) axis(1, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1)) axis(2, at = seq(0, 1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1), las = 2) } }, movie.name = paste0(&quot;boosting_&quot;, max_depth, &quot;_&quot;, eta, &quot;.gif&quot;), autobrowse = FALSE ) } } As the possibilities scale extremely, you must vary some parameters on yourself. You may use for example the following parameters: “eta,” “gamma,” “max_depth,” “min_child_weight,” “subsample,” “colsample_bytree,” “num_parallel_tree,” “monotone_constraints” or “interaction_constraints.” Or you look into the documentation: ?xgboost::xgboost Just some examples: Task We implemented a simple boosted regression tree using R just for fun. Go through the code line by line and try to understand it. Ask, if you have any questions you cannot solve. ... library(tree) set.seed(123) depth = 1L #### Simulate Data x = runif(1000, -5, 5) y = x * sin(x) * 2 + rnorm(1000, 0, cos(x) + 1.8) data = data.frame(x = x, y = y) plot(y~x) #### Helper function for single tree fit. get_model = function(x, y){ control = tree.control(nobs = length(x), mincut = 20L) model = tree(y~x, data.frame(x = x, y = y), control = control) pred = predict(model, data.frame(x = x, y = y)) return(list(model = model, pred = pred)) } #### Boost function. get_boosting_model = function(depth){ pred = NULL m_list = list() for(i in 1:depth){ if(i == 1){ m = get_model(x, y) pred = m$pred }else{ y_res = y - pred m = get_model(x, y_res) pred = pred + m$pred } m_list[[i]] = m$model } model_list &lt;&lt;- m_list # This writes outside function scope! return(pred) } ### Main. pred = get_boosting_model(10L)[order(data$x)] length(model_list) ## [1] 10 plot(model_list[[1]]) plot(y~x) lines(x = data$x[order(data$x)], get_boosting_model(1L)[order(data$x)], col = &#39;red&#39;, lwd = 2) lines(x = data$x[order(data$x)], get_boosting_model(100L)[order(data$x)], col = &#39;green&#39;, lwd = 2) 4.4 Distance-based Algorithms In this chapter, we introduce support-vector machines (SVMs) and other distance-based methods Hint: Distance-based models need scaling! 4.4.1 K-Nearest-Neighbor K-nearest-neighbor (kNN) is a simple algorithm that stores all the available cases and classifies the new data based on a similarity measure. It is mostly used to classify a data point based on how its \\(k\\) nearest neighbors are classified. Let us first see an example: x = scale(iris[,1:4]) y = iris[,5] plot(x[-100,1], x[-100, 3], col = y) points(x[100,1], x[100, 3], col = &quot;blue&quot;, pch = 18, cex = 1.3) Which class would you decide for the blue point? What are the classes of the nearest points? Well, this procedure is used by the k-nearest-neighbors classifier and thus there is actually no “real” learning in a k-nearest-neighbors classification. For applying a k-nearest-neighbors classification, we first have to scale the data set, because we deal with distances and want the same influence of all predictors. Imagine one variable has values from -10.000 to 10.000 and another from -1 to 1. Then the influence of the first variable on the distance to the other points is much stronger than the influence of the second variable. On the iris data set, we have to split the data into training and test set on our own. Then we will follow the usual pipeline. data = iris data[,1:4] = apply(data[,1:4],2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] Fit model and create predictions: library(kknn) set.seed(123) knn = kknn(Species~., train = train, test = test) summary(knn) ## ## Call: ## kknn(formula = Species ~ ., train = train, test = test) ## ## Response: &quot;nominal&quot; ## fit prob.setosa prob.versicolor prob.virginica ## 1 setosa 1 0.00000000 0.0000000 ## 2 setosa 1 0.00000000 0.0000000 ## 3 setosa 1 0.00000000 0.0000000 ## 4 setosa 1 0.00000000 0.0000000 ## 5 setosa 1 0.00000000 0.0000000 ## 6 setosa 1 0.00000000 0.0000000 ## 7 setosa 1 0.00000000 0.0000000 ## 8 setosa 1 0.00000000 0.0000000 ## 9 setosa 1 0.00000000 0.0000000 ## 10 setosa 1 0.00000000 0.0000000 ## 11 setosa 1 0.00000000 0.0000000 ## 12 setosa 1 0.00000000 0.0000000 ## 13 setosa 1 0.00000000 0.0000000 ## 14 versicolor 0 0.86605852 0.1339415 ## 15 versicolor 0 0.74027417 0.2597258 ## 16 versicolor 0 0.91487300 0.0851270 ## 17 versicolor 0 0.98430840 0.0156916 ## 18 versicolor 0 0.91487300 0.0851270 ## 19 versicolor 0 1.00000000 0.0000000 ## 20 versicolor 0 1.00000000 0.0000000 ## 21 versicolor 0 1.00000000 0.0000000 ## 22 versicolor 0 1.00000000 0.0000000 ## 23 versicolor 0 1.00000000 0.0000000 ## 24 versicolor 0 1.00000000 0.0000000 ## 25 versicolor 0 1.00000000 0.0000000 ## 26 versicolor 0 1.00000000 0.0000000 ## 27 versicolor 0 0.86605852 0.1339415 ## 28 versicolor 0 1.00000000 0.0000000 ## 29 virginica 0 0.00000000 1.0000000 ## 30 virginica 0 0.00000000 1.0000000 ## 31 virginica 0 0.00000000 1.0000000 ## 32 virginica 0 0.00000000 1.0000000 ## 33 virginica 0 0.08512700 0.9148730 ## 34 virginica 0 0.22169561 0.7783044 ## 35 virginica 0 0.00000000 1.0000000 ## 36 virginica 0 0.23111986 0.7688801 ## 37 versicolor 0 1.00000000 0.0000000 ## 38 virginica 0 0.04881448 0.9511855 ## 39 versicolor 0 0.64309579 0.3569042 ## 40 versicolor 0 0.67748579 0.3225142 ## 41 virginica 0 0.17288113 0.8271189 ## 42 virginica 0 0.00000000 1.0000000 ## 43 virginica 0 0.00000000 1.0000000 ## 44 virginica 0 0.00000000 1.0000000 ## 45 virginica 0 0.35690421 0.6430958 table(test$Species, fitted(knn)) ## ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 15 0 ## virginica 0 3 14 4.4.2 Support Vector Machines (SVMs) Support vectors machines have a different approach. They try to divide the predictor space into sectors for each class. To do so, a support-vector machine fits the parameters of a hyperplane (a \\(n-1\\) dimensional subspace in a \\(n\\)-dimensional space) in the predictor space by optimizing the distance between the hyperplane and the nearest point from each class. Fitting a support-vector machine: library(e1071) data = iris data[,1:4] = apply(data[,1:4], 2, scale) indices = sample.int(nrow(data), 0.7*nrow(data)) train = data[indices,] test = data[-indices,] sm = svm(Species~., data = train, kernel = &quot;linear&quot;) pred = predict(sm, newdata = test) oldpar = par(mfrow = c(1, 2)) plot(test$Sepal.Length, test$Petal.Length, col = pred, main = &quot;predicted&quot;) plot(test$Sepal.Length, test$Petal.Length, col = test$Species, main = &quot;observed&quot;) par(oldpar) mean(pred == test$Species) # Accuracy. ## [1] 0.9777778 Support-vector machines can only work on linearly separable problems. (A problem is called linearly separable if there exists at least one line in the plane with all of the points of one class on one side of the hyperplane and all the points of the others classes on the other side). If this is not possible, we however, can use the so called kernel trick, which maps the predictor space into a (higher dimensional) space in which the problem is linear separable. After having identified the boundaries in the higher-dimensional space, we can project them back into the original dimensions. x1 = seq(-3, 3, length.out = 100) x2 = seq(-3, 3, length.out = 100) x = expand.grid(x1, x2) y = apply(X, 1, function(t) exp(-t[1]^2 - t[2]^2)) y = ifelse(1/(1+exp(-y)) &lt; 0.62, 0, 1) image(matrix(y, 100, 100)) animation::saveGIF( { for(i in c(&quot;truth&quot;, &quot;linear&quot;, &quot;radial&quot;, &quot;sigmoid&quot;)){ if(i == &quot;truth&quot;){ image(matrix(y, 100,100), main = &quot;Ground truth&quot;, axes = FALSE, las = 2) }else{ sv = e1071::svm(x = x, y = factor(y), kernel = i) image(matrix(as.numeric(as.character(predict(sv, x))), 100, 100), main = paste0(&quot;Kernel: &quot;, i), axes = FALSE, las = 2) axis(1, at = seq(0,1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1)) axis(2, at = seq(0,1, length.out = 10), labels = round(seq(-3, 3, length.out = 10), 1), las = 2) } } }, movie.name = &quot;svm.gif&quot;, autobrowse = FALSE ) As you have seen, this does not work with every kernel. Hence, the problem is to find the actual correct kernel, which is again an optimization procedure and can thus be approximated. Task We will use the Sonar data set to explore support-vector machines and k-neartest-neighbor classifier. library(mlbench) set.seed(123) data(Sonar) data = Sonar indices = sample.int(nrow(Sonar), 0.5 * nrow(Sonar)) Split the Sonar data set from the mlbench library into training- and testset with 50% in each group. Is this a useful split? The response variable is “class.” So you are trying to classify the class. Solution library(mlbench) set.seed(123) data(Sonar) data = Sonar #str(data) # Do not forget scaling! This may be done implicitly by most functions. # Here, it&#39;s done explicitly for teaching purposes. data = cbind.data.frame( scale(data[,-length(data)]), &quot;class&quot; = data[,length(data)] ) n = length(data[,1]) indicesTrain = sample.int(n, (n+1) %/% 2) # Take (at least) 50 % of the data. train = data[indicesTrain,] test = data[-indicesTrain,] labelsTrain = train[,length(train)] labelsTest = test[,length(test)] Until you have strong reasons for that, 50/50 is no really good decision. You waste data/power. Do not forget scaling! Task Fit a standard k-nearest-neighbor classifier and a support vector machine with a linear kernel (check help), and report what fitted better. Solution library(e1071) library(kknn) knn = kknn(class~., train = train, test = test, scale = FALSE, kernel = &quot;rectangular&quot;) predKNN = predict(knn, newdata = test) sm = svm(class~., data = train, scale = FALSE, kernel = &quot;linear&quot;) predSVM = predict(sm, newdata = test) ## K-nearest-neighbor, standard (rectangular) kernel: ## labelsTest ## predKNN M R ## M 46 29 ## R 8 21 ## Correctly classified: 67 / 104 ## Support-vector machine, linear kernel: ## labelsTest ## predSVM M R ## M 41 15 ## R 13 35 ## Correctly classified: 76 / 104 K-nearest neighbor fitted (slightly) better. Task Calculate accuracies of both algorithms. Solution (accKNN = mean(predKNN == labelsTest)) ## [1] 0.6442308 (accSVM = mean(predSVM == labelsTest)) ## [1] 0.7307692 Task Fit again with different kernels and compare accuracies. Solution knn = kknn(class~., train = train, test = test, scale = FALSE, kernel = &quot;optimal&quot;) predKNN = predict(knn, newdata = test) sm = svm(class~., data = train, scale = FALSE, kernel = &quot;radial&quot;) predSVM = predict(sm, newdata = test) (accKNN = mean(predKNN == labelsTest)) ## [1] 0.75 (accSVM = mean(predSVM == labelsTest)) ## [1] 0.8076923 Task Try the fit again with a different seed for training and test set generation. Solution set.seed(42) data = Sonar data = cbind.data.frame( scale(data[,-length(data)]), &quot;class&quot; = data[,length(data)] ) n = length(data[,1]) indicesTrain = sample.int(n, (n+1) %/% 2) train = data[indicesTrain,] test = data[-indicesTrain,] labelsTrain = train[,length(train)] labelsTest = test[,length(test)] ##### knn = kknn(class~., train = train, test = test, scale = FALSE, kernel = &quot;rectangular&quot;) predKNN = predict(knn, newdata = test) sm = svm(class~., data = train, scale = FALSE, kernel = &quot;linear&quot;) predSVM = predict(sm, newdata = test) (accKNN = mean(predKNN == labelsTest)) ## [1] 0.7115385 (accSVM = mean(predSVM == labelsTest)) ## [1] 0.75 ##### knn = kknn(class~., train = train, test = test, scale = FALSE, kernel = &quot;optimal&quot;) predKNN = predict(knn, newdata = test) sm = svm(class~., data = train, scale = FALSE, kernel = &quot;radial&quot;) predSVM = predict(sm, newdata = test) (accKNN = mean(predKNN == labelsTest)) ## [1] 0.8557692 (accSVM = mean(predSVM == labelsTest)) ## [1] 0.8365385 4.5 The Standard Machine Learning Pipeline at the Eexample of the Titanic Data set Before we specialize on any tuning, it is important to understand that machine learning always consists of a pipeline of actions. The typical machine learning workflow consist of: Data cleaning and exploration (EDA = explorative data analysis) for example with tidyverse. Preprocessing and feature selection. Splitting data set into training and test set for evaluation. Model fitting. Model evaluation. New predictions. Here is an (optional) video that explains the entire pipeline from a slightly different perspective: In the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics can be found here : Another good reference is “R for data science” by Hadley Wickham: . For this lecture you need the Titanic data set provided by us. You can find it in GRIPS (datasets.RData in the data set and submission section) or at http://rhsbio6.uni-regensburg.de:8500. We have split the data set already into training and test/prediction data sets (the test/prediction split has one column less than the train split, as the result is not known a priori). 4.5.1 Data Cleaning Load necessary libraries: library(keras) library(tensorflow) library(tidyverse) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. Load data set: library(EcoData) data(titanic_ml) data = titanic_ml Standard summaries: str(data) ## &#39;data.frame&#39;: 1309 obs. of 14 variables: ## $ pclass : int 2 1 3 3 3 3 3 1 3 1 ... ## $ survived : int 1 1 0 0 0 0 0 1 0 1 ... ## $ name : chr &quot;Sinkkonen, Miss. Anna&quot; &quot;Woolner, Mr. Hugh&quot; &quot;Sage, Mr. Douglas Bullen&quot; &quot;Palsson, Master. Paul Folke&quot; ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ... ## $ age : num 30 NA NA 6 30.5 38.5 20 53 NA 42 ... ## $ sibsp : int 0 0 8 3 0 0 0 0 0 0 ... ## $ parch : int 0 0 2 1 0 0 0 0 0 0 ... ## $ ticket : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 221 123 779 542 589 873 472 823 588 834 ... ## $ fare : num 13 35.5 69.55 21.07 8.05 ... ## $ cabin : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 1 94 1 1 1 1 1 1 1 1 ... ## $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 4 4 4 4 4 4 2 4 2 ... ## $ boat : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 3 28 1 1 1 1 1 19 1 15 ... ## $ body : int NA NA NA NA 50 32 NA NA NA NA ... ## $ home.dest: Factor w/ 370 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ... summary(data) ## pclass survived name sex age sibsp parch ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 Min. :0.000 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 Median :0.000 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 Mean :0.385 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 Max. :9.000 ## NA&#39;s :655 NA&#39;s :263 ## ticket fare cabin embarked boat body home.dest ## CA. 2343: 11 Min. : 0.000 :1014 : 2 :823 Min. : 1.0 :564 ## 1601 : 8 1st Qu.: 7.896 C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 New York, NY : 64 ## CA 2144 : 8 Median : 14.454 B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 London : 14 ## 3101295 : 7 Mean : 33.295 G6 : 5 S:914 15 : 37 Mean :160.8 Montreal, PQ : 10 ## 347077 : 7 3rd Qu.: 31.275 B96 B98 : 4 14 : 33 3rd Qu.:256.0 Cornwall / Akron, OH: 9 ## 347082 : 7 Max. :512.329 C22 C26 : 4 4 : 31 Max. :328.0 Paris, France : 9 ## (Other) :1261 NA&#39;s :1 (Other) : 271 (Other):308 NA&#39;s :1188 (Other) :639 head(data) ## pclass survived name sex age sibsp parch ticket fare cabin embarked boat body ## 561 2 1 Sinkkonen, Miss. Anna female 30.0 0 0 250648 13.000 S 10 NA ## 321 1 1 Woolner, Mr. Hugh male NA 0 0 19947 35.500 C52 S D NA ## 1177 3 0 Sage, Mr. Douglas Bullen male NA 8 2 CA. 2343 69.550 S NA ## 1098 3 0 Palsson, Master. Paul Folke male 6.0 3 1 349909 21.075 S NA ## 1252 3 0 Tomlin, Mr. Ernest Portage male 30.5 0 0 364499 8.050 S 50 ## 1170 3 0 Saether, Mr. Simon Sivertsen male 38.5 0 0 SOTON/O.Q. 3101262 7.250 S 32 ## home.dest ## 561 Finland / Washington, DC ## 321 London, England ## 1177 ## 1098 ## 1252 ## 1170 The name variable consists of 1309 unique factors (there are 1309 observations…): length(unique(data$name)) ## [1] 1307 However, there is a title in each name. Let’s extract the titles: We will extract all names and split each name after each comma “,” We will split the second split of the name after a point “.” and extract the titles. first_split = sapply(data$name, function(x) stringr::str_split(x, pattern = &quot;,&quot;)[[1]][2]) titles = sapply(first_split, function(x) strsplit(x, &quot;.&quot;,fixed = TRUE)[[1]][1]) We get 18 unique titles: table(titles) ## titles ## Capt Col Don Dona Dr Jonkheer Lady Major Master ## 1 4 1 1 8 1 1 2 61 ## Miss Mlle Mme Mr Mrs Ms Rev Sir the Countess ## 260 2 1 757 197 2 8 1 1 A few titles have a very low occurrence rate: titles = stringr::str_trim((titles)) titles %&gt;% fct_count() ## # A tibble: 18 × 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Capt 1 ## 2 Col 4 ## 3 Don 1 ## 4 Dona 1 ## 5 Dr 8 ## 6 Jonkheer 1 ## 7 Lady 1 ## 8 Major 2 ## 9 Master 61 ## 10 Miss 260 ## 11 Mlle 2 ## 12 Mme 1 ## 13 Mr 757 ## 14 Mrs 197 ## 15 Ms 2 ## 16 Rev 8 ## 17 Sir 1 ## 18 the Countess 1 We will combine titles with low occurrences into one title, which we can easily do with the forcats package. titles2 = forcats::fct_collapse(titles, officer = c(&quot;Capt&quot;, &quot;Col&quot;, &quot;Major&quot;, &quot;Dr&quot;, &quot;Rev&quot;), royal = c(&quot;Jonkheer&quot;, &quot;Don&quot;, &quot;Sir&quot;, &quot;the Countess&quot;, &quot;Dona&quot;, &quot;Lady&quot;), miss = c(&quot;Miss&quot;, &quot;Mlle&quot;), mrs = c(&quot;Mrs&quot;, &quot;Mme&quot;, &quot;Ms&quot;) ) We can count titles again to see the new number of titles: titles2 %&gt;% fct_count() ## # A tibble: 6 × 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 officer 23 ## 2 royal 6 ## 3 Master 61 ## 4 miss 262 ## 5 mrs 200 ## 6 Mr 757 Add new title variable to data set: data = data %&gt;% mutate(title = titles2) As a second example, we will explore and clean the numeric “age” variable. Explore the variable: summary(data) ## pclass survived name sex age sibsp parch ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 Min. :0.000 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 Median :0.000 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 Mean :0.385 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 Max. :9.000 ## NA&#39;s :655 NA&#39;s :263 ## ticket fare cabin embarked boat body home.dest ## CA. 2343: 11 Min. : 0.000 :1014 : 2 :823 Min. : 1.0 :564 ## 1601 : 8 1st Qu.: 7.896 C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 New York, NY : 64 ## CA 2144 : 8 Median : 14.454 B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 London : 14 ## 3101295 : 7 Mean : 33.295 G6 : 5 S:914 15 : 37 Mean :160.8 Montreal, PQ : 10 ## 347077 : 7 3rd Qu.: 31.275 B96 B98 : 4 14 : 33 3rd Qu.:256.0 Cornwall / Akron, OH: 9 ## 347082 : 7 Max. :512.329 C22 C26 : 4 4 : 31 Max. :328.0 Paris, France : 9 ## (Other) :1261 NA&#39;s :1 (Other) : 271 (Other):308 NA&#39;s :1188 (Other) :639 ## title ## officer: 23 ## royal : 6 ## Master : 61 ## miss :262 ## mrs :200 ## Mr :757 ## sum(is.na(data$age)) / nrow(data) ## [1] 0.2009167 20% NAs! Either we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups. In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age….)) will be done within the specified groups. data = data %&gt;% group_by(sex, pclass, title) %&gt;% mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;% mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %&gt;% ungroup() 4.5.2 Preprocessing and Feature Selection We want to use Keras in our example, but it cannot handle factors and requires the data to be scaled. Normally, one would do this for all predictors, but as we only show the pipeline here, we have sub-selected a bunch of predictors and do this only for them. We first scale the numeric predictors and change the factors with only two groups/levels into integers (this can be handled by Keras). data_sub = data %&gt;% select(survived, sex, age2, fare2, title, pclass) %&gt;% mutate(age2 = scales::rescale(age2, c(0, 1)), fare2 = scales::rescale(fare2, c(0, 1))) %&gt;% mutate(sex = as.integer(sex) - 1L, title = as.integer(title) - 1L, pclass = as.integer(pclass - 1L)) Factors with more than two levels should be one hot encoded (Make columns for every different factor level and write 1 in the respective column for every taken feature value and 0 else. For example: \\(\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}\\)): one_title = k_one_hot(data_sub$title, length(unique(data$title)))$numpy() colnames(one_title) = levels(data$title) one_sex = k_one_hot(data_sub$sex, length(unique(data$sex)))$numpy() colnames(one_sex) = levels(data$sex) one_pclass = k_one_hot(data_sub$pclass, length(unique(data$pclass)))$numpy() colnames(one_pclass) = paste0(1:length(unique(data$pclass)), &quot;pclass&quot;) And we have to add the dummy encoded variables to the data set: data_sub = cbind(data.frame(survived= data_sub$survived), one_title, one_sex, age = data_sub$age2, fare = data_sub$fare2, one_pclass) head(data_sub) ## survived officer royal Master miss mrs Mr female male age fare 1pclass 2pclass 3pclass ## 1 1 0 0 0 1 0 0 1 0 0.37369494 0.02537431 0 1 0 ## 2 1 0 0 0 0 0 1 0 1 0.51774510 0.06929139 1 0 0 ## 3 0 0 0 0 0 0 1 0 1 0.32359053 0.13575256 0 0 1 ## 4 0 0 0 1 0 0 0 0 1 0.07306851 0.04113566 0 0 1 ## 5 0 0 0 0 0 0 1 0 1 0.37995799 0.01571255 0 0 1 ## 6 0 0 0 0 0 0 1 0 1 0.48016680 0.01415106 0 0 1 4.5.3 Split Data for Training and Testing The splitting consists of two splits: An outer split (the original split, remember we got a training and test split without the response “survived”). An inner split (we will split the training data set further into another training and test split with known response). The inner split is important to assess the model’s performance and potential overfitting. Outer split: train = data_sub[!is.na(data_sub$survived),] test = data_sub[is.na(data_sub$survived),] Inner split: indices = sample.int(nrow(train), 0.7 * nrow(train)) sub_train = train[indices,] sub_test = train[-indices,] What is the difference between the two splits? (Tip: have a look at the variable survived.) 4.5.4 Model Fitting In the next step we will fit a Keras model on the training data of the inner split: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dense(units = 20L, input_shape = ncol(sub_train) - 1L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% # Output layer consists of the 1-hot encoded variable &quot;survived&quot; -&gt; 2 units. layer_dense(units = 2L, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_25&quot; ## ______________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ============================================================================================================================== ## dense_79 (Dense) (None, 20) 280 ## ## dense_78 (Dense) (None, 20) 420 ## ## dense_77 (Dense) (None, 20) 420 ## ## dense_76 (Dense) (None, 2) 42 ## ## ============================================================================================================================== ## Total params: 1,162 ## Trainable params: 1,162 ## Non-trainable params: 0 ## ______________________________________________________________________________________________________________________________ model_history = model %&gt;% compile(loss = loss_categorical_crossentropy, optimizer = keras::optimizer_adamax(learning_rate = 0.01)) model_history = model %&gt;% fit(x = as.matrix(sub_train[,-1]), y = to_categorical(sub_train[,1], num_classes = 2L), epochs = 100L, batch_size = 32L, validation_split = 0.2, #Again a test set used by the algorithm. shuffle = TRUE) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Torch library(torch) torch_manual_seed(321L) set.seed(123) model_torch = nn_sequential( nn_linear(in_features = dim(sub_train[,-1])[2], out_features = 20L), nn_relu(), nn_linear(20L, 20L), nn_relu(), nn_linear(20L, 2L) ) opt = optim_adam(params = model_torch$parameters, lr = 0.01) X_torch = torch_tensor(as.matrix(sub_train[,-1])) Y_torch = torch_tensor(sub_train[,1]+1, dtype = torch_long()) for(i in 1:500){ indices = sample.int(nrow(sub_train), 20L) opt$zero_grad() pred = model_torch(X_torch[indices, ]) loss = nnf_cross_entropy(pred, Y_torch[indices], reduction = &quot;mean&quot;) print(loss) loss$backward() opt$step() } ## torch_tensor ## 0.707869 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.686436 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.659611 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.654673 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.627494 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.68805 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.690998 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.652452 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.542723 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.553087 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.664908 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.708627 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.537437 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.43455 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.667144 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.496684 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.570908 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.597058 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.468032 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.649413 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.555161 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.657658 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.587866 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.5941 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.447766 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.604198 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.554323 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.643397 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.536915 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.674608 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.420315 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.32588 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.464963 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.596521 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.530618 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.234835 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.291255 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.929026 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.625346 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.278064 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.726598 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.62555 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.481452 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.673818 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.576145 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.475387 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.457026 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.50311 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.414785 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.385994 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.395334 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.676871 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.393855 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.710248 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.516789 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.530883 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.728959 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.470274 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.51207 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.453122 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.905312 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.4599 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.570826 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.445953 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.468076 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.565009 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.469997 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.600031 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.426152 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.700782 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.352638 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.400375 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.459513 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.568515 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.497609 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.600834 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.798421 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.327998 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.387632 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.63566 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.304985 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.526177 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.592815 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.346981 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.529488 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.623992 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.437405 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.471444 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.481523 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.595864 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.409776 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.408433 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.469692 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.652108 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.457667 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.535109 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.487944 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.588671 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.519266 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.4564 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.420434 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.564046 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.546535 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.655195 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.539748 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.451041 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.488228 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.443963 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.379549 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.592209 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.39709 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.403911 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.286926 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.746202 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.557143 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.343428 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.694007 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.418217 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.512508 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.605517 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.505859 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.57816 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.56861 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.371636 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.584577 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.542919 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.458773 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.673591 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.616572 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.54045 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.482451 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.413941 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.474845 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.419692 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.493641 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.499719 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.47251 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.759404 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.478613 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.446379 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.484713 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.532843 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.320036 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.546959 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.631938 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.367878 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.696314 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.858233 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.694158 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.68525 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.474375 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.547824 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.436111 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.538776 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.505266 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.47963 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.478373 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.517894 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.431919 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.558722 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.459529 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.465366 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.578217 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.603758 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.506026 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.383555 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.419734 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.515685 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.377787 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.251414 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.487529 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.595669 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.421177 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.35105 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.423871 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.474373 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.268141 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.518056 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.31674 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.46961 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.431192 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.329849 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.329375 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.609598 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.52314 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.402313 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.42145 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.634766 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.635958 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.46861 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.277993 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.482526 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.783625 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.481016 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.453809 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.532373 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.614476 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.429902 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.466269 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.52095 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.515713 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.594814 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.620781 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.496087 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.62666 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.574774 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.709346 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.696121 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.466134 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.390352 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.379508 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.518273 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.496471 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.409525 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.467409 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.58821 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.354203 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.54532 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.347564 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.399711 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.689771 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.68454 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.476687 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.44997 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.455688 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.403596 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.388859 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.532802 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.675519 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.496137 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.605388 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.465515 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.41161 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.349016 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.844552 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.45283 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.356554 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.324578 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.501251 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.740512 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.45971 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.4173 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.46211 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.505691 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.461158 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.457387 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.619157 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.540819 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.617918 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.46706 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.355819 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.883221 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.518348 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.44704 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.559336 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.297896 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.412469 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.502253 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.632805 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.374276 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.529211 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.361833 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.480561 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.390684 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.59659 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.2792 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.546276 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.366373 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.431727 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.511097 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.393672 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.510827 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.379485 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.236956 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.453161 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.576676 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.38702 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.511233 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.421102 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.671416 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.449251 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.421628 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.467304 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.471103 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.326304 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.330048 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.355706 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.37242 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.380477 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.445888 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.451437 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.44118 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.545506 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.454936 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.559109 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.554526 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.413272 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.433541 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.35778 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.640296 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.671153 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.271047 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.458804 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.298442 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.422499 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.379167 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.37151 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.29079 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.343601 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.309687 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.640103 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.374724 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.44974 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.349522 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.569582 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.55222 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.75569 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.382098 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.441822 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.608224 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.408617 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.399517 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.3563 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.582844 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.421269 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.458882 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.543267 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.497163 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.482459 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.561153 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.403645 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.417156 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.317258 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.543135 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.518725 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.43794 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.283879 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.341411 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.560607 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.700514 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.652319 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.370861 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.500019 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.701887 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.598595 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.446417 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.415801 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.555131 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.505287 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.663954 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.609121 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.438152 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.531093 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.558249 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.470651 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.476193 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.487094 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.398958 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.370758 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.474866 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.397224 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.429614 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.334949 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.71336 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.414789 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.332019 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.477892 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.317209 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.327091 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.419255 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.273266 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.573711 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.45249 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.346723 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.495111 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.381574 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.244849 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.424235 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.269414 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.607508 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.567444 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.606521 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.357238 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.60519 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.448005 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.650992 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.35432 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.724178 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.449799 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.421666 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.579403 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.945731 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.507651 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.624709 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.477855 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.297618 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.317562 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.578056 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.469425 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.694902 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.502019 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.633915 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.969901 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.416737 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.592856 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.394041 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.392264 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.353905 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.595177 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.359545 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.57412 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.364393 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.584685 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.583028 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.481971 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.383393 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.376592 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.586399 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.413973 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.342848 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.514671 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.406365 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.374424 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.290655 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.425498 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.420754 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.648903 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.859324 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.69948 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.432994 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.366822 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.50001 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.454298 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.475173 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.383777 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.445095 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.415456 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.461693 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.453002 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.519292 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.351406 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.608089 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.560487 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.416949 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.488941 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.562287 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.451058 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.501812 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.530042 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.382543 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.627286 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.569558 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.433292 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.392974 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.483538 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.493791 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.509549 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.519145 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.251333 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.35272 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.542718 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.506634 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.455043 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.526545 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.286775 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.445055 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.462968 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.420297 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.426609 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.22259 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.435372 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.484453 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.410166 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.297978 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.613961 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.479021 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.620836 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.625761 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.464297 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.276816 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.57245 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.380823 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.571787 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.349566 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.430171 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.569651 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.529671 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.428989 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.494857 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.603523 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.595237 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.60492 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.517747 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.492854 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.60878 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.534464 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.49266 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.463699 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.453637 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] ## torch_tensor ## 0.497914 ## [ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward&gt; ] Note: the “nnf_cross_entropy” expects predictions on the scale of the linear predictors (the loss function itself will apply the softmax!). 4.5.5 Model Evaluation We will predict the variable “survived” for the test set of the inner split and calculate the accuracy: pred = model %&gt;% predict(x = as.matrix(sub_test[,-1])) predicted = ifelse(pred[,2] &lt; 0.5, 0, 1) # Ternary operator. observed = sub_test[,1] (accuracy = mean(predicted == observed)) # (...): Show output. ## [1] 0.8121827 Torch model_torch$eval() preds_torch = nnf_softmax(model_torch(torch_tensor(as.matrix(sub_test[,-1]))), dim = 2L) preds_torch = as.matrix(preds_torch) preds_torch = apply(preds_torch, 1, which.max) (accuracy = mean(preds_torch - 1 == observed)) ## [1] 0.7817259 Now we have to use the softmax function. 4.5.6 Predictions and Submission When we are satisfied with the performance of our model in the inner split, we will create predictions for the test data of the outer split. To do so, we take all observations that belong to the outer test split (use the filter function) and remove the survived (NAs) columns: submit = test %&gt;% select(-survived) We cannot assess the performance on the test split because the true survival ratio is unknown, however, we can now submit our predictions to the submission server at http://rhsbio7.uni-regensburg.de:8500. To do so, we have to transform our survived probabilities into actual 0/1 predictions (probabilities are not allowed) and create a .csv file: pred = model %&gt;% predict(as.matrix(submit)) All values &gt; 0.5 will be set to 1 and values &lt; 0.5 to zero. For the submission it is critical to change the predictions into a data.frame, select the second column (the probability to survive), and save it with the write.csv function: write.csv(data.frame(y = ifelse(pred[,2] &lt; 0.5, 0, 1)), file = &quot;Max_1.csv&quot;) The file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself. Annotation: The AUC is (always) higher for probabilities than for 0/1 data (depending on the implementation and definition of the AUC). We expect you to upload 0/1 data (usage scenarios, no theoretical ones)! Hint for cheaters (or if you just forget conversion): Your upload is converted to 0/1 data according to ifelse(… &lt; 0.5, 0, 1). Task The tasks below follow the above section about machine learning pipelines. We will use the titanic_ml data set from the EcoData package to see a pipeline. The goal is to predict if a passenger survives or not. First, we should look at the data and do some feature engineering / selection. Give it a try! (Ideas can be found here.) Solution library(keras) library(tensorflow) library(tidyverse) library(randomForest) library(rpart) library(EcoData) set_random_seed(54321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data(titanic_ml) data = titanic_ml str(data) ## &#39;data.frame&#39;: 1309 obs. of 14 variables: ## $ pclass : int 2 1 3 3 3 3 3 1 3 1 ... ## $ survived : int 1 1 0 0 0 0 0 1 0 1 ... ## $ name : chr &quot;Sinkkonen, Miss. Anna&quot; &quot;Woolner, Mr. Hugh&quot; &quot;Sage, Mr. Douglas Bullen&quot; &quot;Palsson, Master. Paul Folke&quot; ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ... ## $ age : num 30 NA NA 6 30.5 38.5 20 53 NA 42 ... ## $ sibsp : int 0 0 8 3 0 0 0 0 0 0 ... ## $ parch : int 0 0 2 1 0 0 0 0 0 0 ... ## $ ticket : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 221 123 779 542 589 873 472 823 588 834 ... ## $ fare : num 13 35.5 69.55 21.07 8.05 ... ## $ cabin : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 1 94 1 1 1 1 1 1 1 1 ... ## $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 4 4 4 4 4 4 2 4 2 ... ## $ boat : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 3 28 1 1 1 1 1 19 1 15 ... ## $ body : int NA NA NA NA 50 32 NA NA NA NA ... ## $ home.dest: Factor w/ 370 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ... indicesPredict = which(is.na(titanic_ml$survived)) indicesTrain = which(!is.na(titanic_ml$survived)) subset = sample(length(indicesTrain), length(indicesTrain) * 0.7, replace = F) indicesTest = sort(indicesTrain[-subset]) # Mind order of instructions! indicesTrain = sort(indicesTrain[subset]) # Determine labels. As they are fixed from now on, samples must not be left! # Impute when necessary. labelsTrain = data$survived[indicesTrain] labelsTest = data$survived[indicesTest] # Parameter &quot;body&quot; has nearly no information). # Parameter &quot;name&quot; was already processed above. # Parameter &quot;ticket&quot; may include (other) information, but leave it out for now. # Parameter &quot;survived&quot; is the target variable. data = data %&gt;% select(-body, -name, -ticket, -survived) data$pclass = as.factor(data$pclass) # &quot;pclass&quot; makes only sense as a factor. for(i in 1:3){ print(sum(data$cabin[data$pclass == i] == &quot;&quot;) / sum(data$pclass == i)) } ## [1] 0.2074303 ## [1] 0.9169675 ## [1] 0.977433 # Most people have no cabin (21% in 1st, 92% in 2nd and 98% in 3rd class). # Dummy code the availability of a cabin. # This is NOT one-hot encoding! Dummy coded variables use n-1 variables! data$cabin = (data$cabin != &quot;&quot;) * 1 # Impute values for parameter &quot;embarked&quot;: # Leave parameters with too many levels or causal inverse ones (assumed). tmp = data %&gt;% select(-boat, -home.dest) tmp = data.frame(tmp[complete.cases(tmp),]) # Leave samples with missing values. missingIndices = which(tmp$embarked == &quot;&quot;) toPredict = tmp[missingIndices, -8] tmp = tmp[-missingIndices,] # Leave samples that should be predicted. tmp$embarked = droplevels(tmp$embarked) # Remove unused levels (&quot;&quot;). # Random forests and simple regression trees don&#39;t need scaling. # Use a simple regression tree instead of a random forest here, # we have only 2 missing values. # And a simple regression tree is easy to visualize. regressionTree = rpart(embarked ~ ., data = tmp, control = rpart.control(minsplit = 10)) rpart.plot::rpart.plot(regressionTree) prediction = predict(regressionTree, toPredict) for(i in 1:nrow(prediction)){ index = which(rownames(data) == as.integer(rownames(prediction))[i]) data$embarked[index] = colnames(prediction)[which.max(prediction[i,])] } data$embarked = droplevels(data$embarked) # Remove unused levels (&quot;&quot;). table(data$pclass) ## ## 1 2 3 ## 323 277 709 sum(table(data$pclass)) ## [1] 1309 sum(table(data$home.dest)) ## [1] 1309 sum(is.na(data$home.dest)) ## [1] 0 # &quot;pclass&quot;, &quot;sex&quot;, &quot;sibsp&quot;, &quot;parch&quot;, &quot;boat&quot; and &quot;home.dest&quot; have no missing values. summary(data$fare) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 7.896 14.454 33.295 31.275 512.329 1 # Parameter &quot;fare&quot; has 1 NA entry. Impute the mean. data$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE) levels(data$home.dest)[levels(data$home.dest) == &quot;&quot;] = &quot;unknown&quot; levels(data$boat)[levels(data$boat) == &quot;&quot;] = &quot;none&quot; # Impute values for &quot;age&quot;: tmp = data %&gt;% select(-home.dest) # Leave parameters with too many levels. missingIndices = which(is.na(tmp$age)) toPredict = tmp[missingIndices, -3] tmp = tmp[-missingIndices,] # Leave samples that should be predicted. forest = randomForest(x = tmp[,-3], y = tmp$age) prediction = predict(forest, toPredict) for(i in 1:length(prediction)){ index = which(rownames(data) == as.integer(names(prediction))[i]) data$age[index] = prediction[i] } str(data) ## &#39;data.frame&#39;: 1309 obs. of 10 variables: ## $ pclass : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 2 1 3 3 3 3 3 1 3 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ... ## $ age : num 30 36.7 13.9 6 30.5 ... ## $ sibsp : int 0 0 8 3 0 0 0 0 0 0 ... ## $ parch : int 0 0 2 1 0 0 0 0 0 0 ... ## $ fare : num 13 35.5 69.55 21.07 8.05 ... ## $ cabin : num 0 1 0 0 0 0 0 0 0 0 ... ## $ embarked : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 3 3 3 3 3 3 1 3 1 ... ## $ boat : Factor w/ 28 levels &quot;none&quot;,&quot;1&quot;,&quot;10&quot;,..: 3 28 1 1 1 1 1 19 1 15 ... ## $ home.dest: Factor w/ 370 levels &quot;unknown&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ... data = as.data.frame(data) # One-hot encoding of &quot;pclass&quot;, &quot;sex&quot;, &quot;cabin&quot;, &quot;embarked&quot;, &quot;boat&quot; and &quot;home.dest&quot;: for(element in c(&quot;pclass&quot;, &quot;sex&quot;, &quot;cabin&quot;, &quot;embarked&quot;, &quot;boat&quot;, &quot;home.dest&quot;)){ # Build integer representation: # This MUST start at 0, otherwise, the encoding is wrong!! integers = as.integer(data[[element]]) integers = integers - min(integers) # Determine number of classes: num_classes = length(levels(as.factor(data[[element]]))) # Encode: encoded = k_one_hot(integers, num_classes)$numpy() # Copy factor names. colnames(encoded) = paste0(element, &quot;_&quot;, levels(as.factor(data[[element]]))) data = data %&gt;% select(-all_of(element)) # Remove original column. data = cbind.data.frame(data, encoded) # Plug in new (encoded) columns. } # Scale parameters (including one-hot encoded): data = scale(as.matrix(data)) # Split into training, test and prediction set: # Be careful! You need matrices, no data.frames! train = as.matrix(data[indicesTrain,]) test = as.matrix(data[indicesTest,]) predict = as.matrix(data[indicesPredict,]) Task Build a neural network to make predictions and check performance on the hold-out data. Solution ## labelsTest ## prediction 0 1 ## 0 106 14 ## 1 15 62 ## Accuracy: ## [1] 0.8527919 Task Play around with model parameters, optimizer(learning_rate = …), epochs = …, number of hidden nodes in layers: units = …, regularization: kernel_regularizer = …, bias_regularizer = … - Try to maximize the model’s accuracy for the hold-out data. Hint: There are a lot different activation functions like “linear,” “softmax,” “relu,” “leaky_relu,” “gelu,” “selu,” “elu,” “exponential,” “sigmoid,” “tanh,” “softplus,” “softsign,” etc. But be careful, this might be very computation-intensive (especially “gelu,” tanh\" or “softmax”). Every activation function has its own properties, requirements (!), advantages and disadvantages. Choose them wisely! You should get an accuracy of at least 90%. (Before looking into the solution…) Try to be better than the solution. Solution ## labelsTest ## prediction 0 1 ## 0 120 7 ## 1 1 69 ## Accuracy: ## [1] 0.9593909 Task Now try your above solution (the exactly same one!) with another seed to check for overfitting (on the seed!) of your procedure. Solution ## labelsTest ## prediction 0 1 ## 0 113 6 ## 1 8 70 ## Accuracy: ## [1] 0.928934 Task Make predictions and submit them via our submission server. Solution prediction = model %&gt;% predict(x = predict) # Take label with highest probability: prediction = (prediction[,1] &lt; prediction[,2]) * 1 write.csv(data.frame(y = prediction), file = &quot;submission_NN.csv&quot;) 4.6 Bonus - Machine Learning Pipelines with mlr3 As we have seen today, many of the machine learning algorithms are distributed over several packages but the general machine learning pipeline is very similar for all models: feature engineering, feature selection, hyperparameter tuning and cross-validation. The idea of the mlr3 framework is now to provide a general machine learning interface which you can use to build reproducible and automatic machine learning pipelines. The key features of mlr3 are: All common machine learning packages are integrated into mlr3, you can easily switch between different machine learning algorithms. A common ‘language’/workflow to specify machine learning pipelines. Support for different cross-validation strategies. Hyperparameter tuning for all supported machine learning algorithms. Ensemble models. Useful links: mlr3-book (still in work) mlr3 website mlr3 cheatsheet 4.6.1 mlr3 - The Basic Workflow The mlr3 package actually consists of several packages for different tasks (e.g. mlr3tuning for hyperparameter tuning, mlr3pipelines for data preparation pipes). But let’s start with the basic workflow: library(EcoData) library(tidyverse) library(mlr3) library(mlr3learners) library(mlr3pipelines) library(mlr3tuning) library(mlr3measures) data(nasa) str(nasa) ## &#39;data.frame&#39;: 4687 obs. of 40 variables: ## $ Neo.Reference.ID : int 3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ... ## $ Name : int NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ... ## $ Absolute.Magnitude : num 18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ... ## $ Est.Dia.in.KM.min. : num 0.4837 0.1011 0.0291 0.1272 0.1395 ... ## $ Est.Dia.in.KM.max. : num 1.0815 0.226 0.0652 0.2845 0.3119 ... ## $ Est.Dia.in.M.min. : num 483.7 NA 29.1 127.2 139.5 ... ## $ Est.Dia.in.M.max. : num 1081.5 226 65.2 284.5 311.9 ... ## $ Est.Dia.in.Miles.min. : num 0.3005 0.0628 NA 0.0791 0.0867 ... ## $ Est.Dia.in.Miles.max. : num 0.672 0.1404 0.0405 0.1768 0.1938 ... ## $ Est.Dia.in.Feet.min. : num 1586.9 331.5 95.6 417.4 457.7 ... ## $ Est.Dia.in.Feet.max. : num 3548 741 214 933 1023 ... ## $ Close.Approach.Date : Factor w/ 777 levels &quot;1995-01-01&quot;,&quot;1995-01-08&quot;,..: 511 712 472 239 273 145 428 694 87 732 ... ## $ Epoch.Date.Close.Approach : num NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ... ## $ Relative.Velocity.km.per.sec: num 11.22 13.57 5.75 13.84 4.61 ... ## $ Relative.Velocity.km.per.hr : num 40404 48867 20718 49821 16583 ... ## $ Miles.per.hour : num 25105 30364 12873 30957 10304 ... ## $ Miss.Dist..Astronomical. : num NA 0.0671 0.013 0.0583 0.0381 ... ## $ Miss.Dist..lunar. : num 112.7 26.1 NA 22.7 14.8 ... ## $ Miss.Dist..kilometers. : num 43348668 10030753 1949933 NA 5694558 ... ## $ Miss.Dist..miles. : num 26935614 6232821 1211632 5418692 3538434 ... ## $ Orbiting.Body : Factor w/ 1 level &quot;Earth&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Orbit.ID : int NA 8 12 12 91 NA 24 NA NA 212 ... ## $ Orbit.Determination.Date : Factor w/ 2680 levels &quot;2014-06-13 15:20:44&quot;,..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ... ## $ Orbit.Uncertainity : int 0 8 6 0 0 0 1 1 1 0 ... ## $ Minimum.Orbit.Intersection : num NA 0.05594 0.00553 NA 0.0281 ... ## $ Jupiter.Tisserand.Invariant : num 5.58 3.61 4.44 5.5 NA ... ## $ Epoch.Osculation : num 2457800 2457010 NA 2458000 2458000 ... ## $ Eccentricity : num 0.276 0.57 0.344 0.255 0.22 ... ## $ Semi.Major.Axis : num 1.1 NA 1.52 1.11 1.24 ... ## $ Inclination : num 20.06 4.39 5.44 23.9 3.5 ... ## $ Asc.Node.Longitude : num 29.85 1.42 170.68 356.18 183.34 ... ## $ Orbital.Period : num 419 1040 682 427 503 ... ## $ Perihelion.Distance : num 0.794 0.864 0.994 0.828 0.965 ... ## $ Perihelion.Arg : num 41.8 359.3 350 268.2 179.2 ... ## $ Aphelion.Dist : num 1.4 3.15 2.04 1.39 1.51 ... ## $ Perihelion.Time : num 2457736 2456941 2457937 NA 2458070 ... ## $ Mean.Anomaly : num 55.1 NA NA 297.4 310.5 ... ## $ Mean.Motion : num 0.859 0.346 0.528 0.843 0.716 ... ## $ Equinox : Factor w/ 1 level &quot;J2000&quot;: 1 1 NA 1 1 1 1 1 1 1 ... ## $ Hazardous : int 0 0 0 1 1 0 0 0 1 1 ... Let’s drop time, name and ID variable and create a classification task: data = nasa %&gt;% select(-Orbit.Determination.Date, -Close.Approach.Date, -Name, -Neo.Reference.ID) data$Hazardous = as.factor(data$Hazardous) # Create a classification task. task = TaskClassif$new(id = &quot;nasa&quot;, backend = data, target = &quot;Hazardous&quot;, positive = &quot;1&quot;) Create a generic pipeline of data transformation (imputation \\(\\rightarrow\\) scaling \\(\\rightarrow\\) encoding of categorical variables): set.seed(123) # Let&#39;s create the preprocessing graph. preprocessing = po(&quot;imputeoor&quot;) %&gt;&gt;% po(&quot;scale&quot;) %&gt;&gt;% po(&quot;encode&quot;) # Run the task. transformed_task = preprocessing$train(task)[[1]] transformed_task$missings() ## Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude ## 4187 0 0 0 ## Eccentricity Epoch.Date.Close.Approach Epoch.Osculation Est.Dia.in.Feet.max. ## 0 0 0 0 ## Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max. ## 0 0 0 0 ## Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination ## 0 0 0 0 ## Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour ## 0 0 0 0 ## Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers. Miss.Dist..lunar. ## 0 0 0 0 ## Miss.Dist..miles. Orbit.ID Orbit.Uncertainity Orbital.Period ## 0 0 0 0 ## Perihelion.Arg Perihelion.Distance Perihelion.Time Relative.Velocity.km.per.hr ## 0 0 0 0 ## Relative.Velocity.km.per.sec Semi.Major.Axis Equinox.J2000 Equinox..MISSING ## 0 0 0 0 ## Orbiting.Body.Earth Orbiting.Body..MISSING ## 0 0 We can even visualize the preprocessing graph: preprocessing$plot() Now, to test our model (random forest) 10-fold cross-validated, we will do: Specify the missing target rows as validation so that they will be ignored. Specify the cross-validation, the learner (the machine learning model we want to use), and the measurement (AUC). Run (benchmark) our model. set.seed(123) transformed_task$data() ## Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity Epoch.Date.Close.Approach Epoch.Osculation ## 1: 0 -0.81322649 -0.38042005 -1.140837452 -0.315605975 -4.7929881 0.14026773 ## 2: 0 0.02110348 0.94306517 -1.380254611 0.744287645 1.1058704 -0.26325244 ## 3: 0 0.68365964 0.10199889 0.044905370 -0.068280074 0.1591740 -7.76281014 ## 4: 1 -0.10159210 -0.38415066 1.606769281 -0.392030729 -0.7630231 0.24229559 ## 5: 1 -0.15067034 -0.29632490 0.151458877 -0.516897963 -0.6305034 0.24229559 ## --- ## 4683: &lt;NA&gt; -0.32244415 0.69173184 -0.171022906 1.043608082 1.3635097 0.24229559 ## 4684: &lt;NA&gt; 0.46280759 -0.24203066 -0.009803808 -0.006429588 1.3635097 0.05711503 ## 4685: &lt;NA&gt; 1.51798962 -0.56422744 1.514551982 -1.045386877 1.3635097 0.24229559 ## 4686: &lt;NA&gt; 0.16833819 0.14193044 -1.080452287 0.017146757 1.3635097 0.24229559 ## 4687: &lt;NA&gt; -0.05251387 -0.08643345 -0.013006704 -0.579210554 1.3635097 0.24229559 ## Est.Dia.in.Feet.max. Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max. Est.Dia.in.M.min. ## 1: 0.271417899 0.313407647 0.300713440 0.256568684 0.271095311 0.291624502 ## 2: 0.032130074 -0.029173486 -0.020055639 0.057560696 0.031844946 -12.143577263 ## 3: -0.012841645 -0.093558135 -0.080340934 0.020159164 -0.013119734 -0.060269734 ## 4: 0.048493723 -0.005746146 0.001880088 0.071169817 0.048206033 0.015659335 ## 5: 0.056169717 0.005243343 0.012169879 0.077553695 0.055880826 0.025161701 ## --- ## 4683: 0.089353662 0.052751793 0.056653478 0.105151714 0.089059576 0.066241198 ## 4684: -0.003481174 -0.080157032 -0.067793075 0.027943967 -0.003760728 -0.048682099 ## 4685: -0.027260163 -0.114200690 -0.099669182 0.008167747 -0.027535994 -0.078118891 ## 4686: 0.016872584 -0.051017172 -0.040508543 0.044871533 0.016589844 -0.023485512 ## 4687: 0.041493133 -0.015768679 -0.007504312 0.065347651 0.041206539 0.006993074 ## Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion ## 1: 2.620443e-01 0.258651038 0.5442288 0.3840868 -1.02876096 0.31939530 ## 2: 4.153888e-02 0.030928225 -0.5925952 -0.7801632 -4.55056211 -0.71151122 ## 3: 9.711407e-05 -10.258220292 -0.5164818 -0.2872777 -4.55056211 -0.34600512 ## 4: 5.661810e-02 0.046501003 0.8225188 0.3403535 1.02239674 0.28551117 ## 5: 6.369158e-02 0.053806009 -0.6568722 -6.2415005 1.13265516 0.03164827 ## --- ## 4683: 9.427082e-02 0.085386142 0.8222493 -0.6412806 0.01560046 -0.51852041 ## 4684: 8.722856e-03 -0.002961897 1.9818623 0.1346891 1.08051799 0.17477591 ## 4685: -1.318965e-02 -0.025591624 -0.5220442 0.4810091 0.89998250 0.36895738 ## 4686: 2.747899e-02 0.016408144 -0.5912988 -0.3061894 0.22720275 -0.35895074 ## 4687: 5.016700e-02 0.039838758 0.6181969 -0.2665930 0.22740438 -0.31462613 ## Miles.per.hour Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers. Miss.Dist..lunar. ## 1: -0.254130552 -5.45911858 -7.0769260 0.25122963 0.2398625 ## 2: 0.009333354 0.07077092 -0.6830928 -1.08492125 -1.1742128 ## 3: -0.866997591 -0.11099960 -0.9035573 -1.40898698 -4.7878719 ## 4: 0.039031045 -5.45911858 -0.7188386 -4.48402327 -1.2298206 ## 5: -0.995720084 -0.02962490 -0.8013948 -1.25881601 -1.3582490 ## --- ## 4683: 1.403775544 0.30711241 -0.2728622 -0.48191427 -0.5360384 ## 4684: 0.970963141 -0.05962478 -0.7879458 -1.23904708 -1.3373272 ## 4685: -1.150527134 -0.10766868 -0.9303542 -1.44837625 -1.5588644 ## 4686: -0.705980518 0.08529226 -0.7077555 -1.12117355 -1.2125793 ## 4687: -0.239696213 0.50904764 0.1075071 0.07719897 0.0556823 ## Miss.Dist..miles. Orbit.ID Orbit.Uncertainity Orbital.Period Perihelion.Arg Perihelion.Distance Perihelion.Time ## 1: 0.23810770 -9.6514722 -1.0070872 -0.3013135 -1.170536399 -0.01831583 0.10526107 ## 2: -1.18860632 -0.2412680 1.3770116 0.7811097 1.549452700 0.20604472 -0.28203779 ## 3: -1.53463694 -0.1803606 0.7809869 0.1566040 1.470307933 0.61816146 0.20313227 ## 4: -1.24471124 -0.1803606 -1.0070872 -0.2866969 0.769006449 0.09005898 -7.86832915 ## 5: -1.37428752 1.0225620 -1.0070872 -0.1552813 0.006829799 0.52730977 0.26755741 ## --- ## 4683: -0.54472804 -0.1194531 -0.7090748 0.3873214 -0.580282684 -0.65810123 0.03734532 ## 4684: -1.35317867 -0.3021755 1.3770116 -0.2345610 0.839430173 -0.18350549 0.09156633 ## 4685: -1.57669598 -0.3326292 0.7809869 -0.3216884 -1.168210857 0.62646993 0.27629790 ## 4686: -1.22731578 -0.1042262 0.7809869 0.1712806 0.824836889 0.52899080 0.37994517 ## 4687: 0.05228143 -0.2717218 0.4829746 0.1224733 0.016358127 1.22720096 0.37399573 ## Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec Semi.Major.Axis Equinox.J2000 Equinox..MISSING ## 1: -0.28167821 -0.284140684 -0.2791037 1 0 ## 2: -0.00604459 -0.008343348 -7.3370940 1 0 ## 3: -0.92285430 -0.925697621 0.2204883 0 1 ## 4: 0.02502487 0.022744569 -0.2617714 1 0 ## 5: -1.05752264 -1.060445948 -0.1106954 1 0 ## --- ## 4683: 1.45280854 1.451376301 0.4468886 1 0 ## 4684: 1.00000402 0.998302826 -0.2008499 1 0 ## 4685: -1.21948041 -1.222499918 -0.3034586 1 0 ## 4686: -0.75439966 -0.757142920 0.2353030 1 0 ## 4687: -0.26657713 -0.269030636 0.1857979 1 0 ## Orbiting.Body.Earth Orbiting.Body..MISSING ## 1: 1 0 ## 2: 1 0 ## 3: 1 0 ## 4: 1 0 ## 5: 1 0 ## --- ## 4683: 1 0 ## 4684: 1 0 ## 4685: 1 0 ## 4686: 1 0 ## 4687: 1 0 transformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)], &quot;validation&quot;) cv10 = mlr3::rsmp(&quot;cv&quot;, folds = 10L) rf = lrn(&quot;classif.ranger&quot;, predict_type = &quot;prob&quot;) measurement = msr(&quot;classif.auc&quot;) result = mlr3::resample(transformed_task, rf, resampling = cv10, store_models = TRUE) # Calculate the average AUC of the holdouts. result$aggregate(measurement) Very cool! Preprocessing + 10-fold cross-validation model evaluation in a few lines of code! Let’s create the final predictions: pred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task, row_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, &quot;1&quot;, drop = FALSE]) dim(pred) predictions = apply(pred, 1, mean) You could now submit the predictions here. But we are still not happy with the results, let’s do some hyperparameter tuning! 4.6.2 mlr3 - Hyperparameter Tuning Machine learning algorithms have a varying number of hyperparameters which can (!) have a high impact on the predictive performance. To list a few hyperparameters: Random Forest mtry Minimal node size K-nearest-neighbors classification Kernel Number of neighbors Distance metric Boosted Regression Tree nrounds Maximum depth alpha booster eta gamma lambda With mlr3, we can easily extend the above example to do hyperparameter tuning within nested cross-validation (the tuning has its own inner cross-validation). Print the hyperparameter space of our random forest learner: rf$param_set ## &lt;ParamSet&gt; ## id class lower upper nlevels default parents value ## 1: alpha ParamDbl -Inf Inf Inf 0.5 ## 2: always.split.variables ParamUty NA NA Inf &lt;NoDefault[3]&gt; ## 3: class.weights ParamDbl -Inf Inf Inf ## 4: holdout ParamLgl NA NA 2 FALSE ## 5: importance ParamFct NA NA 4 &lt;NoDefault[3]&gt; ## 6: keep.inbag ParamLgl NA NA 2 FALSE ## 7: max.depth ParamInt 0 Inf Inf ## 8: min.node.size ParamInt 1 Inf Inf 1 ## 9: min.prop ParamDbl -Inf Inf Inf 0.1 ## 10: minprop ParamDbl -Inf Inf Inf 0.1 ## 11: mtry ParamInt 1 Inf Inf &lt;NoDefault[3]&gt; ## 12: mtry.ratio ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 13: num.random.splits ParamInt 1 Inf Inf 1 splitrule ## 14: num.threads ParamInt 1 Inf Inf 1 1 ## 15: num.trees ParamInt 1 Inf Inf 500 ## 16: oob.error ParamLgl NA NA 2 TRUE ## 17: regularization.factor ParamUty NA NA Inf 1 ## 18: regularization.usedepth ParamLgl NA NA 2 FALSE ## 19: replace ParamLgl NA NA 2 TRUE ## 20: respect.unordered.factors ParamFct NA NA 3 ignore ## 21: sample.fraction ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 22: save.memory ParamLgl NA NA 2 FALSE ## 23: scale.permutation.importance ParamLgl NA NA 2 FALSE importance ## 24: se.method ParamFct NA NA 2 infjack ## 25: seed ParamInt -Inf Inf Inf ## 26: split.select.weights ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 27: splitrule ParamFct NA NA 2 gini ## 28: verbose ParamLgl NA NA 2 TRUE ## 29: write.forest ParamLgl NA NA 2 TRUE ## id class lower upper nlevels default parents value Define the hyperparameter space of the random forest: library(paradox) rf_pars = paradox::ParamSet$new( list(paradox::ParamInt$new(&quot;min.node.size&quot;, lower = 1, upper = 30L), paradox::ParamInt$new(&quot;mtry&quot;, lower = 1, upper = 30L), paradox::ParamLgl$new(&quot;regularization.usedepth&quot;, default = TRUE))) print(rf_pars) ## &lt;ParamSet&gt; ## id class lower upper nlevels default value ## 1: min.node.size ParamInt 1 30 30 &lt;NoDefault[3]&gt; ## 2: mtry ParamInt 1 30 30 &lt;NoDefault[3]&gt; ## 3: regularization.usedepth ParamLgl NA NA 2 TRUE To set up the tuning pipeline we need: Inner cross-validation resampling object. Tuning criterion (e.g. AUC). Tuning method (e.g. random or block search). Tuning terminator (When should we stop tuning? E.g. after \\(n\\) iterations). set.seed(123) inner3 = mlr3::rsmp(&quot;cv&quot;, folds = 3L) measurement = msr(&quot;classif.auc&quot;) tuner = mlr3tuning::tnr(&quot;random_search&quot;) terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 5L) rf = lrn(&quot;classif.ranger&quot;, predict_type = &quot;prob&quot;) learner_tuner = AutoTuner$new(learner = rf, measure = measurement, tuner = tuner, terminator = terminator, search_space = rf_pars, resampling = inner3) print(learner_tuner) ## &lt;AutoTuner:classif.ranger.tuned&gt; ## * Model: - ## * Search Space: ## &lt;ParamSet&gt; ## id class lower upper nlevels default value ## 1: min.node.size ParamInt 1 30 30 &lt;NoDefault[3]&gt; ## 2: mtry ParamInt 1 30 30 &lt;NoDefault[3]&gt; ## 3: regularization.usedepth ParamLgl NA NA 2 TRUE ## * Packages: mlr3, ranger ## * Predict Type: prob ## * Feature Types: logical, integer, numeric, character, factor, ordered ## * Properties: importance, multiclass, oob_error, twoclass, weights Now we can wrap it normally into the 10-fold cross-validated setup as done previously: set.seed(123) outer3 = mlr3::rsmp(&quot;cv&quot;, folds = 3L) result = mlr3::resample(transformed_task, learner_tuner, resampling = outer3, store_models = TRUE) # Calculate the average AUC of the holdouts. result$aggregate(measurement) Yeah, we were able to improve the performance! Let’s create the final predictions: pred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task, row_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, &quot;1&quot;, drop = FALSE]) dim(pred) predictions = apply(pred, 1, mean) 4.6.3 mlr3 - Hyperparameter Tuning with Oversampling Let’s go one step back, maybe you have noticed that our classes are unbalanced: table(data$Hazardous) ## ## 0 1 ## 412 88 Many machine learning algorithms have problems with unbalanced data because if the imbalance is too strong it is cheaper for the algorithm to focus on only one class (e.g. by predicting only 0s or 1s). You need to keep in mind that machine learning algorithms are greedy and their main focus is to minimize the loss function. There are few techniques to correct for imbalance: Oversampling (oversample the undersampled class). Undersampling (undersample the oversampled class). SMOTE Synthetic Minority Over-sampling Technique (very briefly, we will use a k-nearest-neighbors classification to create new samples around our undersampled class). Here, we will use oversampling which we can do by extending our random forest learner: set.seed(123) rf_over = po(&quot;classbalancing&quot;, id = &quot;over&quot;, adjust = &quot;minor&quot;) %&gt;&gt;% rf # However rf_over is now a &quot;graph&quot;, # but we can easily transform it back into a learner: rf_over_learner = GraphLearner$new(rf_over) print(rf_over_learner) ## &lt;GraphLearner:over.classif.ranger&gt; ## * Model: - ## * Parameters: over.ratio=1, over.reference=all, over.adjust=minor, over.shuffle=TRUE, ## classif.ranger.num.threads=1 ## * Packages: mlr3, mlr3pipelines ## * Predict Type: prob ## * Feature types: logical, integer, numeric, character, factor, ordered, POSIXct ## * Properties: featureless, hotstart_backward, hotstart_forward, importance, loglik, missings, multiclass, ## oob_error, selected_features, twoclass, weights The learner has now a new feature space: rf_over_learner$param_set ## &lt;ParamSetCollection&gt; ## id class lower upper nlevels default parents value ## 1: classif.ranger.alpha ParamDbl -Inf Inf Inf 0.5 ## 2: classif.ranger.always.split.variables ParamUty NA NA Inf &lt;NoDefault[3]&gt; ## 3: classif.ranger.class.weights ParamDbl -Inf Inf Inf ## 4: classif.ranger.holdout ParamLgl NA NA 2 FALSE ## 5: classif.ranger.importance ParamFct NA NA 4 &lt;NoDefault[3]&gt; ## 6: classif.ranger.keep.inbag ParamLgl NA NA 2 FALSE ## 7: classif.ranger.max.depth ParamInt 0 Inf Inf ## 8: classif.ranger.min.node.size ParamInt 1 Inf Inf 1 ## 9: classif.ranger.min.prop ParamDbl -Inf Inf Inf 0.1 ## 10: classif.ranger.minprop ParamDbl -Inf Inf Inf 0.1 ## 11: classif.ranger.mtry ParamInt 1 Inf Inf &lt;NoDefault[3]&gt; ## 12: classif.ranger.mtry.ratio ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 13: classif.ranger.num.random.splits ParamInt 1 Inf Inf 1 classif.ranger.splitrule ## 14: classif.ranger.num.threads ParamInt 1 Inf Inf 1 1 ## 15: classif.ranger.num.trees ParamInt 1 Inf Inf 500 ## 16: classif.ranger.oob.error ParamLgl NA NA 2 TRUE ## 17: classif.ranger.regularization.factor ParamUty NA NA Inf 1 ## 18: classif.ranger.regularization.usedepth ParamLgl NA NA 2 FALSE ## 19: classif.ranger.replace ParamLgl NA NA 2 TRUE ## 20: classif.ranger.respect.unordered.factors ParamFct NA NA 3 ignore ## 21: classif.ranger.sample.fraction ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 22: classif.ranger.save.memory ParamLgl NA NA 2 FALSE ## 23: classif.ranger.scale.permutation.importance ParamLgl NA NA 2 FALSE classif.ranger.importance ## 24: classif.ranger.se.method ParamFct NA NA 2 infjack ## 25: classif.ranger.seed ParamInt -Inf Inf Inf ## 26: classif.ranger.split.select.weights ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 27: classif.ranger.splitrule ParamFct NA NA 2 gini ## 28: classif.ranger.verbose ParamLgl NA NA 2 TRUE ## 29: classif.ranger.write.forest ParamLgl NA NA 2 TRUE ## 30: over.adjust ParamFct NA NA 7 &lt;NoDefault[3]&gt; minor ## 31: over.ratio ParamDbl 0 Inf Inf &lt;NoDefault[3]&gt; 1 ## 32: over.reference ParamFct NA NA 6 &lt;NoDefault[3]&gt; all ## 33: over.shuffle ParamLgl NA NA 2 &lt;NoDefault[3]&gt; TRUE ## id class lower upper nlevels default parents value We can also tune the oversampling rate! set.seed(123) rf_pars_over = paradox::ParamSet$new( list(paradox::ParamInt$new(&quot;over.ratio&quot;, lower = 1, upper = 7L), paradox::ParamInt$new(&quot;classif.ranger.min.node.size&quot;, lower = 1, upper = 30L), paradox::ParamInt$new(&quot;classif.ranger.mtry&quot;, lower = 1, upper = 30L), paradox::ParamLgl$new(&quot;classif.ranger.regularization.usedepth&quot;, default = TRUE))) inner3 = mlr3::rsmp(&quot;cv&quot;, folds = 3L) measurement = msr(&quot;classif.auc&quot;) tuner = mlr3tuning::tnr(&quot;random_search&quot;) terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 5L) learner_tuner_over = AutoTuner$new(learner = rf_over_learner, measure = measurement, tuner = tuner, terminator = terminator, search_space = rf_pars_over, resampling = inner3) print(learner_tuner) ## &lt;AutoTuner:classif.ranger.tuned&gt; ## * Model: - ## * Search Space: ## &lt;ParamSet&gt; ## id class lower upper nlevels default value ## 1: min.node.size ParamInt 1 30 30 &lt;NoDefault[3]&gt; ## 2: mtry ParamInt 1 30 30 &lt;NoDefault[3]&gt; ## 3: regularization.usedepth ParamLgl NA NA 2 TRUE ## * Packages: mlr3, ranger ## * Predict Type: prob ## * Feature Types: logical, integer, numeric, character, factor, ordered ## * Properties: importance, multiclass, oob_error, twoclass, weights set.seed(123) outer3 = mlr3::rsmp(&quot;cv&quot;, folds = 3L) result = mlr3::resample(transformed_task, learner_tuner_over, resampling = outer3, store_models = TRUE) # Calculate the average AUC of the holdouts. result$aggregate(measurement) 5 iterations in the hyperspace is not very much… Let’s create the final predictions: pred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task, row_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, &quot;1&quot;, drop = FALSE]) dim(pred) predictions = apply(pred, 1, mean) Optional bonus task After reading the above chapter about the mlr package, try to transfer it to the titanic data set (use the titanic_ml data set, this has already NAs for the values to predict). Alternatively, you can also use other data sets from our challenge (e.g. the plant-pollinator data set, see the data set chapter 8). Solution library(EcoData) library(tidyverse) library(mlr3) library(mlr3learners) library(mlr3pipelines) library(mlr3tuning) library(mlr3measures) set.seed(123) data(titanic_ml) str(titanic_ml) ## &#39;data.frame&#39;: 1309 obs. of 14 variables: ## $ pclass : int 2 1 3 3 3 3 3 1 3 1 ... ## $ survived : int 1 1 0 0 0 0 0 1 0 1 ... ## $ name : chr &quot;Sinkkonen, Miss. Anna&quot; &quot;Woolner, Mr. Hugh&quot; &quot;Sage, Mr. Douglas Bullen&quot; &quot;Palsson, Master. Paul Folke&quot; ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ... ## $ age : num 30 NA NA 6 30.5 38.5 20 53 NA 42 ... ## $ sibsp : int 0 0 8 3 0 0 0 0 0 0 ... ## $ parch : int 0 0 2 1 0 0 0 0 0 0 ... ## $ ticket : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 221 123 779 542 589 873 472 823 588 834 ... ## $ fare : num 13 35.5 69.55 21.07 8.05 ... ## $ cabin : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 1 94 1 1 1 1 1 1 1 1 ... ## $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 4 4 4 4 4 4 2 4 2 ... ## $ boat : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 3 28 1 1 1 1 1 19 1 15 ... ## $ body : int NA NA NA NA 50 32 NA NA NA NA ... ## $ home.dest: Factor w/ 370 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ... data = titanic_ml %&gt;% select(-name, -ticket, -name, -body) data$pclass = as.factor(data$pclass) data$sex = as.factor(data$sex) data$survived = as.factor(data$survived) # Change easy things manually: data$embarked[data$embarked == &quot;&quot;] = &quot;S&quot; # Fill in &quot;empty&quot; values. data$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (&quot;&quot;). data$cabin = (data$cabin != &quot;&quot;) * 1 # Dummy code the availability of a cabin. data$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE) levels(data$home.dest)[levels(data$home.dest) == &quot;&quot;] = &quot;unknown&quot; levels(data$boat)[levels(data$boat) == &quot;&quot;] = &quot;none&quot; # Create a classification task. task = TaskClassif$new(id = &quot;titanic&quot;, backend = data, target = &quot;survived&quot;, positive = &quot;1&quot;) task$missings() ## survived age boat cabin embarked fare home.dest parch pclass sex sibsp ## 655 263 0 0 0 0 0 0 0 0 0 # Let&#39;s create the preprocessing graph. preprocessing = po(&quot;imputeoor&quot;) %&gt;&gt;% po(&quot;scale&quot;) %&gt;&gt;% po(&quot;encode&quot;) # Run the task. transformed_task = preprocessing$train(task)[[1]] transformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], &quot;validation&quot;) cv10 = mlr3::rsmp(&quot;cv&quot;, folds = 10L) rf = lrn(&quot;classif.ranger&quot;, predict_type = &quot;prob&quot;) measurement = msr(&quot;classif.auc&quot;) # result = mlr3::resample(transformed_task, rf, # resampling = cv10, store_models = TRUE) # # # Calculate the average AUC of the holdouts. # result$aggregate(measurement) # # pred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task, # row_ids = (1:nrow(data))[is.na(data$survived)])$data$prob[, &quot;1&quot;, drop = FALSE]) # # dim(pred) # predictions = round(apply(pred, 1, mean)) # # write.csv(data.frame(y = predictions), file = &quot;submission_RF.csv&quot;) References "],["deep.html", "5 Deep Learning 5.1 Network Architectures 5.2 Case Study: Dropout and Early Stopping in a Deep Neural Network 5.3 Case Study: Fitting a Convolutional Neural Network on MNIST 5.4 Advanced Training Techniques", " 5 Deep Learning In this section, we will discuss both, different (deep) network architectures and different means to regularize and improve those deep architectures. 5.1 Network Architectures 5.1.1 Deep Neural Networks (DNNs) Deep neural networks are basically the same as simple artificial neural networks, only that they have more hidden layers. 5.1.2 Convolutional Neural Networks (CNNs) The main purpose of convolutional neural networks is image recognition. (Sound can be understood as an image as well!) In a convolutional neural network, we have at least one convolution layer, additional to the normal, fully connected deep neural network layers. Neurons in a convolution layer are connected only to a small spatially contiguous area of the input layer (receptive field). We use this structure (feature map) to scan the entire features / neurons (e.g. picture). Think of the feature map as a kernel or filter (or imagine a sliding window with weighted pixels) that is used to scan the image. As the name is already indicating, this operation is a convolution in mathematics. The kernel weights are optimized, but we use the same weights across the entire input neurons (shared weights). The resulting (hidden) convolutional layer after training is called a feature map. You can think of the feature map as a map that shows you where the “shapes” expressed by the kernel appear in the input. One kernel / feature map will not be enough, we typically have many shapes that we want to recognize. Thus, the input layer is typically connected to several feature maps, which can be aggregated and followed by a second layer of feature maps, and so on. You get one convolution map/layer for each kernel of one convolutional layer. 5.1.3 Recurrent Neural Networks (RNNs) Recurrent neural networks are used to model sequential data, i.e. a temporal sequence that exhibits temporal dynamic behavior. Here is a good introduction to the topic: The following code snippet shows you many (technical) things you need for building more complex network structures, even with memory cells: library(deepviz) library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. tf$keras$backend$clear_session() # Resets especially layer counter. inputDimension1 = 50L inputDimension2 = 10L input1 = layer_input(shape = inputDimension1) input2 = layer_input(shape = inputDimension2) modelInput2 = input2 %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = inputDimension2, activation = &quot;gelu&quot;) modelMemory = input1 %&gt;% layer_embedding(input_dim = inputDimension1, output_dim = 64L) %&gt;% layer_lstm(units = 64L) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 2L, activation = &quot;sigmoid&quot;) modelDeep = input1 %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 64L, activation = &quot;relu&quot;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 64L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 64L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 5L, activation = &quot;sigmoid&quot;) modelMain = layer_concatenate(c(modelMemory, modelDeep, modelInput2)) %&gt;% layer_dropout(rate = 0.25) %&gt;% layer_dense(units = 64L, activation = &quot;relu&quot;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 64L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 2L, activation = &quot;sigmoid&quot;) model = keras_model( inputs = c(input1, input2), outputs = c(modelMain) # Use the whole modelMain (resp. its output) as output. ) summary(model) ## Model: &quot;model&quot; ## _________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # Connected to ## ================================================================================================================= ## input_1 (InputLayer) [(None, 50)] 0 [] ## ## dropout_3 (Dropout) (None, 50) 0 [&#39;input_1[0][0]&#39;] ## ## dense_5 (Dense) (None, 64) 3264 [&#39;dropout_3[0][0]&#39;] ## ## embedding (Embedding) (None, 50, 64) 3200 [&#39;input_1[0][0]&#39;] ## ## dropout_2 (Dropout) (None, 64) 0 [&#39;dense_5[0][0]&#39;] ## ## lstm (LSTM) (None, 64) 33024 [&#39;embedding[0][0]&#39;] ## ## dense_4 (Dense) (None, 64) 4160 [&#39;dropout_2[0][0]&#39;] ## ## input_2 (InputLayer) [(None, 10)] 0 [] ## ## dropout_1 (Dropout) (None, 64) 0 [&#39;lstm[0][0]&#39;] ## ## dense_3 (Dense) (None, 64) 4160 [&#39;dense_4[0][0]&#39;] ## ## dropout (Dropout) (None, 10) 0 [&#39;input_2[0][0]&#39;] ## ## dense_1 (Dense) (None, 2) 130 [&#39;dropout_1[0][0]&#39;] ## ## dense_2 (Dense) (None, 5) 325 [&#39;dense_3[0][0]&#39;] ## ## dense (Dense) (None, 10) 110 [&#39;dropout[0][0]&#39;] ## ## concatenate (Concatenate) (None, 17) 0 [&#39;dense_1[0][0]&#39;, ## &#39;dense_2[0][0]&#39;, ## &#39;dense[0][0]&#39;] ## ## dropout_5 (Dropout) (None, 17) 0 [&#39;concatenate[0][0]&#39;] ## ## dense_8 (Dense) (None, 64) 1152 [&#39;dropout_5[0][0]&#39;] ## ## dropout_4 (Dropout) (None, 64) 0 [&#39;dense_8[0][0]&#39;] ## ## dense_7 (Dense) (None, 64) 4160 [&#39;dropout_4[0][0]&#39;] ## ## dense_6 (Dense) (None, 2) 130 [&#39;dense_7[0][0]&#39;] ## ## ================================================================================================================= ## Total params: 53,815 ## Trainable params: 53,815 ## Non-trainable params: 0 ## _________________________________________________________________________________________________________________ # model %&gt;% plot_model() 5.1.4 Natural Language Processing (NLP) Natural language processing is actually more of a task than a network structure, but in the area of deep learning for natural language processing, particular network structures are used. The following video should give you an idea about what NLP is about. See also the blog post linked with the Youtube video with accompanying code. Moreover, here is an article that shows how natural language processing works with Keras, however, written in Python. As a challenge, you can take the code and implement it in R. 5.2 Case Study: Dropout and Early Stopping in a Deep Neural Network Regularization in deep neural networks is very important because of the problem of overfitting. Standard regularization from statistics like \\(L1\\) and \\(L2\\) regularization are often fuzzy and require a lot of tuning. There are more stable and robust methods: Early stopping: Early stopping allows us to stop the training when for instance the test loss does not decrease anymore or the validation loss starts increasing. Dropout: The Dropout layer randomly sets input units to 0 with a frequency of a given rate at each step during training time, which helps prevent overfitting. Dropout is more robust than \\(L1\\) and \\(L2\\), and tuning of the dropout rate can be beneficial but a rate between \\(0.2-0.5\\) often works quite well. Data preparation See 4.6 for explanation about the preprocessing pipeline. library(EcoData) data(nasa) str(nasa) ## &#39;data.frame&#39;: 4687 obs. of 40 variables: ## $ Neo.Reference.ID : int 3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ... ## $ Name : int NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ... ## $ Absolute.Magnitude : num 18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ... ## $ Est.Dia.in.KM.min. : num 0.4837 0.1011 0.0291 0.1272 0.1395 ... ## $ Est.Dia.in.KM.max. : num 1.0815 0.226 0.0652 0.2845 0.3119 ... ## $ Est.Dia.in.M.min. : num 483.7 NA 29.1 127.2 139.5 ... ## $ Est.Dia.in.M.max. : num 1081.5 226 65.2 284.5 311.9 ... ## $ Est.Dia.in.Miles.min. : num 0.3005 0.0628 NA 0.0791 0.0867 ... ## $ Est.Dia.in.Miles.max. : num 0.672 0.1404 0.0405 0.1768 0.1938 ... ## $ Est.Dia.in.Feet.min. : num 1586.9 331.5 95.6 417.4 457.7 ... ## $ Est.Dia.in.Feet.max. : num 3548 741 214 933 1023 ... ## $ Close.Approach.Date : Factor w/ 777 levels &quot;1995-01-01&quot;,&quot;1995-01-08&quot;,..: 511 712 472 239 273 145 428 694 87 732 ... ## $ Epoch.Date.Close.Approach : num NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ... ## $ Relative.Velocity.km.per.sec: num 11.22 13.57 5.75 13.84 4.61 ... ## $ Relative.Velocity.km.per.hr : num 40404 48867 20718 49821 16583 ... ## $ Miles.per.hour : num 25105 30364 12873 30957 10304 ... ## $ Miss.Dist..Astronomical. : num NA 0.0671 0.013 0.0583 0.0381 ... ## $ Miss.Dist..lunar. : num 112.7 26.1 NA 22.7 14.8 ... ## $ Miss.Dist..kilometers. : num 43348668 10030753 1949933 NA 5694558 ... ## $ Miss.Dist..miles. : num 26935614 6232821 1211632 5418692 3538434 ... ## $ Orbiting.Body : Factor w/ 1 level &quot;Earth&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Orbit.ID : int NA 8 12 12 91 NA 24 NA NA 212 ... ## $ Orbit.Determination.Date : Factor w/ 2680 levels &quot;2014-06-13 15:20:44&quot;,..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ... ## $ Orbit.Uncertainity : int 0 8 6 0 0 0 1 1 1 0 ... ## $ Minimum.Orbit.Intersection : num NA 0.05594 0.00553 NA 0.0281 ... ## $ Jupiter.Tisserand.Invariant : num 5.58 3.61 4.44 5.5 NA ... ## $ Epoch.Osculation : num 2457800 2457010 NA 2458000 2458000 ... ## $ Eccentricity : num 0.276 0.57 0.344 0.255 0.22 ... ## $ Semi.Major.Axis : num 1.1 NA 1.52 1.11 1.24 ... ## $ Inclination : num 20.06 4.39 5.44 23.9 3.5 ... ## $ Asc.Node.Longitude : num 29.85 1.42 170.68 356.18 183.34 ... ## $ Orbital.Period : num 419 1040 682 427 503 ... ## $ Perihelion.Distance : num 0.794 0.864 0.994 0.828 0.965 ... ## $ Perihelion.Arg : num 41.8 359.3 350 268.2 179.2 ... ## $ Aphelion.Dist : num 1.4 3.15 2.04 1.39 1.51 ... ## $ Perihelion.Time : num 2457736 2456941 2457937 NA 2458070 ... ## $ Mean.Anomaly : num 55.1 NA NA 297.4 310.5 ... ## $ Mean.Motion : num 0.859 0.346 0.528 0.843 0.716 ... ## $ Equinox : Factor w/ 1 level &quot;J2000&quot;: 1 1 NA 1 1 1 1 1 1 1 ... ## $ Hazardous : int 0 0 0 1 1 0 0 0 1 1 ... library(tidyverse) library(mlr3) library(mlr3pipelines) library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = nasa %&gt;% select(-Orbit.Determination.Date, -Close.Approach.Date, -Name, -Neo.Reference.ID) data$Hazardous = as.factor(data$Hazardous) task = TaskClassif$new(id = &quot;nasa&quot;, backend = data, target = &quot;Hazardous&quot;, positive = &quot;1&quot;) preprocessing = po(&quot;imputeoor&quot;) %&gt;&gt;% po(&quot;scale&quot;) %&gt;&gt;% po(&quot;encode&quot;) data = preprocessing$train(task)[[1]]$data() train = data[!is.na(data$Hazardous),] submit = data[is.na(data$Hazardous),] X = scale(train %&gt;% select(-Hazardous)) Y = train %&gt;% select(Hazardous) Y = to_categorical(as.matrix(Y), 2) Early stopping library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, input_shape = ncol(X)) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = ncol(Y), activation = &quot;softmax&quot;) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(learning_rate = 0.001)) summary(model) ## Model: &quot;sequential_1&quot; ## _____________________________________________________________________ ## Layer (type) Output Shape Param # ## ===================================================================== ## dense_16 (Dense) (None, 50) 1900 ## ## dense_15 (Dense) (None, 50) 2550 ## ## dense_14 (Dense) (None, 50) 2550 ## ## dense_13 (Dense) (None, 2) 102 ## ## ===================================================================== ## Total params: 7,102 ## Trainable params: 7,102 ## Non-trainable params: 0 ## _____________________________________________________________________ model_history = model %&gt;% fit(x = X, y = Y, epochs = 50L, batch_size = 20L, shuffle = TRUE, validation_split = 0.4) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; The validation loss first decreases but then starts increasing again, can you explain this behavior? \\(\\rightarrow\\) Overfitting! Let’s try an \\(L1+L2\\) regularization: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, input_shape = ncol(X), kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %&gt;% layer_dense(units = ncol(Y), activation = &quot;softmax&quot;, kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(learning_rate = 0.001)) summary(model) ## Model: &quot;sequential_4&quot; ## _____________________________________________________________________ ## Layer (type) Output Shape Param # ## ===================================================================== ## dense_28 (Dense) (None, 50) 1900 ## ## dense_27 (Dense) (None, 50) 2550 ## ## dense_26 (Dense) (None, 50) 2550 ## ## dense_25 (Dense) (None, 2) 102 ## ## ===================================================================== ## Total params: 7,102 ## Trainable params: 7,102 ## Non-trainable params: 0 ## _____________________________________________________________________ model_history = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.4) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Better, but the validation loss still starts increasing after 40 epochs. We can use early stopping to end the training before the validation loss starts increasing again! library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, input_shape = ncol(X), kernel_regularizer = regularizer_l1_l2( 0.001, 0.001)) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %&gt;% layer_dense(units = ncol(Y), activation = &quot;softmax&quot;, kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(learning_rate = 0.001)) summary(model) ## Model: &quot;sequential_6&quot; ## ____________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ==================================================================================================================== ## dense_36 (Dense) (None, 50) 1900 ## ## dense_35 (Dense) (None, 50) 2550 ## ## dense_34 (Dense) (None, 50) 2550 ## ## dense_33 (Dense) (None, 2) 102 ## ## ==================================================================================================================== ## Total params: 7,102 ## Trainable params: 7,102 ## Non-trainable params: 0 ## ____________________________________________________________________________________________________________________ early = keras::callback_early_stopping(patience = 5L) model_history = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.4, callbacks = c(early)) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Patience is the number of epochs to wait before aborting the training. Dropout - another type of regularization Srivastava et al. (2014) suggests a dropout rate of 50% for internal hidden layers and 20% for the input layer. One advantage of dropout is that the training is more independent of the number of epochs i.e. the validation loss usually doesn’t start to increase after several epochs. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model = keras_model_sequential() model %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;, input_shape = ncol(X)) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 50L, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = ncol(Y), activation = &quot;softmax&quot;) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, keras::optimizer_adamax(learning_rate = 0.001)) model_history = model %&gt;% fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE, validation_split = 0.4) plot(model_history) ## `geom_smooth()` using formula &#39;y ~ x&#39; Of course, you can still combine early stopping and dropout, which is normally a good idea since it improves training efficiency (e.g. you could start with 1000 epochs and you know training will be aborted if it doesn’t improve anymore). Torch Dropout and early stopping with Torch: library(torch) torch_manual_seed(321L) set.seed(123) model_torch = nn_sequential( nn_dropout(0.2), nn_linear(ncol(X), 50L), nn_relu(), nn_dropout(0.5), nn_linear(50L, 50L), nn_relu(), nn_dropout(0.5), nn_linear(50L, 50L), nn_relu(), nn_dropout(0.5), nn_linear(50L, 2L) ) YT = apply(Y, 1, which.max) dataset_nasa = dataset( name = &quot;nasa&quot;, initialize = function(nasa){ self$X = nasa$X self$Y = nasa$Y }, .getitem = function(i){ X = self$X[i,,drop = FALSE] %&gt;% torch_tensor() Y = self$Y[i] %&gt;% torch_tensor() list(X, Y) }, .length = function(){ nrow(self$X) }) train_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), batch_size = 32, shuffle = TRUE) test_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), batch_size = 32) model_torch$train() opt = optim_adam(model_torch$parameters, 0.01) train_losses = c() test_losses = c() early_epoch = 0 min_loss = Inf patience = 5 for(epoch in 1:50){ if(early_epoch &gt;= patience){ break } train_loss = c() test_loss = c() coro::loop( for(batch in train_dl){ opt$zero_grad() pred = model_torch(batch[[1]]$squeeze()) loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = &quot;mean&quot;) loss$backward() opt$step() train_loss = c(train_loss, loss$item()) } ) coro::loop( for(batch in test_dl){ pred = model_torch(batch[[1]]$squeeze()) loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = &quot;mean&quot;) test_loss = c(test_loss, loss$item()) } ) ### Early stopping ### if(mean(test_loss) &lt; min_loss){ min_loss = mean(test_loss) early_epoch = 0 } else { early_epoch = early_epoch + 1 } ### train_losses = c(train_losses, mean(train_loss)) test_losses = c(test_losses, mean(test_loss)) cat(sprintf(&quot;Loss at epoch %d: %3f\\n&quot;, epoch, mean(train_loss))) } ## Loss at epoch 1: 0.557363 ## Loss at epoch 2: 0.478059 ## Loss at epoch 3: 0.454506 ## Loss at epoch 4: 0.469278 ## Loss at epoch 5: 0.414451 ## Loss at epoch 6: 0.420797 ## Loss at epoch 7: 0.429624 ## Loss at epoch 8: 0.439022 ## Loss at epoch 9: 0.418324 ## Loss at epoch 10: 0.398850 ## Loss at epoch 11: 0.420840 ## Loss at epoch 12: 0.412696 ## Loss at epoch 13: 0.397618 ## Loss at epoch 14: 0.428140 ## Loss at epoch 15: 0.368084 ## Loss at epoch 16: 0.394033 matplot(cbind(train_losses, test_losses), type = &quot;o&quot;, pch = c(15, 16), col = c(&quot;darkblue&quot;, &quot;darkred&quot;), lty = 1, xlab = &quot;Epoch&quot;, ylab = &quot;Loss&quot;, las = 1) legend(&quot;topright&quot;, bty = &quot;n&quot;, col = c(&quot;darkblue&quot;, &quot;darkred&quot;), lty = 1, pch = c(15, 16), legend = c(&quot;Train loss&quot;, &quot;Val loss&quot;) ) Task In this section, we will go through tuning a “deep” neural network, using the NASA data set from kaggle (available via EcoData, see data description via help). You can start immediately, or get inspiration by reading section above (“Case study: dropout and early stopping in a deep neural network”). A basic network for this problem follows: library(tensorflow) library(keras) library(tidyverse) library(missRanger) library(Metrics) library(EcoData) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data(&quot;nasa&quot;) data = nasa data$subset = ifelse(is.na(data$Hazardous), &quot;test&quot;, &quot;train&quot;) ## Explore and clean data. str(data) ## &#39;data.frame&#39;: 4687 obs. of 41 variables: ## $ Neo.Reference.ID : int 3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ... ## $ Name : int NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ... ## $ Absolute.Magnitude : num 18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ... ## $ Est.Dia.in.KM.min. : num 0.4837 0.1011 0.0291 0.1272 0.1395 ... ## $ Est.Dia.in.KM.max. : num 1.0815 0.226 0.0652 0.2845 0.3119 ... ## $ Est.Dia.in.M.min. : num 483.7 NA 29.1 127.2 139.5 ... ## $ Est.Dia.in.M.max. : num 1081.5 226 65.2 284.5 311.9 ... ## $ Est.Dia.in.Miles.min. : num 0.3005 0.0628 NA 0.0791 0.0867 ... ## $ Est.Dia.in.Miles.max. : num 0.672 0.1404 0.0405 0.1768 0.1938 ... ## $ Est.Dia.in.Feet.min. : num 1586.9 331.5 95.6 417.4 457.7 ... ## $ Est.Dia.in.Feet.max. : num 3548 741 214 933 1023 ... ## $ Close.Approach.Date : Factor w/ 777 levels &quot;1995-01-01&quot;,&quot;1995-01-08&quot;,..: 511 712 472 239 273 145 428 694 87 732 ... ## $ Epoch.Date.Close.Approach : num NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ... ## $ Relative.Velocity.km.per.sec: num 11.22 13.57 5.75 13.84 4.61 ... ## $ Relative.Velocity.km.per.hr : num 40404 48867 20718 49821 16583 ... ## $ Miles.per.hour : num 25105 30364 12873 30957 10304 ... ## $ Miss.Dist..Astronomical. : num NA 0.0671 0.013 0.0583 0.0381 ... ## $ Miss.Dist..lunar. : num 112.7 26.1 NA 22.7 14.8 ... ## $ Miss.Dist..kilometers. : num 43348668 10030753 1949933 NA 5694558 ... ## $ Miss.Dist..miles. : num 26935614 6232821 1211632 5418692 3538434 ... ## $ Orbiting.Body : Factor w/ 1 level &quot;Earth&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Orbit.ID : int NA 8 12 12 91 NA 24 NA NA 212 ... ## $ Orbit.Determination.Date : Factor w/ 2680 levels &quot;2014-06-13 15:20:44&quot;,..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ... ## $ Orbit.Uncertainity : int 0 8 6 0 0 0 1 1 1 0 ... ## $ Minimum.Orbit.Intersection : num NA 0.05594 0.00553 NA 0.0281 ... ## $ Jupiter.Tisserand.Invariant : num 5.58 3.61 4.44 5.5 NA ... ## $ Epoch.Osculation : num 2457800 2457010 NA 2458000 2458000 ... ## $ Eccentricity : num 0.276 0.57 0.344 0.255 0.22 ... ## $ Semi.Major.Axis : num 1.1 NA 1.52 1.11 1.24 ... ## $ Inclination : num 20.06 4.39 5.44 23.9 3.5 ... ## $ Asc.Node.Longitude : num 29.85 1.42 170.68 356.18 183.34 ... ## $ Orbital.Period : num 419 1040 682 427 503 ... ## $ Perihelion.Distance : num 0.794 0.864 0.994 0.828 0.965 ... ## $ Perihelion.Arg : num 41.8 359.3 350 268.2 179.2 ... ## $ Aphelion.Dist : num 1.4 3.15 2.04 1.39 1.51 ... ## $ Perihelion.Time : num 2457736 2456941 2457937 NA 2458070 ... ## $ Mean.Anomaly : num 55.1 NA NA 297.4 310.5 ... ## $ Mean.Motion : num 0.859 0.346 0.528 0.843 0.716 ... ## $ Equinox : Factor w/ 1 level &quot;J2000&quot;: 1 1 NA 1 1 1 1 1 1 1 ... ## $ Hazardous : int 0 0 0 1 1 0 0 0 1 1 ... ## $ subset : chr &quot;train&quot; &quot;train&quot; &quot;train&quot; &quot;train&quot; ... summary(data) ## Neo.Reference.ID Name Absolute.Magnitude Est.Dia.in.KM.min. Est.Dia.in.KM.max. Est.Dia.in.M.min. ## Min. :2000433 Min. :2000433 Min. :11.16 Min. : 0.00101 Min. : 0.00226 Min. : 1.011 ## 1st Qu.:3102682 1st Qu.:3102683 1st Qu.:20.10 1st Qu.: 0.03346 1st Qu.: 0.07482 1st Qu.: 33.462 ## Median :3514800 Median :3514800 Median :21.90 Median : 0.11080 Median : 0.24777 Median : 110.804 ## Mean :3272675 Mean :3273113 Mean :22.27 Mean : 0.20523 Mean : 0.45754 Mean : 204.649 ## 3rd Qu.:3690987 3rd Qu.:3690385 3rd Qu.:24.50 3rd Qu.: 0.25384 3rd Qu.: 0.56760 3rd Qu.: 253.837 ## Max. :3781897 Max. :3781897 Max. :32.10 Max. :15.57955 Max. :34.83694 Max. :15579.552 ## NA&#39;s :53 NA&#39;s :57 NA&#39;s :36 NA&#39;s :60 NA&#39;s :23 NA&#39;s :29 ## Est.Dia.in.M.max. Est.Dia.in.Miles.min. Est.Dia.in.Miles.max. Est.Dia.in.Feet.min. Est.Dia.in.Feet.max. ## Min. : 2.26 Min. :0.00063 Min. : 0.00140 Min. : 3.32 Min. : 7.41 ## 1st Qu.: 74.82 1st Qu.:0.02079 1st Qu.: 0.04649 1st Qu.: 109.78 1st Qu.: 245.49 ## Median : 247.77 Median :0.06885 Median : 0.15395 Median : 363.53 Median : 812.88 ## Mean : 458.45 Mean :0.12734 Mean : 0.28486 Mean : 670.44 Mean : 1500.77 ## 3rd Qu.: 567.60 3rd Qu.:0.15773 3rd Qu.: 0.35269 3rd Qu.: 832.80 3rd Qu.: 1862.19 ## Max. :34836.94 Max. :9.68068 Max. :21.64666 Max. :51114.02 Max. :114294.42 ## NA&#39;s :46 NA&#39;s :42 NA&#39;s :50 NA&#39;s :21 NA&#39;s :46 ## Close.Approach.Date Epoch.Date.Close.Approach Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr ## 2016-07-22: 18 Min. :7.889e+11 Min. : 0.3355 Min. : 1208 ## 2015-01-15: 17 1st Qu.:1.016e+12 1st Qu.: 8.4497 1st Qu.: 30399 ## 2015-02-15: 16 Median :1.203e+12 Median :12.9370 Median : 46532 ## 2007-11-08: 15 Mean :1.180e+12 Mean :13.9848 Mean : 50298 ## 2012-01-15: 15 3rd Qu.:1.356e+12 3rd Qu.:18.0774 3rd Qu.: 65068 ## (Other) :4577 Max. :1.473e+12 Max. :44.6337 Max. :160681 ## NA&#39;s : 29 NA&#39;s :43 NA&#39;s :27 NA&#39;s :28 ## Miles.per.hour Miss.Dist..Astronomical. Miss.Dist..lunar. Miss.Dist..kilometers. Miss.Dist..miles. ## Min. : 750.5 Min. :0.00018 Min. : 0.06919 Min. : 26610 Min. : 16535 ## 1st Qu.:18846.7 1st Qu.:0.13341 1st Qu.: 51.89874 1st Qu.:19964907 1st Qu.:12454813 ## Median :28893.7 Median :0.26497 Median :103.19415 Median :39685408 Median :24662435 ## Mean :31228.0 Mean :0.25690 Mean : 99.91366 Mean :38436154 Mean :23885560 ## 3rd Qu.:40436.9 3rd Qu.:0.38506 3rd Qu.:149.59244 3rd Qu.:57540318 3rd Qu.:35714721 ## Max. :99841.2 Max. :0.49988 Max. :194.45491 Max. :74781600 Max. :46467132 ## NA&#39;s :38 NA&#39;s :60 NA&#39;s :30 NA&#39;s :56 NA&#39;s :27 ## Orbiting.Body Orbit.ID Orbit.Determination.Date Orbit.Uncertainity Minimum.Orbit.Intersection ## Earth:4665 Min. : 1.00 2017-06-21 06:17:20: 9 Min. :0.000 Min. :0.00000 ## NA&#39;s : 22 1st Qu.: 9.00 2017-04-06 08:57:13: 8 1st Qu.:0.000 1st Qu.:0.01435 ## Median : 16.00 2017-04-06 09:24:24: 8 Median :3.000 Median :0.04653 ## Mean : 28.34 2017-04-06 08:24:13: 7 Mean :3.521 Mean :0.08191 ## 3rd Qu.: 31.00 2017-04-06 08:26:19: 7 3rd Qu.:6.000 3rd Qu.:0.12150 ## Max. :611.00 (Other) :4622 Max. :9.000 Max. :0.47789 ## NA&#39;s :33 NA&#39;s : 26 NA&#39;s :49 NA&#39;s :137 ## Jupiter.Tisserand.Invariant Epoch.Osculation Eccentricity Semi.Major.Axis Inclination ## Min. :2.196 Min. :2450164 Min. :0.00752 Min. :0.6159 Min. : 0.01451 ## 1st Qu.:4.047 1st Qu.:2458000 1st Qu.:0.24086 1st Qu.:1.0012 1st Qu.: 4.93290 ## Median :5.071 Median :2458000 Median :0.37251 Median :1.2422 Median :10.27694 ## Mean :5.056 Mean :2457723 Mean :0.38267 Mean :1.4009 Mean :13.36159 ## 3rd Qu.:6.017 3rd Qu.:2458000 3rd Qu.:0.51256 3rd Qu.:1.6782 3rd Qu.:19.47848 ## Max. :9.025 Max. :2458020 Max. :0.96026 Max. :5.0720 Max. :75.40667 ## NA&#39;s :56 NA&#39;s :60 NA&#39;s :39 NA&#39;s :53 NA&#39;s :42 ## Asc.Node.Longitude Orbital.Period Perihelion.Distance Perihelion.Arg Aphelion.Dist Perihelion.Time ## Min. : 0.0019 Min. : 176.6 Min. :0.08074 Min. : 0.0069 Min. :0.8038 Min. :2450100 ## 1st Qu.: 83.1849 1st Qu.: 365.9 1st Qu.:0.63038 1st Qu.: 95.6430 1st Qu.:1.2661 1st Qu.:2457815 ## Median :172.6347 Median : 504.9 Median :0.83288 Median :189.7729 Median :1.6182 Median :2457972 ## Mean :172.1717 Mean : 635.5 Mean :0.81316 Mean :184.0185 Mean :1.9864 Mean :2457726 ## 3rd Qu.:254.8804 3rd Qu.: 793.1 3rd Qu.:0.99718 3rd Qu.:271.9535 3rd Qu.:2.4497 3rd Qu.:2458108 ## Max. :359.9059 Max. :4172.2 Max. :1.29983 Max. :359.9931 Max. :8.9839 Max. :2458839 ## NA&#39;s :60 NA&#39;s :46 NA&#39;s :22 NA&#39;s :48 NA&#39;s :38 NA&#39;s :59 ## Mean.Anomaly Mean.Motion Equinox Hazardous subset ## Min. : 0.0032 Min. :0.08628 J2000:4663 Min. :0.000 Length:4687 ## 1st Qu.: 87.0069 1st Qu.:0.45147 NA&#39;s : 24 1st Qu.:0.000 Class :character ## Median :186.0219 Median :0.71137 Median :0.000 Mode :character ## Mean :181.2882 Mean :0.73732 Mean :0.176 ## 3rd Qu.:276.6418 3rd Qu.:0.98379 3rd Qu.:0.000 ## Max. :359.9180 Max. :2.03900 Max. :1.000 ## NA&#39;s :40 NA&#39;s :48 NA&#39;s :4187 # Remove Equinox and other features. data = data %&gt;% select(-Equinox, -Orbiting.Body, -Orbit.Determination.Date, -Close.Approach.Date) # Impute missing values using a random forest. imputed = data %&gt;% select(-Hazardous) %&gt;% missRanger::missRanger(maxiter = 5L, num.trees = 20L) ## ## Missing value imputation by random forests ## ## Variables to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion ## Variables used to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, subset ## iter 1: ................................... ## iter 2: ................................... ## iter 3: ................................... ## iter 4: ................................... # See the usual function call: # data_impute = data %&gt;% select(-Hazardous) # imputed = missRanger::missRanger(data_impute, maxiter = 5L, num.trees = 20L) # Scale data. data = cbind( data %&gt;% select(Hazardous), scale(imputed %&gt;% select(-subset)), data.frame(&quot;subset&quot; = data$subset) ) ## Outer split. train = data[data$subset == &quot;train&quot;, ] test = data[data$subset == &quot;test&quot;, ] train = train %&gt;% select(-subset) test = test %&gt;% select(-subset) ## 10-fold cross-validation: len = nrow(train) ord = sample.int(len) k = 10 cv_indices = lapply(0:9, function(i) sort(ord[(i*len/k + 1):((i+1)*len/k)])) result = matrix(NA, 10L, 2L) colnames(result) = c(&quot;train_auc&quot;, &quot;test_auc&quot;) for(i in 1:10) { indices = cv_indices[[i]] sub_train = train[-indices,] # Leave one &quot;bucket&quot; out. sub_test = train[indices,] # &quot;Deep&quot; neural networks and regularization: model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = ncol(sub_train) - 1L) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.01)) model_history = model %&gt;% fit( x = as.matrix(sub_train %&gt;% select(-Hazardous)), y = as.matrix(sub_train %&gt;% select(Hazardous)), validation_split = 0.2, epochs = 35L, batch = 50L, shuffle = TRUE ) plot(model_history) pred_train = predict(model, as.matrix(sub_train %&gt;% select(-Hazardous))) pred_test = predict(model, as.matrix(sub_test %&gt;% select(-Hazardous))) # AUC: Area under the (ROC) curve, this is a performance measure [0, 1]. # 0.5 is worst for a binary classifier. # 1 perfectly classifies all samples, 0 perfectly misclassifies all samples. result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train) result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test) } print(result) ## train_auc test_auc ## [1,] 0.9981234 0.9268293 ## [2,] 0.9984570 0.9866667 ## [3,] 0.9960277 0.9822222 ## [4,] 0.9965881 0.9650000 ## [5,] 0.9987838 0.9017857 ## [6,] 0.9976351 0.8809524 ## [7,] 0.9932803 0.9824561 ## [8,] 0.9933126 0.9972900 ## [9,] 0.9944652 0.9484127 ## [10,] 0.9968919 0.9642857 (colMeans(result)) ## train_auc test_auc ## 0.9963565 0.9535901 # The model setup seems to be fine. ## Train and predict for outer validation split (on the complete training data): model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = ncol(sub_train) - 1L) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.01)) model_history = model %&gt;% fit( x = as.matrix(train %&gt;% select(-Hazardous)), y = as.matrix(train %&gt;% select(Hazardous)), validation_split = 0.2, epochs = 35L, batch = 50L, shuffle = TRUE ) pred = round(predict(model, as.matrix(test[,-1]))) write.csv(data.frame(y = pred), file = &quot;submission_DNN.csv&quot;) Go trough the code line by line and try to understand it. Especially have a focus on the general machine learning workflow (remember the general steps), the new type of imputation and the hand coded 10-fold cross-validation (the for loop). Task Use early stopping and dropout (i.e. use these options, we explained in the book now) within the same algorithm from above and compare them. Try to tune the network (play around with the number of layers, the width of the layers, dropout layers, early stopping and other regularization and so on) to make better predicitons (monitor the the training and validation loss). In the end, submit predictions to the submission server (if you have time you can also transfer your new knowledge to the titanic data set)! Solution library(tensorflow) library(keras) library(tidyverse) library(missRanger) library(Metrics) library(EcoData) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data(&quot;nasa&quot;) data = nasa data$subset = ifelse(is.na(data$Hazardous), &quot;test&quot;, &quot;train&quot;) data = data %&gt;% select(-Equinox, -Orbiting.Body, -Orbit.Determination.Date, -Close.Approach.Date) imputed = data %&gt;% select(-Hazardous) %&gt;% missRanger::missRanger(maxiter = 5L, num.trees = 20L) ## ## Missing value imputation by random forests ## ## Variables to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion ## Variables used to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, subset ## iter 1: ................................... ## iter 2: ................................... ## iter 3: ................................... ## iter 4: ................................... data = cbind( data %&gt;% select(Hazardous), scale(imputed %&gt;% select(-subset)), data.frame(&quot;subset&quot; = data$subset) ) train = data[data$subset == &quot;train&quot;, ] test = data[data$subset == &quot;test&quot;, ] train = train %&gt;% select(-subset) test = test %&gt;% select(-subset) len = nrow(train) ord = sample.int(len) k = 10 cv_indices = lapply(0:9, function(i) sort(ord[(i*len/k + 1):((i+1)*len/k)])) result = matrix(NA, 10L, 2L) colnames(result) = c(&quot;train_auc&quot;, &quot;test_auc&quot;) for(i in 1:10) { indices = cv_indices[[i]] sub_train = train[-indices,] sub_test = train[indices,] # &quot;Deep&quot; neural networks and regularization: model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = ncol(sub_train) - 1L) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) early_stopping = callback_early_stopping(monitor = &quot;val_loss&quot;, patience = 5L) # You need a validation split for this! model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.01)) model_history = model %&gt;% fit( x = as.matrix(sub_train %&gt;% select(-Hazardous)), y = as.matrix(sub_train %&gt;% select(Hazardous)), callbacks = c(early_stopping), validation_split = 0.2, epochs = 35L, batch = 50L, shuffle = TRUE ) plot(model_history) pred_train = predict(model, as.matrix(sub_train %&gt;% select(-Hazardous))) pred_test = predict(model, as.matrix(sub_test %&gt;% select(-Hazardous))) result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train) result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test) } print(result) ## train_auc test_auc ## [1,] 0.9956669 0.9376694 ## [2,] 0.9939923 0.9911111 ## [3,] 0.9859492 0.9422222 ## [4,] 0.9965536 0.9500000 ## [5,] 0.9976351 0.9107143 ## [6,] 0.9961486 0.8809524 ## [7,] 0.9894807 0.9868421 ## [8,] 0.9908902 1.0000000 ## [9,] 0.9936386 0.9583333 ## [10,] 0.9947297 0.9642857 (colMeans(result)) ## train_auc test_auc ## 0.9934685 0.9522131 # The model setup seems to be fine. ## Train and predict for outer validation split (on the complete training data): model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = ncol(sub_train) - 1L) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) early_stopping = callback_early_stopping(monitor = &quot;val_loss&quot;, patience = 5L) # You need a validation split for this! model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.01)) model_history = model %&gt;% fit( x = as.matrix(train %&gt;% select(-Hazardous)), y = as.matrix(train %&gt;% select(Hazardous)), callbacks = c(early_stopping), validation_split = 0.2, epochs = 35L, batch = 50L, shuffle = TRUE ) pred = round(predict(model, as.matrix(test[,-1]))) write.csv(data.frame(y = pred), file = &quot;submission_DNN_dropout_early.csv&quot;) Better predictions: set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. result = matrix(NA, 10L, 2L) colnames(result) = c(&quot;train_auc&quot;, &quot;test_auc&quot;) for(i in 1:10) { indices = cv_indices[[i]] sub_train = train[-indices,] sub_test = train[indices,] # &quot;Deep&quot; neural networks and regularization: model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = ncol(sub_train) - 1L) %&gt;% layer_dropout(rate = 0.45) %&gt;% layer_dense(units = 50L, activation = &quot;gelu&quot;, kernel_regularizer = regularizer_l1(.00125), bias_regularizer = regularizer_l1_l2(.25) ) %&gt;% layer_dropout(rate = 0.35) %&gt;% layer_dense(units = 30L, activation = &quot;relu&quot;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) early_stopping = callback_early_stopping(monitor = &quot;val_loss&quot;, patience = 8L) # You need a validation split for this! model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.0072)) model_history = model %&gt;% fit( x = as.matrix(sub_train %&gt;% select(-Hazardous)), y = as.matrix(sub_train %&gt;% select(Hazardous)), callbacks = c(early_stopping), validation_split = 0.2, epochs = 50L, batch = 50L, shuffle = TRUE ) plot(model_history) pred_train = predict(model, as.matrix(sub_train %&gt;% select(-Hazardous))) pred_test = predict(model, as.matrix(sub_test %&gt;% select(-Hazardous))) result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train) result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test) } print(result) ## train_auc test_auc ## [1,] 0.9971340 0.9620596 ## [2,] 0.9906766 0.9955556 ## [3,] 0.9927448 0.9866667 ## [4,] 0.9953474 0.9625000 ## [5,] 0.9927027 0.9285714 ## [6,] 0.9948311 0.8839286 ## [7,] 0.9900084 0.9912281 ## [8,] 0.9926644 1.0000000 ## [9,] 0.9923088 0.9543651 ## [10,] 0.9901014 0.9821429 (colMeans(result)) ## train_auc test_auc ## 0.9928520 0.9647018 model = keras_model_sequential() model %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, input_shape = ncol(sub_train) - 1L) %&gt;% layer_dropout(rate = 0.45) %&gt;% layer_dense(units = 50L, activation = &quot;gelu&quot;, kernel_regularizer = regularizer_l1(.00125), bias_regularizer = regularizer_l1_l2(.25) ) %&gt;% layer_dropout(rate = 0.35) %&gt;% layer_dense(units = 30L, activation = &quot;relu&quot;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) early_stopping = callback_early_stopping(monitor = &quot;val_loss&quot;, patience = 8L) # You need a validation split for this! model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.0072)) model_history = model %&gt;% fit( x = as.matrix(train %&gt;% select(-Hazardous)), y = as.matrix(train %&gt;% select(Hazardous)), callbacks = c(early_stopping), validation_split = 0.2, epochs = 50L, batch = 50L, shuffle = TRUE ) pred = round(predict(model, as.matrix(test[,-1]))) write.csv(data.frame(y = pred), file = &quot;submission_DNN_optimal.csv&quot;) Task 5.3 Case Study: Fitting a Convolutional Neural Network on MNIST We will show the use of convolutional neural networks with the MNIST data set. This data set is maybe one of the most famous image data sets. It consists of 60,000 handwritten digits from 0-9. To do so, we define a few helper functions: library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. rotate = function(x){ t(apply(x, 2, rev)) } imgPlot = function(img, title = &quot;&quot;){ col = grey.colors(255) image(rotate(img), col = col, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, main = paste0(&quot;Label: &quot;, as.character(title))) } The MNIST data set is so famous that there is an automatic download function in Keras: data = dataset_mnist() train = data$train test = data$test Let’s visualize a few digits: oldpar = par(mfrow = c(1, 3)) .n = sapply(1:3, function(x) imgPlot(train$x[x,,], train$y[x])) par(oldpar) Similar to the normal machine learning workflow, we have to scale the pixels (from 0-255) to the range of \\([0, 1]\\) and one hot encode the response. For scaling the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2D array/tensor is typically called a matrix. train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) The last dimension denotes the number of channels in the image. In our case we have only one channel because the images are black and white. Most times, we would have at least 3 color channels, for example RGB (red, green, blue) or HSV (hue, saturation, value), sometimes with several additional dimensions like transparency. To build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size \\(2\\times2\\). These are 2D kernels because our images are 2D. For movies for example, one would use 3D kernels (the third dimension would correspond to time and not to the color channels). model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, kernel_size = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) summary(model) ## Model: &quot;sequential_41&quot; ## ____________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ==================================================================================================================== ## conv2d_1 (Conv2D) (None, 27, 27, 16) 80 ## ## max_pooling2d_1 (MaxPooling2D) (None, 13, 13, 16) 0 ## ## conv2d (Conv2D) (None, 11, 11, 16) 2320 ## ## max_pooling2d (MaxPooling2D) (None, 5, 5, 16) 0 ## ## flatten (Flatten) (None, 400) 0 ## ## dense_152 (Dense) (None, 100) 40100 ## ## dense_151 (Dense) (None, 10) 1010 ## ## ==================================================================================================================== ## Total params: 43,510 ## Trainable params: 43,510 ## Non-trainable params: 0 ## ____________________________________________________________________________________________________________________ We additionally used a pooling layer for downsizing the resulting feature maps. Without further specification, a \\(2\\times2\\) pooling layer is taken automatically. Pooling layers take the input feature map and divide it into (in our case) parts of \\(2\\times2\\) size. Then the respective pooling operation is executed. For every input map/layer, you get one (downsized) output map/layer. As we are using the max pooling layer (there are sever other methods like the mean pooling), only the maximum value of these 4 parts is taken and forwarded further. Example input: 1 2 | 5 8 | 3 6 6 5 | 2 4 | 8 1 ------------------------------ 9 4 | 3 7 | 2 5 0 3 | 2 7 | 4 9 We use max pooling for every field: max(1, 2, 6, 5) | max(5, 8, 2, 4) | max(3, 6, 8, 1) ----------------------------------------------------------- max(9, 4, 0, 3) | max(3, 7, 2, 7) | max(2, 5, 4, 9) So the resulting pooled information is: 6 | 8 | 8 ------------------ 9 | 7 | 9 In this example, a \\(4\\times6\\) layer was transformed to a \\(2\\times3\\) layer and thus downsized. This is similar to the biological process called lateral inhibition where active neurons inhibit the activity of neighboring neurons. It’s a loss of information but often very useful for aggregating information and prevent overfitting. After another convolutional and pooling layer we flatten the output. That means the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all weights from the last feature maps). You can imagine that like reshaping a matrix (2D) to a simple 1D vector. Then the full vector is used. Having flattened the layer, we can simply use our typical output layer. Torch Prepare/download data: library(torch) library(torchvision) torch_manual_seed(321L) set.seed(123) train_ds = mnist_dataset( &quot;.&quot;, download = TRUE, train = TRUE, transform = transform_to_tensor ) ## Processing... ## Done! test_ds = mnist_dataset( &quot;.&quot;, download = TRUE, train = FALSE, transform = transform_to_tensor ) Build dataloader: train_dl = dataloader(train_ds, batch_size = 32, shuffle = TRUE) test_dl = dataloader(test_ds, batch_size = 32) first_batch = train_dl$.iter() df = first_batch$.next() df$x$size() ## [1] 32 1 28 28 Build convolutional neural network: We have here to calculate the shapes of our layers on our own: We start with our input of shape (batch_size, 1, 28, 28) sample = df$x sample$size() ## [1] 32 1 28 28 First convolutional layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2) conv1 = nn_conv2d(1, 16L, 2L, stride = 1L) (sample %&gt;% conv1)$size() ## [1] 32 16 27 27 Output: batch_size = 32, number of feature maps = 16, dimensions of each feature map = \\((27 , 27)\\) Wit a kernel size of two and stride = 1 we will lose one pixel in each dimension… Questions: What happens if we increase the stride? What happens if we increase the kernel size? Pooling layer summarizes each feature map (sample %&gt;% conv1 %&gt;% nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size() ## [1] 32 16 13 13 kernel_size = 2L and stride = 2L halfs the pixel dimensions of our image. Fully connected layer Now we have to flatten our final output of the convolutional neural network model to use a normal fully connected layer, but to do so we have to calculate the number of inputs for the fully connected layer: dims = (sample %&gt;% conv1 %&gt;% nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size() # Without the batch size of course. final = prod(dims[-1]) print(final) ## [1] 2704 fc = nn_linear(final, 10L) (sample %&gt;% conv1 %&gt;% nnf_max_pool2d(kernel_size = 2L, stride = 2L) %&gt;% torch_flatten(start_dim = 2L) %&gt;% fc)$size() ## [1] 32 10 Build the network: net = nn_module( &quot;mnist&quot;, initialize = function(){ self$conv1 = nn_conv2d(1, 16L, 2L) self$conv2 = nn_conv2d(16L, 16L, 3L) self$fc1 = nn_linear(400L, 100L) self$fc2 = nn_linear(100L, 10L) }, forward = function(x){ x %&gt;% self$conv1() %&gt;% nnf_relu() %&gt;% nnf_max_pool2d(2) %&gt;% self$conv2() %&gt;% nnf_relu() %&gt;% nnf_max_pool2d(2) %&gt;% torch_flatten(start_dim = 2) %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2() } ) The rest is as usual: First we compile the model. model %&gt;% keras::compile( optimizer = keras::optimizer_adamax(0.01), loss = loss_categorical_crossentropy ) summary(model) ## Model: &quot;sequential_41&quot; ## ____________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ==================================================================================================================== ## conv2d_1 (Conv2D) (None, 27, 27, 16) 80 ## ## max_pooling2d_1 (MaxPooling2D) (None, 13, 13, 16) 0 ## ## conv2d (Conv2D) (None, 11, 11, 16) 2320 ## ## max_pooling2d (MaxPooling2D) (None, 5, 5, 16) 0 ## ## flatten (Flatten) (None, 400) 0 ## ## dense_152 (Dense) (None, 100) 40100 ## ## dense_151 (Dense) (None, 10) 1010 ## ## ==================================================================================================================== ## Total params: 43,510 ## Trainable params: 43,510 ## Non-trainable params: 0 ## ____________________________________________________________________________________________________________________ Then, we train the model: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. epochs = 5L batch_size = 32L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) Torch Train model: library(torch) torch_manual_seed(321L) set.seed(123) model_torch = net() opt = optim_adam(params = model_torch$parameters, lr = 0.01) for(e in 1:3){ losses = c() coro::loop( for(batch in train_dl){ opt$zero_grad() pred = model_torch(batch[[1]]) loss = nnf_cross_entropy(pred, batch[[2]], reduction = &quot;mean&quot;) loss$backward() opt$step() losses = c(losses, loss$item()) } ) cat(sprintf(&quot;Loss at epoch %d: %3f\\n&quot;, e, mean(losses))) } Evaluation: model_torch$eval() test_losses = c() total = 0 correct = 0 coro::loop( for(batch in test_dl){ output = model_torch(batch[[1]]) labels = batch[[2]] loss = nnf_cross_entropy(output, labels) test_losses = c(test_losses, loss$item()) predicted = torch_max(output$data(), dim = 2)[[2]] total = total + labels$size(1) correct = correct + (predicted == labels)$sum()$item() } ) mean(test_losses) test_accuracy = correct/total test_accuracy 5.4 Advanced Training Techniques 5.4.1 Data Augmentation Having to train a convolutional neural network using very little data is a common problem. Data augmentation helps to artificially increase the number of images. The idea is that a convolutional neural network learns specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures. Luckily, it is very easy to use data augmentation in Keras. To show this, we will use our flower data set. We have to define a generator object (a specific object which infinitely draws samples from our data set). In the generator we can turn on the data augmentation. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = EcoData::dataset_flower() train = data$train/255 labels = data$labels model = keras_model_sequential() model %&gt;% layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L), input_shape = c(80L, 80L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L), strides = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) # Data augmentation. aug = image_data_generator(rotation_range = 90, zoom_range = c(0.3), horizontal_flip = TRUE, vertical_flip = TRUE) # Data preparation / splitting. indices = sample.int(nrow(train), 0.1 * nrow(train)) generator = flow_images_from_data(train[-indices,,,], k_one_hot(labels[-indices], num_classes = 5L), generator = aug, batch_size = 25L, shuffle = TRUE) test = train[indices,,,] ## Training loop with early stopping: # As we use an iterator (the generator), validation loss is not applicable. # An available metric is the normal loss. early = keras::callback_early_stopping(patience = 2L, monitor = &quot;loss&quot;) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, optimizer = keras::optimizer_adamax(learning_rate = 0.01)) model %&gt;% fit(generator, epochs = 20L, batch_size = 25L, shuffle = TRUE, callbacks = c(early)) # Predictions on the training set: pred = predict(model, data$train[-indices,,,]) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, labels[-indices]) table(pred) # Predictions on the holdout / test set: pred = predict(model, test) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, labels[indices]) table(pred) # If you want to predict on the holdout for submission, use: pred = predict(model, EcoData::dataset_flower()$test/255) %&gt;% apply(1, which.max) - 1 table(pred) Using data augmentation we can artificially increase the number of images. Torch In Torch, we have to change the transform function (but only for the train dataloader): library(torch) torch_manual_seed(321L) set.seed(123) train_transforms = function(img){ img %&gt;% transform_to_tensor() %&gt;% transform_random_horizontal_flip(p = 0.3) %&gt;% transform_random_resized_crop(size = c(28L, 28L)) %&gt;% transform_random_vertical_flip(0.3) } train_ds = mnist_dataset(&quot;.&quot;, download = TRUE, train = TRUE, transform = train_transforms) test_ds = mnist_dataset(&quot;.&quot;, download = TRUE, train = FALSE, transform = transform_to_tensor) train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE) test_dl = dataloader(test_ds, batch_size = 100L) model_torch = net() opt = optim_adam(params = model_torch$parameters, lr = 0.01) for(e in 1:1){ losses = c() coro::loop( for(batch in train_dl){ opt$zero_grad() pred = model_torch(batch[[1]]) loss = nnf_cross_entropy(pred, batch[[2]], reduction = &quot;mean&quot;) loss$backward() opt$step() losses = c(losses, loss$item()) } ) cat(sprintf(&quot;Loss at epoch %d: %3f\\n&quot;, e, mean(losses))) } model_torch$eval() test_losses = c() total = 0 correct = 0 coro::loop( for(batch in test_dl){ output = model_torch(batch[[1]]) labels = batch[[2]] loss = nnf_cross_entropy(output, labels) test_losses = c(test_losses, loss$item()) predicted = torch_max(output$data(), dim = 2)[[2]] total = total + labels$size(1) correct = correct + (predicted == labels)$sum()$item() } ) test_accuracy = correct/total print(test_accuracy) 5.4.2 Transfer Learning Another approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning. The main idea of transfer learning is that all the convolutional layers have mainly one task - learning to identify highly correlated neighboring features. This knowledge is then used for new tasks. The convolutional layers learn structures such as edges in images and only the top layer, the dense layer is the actual classifier of the convolutional neural network for a specific task. Thus, one could think that we could only train the top layer as classifier. To do so, it will be confronted by sets of different edges/structures and has to decide the label based on these. Again, this sounds very complicated but it is again quite easy with Keras. We will do this now with the CIFAR10 data set, so we have to prepare the data: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = keras::dataset_cifar10() train = data$train test = data$test rm(data) image = train$x[5,,,] image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() train_x = array(train$x/255, c(dim(train$x))) test_x = array(test$x/255, c(dim(test$x))) train_y = to_categorical(train$y, 10) test_y = to_categorical(test$y, 10) rm(train, test, data) ## Warning in rm(train, test, data): object &#39;data&#39; not found Keras provides download functions for all famous architectures/convolutional neural network models which are already trained on the imagenet data set (another famous data set). These trained networks come already without their top layer, so we have to set include_top to false and change the input shape. densenet = application_densenet201(include_top = FALSE, input_shape = c(32L, 32L, 3L)) Now, we will not use a sequential model but just a “keras_model” where we can specify the inputs and outputs. Thereby, the output is our own top layer, but the inputs are the densenet inputs, as these are already pre-trained. model = keras::keras_model( inputs = densenet$input, outputs = layer_flatten( layer_dense(densenet$output, units = 10L, activation = &quot;softmax&quot;) ) ) # Notice that this snippet just creates one (!) new layer. # The densenet&#39;s inputs are connected with the model&#39;s inputs. # The densenet&#39;s outputs are connected with our own layer (with 10 nodes). # This layer is also the output layer of the model. In the next step we want to freeze all layers except for our own last layer. Freezing means that these are not trained: We do not want to train the complete model, we only want to train the last layer. You can check the number of trainable weights via summary(model). model %&gt;% freeze_weights(to = length(model$layers) - 1) summary(model) ## Model: &quot;model_1&quot; ## ____________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # Connected to ## ==================================================================================================================== ## input_3 (InputLayer) [(None, 32, 32, 3)] 0 [] ## ## zero_padding2d (ZeroPadding2D) (None, 38, 38, 3) 0 [&#39;input_3[0][0]&#39;] ## ## conv1/conv (Conv2D) (None, 16, 16, 64) 9408 [&#39;zero_padding2d[0][0]&#39;] ## ## conv1/bn (BatchNormalization) (None, 16, 16, 64) 256 [&#39;conv1/conv[0][0]&#39;] ## ## conv1/relu (Activation) (None, 16, 16, 64) 0 [&#39;conv1/bn[0][0]&#39;] ## ## zero_padding2d_1 (ZeroPadding2D) (None, 18, 18, 64) 0 [&#39;conv1/relu[0][0]&#39;] ## ## pool1 (MaxPooling2D) (None, 8, 8, 64) 0 [&#39;zero_padding2d_1[0][0]&#39;] ## ## conv2_block1_0_bn (BatchNormalizatio (None, 8, 8, 64) 256 [&#39;pool1[0][0]&#39;] ## n) ## ## conv2_block1_0_relu (Activation) (None, 8, 8, 64) 0 [&#39;conv2_block1_0_bn[0][0]&#39;] ## ## conv2_block1_1_conv (Conv2D) (None, 8, 8, 128) 8192 [&#39;conv2_block1_0_relu[0][0]&#39;] ## ## conv2_block1_1_bn (BatchNormalizatio (None, 8, 8, 128) 512 [&#39;conv2_block1_1_conv[0][0]&#39;] ## n) ## ## conv2_block1_1_relu (Activation) (None, 8, 8, 128) 0 [&#39;conv2_block1_1_bn[0][0]&#39;] ## ## conv2_block1_2_conv (Conv2D) (None, 8, 8, 32) 36864 [&#39;conv2_block1_1_relu[0][0]&#39;] ## ## conv2_block1_concat (Concatenate) (None, 8, 8, 96) 0 [&#39;pool1[0][0]&#39;, ## &#39;conv2_block1_2_conv[0][0]&#39;] ## ## conv2_block2_0_bn (BatchNormalizatio (None, 8, 8, 96) 384 [&#39;conv2_block1_concat[0][0]&#39;] ## n) ## ## conv2_block2_0_relu (Activation) (None, 8, 8, 96) 0 [&#39;conv2_block2_0_bn[0][0]&#39;] ## ## conv2_block2_1_conv (Conv2D) (None, 8, 8, 128) 12288 [&#39;conv2_block2_0_relu[0][0]&#39;] ## ## conv2_block2_1_bn (BatchNormalizatio (None, 8, 8, 128) 512 [&#39;conv2_block2_1_conv[0][0]&#39;] ## n) ## ## conv2_block2_1_relu (Activation) (None, 8, 8, 128) 0 [&#39;conv2_block2_1_bn[0][0]&#39;] ## ## conv2_block2_2_conv (Conv2D) (None, 8, 8, 32) 36864 [&#39;conv2_block2_1_relu[0][0]&#39;] ## ## conv2_block2_concat (Concatenate) (None, 8, 8, 128) 0 [&#39;conv2_block1_concat[0][0]&#39;, ## &#39;conv2_block2_2_conv[0][0]&#39;] ## ## conv2_block3_0_bn (BatchNormalizatio (None, 8, 8, 128) 512 [&#39;conv2_block2_concat[0][0]&#39;] ## n) ## ## conv2_block3_0_relu (Activation) (None, 8, 8, 128) 0 [&#39;conv2_block3_0_bn[0][0]&#39;] ## ## conv2_block3_1_conv (Conv2D) (None, 8, 8, 128) 16384 [&#39;conv2_block3_0_relu[0][0]&#39;] ## ## conv2_block3_1_bn (BatchNormalizatio (None, 8, 8, 128) 512 [&#39;conv2_block3_1_conv[0][0]&#39;] ## n) ## ## conv2_block3_1_relu (Activation) (None, 8, 8, 128) 0 [&#39;conv2_block3_1_bn[0][0]&#39;] ## ## conv2_block3_2_conv (Conv2D) (None, 8, 8, 32) 36864 [&#39;conv2_block3_1_relu[0][0]&#39;] ## ## conv2_block3_concat (Concatenate) (None, 8, 8, 160) 0 [&#39;conv2_block2_concat[0][0]&#39;, ## &#39;conv2_block3_2_conv[0][0]&#39;] ## ## conv2_block4_0_bn (BatchNormalizatio (None, 8, 8, 160) 640 [&#39;conv2_block3_concat[0][0]&#39;] ## n) ## ## conv2_block4_0_relu (Activation) (None, 8, 8, 160) 0 [&#39;conv2_block4_0_bn[0][0]&#39;] ## ## conv2_block4_1_conv (Conv2D) (None, 8, 8, 128) 20480 [&#39;conv2_block4_0_relu[0][0]&#39;] ## ## conv2_block4_1_bn (BatchNormalizatio (None, 8, 8, 128) 512 [&#39;conv2_block4_1_conv[0][0]&#39;] ## n) ## ## conv2_block4_1_relu (Activation) (None, 8, 8, 128) 0 [&#39;conv2_block4_1_bn[0][0]&#39;] ## ## conv2_block4_2_conv (Conv2D) (None, 8, 8, 32) 36864 [&#39;conv2_block4_1_relu[0][0]&#39;] ## ## conv2_block4_concat (Concatenate) (None, 8, 8, 192) 0 [&#39;conv2_block3_concat[0][0]&#39;, ## &#39;conv2_block4_2_conv[0][0]&#39;] ## ## conv2_block5_0_bn (BatchNormalizatio (None, 8, 8, 192) 768 [&#39;conv2_block4_concat[0][0]&#39;] ## n) ## ## conv2_block5_0_relu (Activation) (None, 8, 8, 192) 0 [&#39;conv2_block5_0_bn[0][0]&#39;] ## ## conv2_block5_1_conv (Conv2D) (None, 8, 8, 128) 24576 [&#39;conv2_block5_0_relu[0][0]&#39;] ## ## conv2_block5_1_bn (BatchNormalizatio (None, 8, 8, 128) 512 [&#39;conv2_block5_1_conv[0][0]&#39;] ## n) ## ## conv2_block5_1_relu (Activation) (None, 8, 8, 128) 0 [&#39;conv2_block5_1_bn[0][0]&#39;] ## ## conv2_block5_2_conv (Conv2D) (None, 8, 8, 32) 36864 [&#39;conv2_block5_1_relu[0][0]&#39;] ## ## conv2_block5_concat (Concatenate) (None, 8, 8, 224) 0 [&#39;conv2_block4_concat[0][0]&#39;, ## &#39;conv2_block5_2_conv[0][0]&#39;] ## ## conv2_block6_0_bn (BatchNormalizatio (None, 8, 8, 224) 896 [&#39;conv2_block5_concat[0][0]&#39;] ## n) ## ## conv2_block6_0_relu (Activation) (None, 8, 8, 224) 0 [&#39;conv2_block6_0_bn[0][0]&#39;] ## ## conv2_block6_1_conv (Conv2D) (None, 8, 8, 128) 28672 [&#39;conv2_block6_0_relu[0][0]&#39;] ## ## conv2_block6_1_bn (BatchNormalizatio (None, 8, 8, 128) 512 [&#39;conv2_block6_1_conv[0][0]&#39;] ## n) ## ## conv2_block6_1_relu (Activation) (None, 8, 8, 128) 0 [&#39;conv2_block6_1_bn[0][0]&#39;] ## ## conv2_block6_2_conv (Conv2D) (None, 8, 8, 32) 36864 [&#39;conv2_block6_1_relu[0][0]&#39;] ## ## conv2_block6_concat (Concatenate) (None, 8, 8, 256) 0 [&#39;conv2_block5_concat[0][0]&#39;, ## &#39;conv2_block6_2_conv[0][0]&#39;] ## ## pool2_bn (BatchNormalization) (None, 8, 8, 256) 1024 [&#39;conv2_block6_concat[0][0]&#39;] ## ## pool2_relu (Activation) (None, 8, 8, 256) 0 [&#39;pool2_bn[0][0]&#39;] ## ## pool2_conv (Conv2D) (None, 8, 8, 128) 32768 [&#39;pool2_relu[0][0]&#39;] ## ## pool2_pool (AveragePooling2D) (None, 4, 4, 128) 0 [&#39;pool2_conv[0][0]&#39;] ## ## conv3_block1_0_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;pool2_pool[0][0]&#39;] ## n) ## ## conv3_block1_0_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block1_0_bn[0][0]&#39;] ## ## conv3_block1_1_conv (Conv2D) (None, 4, 4, 128) 16384 [&#39;conv3_block1_0_relu[0][0]&#39;] ## ## conv3_block1_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block1_1_conv[0][0]&#39;] ## n) ## ## conv3_block1_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block1_1_bn[0][0]&#39;] ## ## conv3_block1_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block1_1_relu[0][0]&#39;] ## ## conv3_block1_concat (Concatenate) (None, 4, 4, 160) 0 [&#39;pool2_pool[0][0]&#39;, ## &#39;conv3_block1_2_conv[0][0]&#39;] ## ## conv3_block2_0_bn (BatchNormalizatio (None, 4, 4, 160) 640 [&#39;conv3_block1_concat[0][0]&#39;] ## n) ## ## conv3_block2_0_relu (Activation) (None, 4, 4, 160) 0 [&#39;conv3_block2_0_bn[0][0]&#39;] ## ## conv3_block2_1_conv (Conv2D) (None, 4, 4, 128) 20480 [&#39;conv3_block2_0_relu[0][0]&#39;] ## ## conv3_block2_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block2_1_conv[0][0]&#39;] ## n) ## ## conv3_block2_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block2_1_bn[0][0]&#39;] ## ## conv3_block2_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block2_1_relu[0][0]&#39;] ## ## conv3_block2_concat (Concatenate) (None, 4, 4, 192) 0 [&#39;conv3_block1_concat[0][0]&#39;, ## &#39;conv3_block2_2_conv[0][0]&#39;] ## ## conv3_block3_0_bn (BatchNormalizatio (None, 4, 4, 192) 768 [&#39;conv3_block2_concat[0][0]&#39;] ## n) ## ## conv3_block3_0_relu (Activation) (None, 4, 4, 192) 0 [&#39;conv3_block3_0_bn[0][0]&#39;] ## ## conv3_block3_1_conv (Conv2D) (None, 4, 4, 128) 24576 [&#39;conv3_block3_0_relu[0][0]&#39;] ## ## conv3_block3_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block3_1_conv[0][0]&#39;] ## n) ## ## conv3_block3_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block3_1_bn[0][0]&#39;] ## ## conv3_block3_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block3_1_relu[0][0]&#39;] ## ## conv3_block3_concat (Concatenate) (None, 4, 4, 224) 0 [&#39;conv3_block2_concat[0][0]&#39;, ## &#39;conv3_block3_2_conv[0][0]&#39;] ## ## conv3_block4_0_bn (BatchNormalizatio (None, 4, 4, 224) 896 [&#39;conv3_block3_concat[0][0]&#39;] ## n) ## ## conv3_block4_0_relu (Activation) (None, 4, 4, 224) 0 [&#39;conv3_block4_0_bn[0][0]&#39;] ## ## conv3_block4_1_conv (Conv2D) (None, 4, 4, 128) 28672 [&#39;conv3_block4_0_relu[0][0]&#39;] ## ## conv3_block4_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block4_1_conv[0][0]&#39;] ## n) ## ## conv3_block4_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block4_1_bn[0][0]&#39;] ## ## conv3_block4_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block4_1_relu[0][0]&#39;] ## ## conv3_block4_concat (Concatenate) (None, 4, 4, 256) 0 [&#39;conv3_block3_concat[0][0]&#39;, ## &#39;conv3_block4_2_conv[0][0]&#39;] ## ## conv3_block5_0_bn (BatchNormalizatio (None, 4, 4, 256) 1024 [&#39;conv3_block4_concat[0][0]&#39;] ## n) ## ## conv3_block5_0_relu (Activation) (None, 4, 4, 256) 0 [&#39;conv3_block5_0_bn[0][0]&#39;] ## ## conv3_block5_1_conv (Conv2D) (None, 4, 4, 128) 32768 [&#39;conv3_block5_0_relu[0][0]&#39;] ## ## conv3_block5_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block5_1_conv[0][0]&#39;] ## n) ## ## conv3_block5_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block5_1_bn[0][0]&#39;] ## ## conv3_block5_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block5_1_relu[0][0]&#39;] ## ## conv3_block5_concat (Concatenate) (None, 4, 4, 288) 0 [&#39;conv3_block4_concat[0][0]&#39;, ## &#39;conv3_block5_2_conv[0][0]&#39;] ## ## conv3_block6_0_bn (BatchNormalizatio (None, 4, 4, 288) 1152 [&#39;conv3_block5_concat[0][0]&#39;] ## n) ## ## conv3_block6_0_relu (Activation) (None, 4, 4, 288) 0 [&#39;conv3_block6_0_bn[0][0]&#39;] ## ## conv3_block6_1_conv (Conv2D) (None, 4, 4, 128) 36864 [&#39;conv3_block6_0_relu[0][0]&#39;] ## ## conv3_block6_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block6_1_conv[0][0]&#39;] ## n) ## ## conv3_block6_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block6_1_bn[0][0]&#39;] ## ## conv3_block6_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block6_1_relu[0][0]&#39;] ## ## conv3_block6_concat (Concatenate) (None, 4, 4, 320) 0 [&#39;conv3_block5_concat[0][0]&#39;, ## &#39;conv3_block6_2_conv[0][0]&#39;] ## ## conv3_block7_0_bn (BatchNormalizatio (None, 4, 4, 320) 1280 [&#39;conv3_block6_concat[0][0]&#39;] ## n) ## ## conv3_block7_0_relu (Activation) (None, 4, 4, 320) 0 [&#39;conv3_block7_0_bn[0][0]&#39;] ## ## conv3_block7_1_conv (Conv2D) (None, 4, 4, 128) 40960 [&#39;conv3_block7_0_relu[0][0]&#39;] ## ## conv3_block7_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block7_1_conv[0][0]&#39;] ## n) ## ## conv3_block7_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block7_1_bn[0][0]&#39;] ## ## conv3_block7_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block7_1_relu[0][0]&#39;] ## ## conv3_block7_concat (Concatenate) (None, 4, 4, 352) 0 [&#39;conv3_block6_concat[0][0]&#39;, ## &#39;conv3_block7_2_conv[0][0]&#39;] ## ## conv3_block8_0_bn (BatchNormalizatio (None, 4, 4, 352) 1408 [&#39;conv3_block7_concat[0][0]&#39;] ## n) ## ## conv3_block8_0_relu (Activation) (None, 4, 4, 352) 0 [&#39;conv3_block8_0_bn[0][0]&#39;] ## ## conv3_block8_1_conv (Conv2D) (None, 4, 4, 128) 45056 [&#39;conv3_block8_0_relu[0][0]&#39;] ## ## conv3_block8_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block8_1_conv[0][0]&#39;] ## n) ## ## conv3_block8_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block8_1_bn[0][0]&#39;] ## ## conv3_block8_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block8_1_relu[0][0]&#39;] ## ## conv3_block8_concat (Concatenate) (None, 4, 4, 384) 0 [&#39;conv3_block7_concat[0][0]&#39;, ## &#39;conv3_block8_2_conv[0][0]&#39;] ## ## conv3_block9_0_bn (BatchNormalizatio (None, 4, 4, 384) 1536 [&#39;conv3_block8_concat[0][0]&#39;] ## n) ## ## conv3_block9_0_relu (Activation) (None, 4, 4, 384) 0 [&#39;conv3_block9_0_bn[0][0]&#39;] ## ## conv3_block9_1_conv (Conv2D) (None, 4, 4, 128) 49152 [&#39;conv3_block9_0_relu[0][0]&#39;] ## ## conv3_block9_1_bn (BatchNormalizatio (None, 4, 4, 128) 512 [&#39;conv3_block9_1_conv[0][0]&#39;] ## n) ## ## conv3_block9_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block9_1_bn[0][0]&#39;] ## ## conv3_block9_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block9_1_relu[0][0]&#39;] ## ## conv3_block9_concat (Concatenate) (None, 4, 4, 416) 0 [&#39;conv3_block8_concat[0][0]&#39;, ## &#39;conv3_block9_2_conv[0][0]&#39;] ## ## conv3_block10_0_bn (BatchNormalizati (None, 4, 4, 416) 1664 [&#39;conv3_block9_concat[0][0]&#39;] ## on) ## ## conv3_block10_0_relu (Activation) (None, 4, 4, 416) 0 [&#39;conv3_block10_0_bn[0][0]&#39;] ## ## conv3_block10_1_conv (Conv2D) (None, 4, 4, 128) 53248 [&#39;conv3_block10_0_relu[0][0]&#39;] ## ## conv3_block10_1_bn (BatchNormalizati (None, 4, 4, 128) 512 [&#39;conv3_block10_1_conv[0][0]&#39;] ## on) ## ## conv3_block10_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block10_1_bn[0][0]&#39;] ## ## conv3_block10_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block10_1_relu[0][0]&#39;] ## ## conv3_block10_concat (Concatenate) (None, 4, 4, 448) 0 [&#39;conv3_block9_concat[0][0]&#39;, ## &#39;conv3_block10_2_conv[0][0]&#39;] ## ## conv3_block11_0_bn (BatchNormalizati (None, 4, 4, 448) 1792 [&#39;conv3_block10_concat[0][0]&#39;] ## on) ## ## conv3_block11_0_relu (Activation) (None, 4, 4, 448) 0 [&#39;conv3_block11_0_bn[0][0]&#39;] ## ## conv3_block11_1_conv (Conv2D) (None, 4, 4, 128) 57344 [&#39;conv3_block11_0_relu[0][0]&#39;] ## ## conv3_block11_1_bn (BatchNormalizati (None, 4, 4, 128) 512 [&#39;conv3_block11_1_conv[0][0]&#39;] ## on) ## ## conv3_block11_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block11_1_bn[0][0]&#39;] ## ## conv3_block11_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block11_1_relu[0][0]&#39;] ## ## conv3_block11_concat (Concatenate) (None, 4, 4, 480) 0 [&#39;conv3_block10_concat[0][0]&#39;, ## &#39;conv3_block11_2_conv[0][0]&#39;] ## ## conv3_block12_0_bn (BatchNormalizati (None, 4, 4, 480) 1920 [&#39;conv3_block11_concat[0][0]&#39;] ## on) ## ## conv3_block12_0_relu (Activation) (None, 4, 4, 480) 0 [&#39;conv3_block12_0_bn[0][0]&#39;] ## ## conv3_block12_1_conv (Conv2D) (None, 4, 4, 128) 61440 [&#39;conv3_block12_0_relu[0][0]&#39;] ## ## conv3_block12_1_bn (BatchNormalizati (None, 4, 4, 128) 512 [&#39;conv3_block12_1_conv[0][0]&#39;] ## on) ## ## conv3_block12_1_relu (Activation) (None, 4, 4, 128) 0 [&#39;conv3_block12_1_bn[0][0]&#39;] ## ## conv3_block12_2_conv (Conv2D) (None, 4, 4, 32) 36864 [&#39;conv3_block12_1_relu[0][0]&#39;] ## ## conv3_block12_concat (Concatenate) (None, 4, 4, 512) 0 [&#39;conv3_block11_concat[0][0]&#39;, ## &#39;conv3_block12_2_conv[0][0]&#39;] ## ## pool3_bn (BatchNormalization) (None, 4, 4, 512) 2048 [&#39;conv3_block12_concat[0][0]&#39;] ## ## pool3_relu (Activation) (None, 4, 4, 512) 0 [&#39;pool3_bn[0][0]&#39;] ## ## pool3_conv (Conv2D) (None, 4, 4, 256) 131072 [&#39;pool3_relu[0][0]&#39;] ## ## pool3_pool (AveragePooling2D) (None, 2, 2, 256) 0 [&#39;pool3_conv[0][0]&#39;] ## ## conv4_block1_0_bn (BatchNormalizatio (None, 2, 2, 256) 1024 [&#39;pool3_pool[0][0]&#39;] ## n) ## ## conv4_block1_0_relu (Activation) (None, 2, 2, 256) 0 [&#39;conv4_block1_0_bn[0][0]&#39;] ## ## conv4_block1_1_conv (Conv2D) (None, 2, 2, 128) 32768 [&#39;conv4_block1_0_relu[0][0]&#39;] ## ## conv4_block1_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block1_1_conv[0][0]&#39;] ## n) ## ## conv4_block1_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block1_1_bn[0][0]&#39;] ## ## conv4_block1_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block1_1_relu[0][0]&#39;] ## ## conv4_block1_concat (Concatenate) (None, 2, 2, 288) 0 [&#39;pool3_pool[0][0]&#39;, ## &#39;conv4_block1_2_conv[0][0]&#39;] ## ## conv4_block2_0_bn (BatchNormalizatio (None, 2, 2, 288) 1152 [&#39;conv4_block1_concat[0][0]&#39;] ## n) ## ## conv4_block2_0_relu (Activation) (None, 2, 2, 288) 0 [&#39;conv4_block2_0_bn[0][0]&#39;] ## ## conv4_block2_1_conv (Conv2D) (None, 2, 2, 128) 36864 [&#39;conv4_block2_0_relu[0][0]&#39;] ## ## conv4_block2_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block2_1_conv[0][0]&#39;] ## n) ## ## conv4_block2_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block2_1_bn[0][0]&#39;] ## ## conv4_block2_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block2_1_relu[0][0]&#39;] ## ## conv4_block2_concat (Concatenate) (None, 2, 2, 320) 0 [&#39;conv4_block1_concat[0][0]&#39;, ## &#39;conv4_block2_2_conv[0][0]&#39;] ## ## conv4_block3_0_bn (BatchNormalizatio (None, 2, 2, 320) 1280 [&#39;conv4_block2_concat[0][0]&#39;] ## n) ## ## conv4_block3_0_relu (Activation) (None, 2, 2, 320) 0 [&#39;conv4_block3_0_bn[0][0]&#39;] ## ## conv4_block3_1_conv (Conv2D) (None, 2, 2, 128) 40960 [&#39;conv4_block3_0_relu[0][0]&#39;] ## ## conv4_block3_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block3_1_conv[0][0]&#39;] ## n) ## ## conv4_block3_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block3_1_bn[0][0]&#39;] ## ## conv4_block3_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block3_1_relu[0][0]&#39;] ## ## conv4_block3_concat (Concatenate) (None, 2, 2, 352) 0 [&#39;conv4_block2_concat[0][0]&#39;, ## &#39;conv4_block3_2_conv[0][0]&#39;] ## ## conv4_block4_0_bn (BatchNormalizatio (None, 2, 2, 352) 1408 [&#39;conv4_block3_concat[0][0]&#39;] ## n) ## ## conv4_block4_0_relu (Activation) (None, 2, 2, 352) 0 [&#39;conv4_block4_0_bn[0][0]&#39;] ## ## conv4_block4_1_conv (Conv2D) (None, 2, 2, 128) 45056 [&#39;conv4_block4_0_relu[0][0]&#39;] ## ## conv4_block4_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block4_1_conv[0][0]&#39;] ## n) ## ## conv4_block4_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block4_1_bn[0][0]&#39;] ## ## conv4_block4_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block4_1_relu[0][0]&#39;] ## ## conv4_block4_concat (Concatenate) (None, 2, 2, 384) 0 [&#39;conv4_block3_concat[0][0]&#39;, ## &#39;conv4_block4_2_conv[0][0]&#39;] ## ## conv4_block5_0_bn (BatchNormalizatio (None, 2, 2, 384) 1536 [&#39;conv4_block4_concat[0][0]&#39;] ## n) ## ## conv4_block5_0_relu (Activation) (None, 2, 2, 384) 0 [&#39;conv4_block5_0_bn[0][0]&#39;] ## ## conv4_block5_1_conv (Conv2D) (None, 2, 2, 128) 49152 [&#39;conv4_block5_0_relu[0][0]&#39;] ## ## conv4_block5_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block5_1_conv[0][0]&#39;] ## n) ## ## conv4_block5_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block5_1_bn[0][0]&#39;] ## ## conv4_block5_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block5_1_relu[0][0]&#39;] ## ## conv4_block5_concat (Concatenate) (None, 2, 2, 416) 0 [&#39;conv4_block4_concat[0][0]&#39;, ## &#39;conv4_block5_2_conv[0][0]&#39;] ## ## conv4_block6_0_bn (BatchNormalizatio (None, 2, 2, 416) 1664 [&#39;conv4_block5_concat[0][0]&#39;] ## n) ## ## conv4_block6_0_relu (Activation) (None, 2, 2, 416) 0 [&#39;conv4_block6_0_bn[0][0]&#39;] ## ## conv4_block6_1_conv (Conv2D) (None, 2, 2, 128) 53248 [&#39;conv4_block6_0_relu[0][0]&#39;] ## ## conv4_block6_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block6_1_conv[0][0]&#39;] ## n) ## ## conv4_block6_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block6_1_bn[0][0]&#39;] ## ## conv4_block6_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block6_1_relu[0][0]&#39;] ## ## conv4_block6_concat (Concatenate) (None, 2, 2, 448) 0 [&#39;conv4_block5_concat[0][0]&#39;, ## &#39;conv4_block6_2_conv[0][0]&#39;] ## ## conv4_block7_0_bn (BatchNormalizatio (None, 2, 2, 448) 1792 [&#39;conv4_block6_concat[0][0]&#39;] ## n) ## ## conv4_block7_0_relu (Activation) (None, 2, 2, 448) 0 [&#39;conv4_block7_0_bn[0][0]&#39;] ## ## conv4_block7_1_conv (Conv2D) (None, 2, 2, 128) 57344 [&#39;conv4_block7_0_relu[0][0]&#39;] ## ## conv4_block7_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block7_1_conv[0][0]&#39;] ## n) ## ## conv4_block7_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block7_1_bn[0][0]&#39;] ## ## conv4_block7_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block7_1_relu[0][0]&#39;] ## ## conv4_block7_concat (Concatenate) (None, 2, 2, 480) 0 [&#39;conv4_block6_concat[0][0]&#39;, ## &#39;conv4_block7_2_conv[0][0]&#39;] ## ## conv4_block8_0_bn (BatchNormalizatio (None, 2, 2, 480) 1920 [&#39;conv4_block7_concat[0][0]&#39;] ## n) ## ## conv4_block8_0_relu (Activation) (None, 2, 2, 480) 0 [&#39;conv4_block8_0_bn[0][0]&#39;] ## ## conv4_block8_1_conv (Conv2D) (None, 2, 2, 128) 61440 [&#39;conv4_block8_0_relu[0][0]&#39;] ## ## conv4_block8_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block8_1_conv[0][0]&#39;] ## n) ## ## conv4_block8_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block8_1_bn[0][0]&#39;] ## ## conv4_block8_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block8_1_relu[0][0]&#39;] ## ## conv4_block8_concat (Concatenate) (None, 2, 2, 512) 0 [&#39;conv4_block7_concat[0][0]&#39;, ## &#39;conv4_block8_2_conv[0][0]&#39;] ## ## conv4_block9_0_bn (BatchNormalizatio (None, 2, 2, 512) 2048 [&#39;conv4_block8_concat[0][0]&#39;] ## n) ## ## conv4_block9_0_relu (Activation) (None, 2, 2, 512) 0 [&#39;conv4_block9_0_bn[0][0]&#39;] ## ## conv4_block9_1_conv (Conv2D) (None, 2, 2, 128) 65536 [&#39;conv4_block9_0_relu[0][0]&#39;] ## ## conv4_block9_1_bn (BatchNormalizatio (None, 2, 2, 128) 512 [&#39;conv4_block9_1_conv[0][0]&#39;] ## n) ## ## conv4_block9_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block9_1_bn[0][0]&#39;] ## ## conv4_block9_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block9_1_relu[0][0]&#39;] ## ## conv4_block9_concat (Concatenate) (None, 2, 2, 544) 0 [&#39;conv4_block8_concat[0][0]&#39;, ## &#39;conv4_block9_2_conv[0][0]&#39;] ## ## conv4_block10_0_bn (BatchNormalizati (None, 2, 2, 544) 2176 [&#39;conv4_block9_concat[0][0]&#39;] ## on) ## ## conv4_block10_0_relu (Activation) (None, 2, 2, 544) 0 [&#39;conv4_block10_0_bn[0][0]&#39;] ## ## conv4_block10_1_conv (Conv2D) (None, 2, 2, 128) 69632 [&#39;conv4_block10_0_relu[0][0]&#39;] ## ## conv4_block10_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block10_1_conv[0][0]&#39;] ## on) ## ## conv4_block10_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block10_1_bn[0][0]&#39;] ## ## conv4_block10_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block10_1_relu[0][0]&#39;] ## ## conv4_block10_concat (Concatenate) (None, 2, 2, 576) 0 [&#39;conv4_block9_concat[0][0]&#39;, ## &#39;conv4_block10_2_conv[0][0]&#39;] ## ## conv4_block11_0_bn (BatchNormalizati (None, 2, 2, 576) 2304 [&#39;conv4_block10_concat[0][0]&#39;] ## on) ## ## conv4_block11_0_relu (Activation) (None, 2, 2, 576) 0 [&#39;conv4_block11_0_bn[0][0]&#39;] ## ## conv4_block11_1_conv (Conv2D) (None, 2, 2, 128) 73728 [&#39;conv4_block11_0_relu[0][0]&#39;] ## ## conv4_block11_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block11_1_conv[0][0]&#39;] ## on) ## ## conv4_block11_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block11_1_bn[0][0]&#39;] ## ## conv4_block11_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block11_1_relu[0][0]&#39;] ## ## conv4_block11_concat (Concatenate) (None, 2, 2, 608) 0 [&#39;conv4_block10_concat[0][0]&#39;, ## &#39;conv4_block11_2_conv[0][0]&#39;] ## ## conv4_block12_0_bn (BatchNormalizati (None, 2, 2, 608) 2432 [&#39;conv4_block11_concat[0][0]&#39;] ## on) ## ## conv4_block12_0_relu (Activation) (None, 2, 2, 608) 0 [&#39;conv4_block12_0_bn[0][0]&#39;] ## ## conv4_block12_1_conv (Conv2D) (None, 2, 2, 128) 77824 [&#39;conv4_block12_0_relu[0][0]&#39;] ## ## conv4_block12_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block12_1_conv[0][0]&#39;] ## on) ## ## conv4_block12_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block12_1_bn[0][0]&#39;] ## ## conv4_block12_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block12_1_relu[0][0]&#39;] ## ## conv4_block12_concat (Concatenate) (None, 2, 2, 640) 0 [&#39;conv4_block11_concat[0][0]&#39;, ## &#39;conv4_block12_2_conv[0][0]&#39;] ## ## conv4_block13_0_bn (BatchNormalizati (None, 2, 2, 640) 2560 [&#39;conv4_block12_concat[0][0]&#39;] ## on) ## ## conv4_block13_0_relu (Activation) (None, 2, 2, 640) 0 [&#39;conv4_block13_0_bn[0][0]&#39;] ## ## conv4_block13_1_conv (Conv2D) (None, 2, 2, 128) 81920 [&#39;conv4_block13_0_relu[0][0]&#39;] ## ## conv4_block13_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block13_1_conv[0][0]&#39;] ## on) ## ## conv4_block13_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block13_1_bn[0][0]&#39;] ## ## conv4_block13_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block13_1_relu[0][0]&#39;] ## ## conv4_block13_concat (Concatenate) (None, 2, 2, 672) 0 [&#39;conv4_block12_concat[0][0]&#39;, ## &#39;conv4_block13_2_conv[0][0]&#39;] ## ## conv4_block14_0_bn (BatchNormalizati (None, 2, 2, 672) 2688 [&#39;conv4_block13_concat[0][0]&#39;] ## on) ## ## conv4_block14_0_relu (Activation) (None, 2, 2, 672) 0 [&#39;conv4_block14_0_bn[0][0]&#39;] ## ## conv4_block14_1_conv (Conv2D) (None, 2, 2, 128) 86016 [&#39;conv4_block14_0_relu[0][0]&#39;] ## ## conv4_block14_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block14_1_conv[0][0]&#39;] ## on) ## ## conv4_block14_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block14_1_bn[0][0]&#39;] ## ## conv4_block14_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block14_1_relu[0][0]&#39;] ## ## conv4_block14_concat (Concatenate) (None, 2, 2, 704) 0 [&#39;conv4_block13_concat[0][0]&#39;, ## &#39;conv4_block14_2_conv[0][0]&#39;] ## ## conv4_block15_0_bn (BatchNormalizati (None, 2, 2, 704) 2816 [&#39;conv4_block14_concat[0][0]&#39;] ## on) ## ## conv4_block15_0_relu (Activation) (None, 2, 2, 704) 0 [&#39;conv4_block15_0_bn[0][0]&#39;] ## ## conv4_block15_1_conv (Conv2D) (None, 2, 2, 128) 90112 [&#39;conv4_block15_0_relu[0][0]&#39;] ## ## conv4_block15_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block15_1_conv[0][0]&#39;] ## on) ## ## conv4_block15_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block15_1_bn[0][0]&#39;] ## ## conv4_block15_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block15_1_relu[0][0]&#39;] ## ## conv4_block15_concat (Concatenate) (None, 2, 2, 736) 0 [&#39;conv4_block14_concat[0][0]&#39;, ## &#39;conv4_block15_2_conv[0][0]&#39;] ## ## conv4_block16_0_bn (BatchNormalizati (None, 2, 2, 736) 2944 [&#39;conv4_block15_concat[0][0]&#39;] ## on) ## ## conv4_block16_0_relu (Activation) (None, 2, 2, 736) 0 [&#39;conv4_block16_0_bn[0][0]&#39;] ## ## conv4_block16_1_conv (Conv2D) (None, 2, 2, 128) 94208 [&#39;conv4_block16_0_relu[0][0]&#39;] ## ## conv4_block16_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block16_1_conv[0][0]&#39;] ## on) ## ## conv4_block16_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block16_1_bn[0][0]&#39;] ## ## conv4_block16_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block16_1_relu[0][0]&#39;] ## ## conv4_block16_concat (Concatenate) (None, 2, 2, 768) 0 [&#39;conv4_block15_concat[0][0]&#39;, ## &#39;conv4_block16_2_conv[0][0]&#39;] ## ## conv4_block17_0_bn (BatchNormalizati (None, 2, 2, 768) 3072 [&#39;conv4_block16_concat[0][0]&#39;] ## on) ## ## conv4_block17_0_relu (Activation) (None, 2, 2, 768) 0 [&#39;conv4_block17_0_bn[0][0]&#39;] ## ## conv4_block17_1_conv (Conv2D) (None, 2, 2, 128) 98304 [&#39;conv4_block17_0_relu[0][0]&#39;] ## ## conv4_block17_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block17_1_conv[0][0]&#39;] ## on) ## ## conv4_block17_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block17_1_bn[0][0]&#39;] ## ## conv4_block17_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block17_1_relu[0][0]&#39;] ## ## conv4_block17_concat (Concatenate) (None, 2, 2, 800) 0 [&#39;conv4_block16_concat[0][0]&#39;, ## &#39;conv4_block17_2_conv[0][0]&#39;] ## ## conv4_block18_0_bn (BatchNormalizati (None, 2, 2, 800) 3200 [&#39;conv4_block17_concat[0][0]&#39;] ## on) ## ## conv4_block18_0_relu (Activation) (None, 2, 2, 800) 0 [&#39;conv4_block18_0_bn[0][0]&#39;] ## ## conv4_block18_1_conv (Conv2D) (None, 2, 2, 128) 102400 [&#39;conv4_block18_0_relu[0][0]&#39;] ## ## conv4_block18_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block18_1_conv[0][0]&#39;] ## on) ## ## conv4_block18_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block18_1_bn[0][0]&#39;] ## ## conv4_block18_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block18_1_relu[0][0]&#39;] ## ## conv4_block18_concat (Concatenate) (None, 2, 2, 832) 0 [&#39;conv4_block17_concat[0][0]&#39;, ## &#39;conv4_block18_2_conv[0][0]&#39;] ## ## conv4_block19_0_bn (BatchNormalizati (None, 2, 2, 832) 3328 [&#39;conv4_block18_concat[0][0]&#39;] ## on) ## ## conv4_block19_0_relu (Activation) (None, 2, 2, 832) 0 [&#39;conv4_block19_0_bn[0][0]&#39;] ## ## conv4_block19_1_conv (Conv2D) (None, 2, 2, 128) 106496 [&#39;conv4_block19_0_relu[0][0]&#39;] ## ## conv4_block19_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block19_1_conv[0][0]&#39;] ## on) ## ## conv4_block19_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block19_1_bn[0][0]&#39;] ## ## conv4_block19_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block19_1_relu[0][0]&#39;] ## ## conv4_block19_concat (Concatenate) (None, 2, 2, 864) 0 [&#39;conv4_block18_concat[0][0]&#39;, ## &#39;conv4_block19_2_conv[0][0]&#39;] ## ## conv4_block20_0_bn (BatchNormalizati (None, 2, 2, 864) 3456 [&#39;conv4_block19_concat[0][0]&#39;] ## on) ## ## conv4_block20_0_relu (Activation) (None, 2, 2, 864) 0 [&#39;conv4_block20_0_bn[0][0]&#39;] ## ## conv4_block20_1_conv (Conv2D) (None, 2, 2, 128) 110592 [&#39;conv4_block20_0_relu[0][0]&#39;] ## ## conv4_block20_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block20_1_conv[0][0]&#39;] ## on) ## ## conv4_block20_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block20_1_bn[0][0]&#39;] ## ## conv4_block20_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block20_1_relu[0][0]&#39;] ## ## conv4_block20_concat (Concatenate) (None, 2, 2, 896) 0 [&#39;conv4_block19_concat[0][0]&#39;, ## &#39;conv4_block20_2_conv[0][0]&#39;] ## ## conv4_block21_0_bn (BatchNormalizati (None, 2, 2, 896) 3584 [&#39;conv4_block20_concat[0][0]&#39;] ## on) ## ## conv4_block21_0_relu (Activation) (None, 2, 2, 896) 0 [&#39;conv4_block21_0_bn[0][0]&#39;] ## ## conv4_block21_1_conv (Conv2D) (None, 2, 2, 128) 114688 [&#39;conv4_block21_0_relu[0][0]&#39;] ## ## conv4_block21_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block21_1_conv[0][0]&#39;] ## on) ## ## conv4_block21_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block21_1_bn[0][0]&#39;] ## ## conv4_block21_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block21_1_relu[0][0]&#39;] ## ## conv4_block21_concat (Concatenate) (None, 2, 2, 928) 0 [&#39;conv4_block20_concat[0][0]&#39;, ## &#39;conv4_block21_2_conv[0][0]&#39;] ## ## conv4_block22_0_bn (BatchNormalizati (None, 2, 2, 928) 3712 [&#39;conv4_block21_concat[0][0]&#39;] ## on) ## ## conv4_block22_0_relu (Activation) (None, 2, 2, 928) 0 [&#39;conv4_block22_0_bn[0][0]&#39;] ## ## conv4_block22_1_conv (Conv2D) (None, 2, 2, 128) 118784 [&#39;conv4_block22_0_relu[0][0]&#39;] ## ## conv4_block22_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block22_1_conv[0][0]&#39;] ## on) ## ## conv4_block22_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block22_1_bn[0][0]&#39;] ## ## conv4_block22_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block22_1_relu[0][0]&#39;] ## ## conv4_block22_concat (Concatenate) (None, 2, 2, 960) 0 [&#39;conv4_block21_concat[0][0]&#39;, ## &#39;conv4_block22_2_conv[0][0]&#39;] ## ## conv4_block23_0_bn (BatchNormalizati (None, 2, 2, 960) 3840 [&#39;conv4_block22_concat[0][0]&#39;] ## on) ## ## conv4_block23_0_relu (Activation) (None, 2, 2, 960) 0 [&#39;conv4_block23_0_bn[0][0]&#39;] ## ## conv4_block23_1_conv (Conv2D) (None, 2, 2, 128) 122880 [&#39;conv4_block23_0_relu[0][0]&#39;] ## ## conv4_block23_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block23_1_conv[0][0]&#39;] ## on) ## ## conv4_block23_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block23_1_bn[0][0]&#39;] ## ## conv4_block23_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block23_1_relu[0][0]&#39;] ## ## conv4_block23_concat (Concatenate) (None, 2, 2, 992) 0 [&#39;conv4_block22_concat[0][0]&#39;, ## &#39;conv4_block23_2_conv[0][0]&#39;] ## ## conv4_block24_0_bn (BatchNormalizati (None, 2, 2, 992) 3968 [&#39;conv4_block23_concat[0][0]&#39;] ## on) ## ## conv4_block24_0_relu (Activation) (None, 2, 2, 992) 0 [&#39;conv4_block24_0_bn[0][0]&#39;] ## ## conv4_block24_1_conv (Conv2D) (None, 2, 2, 128) 126976 [&#39;conv4_block24_0_relu[0][0]&#39;] ## ## conv4_block24_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block24_1_conv[0][0]&#39;] ## on) ## ## conv4_block24_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block24_1_bn[0][0]&#39;] ## ## conv4_block24_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block24_1_relu[0][0]&#39;] ## ## conv4_block24_concat (Concatenate) (None, 2, 2, 1024) 0 [&#39;conv4_block23_concat[0][0]&#39;, ## &#39;conv4_block24_2_conv[0][0]&#39;] ## ## conv4_block25_0_bn (BatchNormalizati (None, 2, 2, 1024) 4096 [&#39;conv4_block24_concat[0][0]&#39;] ## on) ## ## conv4_block25_0_relu (Activation) (None, 2, 2, 1024) 0 [&#39;conv4_block25_0_bn[0][0]&#39;] ## ## conv4_block25_1_conv (Conv2D) (None, 2, 2, 128) 131072 [&#39;conv4_block25_0_relu[0][0]&#39;] ## ## conv4_block25_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block25_1_conv[0][0]&#39;] ## on) ## ## conv4_block25_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block25_1_bn[0][0]&#39;] ## ## conv4_block25_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block25_1_relu[0][0]&#39;] ## ## conv4_block25_concat (Concatenate) (None, 2, 2, 1056) 0 [&#39;conv4_block24_concat[0][0]&#39;, ## &#39;conv4_block25_2_conv[0][0]&#39;] ## ## conv4_block26_0_bn (BatchNormalizati (None, 2, 2, 1056) 4224 [&#39;conv4_block25_concat[0][0]&#39;] ## on) ## ## conv4_block26_0_relu (Activation) (None, 2, 2, 1056) 0 [&#39;conv4_block26_0_bn[0][0]&#39;] ## ## conv4_block26_1_conv (Conv2D) (None, 2, 2, 128) 135168 [&#39;conv4_block26_0_relu[0][0]&#39;] ## ## conv4_block26_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block26_1_conv[0][0]&#39;] ## on) ## ## conv4_block26_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block26_1_bn[0][0]&#39;] ## ## conv4_block26_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block26_1_relu[0][0]&#39;] ## ## conv4_block26_concat (Concatenate) (None, 2, 2, 1088) 0 [&#39;conv4_block25_concat[0][0]&#39;, ## &#39;conv4_block26_2_conv[0][0]&#39;] ## ## conv4_block27_0_bn (BatchNormalizati (None, 2, 2, 1088) 4352 [&#39;conv4_block26_concat[0][0]&#39;] ## on) ## ## conv4_block27_0_relu (Activation) (None, 2, 2, 1088) 0 [&#39;conv4_block27_0_bn[0][0]&#39;] ## ## conv4_block27_1_conv (Conv2D) (None, 2, 2, 128) 139264 [&#39;conv4_block27_0_relu[0][0]&#39;] ## ## conv4_block27_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block27_1_conv[0][0]&#39;] ## on) ## ## conv4_block27_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block27_1_bn[0][0]&#39;] ## ## conv4_block27_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block27_1_relu[0][0]&#39;] ## ## conv4_block27_concat (Concatenate) (None, 2, 2, 1120) 0 [&#39;conv4_block26_concat[0][0]&#39;, ## &#39;conv4_block27_2_conv[0][0]&#39;] ## ## conv4_block28_0_bn (BatchNormalizati (None, 2, 2, 1120) 4480 [&#39;conv4_block27_concat[0][0]&#39;] ## on) ## ## conv4_block28_0_relu (Activation) (None, 2, 2, 1120) 0 [&#39;conv4_block28_0_bn[0][0]&#39;] ## ## conv4_block28_1_conv (Conv2D) (None, 2, 2, 128) 143360 [&#39;conv4_block28_0_relu[0][0]&#39;] ## ## conv4_block28_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block28_1_conv[0][0]&#39;] ## on) ## ## conv4_block28_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block28_1_bn[0][0]&#39;] ## ## conv4_block28_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block28_1_relu[0][0]&#39;] ## ## conv4_block28_concat (Concatenate) (None, 2, 2, 1152) 0 [&#39;conv4_block27_concat[0][0]&#39;, ## &#39;conv4_block28_2_conv[0][0]&#39;] ## ## conv4_block29_0_bn (BatchNormalizati (None, 2, 2, 1152) 4608 [&#39;conv4_block28_concat[0][0]&#39;] ## on) ## ## conv4_block29_0_relu (Activation) (None, 2, 2, 1152) 0 [&#39;conv4_block29_0_bn[0][0]&#39;] ## ## conv4_block29_1_conv (Conv2D) (None, 2, 2, 128) 147456 [&#39;conv4_block29_0_relu[0][0]&#39;] ## ## conv4_block29_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block29_1_conv[0][0]&#39;] ## on) ## ## conv4_block29_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block29_1_bn[0][0]&#39;] ## ## conv4_block29_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block29_1_relu[0][0]&#39;] ## ## conv4_block29_concat (Concatenate) (None, 2, 2, 1184) 0 [&#39;conv4_block28_concat[0][0]&#39;, ## &#39;conv4_block29_2_conv[0][0]&#39;] ## ## conv4_block30_0_bn (BatchNormalizati (None, 2, 2, 1184) 4736 [&#39;conv4_block29_concat[0][0]&#39;] ## on) ## ## conv4_block30_0_relu (Activation) (None, 2, 2, 1184) 0 [&#39;conv4_block30_0_bn[0][0]&#39;] ## ## conv4_block30_1_conv (Conv2D) (None, 2, 2, 128) 151552 [&#39;conv4_block30_0_relu[0][0]&#39;] ## ## conv4_block30_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block30_1_conv[0][0]&#39;] ## on) ## ## conv4_block30_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block30_1_bn[0][0]&#39;] ## ## conv4_block30_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block30_1_relu[0][0]&#39;] ## ## conv4_block30_concat (Concatenate) (None, 2, 2, 1216) 0 [&#39;conv4_block29_concat[0][0]&#39;, ## &#39;conv4_block30_2_conv[0][0]&#39;] ## ## conv4_block31_0_bn (BatchNormalizati (None, 2, 2, 1216) 4864 [&#39;conv4_block30_concat[0][0]&#39;] ## on) ## ## conv4_block31_0_relu (Activation) (None, 2, 2, 1216) 0 [&#39;conv4_block31_0_bn[0][0]&#39;] ## ## conv4_block31_1_conv (Conv2D) (None, 2, 2, 128) 155648 [&#39;conv4_block31_0_relu[0][0]&#39;] ## ## conv4_block31_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block31_1_conv[0][0]&#39;] ## on) ## ## conv4_block31_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block31_1_bn[0][0]&#39;] ## ## conv4_block31_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block31_1_relu[0][0]&#39;] ## ## conv4_block31_concat (Concatenate) (None, 2, 2, 1248) 0 [&#39;conv4_block30_concat[0][0]&#39;, ## &#39;conv4_block31_2_conv[0][0]&#39;] ## ## conv4_block32_0_bn (BatchNormalizati (None, 2, 2, 1248) 4992 [&#39;conv4_block31_concat[0][0]&#39;] ## on) ## ## conv4_block32_0_relu (Activation) (None, 2, 2, 1248) 0 [&#39;conv4_block32_0_bn[0][0]&#39;] ## ## conv4_block32_1_conv (Conv2D) (None, 2, 2, 128) 159744 [&#39;conv4_block32_0_relu[0][0]&#39;] ## ## conv4_block32_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block32_1_conv[0][0]&#39;] ## on) ## ## conv4_block32_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block32_1_bn[0][0]&#39;] ## ## conv4_block32_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block32_1_relu[0][0]&#39;] ## ## conv4_block32_concat (Concatenate) (None, 2, 2, 1280) 0 [&#39;conv4_block31_concat[0][0]&#39;, ## &#39;conv4_block32_2_conv[0][0]&#39;] ## ## conv4_block33_0_bn (BatchNormalizati (None, 2, 2, 1280) 5120 [&#39;conv4_block32_concat[0][0]&#39;] ## on) ## ## conv4_block33_0_relu (Activation) (None, 2, 2, 1280) 0 [&#39;conv4_block33_0_bn[0][0]&#39;] ## ## conv4_block33_1_conv (Conv2D) (None, 2, 2, 128) 163840 [&#39;conv4_block33_0_relu[0][0]&#39;] ## ## conv4_block33_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block33_1_conv[0][0]&#39;] ## on) ## ## conv4_block33_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block33_1_bn[0][0]&#39;] ## ## conv4_block33_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block33_1_relu[0][0]&#39;] ## ## conv4_block33_concat (Concatenate) (None, 2, 2, 1312) 0 [&#39;conv4_block32_concat[0][0]&#39;, ## &#39;conv4_block33_2_conv[0][0]&#39;] ## ## conv4_block34_0_bn (BatchNormalizati (None, 2, 2, 1312) 5248 [&#39;conv4_block33_concat[0][0]&#39;] ## on) ## ## conv4_block34_0_relu (Activation) (None, 2, 2, 1312) 0 [&#39;conv4_block34_0_bn[0][0]&#39;] ## ## conv4_block34_1_conv (Conv2D) (None, 2, 2, 128) 167936 [&#39;conv4_block34_0_relu[0][0]&#39;] ## ## conv4_block34_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block34_1_conv[0][0]&#39;] ## on) ## ## conv4_block34_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block34_1_bn[0][0]&#39;] ## ## conv4_block34_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block34_1_relu[0][0]&#39;] ## ## conv4_block34_concat (Concatenate) (None, 2, 2, 1344) 0 [&#39;conv4_block33_concat[0][0]&#39;, ## &#39;conv4_block34_2_conv[0][0]&#39;] ## ## conv4_block35_0_bn (BatchNormalizati (None, 2, 2, 1344) 5376 [&#39;conv4_block34_concat[0][0]&#39;] ## on) ## ## conv4_block35_0_relu (Activation) (None, 2, 2, 1344) 0 [&#39;conv4_block35_0_bn[0][0]&#39;] ## ## conv4_block35_1_conv (Conv2D) (None, 2, 2, 128) 172032 [&#39;conv4_block35_0_relu[0][0]&#39;] ## ## conv4_block35_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block35_1_conv[0][0]&#39;] ## on) ## ## conv4_block35_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block35_1_bn[0][0]&#39;] ## ## conv4_block35_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block35_1_relu[0][0]&#39;] ## ## conv4_block35_concat (Concatenate) (None, 2, 2, 1376) 0 [&#39;conv4_block34_concat[0][0]&#39;, ## &#39;conv4_block35_2_conv[0][0]&#39;] ## ## conv4_block36_0_bn (BatchNormalizati (None, 2, 2, 1376) 5504 [&#39;conv4_block35_concat[0][0]&#39;] ## on) ## ## conv4_block36_0_relu (Activation) (None, 2, 2, 1376) 0 [&#39;conv4_block36_0_bn[0][0]&#39;] ## ## conv4_block36_1_conv (Conv2D) (None, 2, 2, 128) 176128 [&#39;conv4_block36_0_relu[0][0]&#39;] ## ## conv4_block36_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block36_1_conv[0][0]&#39;] ## on) ## ## conv4_block36_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block36_1_bn[0][0]&#39;] ## ## conv4_block36_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block36_1_relu[0][0]&#39;] ## ## conv4_block36_concat (Concatenate) (None, 2, 2, 1408) 0 [&#39;conv4_block35_concat[0][0]&#39;, ## &#39;conv4_block36_2_conv[0][0]&#39;] ## ## conv4_block37_0_bn (BatchNormalizati (None, 2, 2, 1408) 5632 [&#39;conv4_block36_concat[0][0]&#39;] ## on) ## ## conv4_block37_0_relu (Activation) (None, 2, 2, 1408) 0 [&#39;conv4_block37_0_bn[0][0]&#39;] ## ## conv4_block37_1_conv (Conv2D) (None, 2, 2, 128) 180224 [&#39;conv4_block37_0_relu[0][0]&#39;] ## ## conv4_block37_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block37_1_conv[0][0]&#39;] ## on) ## ## conv4_block37_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block37_1_bn[0][0]&#39;] ## ## conv4_block37_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block37_1_relu[0][0]&#39;] ## ## conv4_block37_concat (Concatenate) (None, 2, 2, 1440) 0 [&#39;conv4_block36_concat[0][0]&#39;, ## &#39;conv4_block37_2_conv[0][0]&#39;] ## ## conv4_block38_0_bn (BatchNormalizati (None, 2, 2, 1440) 5760 [&#39;conv4_block37_concat[0][0]&#39;] ## on) ## ## conv4_block38_0_relu (Activation) (None, 2, 2, 1440) 0 [&#39;conv4_block38_0_bn[0][0]&#39;] ## ## conv4_block38_1_conv (Conv2D) (None, 2, 2, 128) 184320 [&#39;conv4_block38_0_relu[0][0]&#39;] ## ## conv4_block38_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block38_1_conv[0][0]&#39;] ## on) ## ## conv4_block38_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block38_1_bn[0][0]&#39;] ## ## conv4_block38_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block38_1_relu[0][0]&#39;] ## ## conv4_block38_concat (Concatenate) (None, 2, 2, 1472) 0 [&#39;conv4_block37_concat[0][0]&#39;, ## &#39;conv4_block38_2_conv[0][0]&#39;] ## ## conv4_block39_0_bn (BatchNormalizati (None, 2, 2, 1472) 5888 [&#39;conv4_block38_concat[0][0]&#39;] ## on) ## ## conv4_block39_0_relu (Activation) (None, 2, 2, 1472) 0 [&#39;conv4_block39_0_bn[0][0]&#39;] ## ## conv4_block39_1_conv (Conv2D) (None, 2, 2, 128) 188416 [&#39;conv4_block39_0_relu[0][0]&#39;] ## ## conv4_block39_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block39_1_conv[0][0]&#39;] ## on) ## ## conv4_block39_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block39_1_bn[0][0]&#39;] ## ## conv4_block39_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block39_1_relu[0][0]&#39;] ## ## conv4_block39_concat (Concatenate) (None, 2, 2, 1504) 0 [&#39;conv4_block38_concat[0][0]&#39;, ## &#39;conv4_block39_2_conv[0][0]&#39;] ## ## conv4_block40_0_bn (BatchNormalizati (None, 2, 2, 1504) 6016 [&#39;conv4_block39_concat[0][0]&#39;] ## on) ## ## conv4_block40_0_relu (Activation) (None, 2, 2, 1504) 0 [&#39;conv4_block40_0_bn[0][0]&#39;] ## ## conv4_block40_1_conv (Conv2D) (None, 2, 2, 128) 192512 [&#39;conv4_block40_0_relu[0][0]&#39;] ## ## conv4_block40_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block40_1_conv[0][0]&#39;] ## on) ## ## conv4_block40_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block40_1_bn[0][0]&#39;] ## ## conv4_block40_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block40_1_relu[0][0]&#39;] ## ## conv4_block40_concat (Concatenate) (None, 2, 2, 1536) 0 [&#39;conv4_block39_concat[0][0]&#39;, ## &#39;conv4_block40_2_conv[0][0]&#39;] ## ## conv4_block41_0_bn (BatchNormalizati (None, 2, 2, 1536) 6144 [&#39;conv4_block40_concat[0][0]&#39;] ## on) ## ## conv4_block41_0_relu (Activation) (None, 2, 2, 1536) 0 [&#39;conv4_block41_0_bn[0][0]&#39;] ## ## conv4_block41_1_conv (Conv2D) (None, 2, 2, 128) 196608 [&#39;conv4_block41_0_relu[0][0]&#39;] ## ## conv4_block41_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block41_1_conv[0][0]&#39;] ## on) ## ## conv4_block41_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block41_1_bn[0][0]&#39;] ## ## conv4_block41_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block41_1_relu[0][0]&#39;] ## ## conv4_block41_concat (Concatenate) (None, 2, 2, 1568) 0 [&#39;conv4_block40_concat[0][0]&#39;, ## &#39;conv4_block41_2_conv[0][0]&#39;] ## ## conv4_block42_0_bn (BatchNormalizati (None, 2, 2, 1568) 6272 [&#39;conv4_block41_concat[0][0]&#39;] ## on) ## ## conv4_block42_0_relu (Activation) (None, 2, 2, 1568) 0 [&#39;conv4_block42_0_bn[0][0]&#39;] ## ## conv4_block42_1_conv (Conv2D) (None, 2, 2, 128) 200704 [&#39;conv4_block42_0_relu[0][0]&#39;] ## ## conv4_block42_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block42_1_conv[0][0]&#39;] ## on) ## ## conv4_block42_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block42_1_bn[0][0]&#39;] ## ## conv4_block42_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block42_1_relu[0][0]&#39;] ## ## conv4_block42_concat (Concatenate) (None, 2, 2, 1600) 0 [&#39;conv4_block41_concat[0][0]&#39;, ## &#39;conv4_block42_2_conv[0][0]&#39;] ## ## conv4_block43_0_bn (BatchNormalizati (None, 2, 2, 1600) 6400 [&#39;conv4_block42_concat[0][0]&#39;] ## on) ## ## conv4_block43_0_relu (Activation) (None, 2, 2, 1600) 0 [&#39;conv4_block43_0_bn[0][0]&#39;] ## ## conv4_block43_1_conv (Conv2D) (None, 2, 2, 128) 204800 [&#39;conv4_block43_0_relu[0][0]&#39;] ## ## conv4_block43_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block43_1_conv[0][0]&#39;] ## on) ## ## conv4_block43_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block43_1_bn[0][0]&#39;] ## ## conv4_block43_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block43_1_relu[0][0]&#39;] ## ## conv4_block43_concat (Concatenate) (None, 2, 2, 1632) 0 [&#39;conv4_block42_concat[0][0]&#39;, ## &#39;conv4_block43_2_conv[0][0]&#39;] ## ## conv4_block44_0_bn (BatchNormalizati (None, 2, 2, 1632) 6528 [&#39;conv4_block43_concat[0][0]&#39;] ## on) ## ## conv4_block44_0_relu (Activation) (None, 2, 2, 1632) 0 [&#39;conv4_block44_0_bn[0][0]&#39;] ## ## conv4_block44_1_conv (Conv2D) (None, 2, 2, 128) 208896 [&#39;conv4_block44_0_relu[0][0]&#39;] ## ## conv4_block44_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block44_1_conv[0][0]&#39;] ## on) ## ## conv4_block44_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block44_1_bn[0][0]&#39;] ## ## conv4_block44_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block44_1_relu[0][0]&#39;] ## ## conv4_block44_concat (Concatenate) (None, 2, 2, 1664) 0 [&#39;conv4_block43_concat[0][0]&#39;, ## &#39;conv4_block44_2_conv[0][0]&#39;] ## ## conv4_block45_0_bn (BatchNormalizati (None, 2, 2, 1664) 6656 [&#39;conv4_block44_concat[0][0]&#39;] ## on) ## ## conv4_block45_0_relu (Activation) (None, 2, 2, 1664) 0 [&#39;conv4_block45_0_bn[0][0]&#39;] ## ## conv4_block45_1_conv (Conv2D) (None, 2, 2, 128) 212992 [&#39;conv4_block45_0_relu[0][0]&#39;] ## ## conv4_block45_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block45_1_conv[0][0]&#39;] ## on) ## ## conv4_block45_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block45_1_bn[0][0]&#39;] ## ## conv4_block45_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block45_1_relu[0][0]&#39;] ## ## conv4_block45_concat (Concatenate) (None, 2, 2, 1696) 0 [&#39;conv4_block44_concat[0][0]&#39;, ## &#39;conv4_block45_2_conv[0][0]&#39;] ## ## conv4_block46_0_bn (BatchNormalizati (None, 2, 2, 1696) 6784 [&#39;conv4_block45_concat[0][0]&#39;] ## on) ## ## conv4_block46_0_relu (Activation) (None, 2, 2, 1696) 0 [&#39;conv4_block46_0_bn[0][0]&#39;] ## ## conv4_block46_1_conv (Conv2D) (None, 2, 2, 128) 217088 [&#39;conv4_block46_0_relu[0][0]&#39;] ## ## conv4_block46_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block46_1_conv[0][0]&#39;] ## on) ## ## conv4_block46_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block46_1_bn[0][0]&#39;] ## ## conv4_block46_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block46_1_relu[0][0]&#39;] ## ## conv4_block46_concat (Concatenate) (None, 2, 2, 1728) 0 [&#39;conv4_block45_concat[0][0]&#39;, ## &#39;conv4_block46_2_conv[0][0]&#39;] ## ## conv4_block47_0_bn (BatchNormalizati (None, 2, 2, 1728) 6912 [&#39;conv4_block46_concat[0][0]&#39;] ## on) ## ## conv4_block47_0_relu (Activation) (None, 2, 2, 1728) 0 [&#39;conv4_block47_0_bn[0][0]&#39;] ## ## conv4_block47_1_conv (Conv2D) (None, 2, 2, 128) 221184 [&#39;conv4_block47_0_relu[0][0]&#39;] ## ## conv4_block47_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block47_1_conv[0][0]&#39;] ## on) ## ## conv4_block47_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block47_1_bn[0][0]&#39;] ## ## conv4_block47_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block47_1_relu[0][0]&#39;] ## ## conv4_block47_concat (Concatenate) (None, 2, 2, 1760) 0 [&#39;conv4_block46_concat[0][0]&#39;, ## &#39;conv4_block47_2_conv[0][0]&#39;] ## ## conv4_block48_0_bn (BatchNormalizati (None, 2, 2, 1760) 7040 [&#39;conv4_block47_concat[0][0]&#39;] ## on) ## ## conv4_block48_0_relu (Activation) (None, 2, 2, 1760) 0 [&#39;conv4_block48_0_bn[0][0]&#39;] ## ## conv4_block48_1_conv (Conv2D) (None, 2, 2, 128) 225280 [&#39;conv4_block48_0_relu[0][0]&#39;] ## ## conv4_block48_1_bn (BatchNormalizati (None, 2, 2, 128) 512 [&#39;conv4_block48_1_conv[0][0]&#39;] ## on) ## ## conv4_block48_1_relu (Activation) (None, 2, 2, 128) 0 [&#39;conv4_block48_1_bn[0][0]&#39;] ## ## conv4_block48_2_conv (Conv2D) (None, 2, 2, 32) 36864 [&#39;conv4_block48_1_relu[0][0]&#39;] ## ## conv4_block48_concat (Concatenate) (None, 2, 2, 1792) 0 [&#39;conv4_block47_concat[0][0]&#39;, ## &#39;conv4_block48_2_conv[0][0]&#39;] ## ## pool4_bn (BatchNormalization) (None, 2, 2, 1792) 7168 [&#39;conv4_block48_concat[0][0]&#39;] ## ## pool4_relu (Activation) (None, 2, 2, 1792) 0 [&#39;pool4_bn[0][0]&#39;] ## ## pool4_conv (Conv2D) (None, 2, 2, 896) 1605632 [&#39;pool4_relu[0][0]&#39;] ## ## pool4_pool (AveragePooling2D) (None, 1, 1, 896) 0 [&#39;pool4_conv[0][0]&#39;] ## ## conv5_block1_0_bn (BatchNormalizatio (None, 1, 1, 896) 3584 [&#39;pool4_pool[0][0]&#39;] ## n) ## ## conv5_block1_0_relu (Activation) (None, 1, 1, 896) 0 [&#39;conv5_block1_0_bn[0][0]&#39;] ## ## conv5_block1_1_conv (Conv2D) (None, 1, 1, 128) 114688 [&#39;conv5_block1_0_relu[0][0]&#39;] ## ## conv5_block1_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block1_1_conv[0][0]&#39;] ## n) ## ## conv5_block1_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block1_1_bn[0][0]&#39;] ## ## conv5_block1_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block1_1_relu[0][0]&#39;] ## ## conv5_block1_concat (Concatenate) (None, 1, 1, 928) 0 [&#39;pool4_pool[0][0]&#39;, ## &#39;conv5_block1_2_conv[0][0]&#39;] ## ## conv5_block2_0_bn (BatchNormalizatio (None, 1, 1, 928) 3712 [&#39;conv5_block1_concat[0][0]&#39;] ## n) ## ## conv5_block2_0_relu (Activation) (None, 1, 1, 928) 0 [&#39;conv5_block2_0_bn[0][0]&#39;] ## ## conv5_block2_1_conv (Conv2D) (None, 1, 1, 128) 118784 [&#39;conv5_block2_0_relu[0][0]&#39;] ## ## conv5_block2_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block2_1_conv[0][0]&#39;] ## n) ## ## conv5_block2_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block2_1_bn[0][0]&#39;] ## ## conv5_block2_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block2_1_relu[0][0]&#39;] ## ## conv5_block2_concat (Concatenate) (None, 1, 1, 960) 0 [&#39;conv5_block1_concat[0][0]&#39;, ## &#39;conv5_block2_2_conv[0][0]&#39;] ## ## conv5_block3_0_bn (BatchNormalizatio (None, 1, 1, 960) 3840 [&#39;conv5_block2_concat[0][0]&#39;] ## n) ## ## conv5_block3_0_relu (Activation) (None, 1, 1, 960) 0 [&#39;conv5_block3_0_bn[0][0]&#39;] ## ## conv5_block3_1_conv (Conv2D) (None, 1, 1, 128) 122880 [&#39;conv5_block3_0_relu[0][0]&#39;] ## ## conv5_block3_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block3_1_conv[0][0]&#39;] ## n) ## ## conv5_block3_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block3_1_bn[0][0]&#39;] ## ## conv5_block3_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block3_1_relu[0][0]&#39;] ## ## conv5_block3_concat (Concatenate) (None, 1, 1, 992) 0 [&#39;conv5_block2_concat[0][0]&#39;, ## &#39;conv5_block3_2_conv[0][0]&#39;] ## ## conv5_block4_0_bn (BatchNormalizatio (None, 1, 1, 992) 3968 [&#39;conv5_block3_concat[0][0]&#39;] ## n) ## ## conv5_block4_0_relu (Activation) (None, 1, 1, 992) 0 [&#39;conv5_block4_0_bn[0][0]&#39;] ## ## conv5_block4_1_conv (Conv2D) (None, 1, 1, 128) 126976 [&#39;conv5_block4_0_relu[0][0]&#39;] ## ## conv5_block4_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block4_1_conv[0][0]&#39;] ## n) ## ## conv5_block4_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block4_1_bn[0][0]&#39;] ## ## conv5_block4_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block4_1_relu[0][0]&#39;] ## ## conv5_block4_concat (Concatenate) (None, 1, 1, 1024) 0 [&#39;conv5_block3_concat[0][0]&#39;, ## &#39;conv5_block4_2_conv[0][0]&#39;] ## ## conv5_block5_0_bn (BatchNormalizatio (None, 1, 1, 1024) 4096 [&#39;conv5_block4_concat[0][0]&#39;] ## n) ## ## conv5_block5_0_relu (Activation) (None, 1, 1, 1024) 0 [&#39;conv5_block5_0_bn[0][0]&#39;] ## ## conv5_block5_1_conv (Conv2D) (None, 1, 1, 128) 131072 [&#39;conv5_block5_0_relu[0][0]&#39;] ## ## conv5_block5_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block5_1_conv[0][0]&#39;] ## n) ## ## conv5_block5_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block5_1_bn[0][0]&#39;] ## ## conv5_block5_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block5_1_relu[0][0]&#39;] ## ## conv5_block5_concat (Concatenate) (None, 1, 1, 1056) 0 [&#39;conv5_block4_concat[0][0]&#39;, ## &#39;conv5_block5_2_conv[0][0]&#39;] ## ## conv5_block6_0_bn (BatchNormalizatio (None, 1, 1, 1056) 4224 [&#39;conv5_block5_concat[0][0]&#39;] ## n) ## ## conv5_block6_0_relu (Activation) (None, 1, 1, 1056) 0 [&#39;conv5_block6_0_bn[0][0]&#39;] ## ## conv5_block6_1_conv (Conv2D) (None, 1, 1, 128) 135168 [&#39;conv5_block6_0_relu[0][0]&#39;] ## ## conv5_block6_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block6_1_conv[0][0]&#39;] ## n) ## ## conv5_block6_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block6_1_bn[0][0]&#39;] ## ## conv5_block6_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block6_1_relu[0][0]&#39;] ## ## conv5_block6_concat (Concatenate) (None, 1, 1, 1088) 0 [&#39;conv5_block5_concat[0][0]&#39;, ## &#39;conv5_block6_2_conv[0][0]&#39;] ## ## conv5_block7_0_bn (BatchNormalizatio (None, 1, 1, 1088) 4352 [&#39;conv5_block6_concat[0][0]&#39;] ## n) ## ## conv5_block7_0_relu (Activation) (None, 1, 1, 1088) 0 [&#39;conv5_block7_0_bn[0][0]&#39;] ## ## conv5_block7_1_conv (Conv2D) (None, 1, 1, 128) 139264 [&#39;conv5_block7_0_relu[0][0]&#39;] ## ## conv5_block7_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block7_1_conv[0][0]&#39;] ## n) ## ## conv5_block7_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block7_1_bn[0][0]&#39;] ## ## conv5_block7_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block7_1_relu[0][0]&#39;] ## ## conv5_block7_concat (Concatenate) (None, 1, 1, 1120) 0 [&#39;conv5_block6_concat[0][0]&#39;, ## &#39;conv5_block7_2_conv[0][0]&#39;] ## ## conv5_block8_0_bn (BatchNormalizatio (None, 1, 1, 1120) 4480 [&#39;conv5_block7_concat[0][0]&#39;] ## n) ## ## conv5_block8_0_relu (Activation) (None, 1, 1, 1120) 0 [&#39;conv5_block8_0_bn[0][0]&#39;] ## ## conv5_block8_1_conv (Conv2D) (None, 1, 1, 128) 143360 [&#39;conv5_block8_0_relu[0][0]&#39;] ## ## conv5_block8_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block8_1_conv[0][0]&#39;] ## n) ## ## conv5_block8_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block8_1_bn[0][0]&#39;] ## ## conv5_block8_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block8_1_relu[0][0]&#39;] ## ## conv5_block8_concat (Concatenate) (None, 1, 1, 1152) 0 [&#39;conv5_block7_concat[0][0]&#39;, ## &#39;conv5_block8_2_conv[0][0]&#39;] ## ## conv5_block9_0_bn (BatchNormalizatio (None, 1, 1, 1152) 4608 [&#39;conv5_block8_concat[0][0]&#39;] ## n) ## ## conv5_block9_0_relu (Activation) (None, 1, 1, 1152) 0 [&#39;conv5_block9_0_bn[0][0]&#39;] ## ## conv5_block9_1_conv (Conv2D) (None, 1, 1, 128) 147456 [&#39;conv5_block9_0_relu[0][0]&#39;] ## ## conv5_block9_1_bn (BatchNormalizatio (None, 1, 1, 128) 512 [&#39;conv5_block9_1_conv[0][0]&#39;] ## n) ## ## conv5_block9_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block9_1_bn[0][0]&#39;] ## ## conv5_block9_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block9_1_relu[0][0]&#39;] ## ## conv5_block9_concat (Concatenate) (None, 1, 1, 1184) 0 [&#39;conv5_block8_concat[0][0]&#39;, ## &#39;conv5_block9_2_conv[0][0]&#39;] ## ## conv5_block10_0_bn (BatchNormalizati (None, 1, 1, 1184) 4736 [&#39;conv5_block9_concat[0][0]&#39;] ## on) ## ## conv5_block10_0_relu (Activation) (None, 1, 1, 1184) 0 [&#39;conv5_block10_0_bn[0][0]&#39;] ## ## conv5_block10_1_conv (Conv2D) (None, 1, 1, 128) 151552 [&#39;conv5_block10_0_relu[0][0]&#39;] ## ## conv5_block10_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block10_1_conv[0][0]&#39;] ## on) ## ## conv5_block10_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block10_1_bn[0][0]&#39;] ## ## conv5_block10_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block10_1_relu[0][0]&#39;] ## ## conv5_block10_concat (Concatenate) (None, 1, 1, 1216) 0 [&#39;conv5_block9_concat[0][0]&#39;, ## &#39;conv5_block10_2_conv[0][0]&#39;] ## ## conv5_block11_0_bn (BatchNormalizati (None, 1, 1, 1216) 4864 [&#39;conv5_block10_concat[0][0]&#39;] ## on) ## ## conv5_block11_0_relu (Activation) (None, 1, 1, 1216) 0 [&#39;conv5_block11_0_bn[0][0]&#39;] ## ## conv5_block11_1_conv (Conv2D) (None, 1, 1, 128) 155648 [&#39;conv5_block11_0_relu[0][0]&#39;] ## ## conv5_block11_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block11_1_conv[0][0]&#39;] ## on) ## ## conv5_block11_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block11_1_bn[0][0]&#39;] ## ## conv5_block11_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block11_1_relu[0][0]&#39;] ## ## conv5_block11_concat (Concatenate) (None, 1, 1, 1248) 0 [&#39;conv5_block10_concat[0][0]&#39;, ## &#39;conv5_block11_2_conv[0][0]&#39;] ## ## conv5_block12_0_bn (BatchNormalizati (None, 1, 1, 1248) 4992 [&#39;conv5_block11_concat[0][0]&#39;] ## on) ## ## conv5_block12_0_relu (Activation) (None, 1, 1, 1248) 0 [&#39;conv5_block12_0_bn[0][0]&#39;] ## ## conv5_block12_1_conv (Conv2D) (None, 1, 1, 128) 159744 [&#39;conv5_block12_0_relu[0][0]&#39;] ## ## conv5_block12_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block12_1_conv[0][0]&#39;] ## on) ## ## conv5_block12_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block12_1_bn[0][0]&#39;] ## ## conv5_block12_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block12_1_relu[0][0]&#39;] ## ## conv5_block12_concat (Concatenate) (None, 1, 1, 1280) 0 [&#39;conv5_block11_concat[0][0]&#39;, ## &#39;conv5_block12_2_conv[0][0]&#39;] ## ## conv5_block13_0_bn (BatchNormalizati (None, 1, 1, 1280) 5120 [&#39;conv5_block12_concat[0][0]&#39;] ## on) ## ## conv5_block13_0_relu (Activation) (None, 1, 1, 1280) 0 [&#39;conv5_block13_0_bn[0][0]&#39;] ## ## conv5_block13_1_conv (Conv2D) (None, 1, 1, 128) 163840 [&#39;conv5_block13_0_relu[0][0]&#39;] ## ## conv5_block13_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block13_1_conv[0][0]&#39;] ## on) ## ## conv5_block13_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block13_1_bn[0][0]&#39;] ## ## conv5_block13_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block13_1_relu[0][0]&#39;] ## ## conv5_block13_concat (Concatenate) (None, 1, 1, 1312) 0 [&#39;conv5_block12_concat[0][0]&#39;, ## &#39;conv5_block13_2_conv[0][0]&#39;] ## ## conv5_block14_0_bn (BatchNormalizati (None, 1, 1, 1312) 5248 [&#39;conv5_block13_concat[0][0]&#39;] ## on) ## ## conv5_block14_0_relu (Activation) (None, 1, 1, 1312) 0 [&#39;conv5_block14_0_bn[0][0]&#39;] ## ## conv5_block14_1_conv (Conv2D) (None, 1, 1, 128) 167936 [&#39;conv5_block14_0_relu[0][0]&#39;] ## ## conv5_block14_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block14_1_conv[0][0]&#39;] ## on) ## ## conv5_block14_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block14_1_bn[0][0]&#39;] ## ## conv5_block14_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block14_1_relu[0][0]&#39;] ## ## conv5_block14_concat (Concatenate) (None, 1, 1, 1344) 0 [&#39;conv5_block13_concat[0][0]&#39;, ## &#39;conv5_block14_2_conv[0][0]&#39;] ## ## conv5_block15_0_bn (BatchNormalizati (None, 1, 1, 1344) 5376 [&#39;conv5_block14_concat[0][0]&#39;] ## on) ## ## conv5_block15_0_relu (Activation) (None, 1, 1, 1344) 0 [&#39;conv5_block15_0_bn[0][0]&#39;] ## ## conv5_block15_1_conv (Conv2D) (None, 1, 1, 128) 172032 [&#39;conv5_block15_0_relu[0][0]&#39;] ## ## conv5_block15_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block15_1_conv[0][0]&#39;] ## on) ## ## conv5_block15_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block15_1_bn[0][0]&#39;] ## ## conv5_block15_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block15_1_relu[0][0]&#39;] ## ## conv5_block15_concat (Concatenate) (None, 1, 1, 1376) 0 [&#39;conv5_block14_concat[0][0]&#39;, ## &#39;conv5_block15_2_conv[0][0]&#39;] ## ## conv5_block16_0_bn (BatchNormalizati (None, 1, 1, 1376) 5504 [&#39;conv5_block15_concat[0][0]&#39;] ## on) ## ## conv5_block16_0_relu (Activation) (None, 1, 1, 1376) 0 [&#39;conv5_block16_0_bn[0][0]&#39;] ## ## conv5_block16_1_conv (Conv2D) (None, 1, 1, 128) 176128 [&#39;conv5_block16_0_relu[0][0]&#39;] ## ## conv5_block16_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block16_1_conv[0][0]&#39;] ## on) ## ## conv5_block16_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block16_1_bn[0][0]&#39;] ## ## conv5_block16_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block16_1_relu[0][0]&#39;] ## ## conv5_block16_concat (Concatenate) (None, 1, 1, 1408) 0 [&#39;conv5_block15_concat[0][0]&#39;, ## &#39;conv5_block16_2_conv[0][0]&#39;] ## ## conv5_block17_0_bn (BatchNormalizati (None, 1, 1, 1408) 5632 [&#39;conv5_block16_concat[0][0]&#39;] ## on) ## ## conv5_block17_0_relu (Activation) (None, 1, 1, 1408) 0 [&#39;conv5_block17_0_bn[0][0]&#39;] ## ## conv5_block17_1_conv (Conv2D) (None, 1, 1, 128) 180224 [&#39;conv5_block17_0_relu[0][0]&#39;] ## ## conv5_block17_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block17_1_conv[0][0]&#39;] ## on) ## ## conv5_block17_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block17_1_bn[0][0]&#39;] ## ## conv5_block17_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block17_1_relu[0][0]&#39;] ## ## conv5_block17_concat (Concatenate) (None, 1, 1, 1440) 0 [&#39;conv5_block16_concat[0][0]&#39;, ## &#39;conv5_block17_2_conv[0][0]&#39;] ## ## conv5_block18_0_bn (BatchNormalizati (None, 1, 1, 1440) 5760 [&#39;conv5_block17_concat[0][0]&#39;] ## on) ## ## conv5_block18_0_relu (Activation) (None, 1, 1, 1440) 0 [&#39;conv5_block18_0_bn[0][0]&#39;] ## ## conv5_block18_1_conv (Conv2D) (None, 1, 1, 128) 184320 [&#39;conv5_block18_0_relu[0][0]&#39;] ## ## conv5_block18_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block18_1_conv[0][0]&#39;] ## on) ## ## conv5_block18_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block18_1_bn[0][0]&#39;] ## ## conv5_block18_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block18_1_relu[0][0]&#39;] ## ## conv5_block18_concat (Concatenate) (None, 1, 1, 1472) 0 [&#39;conv5_block17_concat[0][0]&#39;, ## &#39;conv5_block18_2_conv[0][0]&#39;] ## ## conv5_block19_0_bn (BatchNormalizati (None, 1, 1, 1472) 5888 [&#39;conv5_block18_concat[0][0]&#39;] ## on) ## ## conv5_block19_0_relu (Activation) (None, 1, 1, 1472) 0 [&#39;conv5_block19_0_bn[0][0]&#39;] ## ## conv5_block19_1_conv (Conv2D) (None, 1, 1, 128) 188416 [&#39;conv5_block19_0_relu[0][0]&#39;] ## ## conv5_block19_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block19_1_conv[0][0]&#39;] ## on) ## ## conv5_block19_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block19_1_bn[0][0]&#39;] ## ## conv5_block19_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block19_1_relu[0][0]&#39;] ## ## conv5_block19_concat (Concatenate) (None, 1, 1, 1504) 0 [&#39;conv5_block18_concat[0][0]&#39;, ## &#39;conv5_block19_2_conv[0][0]&#39;] ## ## conv5_block20_0_bn (BatchNormalizati (None, 1, 1, 1504) 6016 [&#39;conv5_block19_concat[0][0]&#39;] ## on) ## ## conv5_block20_0_relu (Activation) (None, 1, 1, 1504) 0 [&#39;conv5_block20_0_bn[0][0]&#39;] ## ## conv5_block20_1_conv (Conv2D) (None, 1, 1, 128) 192512 [&#39;conv5_block20_0_relu[0][0]&#39;] ## ## conv5_block20_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block20_1_conv[0][0]&#39;] ## on) ## ## conv5_block20_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block20_1_bn[0][0]&#39;] ## ## conv5_block20_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block20_1_relu[0][0]&#39;] ## ## conv5_block20_concat (Concatenate) (None, 1, 1, 1536) 0 [&#39;conv5_block19_concat[0][0]&#39;, ## &#39;conv5_block20_2_conv[0][0]&#39;] ## ## conv5_block21_0_bn (BatchNormalizati (None, 1, 1, 1536) 6144 [&#39;conv5_block20_concat[0][0]&#39;] ## on) ## ## conv5_block21_0_relu (Activation) (None, 1, 1, 1536) 0 [&#39;conv5_block21_0_bn[0][0]&#39;] ## ## conv5_block21_1_conv (Conv2D) (None, 1, 1, 128) 196608 [&#39;conv5_block21_0_relu[0][0]&#39;] ## ## conv5_block21_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block21_1_conv[0][0]&#39;] ## on) ## ## conv5_block21_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block21_1_bn[0][0]&#39;] ## ## conv5_block21_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block21_1_relu[0][0]&#39;] ## ## conv5_block21_concat (Concatenate) (None, 1, 1, 1568) 0 [&#39;conv5_block20_concat[0][0]&#39;, ## &#39;conv5_block21_2_conv[0][0]&#39;] ## ## conv5_block22_0_bn (BatchNormalizati (None, 1, 1, 1568) 6272 [&#39;conv5_block21_concat[0][0]&#39;] ## on) ## ## conv5_block22_0_relu (Activation) (None, 1, 1, 1568) 0 [&#39;conv5_block22_0_bn[0][0]&#39;] ## ## conv5_block22_1_conv (Conv2D) (None, 1, 1, 128) 200704 [&#39;conv5_block22_0_relu[0][0]&#39;] ## ## conv5_block22_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block22_1_conv[0][0]&#39;] ## on) ## ## conv5_block22_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block22_1_bn[0][0]&#39;] ## ## conv5_block22_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block22_1_relu[0][0]&#39;] ## ## conv5_block22_concat (Concatenate) (None, 1, 1, 1600) 0 [&#39;conv5_block21_concat[0][0]&#39;, ## &#39;conv5_block22_2_conv[0][0]&#39;] ## ## conv5_block23_0_bn (BatchNormalizati (None, 1, 1, 1600) 6400 [&#39;conv5_block22_concat[0][0]&#39;] ## on) ## ## conv5_block23_0_relu (Activation) (None, 1, 1, 1600) 0 [&#39;conv5_block23_0_bn[0][0]&#39;] ## ## conv5_block23_1_conv (Conv2D) (None, 1, 1, 128) 204800 [&#39;conv5_block23_0_relu[0][0]&#39;] ## ## conv5_block23_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block23_1_conv[0][0]&#39;] ## on) ## ## conv5_block23_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block23_1_bn[0][0]&#39;] ## ## conv5_block23_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block23_1_relu[0][0]&#39;] ## ## conv5_block23_concat (Concatenate) (None, 1, 1, 1632) 0 [&#39;conv5_block22_concat[0][0]&#39;, ## &#39;conv5_block23_2_conv[0][0]&#39;] ## ## conv5_block24_0_bn (BatchNormalizati (None, 1, 1, 1632) 6528 [&#39;conv5_block23_concat[0][0]&#39;] ## on) ## ## conv5_block24_0_relu (Activation) (None, 1, 1, 1632) 0 [&#39;conv5_block24_0_bn[0][0]&#39;] ## ## conv5_block24_1_conv (Conv2D) (None, 1, 1, 128) 208896 [&#39;conv5_block24_0_relu[0][0]&#39;] ## ## conv5_block24_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block24_1_conv[0][0]&#39;] ## on) ## ## conv5_block24_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block24_1_bn[0][0]&#39;] ## ## conv5_block24_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block24_1_relu[0][0]&#39;] ## ## conv5_block24_concat (Concatenate) (None, 1, 1, 1664) 0 [&#39;conv5_block23_concat[0][0]&#39;, ## &#39;conv5_block24_2_conv[0][0]&#39;] ## ## conv5_block25_0_bn (BatchNormalizati (None, 1, 1, 1664) 6656 [&#39;conv5_block24_concat[0][0]&#39;] ## on) ## ## conv5_block25_0_relu (Activation) (None, 1, 1, 1664) 0 [&#39;conv5_block25_0_bn[0][0]&#39;] ## ## conv5_block25_1_conv (Conv2D) (None, 1, 1, 128) 212992 [&#39;conv5_block25_0_relu[0][0]&#39;] ## ## conv5_block25_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block25_1_conv[0][0]&#39;] ## on) ## ## conv5_block25_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block25_1_bn[0][0]&#39;] ## ## conv5_block25_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block25_1_relu[0][0]&#39;] ## ## conv5_block25_concat (Concatenate) (None, 1, 1, 1696) 0 [&#39;conv5_block24_concat[0][0]&#39;, ## &#39;conv5_block25_2_conv[0][0]&#39;] ## ## conv5_block26_0_bn (BatchNormalizati (None, 1, 1, 1696) 6784 [&#39;conv5_block25_concat[0][0]&#39;] ## on) ## ## conv5_block26_0_relu (Activation) (None, 1, 1, 1696) 0 [&#39;conv5_block26_0_bn[0][0]&#39;] ## ## conv5_block26_1_conv (Conv2D) (None, 1, 1, 128) 217088 [&#39;conv5_block26_0_relu[0][0]&#39;] ## ## conv5_block26_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block26_1_conv[0][0]&#39;] ## on) ## ## conv5_block26_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block26_1_bn[0][0]&#39;] ## ## conv5_block26_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block26_1_relu[0][0]&#39;] ## ## conv5_block26_concat (Concatenate) (None, 1, 1, 1728) 0 [&#39;conv5_block25_concat[0][0]&#39;, ## &#39;conv5_block26_2_conv[0][0]&#39;] ## ## conv5_block27_0_bn (BatchNormalizati (None, 1, 1, 1728) 6912 [&#39;conv5_block26_concat[0][0]&#39;] ## on) ## ## conv5_block27_0_relu (Activation) (None, 1, 1, 1728) 0 [&#39;conv5_block27_0_bn[0][0]&#39;] ## ## conv5_block27_1_conv (Conv2D) (None, 1, 1, 128) 221184 [&#39;conv5_block27_0_relu[0][0]&#39;] ## ## conv5_block27_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block27_1_conv[0][0]&#39;] ## on) ## ## conv5_block27_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block27_1_bn[0][0]&#39;] ## ## conv5_block27_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block27_1_relu[0][0]&#39;] ## ## conv5_block27_concat (Concatenate) (None, 1, 1, 1760) 0 [&#39;conv5_block26_concat[0][0]&#39;, ## &#39;conv5_block27_2_conv[0][0]&#39;] ## ## conv5_block28_0_bn (BatchNormalizati (None, 1, 1, 1760) 7040 [&#39;conv5_block27_concat[0][0]&#39;] ## on) ## ## conv5_block28_0_relu (Activation) (None, 1, 1, 1760) 0 [&#39;conv5_block28_0_bn[0][0]&#39;] ## ## conv5_block28_1_conv (Conv2D) (None, 1, 1, 128) 225280 [&#39;conv5_block28_0_relu[0][0]&#39;] ## ## conv5_block28_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block28_1_conv[0][0]&#39;] ## on) ## ## conv5_block28_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block28_1_bn[0][0]&#39;] ## ## conv5_block28_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block28_1_relu[0][0]&#39;] ## ## conv5_block28_concat (Concatenate) (None, 1, 1, 1792) 0 [&#39;conv5_block27_concat[0][0]&#39;, ## &#39;conv5_block28_2_conv[0][0]&#39;] ## ## conv5_block29_0_bn (BatchNormalizati (None, 1, 1, 1792) 7168 [&#39;conv5_block28_concat[0][0]&#39;] ## on) ## ## conv5_block29_0_relu (Activation) (None, 1, 1, 1792) 0 [&#39;conv5_block29_0_bn[0][0]&#39;] ## ## conv5_block29_1_conv (Conv2D) (None, 1, 1, 128) 229376 [&#39;conv5_block29_0_relu[0][0]&#39;] ## ## conv5_block29_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block29_1_conv[0][0]&#39;] ## on) ## ## conv5_block29_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block29_1_bn[0][0]&#39;] ## ## conv5_block29_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block29_1_relu[0][0]&#39;] ## ## conv5_block29_concat (Concatenate) (None, 1, 1, 1824) 0 [&#39;conv5_block28_concat[0][0]&#39;, ## &#39;conv5_block29_2_conv[0][0]&#39;] ## ## conv5_block30_0_bn (BatchNormalizati (None, 1, 1, 1824) 7296 [&#39;conv5_block29_concat[0][0]&#39;] ## on) ## ## conv5_block30_0_relu (Activation) (None, 1, 1, 1824) 0 [&#39;conv5_block30_0_bn[0][0]&#39;] ## ## conv5_block30_1_conv (Conv2D) (None, 1, 1, 128) 233472 [&#39;conv5_block30_0_relu[0][0]&#39;] ## ## conv5_block30_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block30_1_conv[0][0]&#39;] ## on) ## ## conv5_block30_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block30_1_bn[0][0]&#39;] ## ## conv5_block30_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block30_1_relu[0][0]&#39;] ## ## conv5_block30_concat (Concatenate) (None, 1, 1, 1856) 0 [&#39;conv5_block29_concat[0][0]&#39;, ## &#39;conv5_block30_2_conv[0][0]&#39;] ## ## conv5_block31_0_bn (BatchNormalizati (None, 1, 1, 1856) 7424 [&#39;conv5_block30_concat[0][0]&#39;] ## on) ## ## conv5_block31_0_relu (Activation) (None, 1, 1, 1856) 0 [&#39;conv5_block31_0_bn[0][0]&#39;] ## ## conv5_block31_1_conv (Conv2D) (None, 1, 1, 128) 237568 [&#39;conv5_block31_0_relu[0][0]&#39;] ## ## conv5_block31_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block31_1_conv[0][0]&#39;] ## on) ## ## conv5_block31_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block31_1_bn[0][0]&#39;] ## ## conv5_block31_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block31_1_relu[0][0]&#39;] ## ## conv5_block31_concat (Concatenate) (None, 1, 1, 1888) 0 [&#39;conv5_block30_concat[0][0]&#39;, ## &#39;conv5_block31_2_conv[0][0]&#39;] ## ## conv5_block32_0_bn (BatchNormalizati (None, 1, 1, 1888) 7552 [&#39;conv5_block31_concat[0][0]&#39;] ## on) ## ## conv5_block32_0_relu (Activation) (None, 1, 1, 1888) 0 [&#39;conv5_block32_0_bn[0][0]&#39;] ## ## conv5_block32_1_conv (Conv2D) (None, 1, 1, 128) 241664 [&#39;conv5_block32_0_relu[0][0]&#39;] ## ## conv5_block32_1_bn (BatchNormalizati (None, 1, 1, 128) 512 [&#39;conv5_block32_1_conv[0][0]&#39;] ## on) ## ## conv5_block32_1_relu (Activation) (None, 1, 1, 128) 0 [&#39;conv5_block32_1_bn[0][0]&#39;] ## ## conv5_block32_2_conv (Conv2D) (None, 1, 1, 32) 36864 [&#39;conv5_block32_1_relu[0][0]&#39;] ## ## conv5_block32_concat (Concatenate) (None, 1, 1, 1920) 0 [&#39;conv5_block31_concat[0][0]&#39;, ## &#39;conv5_block32_2_conv[0][0]&#39;] ## ## bn (BatchNormalization) (None, 1, 1, 1920) 7680 [&#39;conv5_block32_concat[0][0]&#39;] ## ## relu (Activation) (None, 1, 1, 1920) 0 [&#39;bn[0][0]&#39;] ## ## dense_153 (Dense) (None, 1, 1, 10) 19210 [&#39;relu[0][0]&#39;] ## ## flatten_1 (Flatten) (None, 10) 0 [&#39;dense_153[0][0]&#39;] ## ## ==================================================================================================================== ## Total params: 18,341,194 ## Trainable params: 0 ## Non-trainable params: 18,341,194 ## ____________________________________________________________________________________________________________________ And then the usual training: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. model %&gt;% keras::compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax()) model %&gt;% fit( x = train_x, y = train_y, epochs = 1L, batch_size = 32L, shuffle = TRUE, validation_split = 0.2 ) We have seen, that transfer learning can easily be done using Keras. Torch In Torch, we have to change the transform function (but only for the train dataloader): library(torchvision) library(torch) torch_manual_seed(321L) set.seed(123) train_ds = cifar10_dataset(&quot;.&quot;, download = TRUE, train = TRUE, transform = transform_to_tensor) test_ds = cifar10_dataset(&quot;.&quot;, download = TRUE, train = FALSE, transform = transform_to_tensor) train_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE) test_dl = dataloader(test_ds, batch_size = 100L) model_torch = model_resnet18(pretrained = TRUE) # We will set all model parameters to constant values: model_torch$parameters %&gt;% purrr::walk(function(param) param$requires_grad_(FALSE)) # Let&#39;s replace the last layer (last layer is named &#39;fc&#39;) with our own layer: inFeat = model_torch$fc$in_features model_torch$fc = nn_linear(inFeat, out_features = 10L) opt = optim_adam(params = model_torch$parameters, lr = 0.01) for(e in 1:1){ losses = c() coro::loop( for(batch in train_dl){ opt$zero_grad() pred = model_torch(batch[[1]]) loss = nnf_cross_entropy(pred, batch[[2]], reduction = &quot;mean&quot;) loss$backward() opt$step() losses = c(losses, loss$item()) } ) cat(sprintf(&quot;Loss at epoch %d: %3f\\n&quot;, e, mean(losses))) } model_torch$eval() test_losses = c() total = 0 correct = 0 coro::loop( for(batch in test_dl){ output = model_torch(batch[[1]]) labels = batch[[2]] loss = nnf_cross_entropy(output, labels) test_losses = c(test_losses, loss$item()) predicted = torch_max(output$data(), dim = 2)[[2]] total = total + labels$size(1) correct = correct + (predicted == labels)$sum()$item() } ) test_accuracy = correct/total print(test_accuracy) Flower data set Let’s do that with our flower data set: library(keras) library(tensorflow) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = EcoData::dataset_flower() train = data$train test = data$test labels = data$labels densenet = keras::application_densenet201(include_top = FALSE, input_shape = list(80L, 80L, 3L)) keras::freeze_weights(densenet) model = keras_model(inputs = densenet$input, outputs = densenet$output %&gt;% layer_flatten() %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 200L) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;)) # Data augmentation. aug = image_data_generator(rotation_range = 180, zoom_range = 0.4, width_shift_range = 0.2, height_shift_range = 0.2, vertical_flip = TRUE, horizontal_flip = TRUE, preprocessing_function = imagenet_preprocess_input) # Data preparation / splitting. indices = sample.int(nrow(train), 0.1 * nrow(train)) generator = flow_images_from_data(train[-indices,,,]/255, k_one_hot(labels[-indices], num_classes = 5L), batch_size = 25L, shuffle = TRUE, generator = aug) test = imagenet_preprocess_input(train[indices,,,]) test_labels = k_one_hot(labels[indices], num_classes = 5L) ## Training loop with early stopping: # As we use an iterator (the generator), validation loss is not applicable. # An available metric is the normal loss. early = keras::callback_early_stopping(patience = 2L, monitor = &quot;loss&quot;) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, optimizer = keras::optimizer_rmsprop(learning_rate = 0.0005)) model %&gt;% fit(generator, epochs = 8L, batch_size = 45L, shuffle = TRUE, callbacks = c(early)) pred = predict(model, imagenet_preprocess_input(data$test)) pred = apply(pred, 1, which.max) - 1 5.4.3 Influence of Batch Size and Learning Rate In this chapter, the influence of batch size and learning rate is explored using the MNIST data set. If you are more interested in this topic (you should be), read this article. 5.4.3.1 Batch Size Different batch sizes may massively influence the outcome of a training step. Finding a suitable batch size is a task itself. As a general rule of thumb: The lower the batch size, the longer the calculations take, the less (!) memory is needed and the more accurate the training (including less overfitting). The higher the batch size, the wider the training steps (like with a higher learning rate). Changing batch sizes and learning rates is always possible and this might “heal” previous mistakes. Maybe you have to “push” the system out of its local neighborhood. It also depends on the respective problem. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, kernel_size = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) model %&gt;% keras::compile( optimizer = keras::optimizer_adamax(learning_rate = 0.01), loss = loss_categorical_crossentropy ) epochs = 5L batch_size = 32L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) pred = model %&gt;% predict(test_x) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, test$y) # 0.9884 ## [1] 0.9873 Higher batch size: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, kernel_size = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) model %&gt;% keras::compile( optimizer = keras::optimizer_adamax(learning_rate = 0.01), loss = loss_categorical_crossentropy ) epochs = 5L batch_size = 100L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) pred = model %&gt;% predict(test_x) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, test$y) # 0.9864 ## [1] 0.9866 Lower batch size: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, kernel_size = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) model %&gt;% keras::compile( optimizer = keras::optimizer_adamax(learning_rate = 0.01), loss = loss_categorical_crossentropy ) epochs = 5L batch_size = 10L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) pred = model %&gt;% predict(test_x) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, test$y) # 0.9869 ## [1] 0.9875 Lowest (1) batch size: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, kernel_size = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) model %&gt;% keras::compile( optimizer = keras::optimizer_adamax(learning_rate = 0.01), loss = loss_categorical_crossentropy ) epochs = 5L batch_size = 1L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) pred = model %&gt;% predict(test_x) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, test$y) # 0.982 ## [1] 0.9834 Highest (complete) batch size: This might not run, because too much memory is needed. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, kernel_size = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) model %&gt;% keras::compile( optimizer = keras::optimizer_adamax(learning_rate = 0.01), loss = loss_categorical_crossentropy ) epochs = 5L batch_size = nrow(test_x) model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) pred = model %&gt;% predict(test_x) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, test$y) # ??? 5.4.3.2 Learning Rate Choosing a high learning rate at the beginning may yield acceptable results relatively fast. It would take much more time to get there with a small learning rate. But keeping the learning rate this high may result in jumping over the desired optimal values. So decreasing the learning rate with time might help. TensorFlow / Keras can manage a changing learning rate. This may also be a periodic function or an increase one. You are not limited to a decreasing learning rate. Defining own learning rates is a bit more complicated than just using the inbuilt Keras functions. If you need a self-made learning rate, you can find some recipe here. An example of the inbuilt functions for managing learning rates in Keras: The function declaration of the adamax optimizer is as follows: optimizer_adamax( learning_rate = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = NULL, decay = 0, clipnorm = NULL, clipvalue = NULL, ... ) # learning_rate: float &gt;= 0. Learning rate. # beta_1: The exponential decay rate for the 1st moment estimates. # float, 0 &lt; beta &lt; 1. Generally close to 1. # beta_2: The exponential decay rate for the 2nd moment estimates. # float, 0 &lt; beta &lt; 1. Generally close to 1. # epsilon: float &gt;= 0. Fuzz factor. If NULL, defaults to k_epsilon(). # decay: float &gt;= 0. Learning rate decay over each update. # clipnorm: Gradients will be clipped when their L2 norm exceeds this value. # clipvalue: Gradients will be clipped when their absolute value exceeds this value. You can easily specify a decay this way. Mind interval boundaries and suitable parameter values! library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = dataset_mnist() train = data$train test = data$test train_x = array(train$x/255, c(dim(train$x), 1)) test_x = array(test$x/255, c(dim(test$x), 1)) train_y = to_categorical(train$y, 10) model = keras_model_sequential() model %&gt;% layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L, kernel_size = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(100L, activation = &quot;relu&quot;) %&gt;% layer_dense(10L, activation = &quot;softmax&quot;) model %&gt;% keras::compile( optimizer = keras::optimizer_adamax(learning_rate = 0.02, decay = 0.002), loss = loss_categorical_crossentropy ) epochs = 5L batch_size = 32L model %&gt;% fit( x = train_x, y = train_y, epochs = epochs, batch_size = batch_size, shuffle = TRUE, validation_split = 0.2 ) pred = model %&gt;% predict(test_x) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, test$y) # 0.9882 ## [1] 0.9883 Except for the decay in learning rate, this example is identical to the first one concerning the batch size (0.9884). 5.4.3.3 Conclusion There is a (very) complex interplay of batch size, learning rate and optimization algorithm. This topic is to deep for this course. Most times, doing a longer training or increasing the batch size rather than decreasing the learning rate is recommended. But this is a matter of further research and also personal attitude. Of course, it also depends on the respective problem. 5.4.3.4 Caveat About Learning Rates and Activation Functions (already mentioned in the intro) Depending on activation functions, it might occur that the network won’t get updated, even with high learning rates (called vanishing gradient, especially for “sigmoid” functions). Furthermore, updates might overshoot (called exploding gradients) or activation functions will result in many zeros (especially for “relu,” dying relu). In general, the first layers of a network tend to learn (much) more slowly than subsequent ones. Task The next exercise is on the flower data set in the Ecodata package. Follow the steps, we did above and build your own convolutional neural network. In the end, submit your predictions to the submission server. If you have extra time, have a look at kaggle and find the flower data set challenge for specific architectures tailored for this data set. Solution The following code shows different behavior in the context of data augmentation and model complexity. The topic of overfitting can be seen cleary: Compare the simple model and its performance on the training and the test data. Then compare the more complex or even the regularized models and their performance on training and test data. You see that the very simple models tend to overfit. library(tensorflow) library(keras) flowerCNN = function( networkSize = c(&quot;small&quot;, &quot;medium&quot;, &quot;big&quot;)[3], useGenerator = c(1, 2, NA)[3], batch_size = 25L, epochs = 10L, learning_rate = 0.01, percentageTrain = 0.9 ){ gc(FALSE) # Clean up system (use garbage collection). set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. ############### # Prepare training and test sets: train = EcoData::dataset_flower()$train/255 indicesTrain = sample.int(nrow(train), percentageTrain * nrow(train)) test = train[-indicesTrain,,,] train = train[indicesTrain,,,] labelsTrain = EcoData::dataset_flower()$labels labelsTest = labelsTrain[-indicesTrain] labelsTrain = labelsTrain[indicesTrain] ############### # Models: model = keras_model_sequential() if(networkSize == &quot;small&quot;){ modelString = &quot;small model&quot; model %&gt;% layer_conv_2d(filters = 4L, kernel_size = 2L, input_shape = list(80L, 80L, 3L)) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) }else if(networkSize == &quot;medium&quot;){ modelString = &quot;medium model&quot; model %&gt;% layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L), input_shape = c(80L, 80L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L), strides = c(2L, 2L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) } else if(networkSize == &quot;big&quot;){ modelString = &quot;big model&quot; model %&gt;% layer_conv_2d(filter = 48L, kernel_size = c(5L, 5L), input_shape = c(80L, 80L, 3L), activation = &quot;leaky_relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L), activation = &quot;leaky_relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L), strides = c(2L, 2L), activation = &quot;leaky_relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dropout(0.35) %&gt;% layer_dense(units = 256L, activation = &quot;relu&quot;, bias_regularizer = regularizer_l2(.25) ) %&gt;% layer_dropout(0.4) %&gt;% layer_dense(units = 128L, activation = &quot;leaky_relu&quot;) %&gt;% layer_dropout(0.4) %&gt;% layer_dense(units = 64L, activation = &quot;leaky_relu&quot;) %&gt;% layer_dropout(0.4) %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) } ############### # Generators for augmentation: if(!is.na(useGenerator)){ if(useGenerator == 1){ generatorString = &quot;generator 1&quot; generator = keras::flow_images_from_data( x = train, y = k_one_hot(labelsTrain, num_classes = 5L), generator = keras::image_data_generator( rotation_range = 180, zoom_range = c(0.3), horizontal_flip = TRUE, vertical_flip = TRUE, samplewise_center = TRUE, samplewise_std_normalization = TRUE), batch_size = batch_size, shuffle = TRUE ) }else if(useGenerator == 2){ generatorString = &quot;generator 2&quot; generator = keras::flow_images_from_data( x = train, y = keras::k_one_hot(labelsTrain, 5L), batch_size = batch_size ) } }else{ generatorString = &quot;no generator&quot; } ############### # Model selection and compilation: model %&gt;% keras::compile(loss = loss_categorical_crossentropy, optimizer = keras::optimizer_adamax(learning_rate = learning_rate)) if(is.na(useGenerator)){ # Use no generator. model %&gt;% fit(x = train, y = to_categorical(matrix(labelsTrain, ncol = 1L), 5L), epochs = epochs, batch_size = batch_size, shuffle = TRUE) }else{ model %&gt;% fit(generator, epochs = epochs, batch_size = batch_size, shuffle = TRUE) } ############### # Predictions: cat(paste0(&quot;\\nModalities: &quot;, modelString, &quot;, &quot;, generatorString, &quot;\\n&quot;)) # Predictions on the training set: predTrain = predict(model, train) %&gt;% apply(1, which.max) - 1 cat(paste0(&quot;\\nAccuracy on training set: &quot;, round(Metrics::accuracy(predTrain, labelsTrain), 2), &quot;\\n&quot;)) print(round(table(predTrain) / nrow(train), 2)) # Predictions on the test set: predTest = predict(model, test) %&gt;% apply(1, which.max) - 1 cat(paste0(&quot;\\nAccuracy on test set: &quot;, round(Metrics::accuracy(predTest, labelsTest), 2), &quot;\\n&quot;)) print(round(table(predTest) / nrow(test), 2)) # Predictions on the holdout for submission: predHoldout = predict(model, EcoData::dataset_flower()$test/255) %&gt;% apply(1, which.max) - 1 return(predHoldout) } for(networkSize in c(&quot;small&quot;, &quot;medium&quot;, &quot;big&quot;)){ for(useGenerator in c(1, 2, NA)){ pred = flowerCNN(networkSize = networkSize, useGenerator = useGenerator) } } ## ## Modalities: small model, generator 1 ## ## Accuracy on training set: 0.31 ## predTrain ## 0 1 2 3 4 ## 0.65 0.11 0.19 0.00 0.05 ## ## Accuracy on test set: 0.32 ## predTest ## 0 1 2 3 4 ## 0.64 0.11 0.19 0.00 0.06 ## ## Modalities: small model, generator 2 ## ## Accuracy on training set: 0.89 ## predTrain ## 0 1 2 3 4 ## 0.16 0.26 0.16 0.18 0.24 ## ## Accuracy on test set: 0.45 ## predTest ## 0 1 2 3 4 ## 0.11 0.31 0.13 0.24 0.22 ## ## Modalities: small model, no generator ## ## Accuracy on training set: 0.91 ## predTrain ## 0 1 2 3 4 ## 0.16 0.27 0.17 0.17 0.23 ## ## Accuracy on test set: 0.46 ## predTest ## 0 1 2 3 4 ## 0.13 0.31 0.17 0.22 0.17 ## ## Modalities: medium model, generator 1 ## ## Accuracy on training set: 0.25 ## predTrain ## 1 2 ## 0.99 0.01 ## ## Accuracy on test set: 0.25 ## predTest ## 1 2 ## 0.99 0.01 ## ## Modalities: medium model, generator 2 ## ## Accuracy on training set: 0.7 ## predTrain ## 0 1 2 3 4 ## 0.16 0.34 0.09 0.20 0.20 ## ## Accuracy on test set: 0.67 ## predTest ## 0 1 2 3 4 ## 0.17 0.33 0.10 0.20 0.19 ## ## Modalities: medium model, no generator ## ## Accuracy on training set: 0.68 ## predTrain ## 0 1 2 3 4 ## 0.21 0.24 0.28 0.17 0.10 ## ## Accuracy on test set: 0.66 ## predTest ## 0 1 2 3 4 ## 0.20 0.23 0.30 0.17 0.10 ## ## Modalities: big model, generator 1 ## ## Accuracy on training set: 0.3 ## predTrain ## 1 2 4 ## 0.91 0.00 0.09 ## ## Accuracy on test set: 0.3 ## predTest ## 1 4 ## 0.89 0.11 ## ## Modalities: big model, generator 2 ## ## Accuracy on training set: 0.71 ## predTrain ## 0 1 2 3 4 ## 0.13 0.33 0.09 0.20 0.25 ## ## Accuracy on test set: 0.65 ## predTest ## 0 1 2 3 4 ## 0.12 0.35 0.08 0.20 0.25 ## ## Modalities: big model, no generator ## ## Accuracy on training set: 0.69 ## predTrain ## 0 1 2 3 4 ## 0.16 0.21 0.25 0.17 0.20 ## ## Accuracy on test set: 0.63 ## predTest ## 0 1 2 3 4 ## 0.15 0.23 0.24 0.15 0.23 Even more complex model: The following snippet offers a solution for data generation with oversampling and undersampling, because the distribution of classes is not equal in the flower data set. getData = function(oversample = TRUE, undersample = FALSE){ # &quot;undersample&quot; has priority over &quot;oversample&quot;. # As the whole task is very compute-intensive and needs much memory, # pack data acquisition in a function. # The used local memory is cleaned automatically at the end of the scope. data = EcoData::dataset_flower() # &lt;&lt;-: Global scope. trainLocal = data$train/255 labelsLocal = data$labels print(table(labelsLocal)) # The classes are not equally distributed. # Many models tend to predict class 1 overproportionally often. if(undersample){ n = min(table(labelsLocal)) # Minimal size of classes times number of classes. total = n * length(levels(as.factor(labelsLocal))) newIndices = rep(FALSE, total) for(i in 1:length(levels(as.factor(labelsLocal)))){ newIndices[sample(which(labelsLocal == i - 1), n, replace = FALSE)] = TRUE } newIndices = which(newIndices) trainingSet &lt;&lt;- trainLocal[newIndices,,,] flowerLabels &lt;&lt;- labelsLocal[newIndices] print(table(flowerLabels)) return() } if(!oversample){ trainingSet &lt;&lt;- trainLocal flowerLabels &lt;&lt;- labelsLocal return() } n = round(max(table(labelsLocal)) + 14) # Number of samples to extend each class to. ## Sample new data (with replacement): for(i in 1:length(levels(as.factor(labelsLocal)))){ missing = n - table(labelsLocal)[i] # Number of elements missing compared to n. indices = which(labelsLocal == i - 1) # Indices of all elements of class i. newIndices = sample(indices, missing, replace = TRUE) trainLocal &lt;&lt;- abind::abind(trainLocal, trainLocal[newIndices,,,], along = 1) # As only new indices are added, there is no confusion with using the old ones. labelsLocal = c(labelsLocal, rep(as.integer(i - 1), missing)) } trainingSet &lt;&lt;- trainLocal flowerLabels &lt;&lt;- labelsLocal print(table(flowerLabels)) } Read in for example the following way: if(!exists(&quot;done&quot;)){ # Do not calculate this more often than 1 time. trainingSet = c() flowerLabels = c() getData(oversample = FALSE, undersample = FALSE) done = 1 } Be careful, this exercise uses A LOT OF MEMORY!! If you have a SSD, you might want to turn pagefile / swap off. If you don’t have at least 8 GB RAM, don’t try to run this exercise, it won’t work. To avoid crashes of your system, you might want to do some memory management, like: After model training, unload the training set and load the test set. Generally remove data that is used no longer. Lazy load new images (not shown here) out of a generator (later shown in section 7, but with manually written training loop). library(keras) library(tensorflow) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. if(!exists(&quot;doneWithTest&quot;)){ # Do not calculate this more often than 1 time. trainingSet = c() flowerLabels = c() getData(oversample = FALSE, undersample = FALSE) doneWithTest = 1 } ## labelsLocal ## 0 1 2 3 4 ## 538 736 548 513 688 model = keras_model_sequential() model %&gt;% layer_conv_2d(filter = 48L, kernel_size = c(4L, 4L), input_shape = c(80L, 80L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L), activation = &quot;elu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 64L, kernel_size = c(2L, 2L), activation = &quot;gelu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dropout(0.33) %&gt;% layer_dense(units = 750L, activation = &quot;gelu&quot;, bias_regularizer = regularizer_l1(.75), kernel_regularizer = regularizer_l2(.0055) ) %&gt;% layer_dropout(0.4) %&gt;% layer_dense(units = 175L, activation = &quot;gelu&quot;) %&gt;% layer_dropout(0.35) %&gt;% layer_dense(units = 75L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 50L, activation = &quot;gelu&quot;, bias_regularizer = regularizer_l2(.75), kernel_regularizer = regularizer_l1(.0055) ) %&gt;% layer_dropout(0.3) %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax(learning_rate = 0.011)) model %&gt;% fit(x = trainingSet, y = to_categorical(matrix(flowerLabels, ncol = 1L), 5L), epochs = 50L, batch_size = 100L, shuffle = TRUE, validation_split = 0.2) pred = model %&gt;% predict(trainingSet) pred_classes = apply(pred, 1, which.max) print(table(pred_classes)) ## pred_classes ## 1 2 3 4 ## 531 854 955 683 Metrics::accuracy(pred_classes - 1L, flowerLabels) ## [1] 0.6218988 #pred_classes = model %&gt;% predict(EcoData::dataset_flower()$test/255) %&gt;% # apply(1, which.max) - 1L # Do not forget &quot;/255&quot; and &quot;- 1L&quot;!! #write.csv(pred_classes, file = &quot;flower_CNN.csv&quot;) As you can see, the network works in principle (76% accuracy for training data). Mind, that this is not a binary classification problem and we are expecting roughly 20% accuracy by chance. Just a little hint: More complex networks are not always better. This won’t be shown explicitly (as it is very computing-intensive). You can try for example 64 filter kernels per layer or copy one of the convolutional layers (including pooling layer) to see what happens. Now, we are training without holdouts to get the most power. library(keras) library(tensorflow) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. if(!exists(&quot;doneWithoutTest&quot;)){ # Do not calculate this more often than 1 time. trainingSet = c() flowerLabels = c() getData(oversample = FALSE, undersample = FALSE) doneWithoutTest = 1 } ## labelsLocal ## 0 1 2 3 4 ## 538 736 548 513 688 model = keras_model_sequential() model %&gt;% layer_conv_2d(filter = 48L, kernel_size = c(4L, 4L), input_shape = c(80L, 80L, 3L), activation = &quot;relu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L), activation = &quot;elu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_conv_2d(filter = 64L, kernel_size = c(2L, 2L), activation = &quot;gelu&quot;) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dropout(0.33) %&gt;% layer_dense(units = 750L, activation = &quot;gelu&quot;, bias_regularizer = regularizer_l1(.75), kernel_regularizer = regularizer_l2(.0055) ) %&gt;% layer_dropout(0.4) %&gt;% layer_dense(units = 175L, activation = &quot;gelu&quot;) %&gt;% layer_dropout(0.35) %&gt;% layer_dense(units = 75L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 50L, activation = &quot;gelu&quot;, bias_regularizer = regularizer_l2(.75), kernel_regularizer = regularizer_l1(.0055) ) %&gt;% layer_dropout(0.3) %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) model %&gt;% keras::compile(loss = loss_categorical_crossentropy, optimizer = optimizer_adamax(learning_rate = 0.011)) model %&gt;% fit(x = trainingSet, y = to_categorical(matrix(flowerLabels, ncol = 1L), 5L), epochs = 50L, batch_size = 100L, shuffle = TRUE) pred_classes = apply(model %&gt;% predict(trainingSet), 1, which.max) print(table(pred_classes)) ## pred_classes ## 1 2 3 4 5 ## 491 618 156 964 794 Metrics::accuracy(pred_classes - 1L, flowerLabels) ## [1] 0.621568 pred_classes = model %&gt;% predict(EcoData::dataset_flower()$test/255) %&gt;% apply(1, which.max) - 1L # Do not forget &quot;/255&quot; and &quot;- 1L&quot;!! write.csv(pred_classes, file = &quot;flower_CNN.csv&quot;) Maybe you can find (much?) better networks, that fit already better than 84% on the training data. Mind, that there are 5 classes. If your model predicts only 3 or 4 of them, is is not surprising, that the accuracy is low. References "],["interpretation.html", "6 Interpretation and Causality With Machine Learning 6.1 Explainable AI 6.2 Causal Inference and Machine Learning", " 6 Interpretation and Causality With Machine Learning 6.1 Explainable AI The goal of explainable AI (xAI, aka interpretable machine learning) is to explain why a fitted machine learning model makes certain predictions. A typical example is to understand how important different variables are for predictions. The incentives for doing so range from a better technical understanding of the models over understanding which data is important for improving predictions to questions of fairness and discrimination (e.g. to understand if an algorithm uses skin color to make a decision). 6.1.1 A Practical Example In this lecture we will work with another famous data set, the Boston housing data set: We will fit a random forest and use the iml package for xAI, see https://christophm.github.io/interpretable-ml-book/. library(iml) library(randomForest) set.seed(123) data(&quot;Boston&quot;, package = &quot;MASS&quot;) rf = randomForest(medv ~ ., data = Boston, ntree = 50) xAI packages are written generic, i.e. they can handle almost all machine learning models. When we want to use them, we first have to create a predictor object, that holds the model and the data. The iml package uses R6 classes, that means new objects can be created by calling Predictor$new(). (Do not worry if you do not know what R6 classes are, just use the command.) X = Boston[which(names(Boston) != &quot;medv&quot;)] predictor = Predictor$new(rf, data = X, y = Boston$medv) # &quot;Predictor&quot; is an object generator. 6.1.2 Feature Importance Feature importance should not be mistaken with the random forest variable importance though they are related. It tells us how important the individual variables are for predictions, can be calculated for all machine learning models and is based on a permutation approach (have a look at the book): imp = FeatureImp$new(predictor, loss = &quot;mae&quot;) plot(imp) 6.1.3 Partial Dependencies Partial dependencies are similar to allEffects plots for normal regressions. The idea is to visualize “marginal effects” of predictors (with the “feature” argument we specify the variable we want to visualize): eff = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;pdp&quot;, grid.size = 30) plot(eff) Partial dependencies can also be plotted for single observations: eff = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;pdp+ice&quot;, grid.size = 30) plot(eff) One disadvantage of partial dependencies is that they are sensitive to correlated predictors. Accumulated local effects can be used for accounting for correlation of predictors. 6.1.4 Accumulated Local Effects Accumulated local effects (ALE) are basically partial dependencies plots but try to correct for correlations between predictors. ale = FeatureEffect$new(predictor, feature = &quot;rm&quot;, method = &quot;ale&quot;) ale$plot() If there is no collinearity, you shouldn’t see much difference between partial dependencies and ALE plots. 6.1.5 Friedman’s H-statistic The H-statistic can be used to find interactions between predictors. However, again, keep in mind that the H-statistic is sensible to correlation between predictors: interact = Interaction$new(predictor, &quot;lstat&quot;) plot(interact) 6.1.6 Global Explainer - Simplifying the Machine Learning Model Another idea is simplifying the machine learning model with another simpler model such as a decision tree. We create predictions with the machine learning model for a lot of different input values and then we fit a decision tree on these predictions. We can then interpret the easier model. library(partykit) tree = TreeSurrogate$new(predictor, maxdepth = 2) plot(tree$tree) 6.1.7 Local Explainer - LIME Explaining Single Instances (observations) The global approach is to simplify the entire machine learning-black-box model via a simpler model, which is then interpretable. However, sometimes we are only interested in understanding how single predictions are generated. The LIME (Local interpretable model-agnostic explanations) approach explores the feature space around one observation and based on this locally fits a simpler model (e.g. a linear model): lime.explain = LocalModel$new(predictor, x.interest = X[1,]) lime.explain$results ## beta x.recoded effect x.original feature feature.value ## rm 4.1893817 6.575 27.545185 6.575 rm rm=6.575 ## ptratio -0.5307031 15.300 -8.119758 15.3 ptratio ptratio=15.3 ## lstat -0.4398104 4.980 -2.190256 4.98 lstat lstat=4.98 plot(lime.explain) 6.1.8 Local Explainer - Shapley The Shapley method computes the so called Shapley value, feature contributions for single predictions, and is based on an approach from cooperative game theory. The idea is that each feature value of the instance is a “player” in a game, where the prediction is the reward. The Shapley value tells us how to fairly distribute the reward among the features. shapley = Shapley$new(predictor, x.interest = X[1,]) shapley$plot() 6.2 Causal Inference and Machine Learning xAI aims at explaining how predictions are being made. In general, xAI != causality. xAI methods measure which variables are used for predictions by the algorithm, or how far variables improve predictions. The important point to note here: If a variable causes something, we could also expect that it helps predicting the very thing. The opposite, however, is not generally true - very often it is possible that a variable that doesn’t cause anything can predict something. In statistics courses (in particular our course: Advanced Biostatistics), we discuss the issue of causality at full length. Here, we don’t want to go into the details, but again, you should in general resist to interpret indicators of importance in xAI as causal effects. They tell you something about what’s going on in the algorithm, not about what’s going on in reality. 6.2.1 Causal Inference on Static Data Methods for causal inference depend on whether we have dynamic or static data. The latter is the more common case. With static data, the problem is confounding. If you have several correlated predictors, you can get spurious correlations between a given predictor and the response, although there is no causal effect in general. Multiple regression and few other methods are able to correct for other predictors and thus isolate the causal effect. The same is not necessarily true for machine learning algorithms and xAI methods. This is not a bug, but a feature - for making good predictions, it is often no problem, but rather an advantage to also use non-causal predictors. Here an example for the indicators of variable importance in the random forest algorithm. The purpose of this script is to show that random forest variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are separable and would be correctly separated by an lm / ANOVA. We first simulate a data set with 2 predictors that are strongly correlated, but only one of them has an effect on the response. set.seed(123) # Simulation parameters. n = 1000 col = 0.7 # Create collinear predictors. x1 = runif(n) x2 = col * x1 + (1-col) * runif(n) # Response is only influenced by x1. y = x1 + rnorm(n) lm / anova correctly identify \\(x1\\) as causal variable. summary(lm(y ~ x1 + x2)) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0709 -0.6939 0.0102 0.6976 3.3373 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02837 0.08705 0.326 0.744536 ## x1 1.07383 0.27819 3.860 0.000121 *** ## x2 -0.04547 0.37370 -0.122 0.903186 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.011 on 997 degrees of freedom ## Multiple R-squared: 0.08104, Adjusted R-squared: 0.0792 ## F-statistic: 43.96 on 2 and 997 DF, p-value: &lt; 2.2e-16 Fit random forest and show variable importance: set.seed(123) fit = randomForest(y ~ x1 + x2, importance = TRUE) varImpPlot(fit) Variable importance is now split nearly evenly. Task: understand why this is - remember: How the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built. Remember that as \\(x1 \\propto x2\\), we can use \\(x2\\) as a replacement for \\(x1\\). Remember that the variable importance measures the average contributions of the different variables in the trees of the forest. 6.2.2 Structural Equation Models If causal relationships get more complicated, it will not be possible to adjust correctly with a simple lm. In this case, in statistics, we will usually use structural equation models (SEMs). Structural equation models are designed to estimate entire causal diagrams. There are two main SEM packages in R: For anything that is non-normal, you will currently have to estimate the directed acyclic graph (that depicts causal relations) piece-wise with CRAN package piecewiseSEM. Example for a vegetation data set: library(piecewiseSEM) mod = psem( lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, data = keeley), lm(firesev ~ elev + age + cover, data = keeley), lm(cover ~ age + elev + hetero + abiotic, data = keeley) ) summary(mod) plot(mod) For linear structural equation models, we can estimate the entire directed acyclic graph at once. This also allows having unobserved variables in the directed acyclic graph. One of the most popular packages for this is lavaan. library(lavaan) mod = &quot; rich ~ distance + elev + abiotic + age + hetero + firesev + cover firesev ~ elev + age + cover cover ~ age + elev + abiotic &quot; fit = sem(mod, data = keeley) summary(fit) ## lavaan 0.6-9 ended normally after 77 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 10.437 ## Degrees of freedom 5 ## P-value (Chi-square) 0.064 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## rich ~ ## distance 0.616 0.177 3.485 0.000 ## elev -0.009 0.006 -1.644 0.100 ## abiotic 0.488 0.156 3.134 0.002 ## age 0.024 0.105 0.229 0.819 ## hetero 44.414 9.831 4.517 0.000 ## firesev -1.018 0.759 -1.341 0.180 ## cover 12.400 3.841 3.228 0.001 ## firesev ~ ## elev -0.001 0.001 -0.951 0.342 ## age 0.047 0.013 3.757 0.000 ## cover -1.521 0.509 -2.991 0.003 ## cover ~ ## age -0.009 0.002 -3.875 0.000 ## elev 0.000 0.000 2.520 0.012 ## abiotic -0.000 0.004 -0.115 0.909 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .rich 97.844 14.586 6.708 0.000 ## .firesev 1.887 0.281 6.708 0.000 ## .cover 0.081 0.012 6.708 0.000 The default plot options are not so nice as before. library(lavaanPlot) lavaanPlot(model = fit) Another plotting option is using semPlot. library(semPlot) semPaths(fit) 6.2.3 Automatic Causal Discovery But how to get the causal graph? In statistics, it is common to “guess” it and afterwards do residual checks, in the same way as we guess the structure of a regression. For more complicated problems, however, this is unsatisfying. Some groups therefore work on so-called causal discovery algorithms, i.e. algorithms that automatically generate causal graphs from data. One of the most classic algorithms of this sort is the PC algorithm. Here an example using the pcalg package: library(pcalg) Loading the data: data(&quot;gmG&quot;, package = &quot;pcalg&quot;) # Loads data sets gmG and gmG8. suffStat = list(C = cor(gmG8$x), n = nrow(gmG8$x)) varNames = gmG8$g@nodes First, the skeleton algorithm creates a basic graph without connections (a skeleton of the graph). skel.gmG8 = skeleton(suffStat, indepTest = gaussCItest, labels = varNames, alpha = 0.01) Rgraphviz::plot(skel.gmG8) What is missing here is the direction of the errors. The PC algorithm now makes tests for conditional independence, which allows fixing a part (but typically not all) of the directions of the causal arrows. pc.gmG8 = pc(suffStat, indepTest = gaussCItest, labels = varNames, alpha = 0.01) Rgraphviz::plot(pc.gmG8 ) 6.2.4 Causal Inference on Dynamic Data When working with dynamic data, we can use an additional piece of information - the cause usually precedes the effect, which means that we can test for a time-lag between cause and effect to determine the direction of causality. This way of testing for causality is known as Granger causality, or Granger methods. Here an example: library(lmtest) ## What came first: the chicken or the egg? data(ChickEgg) grangertest(egg ~ chicken, order = 3, data = ChickEgg) ## Granger causality test ## ## Model 1: egg ~ Lags(egg, 1:3) + Lags(chicken, 1:3) ## Model 2: egg ~ Lags(egg, 1:3) ## Res.Df Df F Pr(&gt;F) ## 1 44 ## 2 47 -3 0.5916 0.6238 grangertest(chicken ~ egg, order = 3, data = ChickEgg) ## Granger causality test ## ## Model 1: chicken ~ Lags(chicken, 1:3) + Lags(egg, 1:3) ## Model 2: chicken ~ Lags(chicken, 1:3) ## Res.Df Df F Pr(&gt;F) ## 1 44 ## 2 47 -3 5.405 0.002966 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.2.5 Outlook for Machine Learning As we have seen, there are already a few methods / algorithms for discovering causality from large data sets, but the systematic transfer of these concepts to machine learning, in particular deep learning, is still at its infancy. At the moment, this field is actively researched and changes extremely fast, so we recommend using Google to see what is currently going on. Particular in business and industry, there is a large interest in learning about causal effect from large data sets. In our opinion, a great topic for young scientists to specialize on. Task Use one of the non-image based data sets (preferably Wine, which is also described in the data sets section 8 but wasn’t used yet, but you can also use Nasa or Titanic) and fit a random forest. Explore / interpret the fitted model using iml (see also the book: https://christophm.github.io/interpretable-ml-book/). Solution library(randomForest) library(&quot;iml&quot;) set.seed(1234) data = as.data.frame(EcoData::wine) submission = data[which(is.na(data$quality)), -which(colnames(data) == &quot;quality&quot;)] data = data[complete.cases(data), ] # Removes sumbmission data as well. # Remark: Features don&#39;t need to be scaled for regression trees. rf = randomForest(quality ~ ., data = data) pred = round(predict(rf, data)) table(pred, data$quality) ## ## pred 3 4 5 6 7 8 ## 4 2 4 0 0 0 0 ## 5 0 6 133 0 0 0 ## 6 0 0 3 113 10 0 ## 7 0 0 0 0 25 3 (accuracy = mean(pred == data$quality)) # Fits pretty well (on the training data...) ## [1] 0.9197324 # For submission: #write.csv(round(predict(rf, submission)), file = &quot;wine_RF.csv&quot;) # Standard depiction of importance: rf$importance ## IncNodePurity ## fixed.acidity 12.279077 ## volatile.acidity 21.997368 ## citric.acid 13.751476 ## residual.sugar 10.787817 ## chlorides 11.587301 ## free.sulfur.dioxide 9.966824 ## total.sulfur.dioxide 13.977669 ## density 17.068801 ## pH 10.302109 ## sulphates 20.273156 ## alcohol 37.316264 # IML: predictor = Predictor$new( rf, data = data[,which(names(data) != &quot;quality&quot;)], y = data$quality) # Mind: This is stochastical! importance = FeatureImp$new(predictor, loss = &quot;mae&quot;) plot(importance) # Comparison between standard importance and IML importance: importanceRf = rownames(rf$importance)[order(rf$importance, decreasing = TRUE)] importanceIML = importance$results[1] comparison = cbind(importanceIML, importanceRf) colnames(comparison) = c(&quot;IML&quot;, &quot;RF&quot;) as.matrix(comparison) ## IML RF ## [1,] &quot;alcohol&quot; &quot;alcohol&quot; ## [2,] &quot;sulphates&quot; &quot;volatile.acidity&quot; ## [3,] &quot;volatile.acidity&quot; &quot;sulphates&quot; ## [4,] &quot;density&quot; &quot;density&quot; ## [5,] &quot;citric.acid&quot; &quot;total.sulfur.dioxide&quot; ## [6,] &quot;total.sulfur.dioxide&quot; &quot;citric.acid&quot; ## [7,] &quot;fixed.acidity&quot; &quot;fixed.acidity&quot; ## [8,] &quot;chlorides&quot; &quot;chlorides&quot; ## [9,] &quot;pH&quot; &quot;residual.sugar&quot; ## [10,] &quot;free.sulfur.dioxide&quot; &quot;pH&quot; ## [11,] &quot;residual.sugar&quot; &quot;free.sulfur.dioxide&quot; Mind that feature importance, and the random forest’s variable importance are related but not equal! Variable importance is a measure for determining importance while creating the forest (i.e. for fitting). Feature importance is a measure for how important a variable is for prediction. Maybe you want to see other explanation methods as well. Surely you can use the other techniques of this section on your own. Task As we show in section 6.2.1 of this chapter, a random forest will split variable importance across collinear predictors, while a linear regression model (lm()) can identify which predictor is causally affecting the response (at least in theory, if all confounders are controlled). What about a boosted regression tree or an artificial neural network? Take the random forest example and add a boosted regression tree (easier, you can use for example https://rdrr.io/cran/xgboost/man/xgb.importance.html) or an artificial neural network, and have a look if those are better than the random forest at identifying causal predictors. Solution library(xgboost) set.seed(1234) data = as.data.frame(EcoData::wine) submission = data[which(is.na(data$quality)), -which(colnames(data) == &quot;quality&quot;)] data = data[complete.cases(data), ] # Removes sumbmission data as well. data_xg = xgb.DMatrix( data = as.matrix(data[,which(names(data) != &quot;quality&quot;)]), label = data$quality ) brt = xgboost(data_xg, nrounds = 24) ## [1] train-rmse:3.656523 ## [2] train-rmse:2.609494 ## [3] train-rmse:1.884807 ## [4] train-rmse:1.384918 ## [5] train-rmse:1.037362 ## [6] train-rmse:0.800110 ## [7] train-rmse:0.629324 ## [8] train-rmse:0.508917 ## [9] train-rmse:0.426155 ## [10] train-rmse:0.369580 ## [11] train-rmse:0.313017 ## [12] train-rmse:0.274227 ## [13] train-rmse:0.236959 ## [14] train-rmse:0.207364 ## [15] train-rmse:0.195811 ## [16] train-rmse:0.182500 ## [17] train-rmse:0.173310 ## [18] train-rmse:0.154747 ## [19] train-rmse:0.144045 ## [20] train-rmse:0.139083 ## [21] train-rmse:0.129605 ## [22] train-rmse:0.118541 ## [23] train-rmse:0.110689 ## [24] train-rmse:0.097798 pred = round(predict(brt, newdata = data_xg)) # Only integers are allowed. table(pred, data$quality) ## ## pred 3 4 5 6 7 8 ## 3 2 0 0 0 0 0 ## 4 0 10 0 0 0 0 ## 5 0 0 136 0 0 0 ## 6 0 0 0 113 1 0 ## 7 0 0 0 0 34 0 ## 8 0 0 0 0 0 3 (accuracy = mean(pred == data$quality)) # Fits very well (on the training data...) ## [1] 0.9966555 # For submission: #write.csv(round(predict(rf, submission)), file = &quot;wine_RF.csv&quot;) # Look at variable importance: xgboost::xgb.importance(model = brt) ## Feature Gain Cover Frequency ## 1: alcohol 0.28363726 0.13667721 0.07073509 ## 2: sulphates 0.11331763 0.07015405 0.06657420 ## 3: fixed.acidity 0.09844523 0.11359510 0.19278779 ## 4: volatile.acidity 0.09582794 0.07098397 0.12760055 ## 5: total.sulfur.dioxide 0.09207986 0.09147259 0.07212205 ## 6: density 0.07374576 0.14910006 0.08321775 ## 7: chlorides 0.06025521 0.07972405 0.08876560 ## 8: residual.sugar 0.05307944 0.07202137 0.08044383 ## 9: free.sulfur.dioxide 0.04602742 0.04743503 0.06518724 ## 10: pH 0.04477577 0.12562892 0.07489598 ## 11: citric.acid 0.03880847 0.04320764 0.07766990 Every method yields slightly different results, but the main ingredient is alcohol (and sulphates). Task If you’re done with the previous tasks and have still time and appetite, improve the submissions for our competition, in particular for the Wine data set. Possible ideas: Use MLR framework (section 4.6). Try Transfer learning (section 5.4.2). This was the winner of last years challenge. Search on kaggle for more ideas / try to copy the ideas. This was the winner two years ago. A little example for the (unbalanced!) Wine data set Solution library(tensorflow) library(keras) set_random_seed(123L, disable_gpu = FALSE) # Already sets R&#39;s random seed. readin = function(percentageTest = 0.2, aggregate = 0){ # Parameter &quot;aggregate&quot; packs the classes with very low abundances into one. # If &quot;aggregate&quot; equals to NA, NaN, Null, 0 or FALSE, no aggregation is performed. # Else, the given number is the boundary. # Every class with less elements than the boundary is aggregated into one. # WARNING: These classes cannot be distinguished from then on! # Using the predictions for submission needs further processing! # Just for random selection of features, independent of the amount of function calls. set.seed(12345) train = as.data.frame(EcoData::wine) indicesTrain = which(!is.na(train$quality)) labelsTrain = train$quality[indicesTrain] labelsTrain = labelsTrain - min(labelsTrain) # Start at 0 (for softmax). train = train[, -which(colnames(train) == &quot;quality&quot;)] if(!is.na(aggregate) &amp; aggregate){ indices = names(table(labelsTrain)[ table(labelsTrain) &lt; aggregate &amp; table(labelsTrain) &gt; 0 ]) if(length(indices)){ labelsTrain[labelsTrain %in% indices] = -1 labelsTrain = as.factor(labelsTrain) levels(labelsTrain) = 1:length(levels(labelsTrain)) - 1 labelsTrain = as.integer(labelsTrain) } } # Impute missing values (before any splitting, to get the highest power): train = missRanger::missRanger( data = train, maxiter = 10L, seed = 123, num.trees = 200L ) # Separate submission data (mind scaling!): submission = scale(train[-indicesTrain,]) train = scale(train[indicesTrain,]) # Very asymmetric training data: cat(paste0(&quot;Size of training set: &quot;, length(labelsTrain), &quot;\\n&quot;)) print(table(labelsTrain)) if(percentageTest == 0){ return(list( &quot;labelsTrain&quot; = labelsTrain, &quot;labelsTest&quot; = list(), &quot;train&quot; = train, &quot;test&quot; = list(), &quot;submission&quot; = submission )) } # Split into training and test set: len = nrow(train) indicesTest = sample(x = 1:len, size = percentageTest * len, replace = FALSE) test = as.data.frame(train[indicesTest,]) labelsTest = labelsTrain[indicesTest] train = as.data.frame(train[-indicesTest,]) labelsTrain = labelsTrain[-indicesTest] return(list( &quot;labelsTrain&quot; = labelsTrain, &quot;labelsTest&quot; = labelsTest, &quot;train&quot; = train, &quot;test&quot; = test, &quot;submission&quot; = submission )) } retVal = readin(aggregate = 0) ## ## Missing value imputation by random forests ## ## Variables to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates ## Variables used to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol ## iter 1: .......... ## iter 2: .......... ## iter 3: .......... ## Size of training set: 694 ## labelsTrain ## 0 1 2 3 4 5 ## 7 25 309 261 84 8 labelsTrain = retVal[[&quot;labelsTrain&quot;]] labelsTest = retVal[[&quot;labelsTest&quot;]] train = retVal[[&quot;train&quot;]] test = retVal[[&quot;test&quot;]] submission = retVal[[&quot;submission&quot;]] rm(retVal) classNumber = length(table(labelsTrain)) model = keras_model_sequential() model %&gt;% layer_dense(units = 200L, activation = &quot;leaky_relu&quot;, kernel_regularizer = regularizer_l2(0.00035), input_shape = ncol(train)) %&gt;% layer_dropout(0.45) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, bias_regularizer = regularizer_l1_l2(0.5)) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 100L, activation = &quot;leaky_relu&quot;, kernel_regularizer = regularizer_l2(0.00035), bias_regularizer = regularizer_l1_l2(0.1)) %&gt;% layer_dropout(0.25) %&gt;% layer_dense(units = 50L, activation = &quot;gelu&quot;) %&gt;% layer_dense(units = 25L, activation = &quot;elu&quot;) %&gt;% layer_dropout(0.35) %&gt;% # We need probabilities. So we use the softmax function. # Remember, the labels MUST start at 0! layer_dense(units = classNumber, activation = &quot;softmax&quot;) model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(learning_rate = 0.015)) model_history = model %&gt;% # Mind the matrix property (no data.frame)! fit(x = as.matrix(train), y = k_one_hot(labelsTrain, classNumber), epochs = 80L, batch = 12L, shuffle = TRUE) plot(model_history) # Accuracy on training set (!) pred = predict(model, as.matrix(train)) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, labelsTrain) ## [1] 0.7446043 table(pred, labelsTrain) ## labelsTrain ## pred 0 1 2 3 4 5 ## 0 2 1 0 0 0 0 ## 1 3 8 0 2 0 0 ## 2 2 10 217 53 5 0 ## 3 0 4 20 159 31 4 ## 4 0 0 1 3 28 3 # Accuracy on test set pred = predict(model, as.matrix(test)) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, labelsTest) ## [1] 0.6304348 table(pred, labelsTest) ## labelsTest ## pred 1 2 3 4 5 ## 1 0 4 2 0 0 ## 2 2 55 17 3 0 ## 3 0 11 25 10 1 ## 4 0 1 0 7 0 Recognize overfitting of your model selection strategy by changing the seed few times (while keeping the model constant) and increase the percentage of test data. Furthermore, consider fitting a random forest for good quality as well. For the final predictions, we use the whole data set without holdouts: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. retVal = readin(percentageTest = 0, aggregate = 0) ## ## Missing value imputation by random forests ## ## Variables to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates ## Variables used to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol ## iter 1: .......... ## iter 2: .......... ## iter 3: .......... ## Size of training set: 694 ## labelsTrain ## 0 1 2 3 4 5 ## 7 25 309 261 84 8 labelsTrain = retVal[[&quot;labelsTrain&quot;]] labelsTest = retVal[[&quot;labelsTest&quot;]] train = retVal[[&quot;train&quot;]] test = retVal[[&quot;test&quot;]] submission = retVal[[&quot;submission&quot;]] rm(retVal) classNumber = length(table(labelsTrain)) model = keras_model_sequential() model %&gt;% layer_dense(units = 200L, activation = &quot;leaky_relu&quot;, kernel_regularizer = regularizer_l2(0.00035), input_shape = ncol(train)) %&gt;% layer_dropout(0.45) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;, bias_regularizer = regularizer_l1_l2(0.5)) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(units = 100L, activation = &quot;leaky_relu&quot;, kernel_regularizer = regularizer_l2(0.00035), bias_regularizer = regularizer_l1_l2(0.1)) %&gt;% layer_dropout(0.25) %&gt;% layer_dense(units = 50L, activation = &quot;gelu&quot;) %&gt;% layer_dense(units = 25L, activation = &quot;elu&quot;) %&gt;% layer_dropout(0.35) %&gt;% # We need probabilities. So we use the softmax function. # Remember, the labels MUST start at 0! layer_dense(units = classNumber, activation = &quot;softmax&quot;) model %&gt;% keras::compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(learning_rate = 0.015)) model_history = model %&gt;% # Mind the matrix property (no data.frame)! fit(x = as.matrix(train), y = k_one_hot(labelsTrain, classNumber), epochs = 80L, batch = 12L, shuffle = TRUE) plot(model_history) # Accuracy on training set (!) pred = predict(model, as.matrix(train)) %&gt;% apply(1, which.max) - 1 Metrics::accuracy(pred, labelsTrain) ## [1] 0.7853026 table(pred, labelsTrain) ## labelsTrain ## pred 0 1 2 3 4 5 ## 0 1 0 0 0 0 0 ## 1 0 5 0 0 0 0 ## 2 6 15 267 33 4 0 ## 3 0 5 35 211 19 1 ## 4 0 0 7 17 61 7 # Reverse subtraction (for start at 0) and create submission file. write.csv(pred + min(as.data.frame(EcoData::wine)$quality, na.rm = TRUE), file = &quot;wine_NN.csv&quot;) "],["gan.html", "7 Generative Modeling and Reinforcement Learning 7.1 Autoencoder 7.2 Generative Adversarial Networks (GANs) 7.3 Reinforcement learning", " 7 Generative Modeling and Reinforcement Learning We will explore more machine learning ideas today. 7.1 Autoencoder An autoencoder (AE) is a type of artificial neural network for unsupervised learning. The idea is similar to data compression: The first part of the network compresses (encodes) the data to a low dimensional space (e.g. 2-4 dimensions) and the second part of the network decompresses (reverses the encoding) and learns to reconstruct the data (think of a hourglass). Why is this useful? The method is similar to a dimension reduction technique (e.g. PCA) but with the advantage that we don’t have to make any distributional assumptions (but see PCA). For instance, we could first train an autoencoder on genomic expression data with thousands of features, compress them into 2-4 dimensions, and then use them for clustering. 7.1.1 Autoencoder - Deep Neural Network MNIST We now will write an autoencoder for the MNIST data set. Let’s start with the (usual) MNIST example: library(keras) library(tensorflow) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = keras::dataset_mnist() We don’t need the labels here, our images will be the inputs and at the same time the outputs of our final autoencoder. rotate = function(x){ t(apply(x, 2, rev)) } imgPlot = function(img, title = &quot;&quot;){ col = grey.colors(255) if(title != &quot;&quot;){ main = paste0(&quot;Label: &quot;, as.character(title)) } else{ main = &quot;&quot; } image(rotate(img), col = col, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, main = main) } train = data[[1]] test = data[[2]] train_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L)) test_x = array(test[[1]]/255, c(dim(test[[1]])[1], 784L)) Our encoder: image (784 dimensions) \\(\\rightarrow\\) 2 dimensions down_size_model = keras_model_sequential() down_size_model %&gt;% layer_dense(units = 100L, input_shape = c(784L), activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 2L, activation = &quot;linear&quot;) Our decoder: 2 dimensions \\(\\rightarrow\\) 784 dimensions (our image) up_size_model = keras_model_sequential() up_size_model %&gt;% layer_dense(units = 20L, input_shape = c(2L), activation = &quot;relu&quot;) %&gt;% layer_dense(units = 100L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 784L, activation = &quot;sigmoid&quot;) We can use the non-sequential model type to connect the two models. (We did the same in the transfer learning chapter.) autoencoder = keras_model(inputs = down_size_model$input, outputs = up_size_model(down_size_model$output)) autoencoder$compile(loss = loss_binary_crossentropy, optimizer = optimizer_adamax(0.01)) summary(autoencoder) ## Model: &quot;model_2&quot; ## ________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================================================================== ## dense_212_input (InputLayer) [(None, 784)] 0 ## ## dense_212 (Dense) (None, 100) 78500 ## ## dense_211 (Dense) (None, 20) 2020 ## ## dense_210 (Dense) (None, 2) 42 ## ## sequential_62 (Sequential) (None, 784) 81344 ## ## ======================================================================================================== ## Total params: 161,906 ## Trainable params: 161,906 ## Non-trainable params: 0 ## ________________________________________________________________________________________________________ We will now show an example of an image before and after the unfitted autoencoder, so we see that we have to train the autoencoder. image = autoencoder(train_x[1,,drop = FALSE]) oldpar = par(mfrow = c(1, 2)) imgPlot(array(train_x[1,,drop = FALSE], c(28, 28)), title = &quot;Before&quot;) imgPlot(array(image$numpy(), c(28, 28)), title = &quot;After&quot;) par(oldpar) Fit the autoencoder (inputs == outputs!): library(tensorflow) library(keras) set_random_seed(123L, disable_gpu = FALSE) # Already sets R&#39;s random seed. autoencoder %&gt;% fit(x = train_x, y = train_x, epochs = 5L, batch_size = 128L) Visualization of the latent variables: pred_dim = down_size_model(test_x) reconstr_pred = up_size_model(pred_dim) imgPlot(array(reconstr_pred[10,]$numpy(), dim = c(28L, 28L)), title = &quot;&quot;) ownColors = c(&quot;limegreen&quot;, &quot;purple&quot;, &quot;yellow&quot;, &quot;grey&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;red&quot;, &quot;navy&quot;, &quot;sienna&quot;, &quot;springgreen&quot;) oldpar = par(mfrow = c(1, 1)) plot(pred_dim$numpy()[,1], pred_dim$numpy()[,2], col = ownColors[test[[2]]+1L]) par(oldpar) The picture above shows the 2-dimensional encoded values of the numbers in the MNIST data set and the number they are depicting via the respective color. 7.1.2 Autoencoder - MNIST Convolutional Neural Networks We can also use convolutional neural networks instead or on the side of deep neural networks. Moreover, there is an inverse convolutional layer (layer_conv_2d_transpose; “deconvolution”): Prepare data: data = tf$keras$datasets$mnist$load_data() train = data[[1]] train_x = array(train[[1]]/255, c(dim(train[[1]]), 1L)) test_x = array(data[[2]][[1]]/255, c(dim(data[[2]][[1]]/255), 1L)) Then define the downsize model: down_size_model = keras_model_sequential() down_size_model %&gt;% layer_conv_2d(filters = 32L, activation = &quot;relu&quot;, kernel_size = c(2L, 2L), input_shape = c(28L, 28L, 1L), strides = c(4L, 4L)) %&gt;% layer_conv_2d(filters = 16L, activation = &quot;relu&quot;, kernel_size = c(7L, 7L), strides = c(1L, 1L)) %&gt;% layer_flatten() %&gt;% layer_dense(units = 2L, activation = &quot;linear&quot;) Define the upsize model: up_size_model = keras_model_sequential() up_size_model %&gt;% layer_dense(units = 8L, activation = &quot;relu&quot;, input_shape = c(2L)) %&gt;% layer_reshape(target_shape = c(1L, 1L, 8L)) %&gt;% layer_conv_2d_transpose(filters = 16L, kernel_size = c(7, 7), activation = &quot;relu&quot;, strides = c(1L, 1L)) %&gt;% layer_conv_2d_transpose(filters = 32L, activation = &quot;relu&quot;, kernel_size = c(2, 2), strides = c(4L, 4L)) %&gt;% layer_conv_2d(filters = 1, kernel_size = c(1L, 1L), strides = c(1L, 1L), activation = &quot;sigmoid&quot;) Combine the two models and fit it: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. autoencoder = tf$keras$models$Model(inputs = down_size_model$input, outputs = up_size_model(down_size_model$output)) autoencoder$compile(loss = loss_binary_crossentropy, optimizer = optimizer_rmsprop(0.001)) autoencoder$fit(x = tf$constant(train_x), y = tf$constant(train_x), epochs = 10L, batch_size = 128L) ## &lt;keras.callbacks.History&gt; Test it: pred_dim = down_size_model(tf$constant(test_x, &quot;float32&quot;)) reconstr_pred = autoencoder(tf$constant(test_x, &quot;float32&quot;)) imgPlot(reconstr_pred[10,,,]$numpy()[,,1]) ownColors = c(&quot;limegreen&quot;, &quot;purple&quot;, &quot;yellow&quot;, &quot;grey&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;red&quot;, &quot;navy&quot;, &quot;sienna&quot;, &quot;springgreen&quot;) plot(pred_dim[,1]$numpy(), pred_dim[,2]$numpy(), col = ownColors[test[[2]]+1L]) ## Generate new images! new = matrix(c(10, 10), 1, 2) imgPlot(array(up_size_model(new)$numpy(), c(28L, 28L))) new = matrix(c(5, 5), 1, 2) imgPlot(array(up_size_model(new)$numpy(), c(28L, 28L))) 7.1.3 Variational Autoencoder (VAE) The difference between a variational and a normal autoencoder is that a variational autoencoder assumes a distribution for the latent variables (latent variables cannot be observed and are composed of other variables) and the parameters of this distribution are learned. Thus new objects can be generated by inserting valid (!) (with regard to the assumed distribution) “seeds” to the decoder. To achieve the property that more or less randomly chosen points in the low dimensional latent space are meaningful and yield suitable results after decoding, the latent space/training process must be regularized. In this process, the input to the VAE is encoded to a distribution in the latent space rather than a single point. For building variational autoencoders, we will use TensorFlow probability, but first, we need to split the data again. library(tfprobability) ## ## Attaching package: &#39;tfprobability&#39; ## The following object is masked from &#39;package:Rgraphviz&#39;: ## ## shape data = tf$keras$datasets$mnist$load_data() train = data[[1]] train_x = array(train[[1]]/255, c(dim(train[[1]]), 1L)) We will use TensorFlow probability to define priors for our latent variables. library(tfprobability) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. tfp = reticulate::import(&quot;tensorflow_probability&quot;) Build the two networks: encoded = 2L prior = tfd_independent(tfd_normal(c(0.0, 0.0), 1.0), 1L) down_size_model = keras_model_sequential() down_size_model %&gt;% layer_conv_2d(filters = 32L, activation = &quot;relu&quot;, kernel_size = c(2L, 2L), input_shape = c(28L, 28L, 1L), strides = c(4L, 4L)) %&gt;% layer_conv_2d(filters = 16L, activation = &quot;relu&quot;, kernel_size = c(7L, 7L), strides = c(1L, 1L)) %&gt;% layer_flatten() %&gt;% layer_dense(units = 4L, activation = &quot;linear&quot;) %&gt;% layer_independent_normal(2L, activity_regularizer = tfp$layers$KLDivergenceRegularizer(distribution_b = prior)) up_size_model = keras_model_sequential() up_size_model %&gt;% layer_dense(units = 8L, activation = &quot;relu&quot;, input_shape = c(2L)) %&gt;% layer_reshape(target_shape = c(1L, 1L, 8L)) %&gt;% layer_conv_2d_transpose(filters = 16L, kernel_size = c(7, 7), activation = &quot;relu&quot;, strides = c(1L, 1L), use_bias = FALSE) %&gt;% layer_conv_2d_transpose(filters = 32L, activation = &quot;relu&quot;, kernel_size = c(2L, 2L), strides = c(4L, 4L), use_bias = FALSE) %&gt;% layer_conv_2d(filters = 1, kernel_size = c(1L, 1L), strides = c(1L, 1L), activation = &quot;sigmoid&quot;, use_bias = FALSE) VAE = keras_model(inputs = down_size_model$inputs, outputs = up_size_model(down_size_model$outputs)) Compile and fit model: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. loss_binary = function(true, pred){ return(loss_binary_crossentropy(true, pred) * 28.0 * 28.0) } VAE$compile(loss = loss_binary, optimizer = optimizer_adamax()) VAE$fit(train_x, train_x, epochs = 5L) ## &lt;keras.callbacks.History&gt; And show that it works: dist = down_size_model(train_x[1:2000,,,,drop = FALSE]) images = up_size_model(dist$sample()[1:5,]) ownColors = c(&quot;limegreen&quot;, &quot;purple&quot;, &quot;yellow&quot;, &quot;grey&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;red&quot;, &quot;navy&quot;, &quot;sienna&quot;, &quot;springgreen&quot;) oldpar = par(mfrow = c(1, 1)) imgPlot(images[1,,,1]$numpy()) plot(dist$mean()$numpy()[,1], dist$mean()$numpy()[,2], col = ownColors[train[[2]]+1L]) par(oldpar) 7.2 Generative Adversarial Networks (GANs) The idea of a generative adversarial network (GAN) is that two neural networks contest against each other in a “game.” One network is creating data and is trying to “trick” the other network into deciding the generated data is real. The generator (similar to the decoder in autoencoders) creates new images from noise. The discriminator is getting a mix of true (from the data set) and artificially generated images from the generator. Thereby, the loss of the generator rises when fakes are identified as fakes by the discriminator (simple binary cross entropy loss, 0/1…). The loss of the discriminator rises when fakes are identified as real images (class 0) or real images as fakes (class 1), again with binary cross entropy. Binary cross entropy: Entropy or Shannon entropy (named after Claude Shannon) \\(\\mathbf{H}\\) (uppercase “eta”) in context of information theory is the expected value of information content or the mean/average information content of an “event” compared to all possible outcomes. Encountering an event with low probability holds more information than encountering an event with high probability. Binary cross entropy is a measure to determine the similarity of two (discrete) probability distributions \\(A~(\\mathrm{true~distribution}), B~(\\mathrm{predicted~distribution})\\) according to the inherent information. It is not (!) symmetric, in general: \\(\\textbf{H}_{A}(B) \\neq \\textbf{H}_{B}(A)\\). The minimum value depends on the distribution of \\(A\\) and is the entropy of \\(A\\): \\[\\mathrm{min}~\\textbf{H}_{A}(B) = \\underset{B}{\\mathrm{min}}~\\textbf{H}_{A}(B) = \\textbf{H}_{A}(B = A) = \\textbf{H}_{A}(A) = \\textbf{H}(A)\\] The setup: Outcomes \\(y_{i} \\in \\{0, 1\\}\\) (labels). Predictions \\(\\hat{y}_{i} \\in[0, 1]\\) (probabilities). The binary cross entropy or log loss of a system of outcomes/predictions is then defined as follows: \\[ \\textbf{H}_{A}(B) = -\\frac{1}{N} \\sum_{i = 1}^{N} y_{i} \\cdot \\mathrm{log} \\left( p(y_{i}) \\right) + (1 -y_{i}) \\cdot \\mathrm{log} \\left( 1-p(y_{i}) \\right) =\\\\ = -\\frac{1}{N} \\sum_{i = 1}^{N} y_{i} \\cdot \\mathrm{log} (\\hat{y}_{i}) + (1 -y_{i}) \\cdot \\mathrm{log} \\left( 1- \\hat{y}_{i} \\right) \\] High predicted probabilities of having the label for originally labeled data (1) yield a low loss as well as predicting a low probability of having the label for originally unlabeled data (0). Mind the properties of probabilities and the logarithm. A possible application of generative adversarial networks is to create pictures that look like real photographs e.g. https://thispersondoesnotexist.com/. Visit that site (several times)!. However, the application of generative adversarial networks today is much wider than just the creation of data. For example, generative adversarial networks can also be used to “augment” data, i.e. to create new data and thereby improve the fitted model. 7.2.1 MNIST - Generative Adversarial Networks Based on Deep Neural Networks We will now explore this on the MNIST data set. library(keras) library(tensorflow) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. rotate = function(x){ t(apply(x, 2, rev)) } imgPlot = function(img, title = &quot;&quot;){ col = grey.colors(255) image(rotate(img), col = col, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, main = paste0(&quot;Label: &quot;, as.character(title))) } We don’t need the test set here. data = dataset_mnist() train = data$train train_x = array((train$x-127.5)/127.5, c(dim(train$x)[1], 784L)) We need a function to sample images for the discriminator. batch_size = 32L dataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, &quot;float32&quot;)) dataset$batch(batch_size) ## &lt;BatchDataset shapes: (None, 784), types: tf.float32&gt; Define generator model: get_generator = function(){ generator = keras_model_sequential() generator %&gt;% layer_dense(units = 200L, input_shape = c(100L)) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 200L) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 784L, activation = &quot;tanh&quot;) return(generator) } And we also test the generator model: generator = get_generator() sample = tf$random$normal(c(1L, 100L)) imgPlot(array(generator(sample)$numpy(), c(28L, 28L))) In the discriminator, noise (random vector with 100 values) is passed through the network such that the output corresponds to the number of pixels of one MNIST image (784). We therefore define the discriminator function now. get_discriminator = function(){ discriminator = keras_model_sequential() discriminator %&gt;% layer_dense(units = 200L, input_shape = c(784L)) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 100L) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) return(discriminator) } And we also test the discriminator function. discriminator = get_discriminator() discriminator(generator(tf$random$normal(c(1L, 100L)))) ## tf.Tensor([[0.5089391]], shape=(1, 1), dtype=float32) We also have to define the loss functions for both networks.We use the already known binary cross entropy. However, we have to encode the real and predicted values for the two networks individually. The discriminator will get two losses - one for identifying fake images as fake, and one for identifying real MNIST images as real images. The generator will just get one loss - was it able to deceive the discriminator? ce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE) loss_discriminator = function(real, fake){ real_loss = ce(tf$ones_like(real), real) fake_loss = ce(tf$zeros_like(fake), fake) return(real_loss + fake_loss) } loss_generator = function(fake){ return(ce(tf$ones_like(fake), fake)) } Each network will get its own optimizer (in a GAN the networks are treated independently): gen_opt = tf$keras$optimizers$RMSprop(1e-4) disc_opt = tf$keras$optimizers$RMSprop(1e-4) We have to write our own training loop here (we cannot use the fit function). In each iteration (for each batch) we will do the following (the GradientTape records computations to do automatic differentiation): Sample noise. Generator creates images from the noise. Discriminator makes predictions for fake images and real images (response is a probability between [0,1]). Calculate loss for generator. Calculate loss for discriminator. Calculate gradients for weights and the loss. Update weights of generator. Update weights of discriminator. Return losses. generator = get_generator() discriminator = get_discriminator() train_step = function(images){ noise = tf$random$normal(c(128L, 100L)) with(tf$GradientTape(persistent = TRUE) %as% tape, { gen_images = generator(noise) fake_output = discriminator(gen_images) real_output = discriminator(images) gen_loss = loss_generator(fake_output) disc_loss = loss_discriminator(real_output, fake_output) } ) gen_grads = tape$gradient(gen_loss, generator$weights) disc_grads = tape$gradient(disc_loss, discriminator$weights) rm(tape) gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights))) disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights))) return(c(gen_loss, disc_loss)) } train_step = tf$`function`(reticulate::py_func(train_step)) Now we can finally train our networks in a training loop: Create networks. Get batch of images. Run train_step function. Print losses. Repeat step 2-4 for number of epochs. library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. batch_size = 128L epochs = 20L steps = as.integer(nrow(train_x)/batch_size) counter = 1 gen_loss = c() disc_loss = c() dataset2 = dataset$prefetch(tf$data$AUTOTUNE) for(e in 1:epochs){ dat = reticulate::as_iterator(dataset2$batch(batch_size)) coro::loop( for(images in dat){ losses = train_step(images) gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy()) disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy()) } ) if(epochs %% 5 == 0){ #Print output every 5 steps. cat(&quot;Gen: &quot;, mean(gen_loss), &quot; Disc: &quot;, mean(disc_loss), &quot; \\n&quot;) } noise = tf$random$normal(c(1L, 100L)) if(epochs %% 10 == 0){ #Plot image every 10 steps. imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), &quot;Gen&quot;) } } ## Gen: 0.6883632 Disc: 1.086055 ## Gen: 0.7164299 Disc: 1.163737 ## Gen: 0.7246124 Disc: 1.169931 ## Gen: 0.7456345 Disc: 1.170835 ## Gen: 0.7647617 Disc: 1.166425 ## Gen: 0.7748129 Disc: 1.182352 ## Gen: 0.854375 Disc: 1.087074 ## Gen: 0.88686 Disc: 1.14963 ## Gen: 0.8885204 Disc: 1.210371 ## Gen: 0.8914993 Disc: 1.249861 ## Gen: 0.8898634 Disc: 1.284225 ## Gen: 0.8886764 Disc: 1.306691 ## Gen: 0.8881942 Disc: 1.322189 ## Gen: 0.888625 Disc: 1.332328 ## Gen: 0.8893886 Disc: 1.33952 ## Gen: 0.890981 Disc: 1.344293 ## Gen: 0.8932191 Disc: 1.346911 ## Gen: 0.8957275 Disc: 1.34975 ## Gen: 0.900085 Disc: 1.351894 ## Gen: 0.9044505 Disc: 1.35396 7.2.2 Flower - GAN We can now also do the same for the flower data set. We will write this completely on our own following the steps also done for the MNIST data set. library(keras) library(tidyverse) library(tensorflow) library(EcoData) data = EcoData::dataset_flower() train = (data$train-127.5)/127.5 test = (data$test-127.5)/127.5 train_x = abind::abind(list(train, test), along = 1L) dataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, &quot;float32&quot;)) Define the generator model and test it: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. get_generator = function(){ generator = keras_model_sequential() generator %&gt;% layer_dense(units = 20L*20L*128L, input_shape = c(100L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_reshape(c(20L, 20L, 128L)) %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters = 256L, kernel_size = c(3L, 3L), padding = &quot;same&quot;, strides = c(1L, 1L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters = 128L, kernel_size = c(5L, 5L), padding = &quot;same&quot;, strides = c(1L, 1L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters = 64L, kernel_size = c(5L, 5L), padding = &quot;same&quot;, strides = c(2L, 2L), use_bias = FALSE) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d_transpose(filters = 3L, kernel_size = c(5L, 5L), padding = &quot;same&quot;, strides = c(2L, 2L), activation = &quot;tanh&quot;, use_bias = FALSE) return(generator) } generator = get_generator() image = generator(tf$random$normal(c(1L,100L)))$numpy()[1,,,] image = scales::rescale(image, to = c(0, 255)) image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() Define the discriminator and test it: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. get_discriminator = function(){ discriminator = keras_model_sequential() discriminator %&gt;% layer_conv_2d(filters = 64L, kernel_size = c(5L, 5L), strides = c(2L, 2L), padding = &quot;same&quot;, input_shape = c(80L, 80L, 3L)) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d(filters = 128L, kernel_size = c(5L, 5L), strides = c(2L, 2L), padding = &quot;same&quot;) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_conv_2d(filters = 256L, kernel_size = c(3L, 3L), strides = c(2L, 2L), padding = &quot;same&quot;) %&gt;% layer_activation_leaky_relu() %&gt;% layer_dropout(0.3) %&gt;% layer_flatten() %&gt;% layer_dense(units = 1L, activation = &quot;sigmoid&quot;) return(discriminator) } discriminator = get_discriminator() discriminator ## Model ## Model: &quot;sequential_72&quot; ## __________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================================================== ## conv2d_47 (Conv2D) (None, 40, 40, 64) 4864 ## ## leaky_re_lu_14 (LeakyReLU) (None, 40, 40, 64) 0 ## ## dropout_106 (Dropout) (None, 40, 40, 64) 0 ## ## conv2d_46 (Conv2D) (None, 20, 20, 128) 204928 ## ## leaky_re_lu_13 (LeakyReLU) (None, 20, 20, 128) 0 ## ## dropout_105 (Dropout) (None, 20, 20, 128) 0 ## ## conv2d_45 (Conv2D) (None, 10, 10, 256) 295168 ## ## leaky_re_lu_12 (LeakyReLU) (None, 10, 10, 256) 0 ## ## dropout_104 (Dropout) (None, 10, 10, 256) 0 ## ## flatten_20 (Flatten) (None, 25600) 0 ## ## dense_233 (Dense) (None, 1) 25601 ## ## ================================================================================================================== ## Total params: 530,561 ## Trainable params: 530,561 ## Non-trainable params: 0 ## __________________________________________________________________________________________________________________ discriminator(generator(tf$random$normal(c(1L, 100L)))) ## tf.Tensor([[0.4999608]], shape=(1, 1), dtype=float32) Define the loss functions: ce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE, label_smoothing = 0.1) loss_discriminator = function(real, fake){ real_loss = ce(tf$ones_like(real), real) fake_loss = ce(tf$zeros_like(fake), fake) return(real_loss+fake_loss) } loss_generator = function(fake){ return(ce(tf$ones_like(fake), fake)) } Define the optimizers and the batch function: gen_opt = tf$keras$optimizers$RMSprop(1e-4) disc_opt = tf$keras$optimizers$RMSprop(1e-4) Define functions for the generator and discriminator: generator = get_generator() discriminator = get_discriminator() train_step = function(images){ noise = tf$random$normal(c(32L, 100L)) with(tf$GradientTape(persistent = TRUE) %as% tape, { gen_images = generator(noise) real_output = discriminator(images) fake_output = discriminator(gen_images) gen_loss = loss_generator(fake_output) disc_loss = loss_discriminator(real_output, fake_output) } ) gen_grads = tape$gradient(gen_loss, generator$weights) disc_grads = tape$gradient(disc_loss, discriminator$weights) rm(tape) gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights))) disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights))) return(c(gen_loss, disc_loss)) } train_step = tf$`function`(reticulate::py_func(train_step)) Do the training: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. batch_size = 32L epochs = 30L steps = as.integer(dim(train_x)[1]/batch_size) counter = 1 gen_loss = c() disc_loss = c() dataset = dataset$prefetch(tf$data$AUTOTUNE) for(e in 1:epochs){ dat = reticulate::as_iterator(dataset$batch(batch_size)) coro::loop( for(images in dat){ losses = train_step(images) gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy()) disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy()) } ) noise = tf$random$normal(c(1L, 100L)) image = generator(noise)$numpy()[1,,,] image = scales::rescale(image, to = c(0, 255)) if(e %% 10 == 0){ image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() } cat(&quot;Gen: &quot;, mean(gen_loss), &quot; Disc: &quot;, mean(disc_loss), &quot; \\n&quot;) } ## Gen: 1.149357 Disc: 0.8001182 ## Gen: 1.5089 Disc: 0.7430979 ## Gen: 1.698984 Disc: 0.7419785 ## Gen: 1.790281 Disc: 0.7449237 ## Gen: 1.82084 Disc: 0.7540591 ## Gen: 1.796408 Disc: 0.7812456 ## Gen: 1.735721 Disc: 0.8152333 ## Gen: 1.655125 Disc: 0.8589529 ## Gen: 1.599357 Disc: 0.8842102 ## Gen: 1.549184 Disc: 0.9052329 ## Gen: 1.504676 Disc: 0.9234332 ## Gen: 1.46387 Disc: 0.9415495 ## Gen: 1.439547 Disc: 0.9518118 ## Gen: 1.411166 Disc: 0.963441 ## Gen: 1.38764 Disc: 0.9735227 ## Gen: 1.361992 Disc: 0.9863029 ## Gen: 1.334074 Disc: 1.002124 ## Gen: 1.307651 Disc: 1.017834 ## Gen: 1.283841 Disc: 1.030556 ## Gen: 1.262961 Disc: 1.041403 ## Gen: 1.245325 Disc: 1.049907 ## Gen: 1.228488 Disc: 1.058534 ## Gen: 1.213941 Disc: 1.065773 ## Gen: 1.199879 Disc: 1.072757 ## Gen: 1.187363 Disc: 1.078638 ## Gen: 1.176552 Disc: 1.083673 ## Gen: 1.16801 Disc: 1.086775 ## Gen: 1.161602 Disc: 1.089686 ## Gen: 1.15672 Disc: 1.091906 ## Gen: 1.151876 Disc: 1.093815 library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. noise = tf$random$normal(c(1L, 100L)) image = generator(noise)$numpy()[1,,,] image = scales::rescale(image, to = c(0, 255)) image %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() More images: 7.3 Reinforcement learning This is just a teaser, run/adapt it if you like. Objective: Train a neural network capable of balancing a pole. The environment is run on a local server, please install gym. Or go through this colab book. library(keras) library(tensorflow) library(gym) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. remote_base =´ &quot;http://127.0.0.1:5000&quot; client = create_GymClient(remote_base) env = gym::env_create(client, &quot;CartPole-v1&quot;) gym::env_list_all(client) env_reset(client, env) # action = env_action_space_sample(client, env) step = env_step(client, env, 1) env_reset(client, env) goal_steps = 500 score_requirement = 60 intial_games = 1000 state_size = 4L action_size = 2L gamma = 0.95 epsilon = 0.95 epsilon_min = 0.01 epsilon_decay = 0.995 model = keras_model_sequential() model %&gt;% layer_dense(input_shape = c(4L), units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(units = 20L, activation = &quot;relu&quot;) %&gt;% layer_dense(2L, activation = &quot;linear&quot;) model %&gt;% keras::compile(loss = loss_mean_squared_error, optimizer = optimizer_adamax()) memory = matrix(0, nrow = 8000L, 11L) counter = 1 remember = function(memory, state, action, reward, next_state, done){ memory[counter,] = as.numeric(c(state, action, reward, next_state, done)) counter &lt;&lt;- counter+1 return(memory) } # memory: state 1:4, action 5, reward 6, next_state 7:10, done 11 act = function(state){ if(runif(1) &lt;= epsilon){ return(sample(0:1, 1)) } act_prob = predict(model, matrix(state, nrow = 1L)) return(which.max(act_prob) - 1L) } replay = function(batch_size = 25L, memory, counter){ indices = sample.int(counter, batch_size) batch = memory[indices,,drop = FALSE] for(i in 1:nrow(batch)){ target = batch[i,6] # Reward. action = batch[i,5] # Action. state = matrix(memory[i, 1:4], nrow = 1L) next_state = matrix(memory[i,7:10], nrow =1L) if(!batch[i,11]){ # Not done. target = (batch[i,6] + gamma * predict(model, matrix(next_state, nrow = 1L)))[1,1] } target_f = predict(model, matrix(state, nrow = 1L)) target_f[action+1L] = target model$fit(x = state, y = target_f, epochs = 1L, verbose = 0L) if(epsilon &gt; epsilon_min){ epsilon &lt;&lt;- epsilon_decay*epsilon } } } done = 0 for(e in 1:100){ state = unlist(env_reset(client, env)) for(time in 1:500){ action = act(state) response = env_step(client, env, action = action) done = as.integer(response$done) if(!done){ reward = response$reward } else{ reward = -10 } next_state = unlist(response$observation) memory = remember(memory, state, action, reward, next_state, done) state = next_state if(done){ cat(&quot;episode&quot;, e/500, &quot; score: &quot;, time, &quot; eps: &quot;, epsilon, &quot;\\n&quot;) break() } if(counter &gt; 32L){ replay(32L, memory, counter-1L) } } } Task Read section 7.1.3 on variational autoencoders and try to transfer the examples with MNIST to our flower data set. Solution Split the data: library(keras) library(tensorflow) library(tfprobability) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. data = EcoData::dataset_flower() test = data$test/255 train = data$train/255 rm(data) Build the variational autoencoder: encoded = 10L prior = tfp$distributions$Independent( tfp$distributions$Normal(loc=tf$zeros(encoded), scale = 1.), reinterpreted_batch_ndims = 1L ) down_size_model = tf$keras$models$Sequential(list( tf$keras$layers$InputLayer(input_shape = c(80L, 80L, 3L)), tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu, kernel_size = 5L, strides = 1L), tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu, kernel_size = 5L, strides = 2L), tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu, kernel_size = 5L, strides = 1L), tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu, kernel_size = 5L, strides = 2L), tf$keras$layers$Conv2D(filters = 128L, activation = tf$nn$leaky_relu, kernel_size = 7L, strides = 1L), tf$keras$layers$Flatten(), tf$keras$layers$Dense(units = tfp$layers$MultivariateNormalTriL$params_size(encoded), activation = NULL), tfp$layers$MultivariateNormalTriL( encoded, activity_regularizer = tfp$layers$KLDivergenceRegularizer(prior, weight = 0.0002) ) )) up_size_model = tf$keras$models$Sequential(list( tf$keras$layers$InputLayer(input_shape = encoded), tf$keras$layers$Dense(units = 8192L, activation = &quot;relu&quot;), tf$keras$layers$Reshape(target_shape = c(8L, 8L, 128L)), tf$keras$layers$Conv2DTranspose(filters = 128L, kernel_size = 7L, activation = tf$nn$leaky_relu, strides = 1L, use_bias = FALSE), tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L, activation = tf$nn$leaky_relu, strides = 2L, use_bias = FALSE), tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L, activation = tf$nn$leaky_relu, strides = 1L, use_bias = FALSE), tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L, activation = tf$nn$leaky_relu, strides = 2L, use_bias = FALSE), tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L, activation = tf$nn$leaky_relu, strides = 1L, use_bias = FALSE), tf$keras$layers$Conv2DTranspose(filters = 3L, kernel_size = c(4L, 4L), activation = &quot;sigmoid&quot;, strides = c(1L, 1L), use_bias = FALSE) )) VAE = tf$keras$models$Model(inputs = down_size_model$inputs, outputs = up_size_model(down_size_model$outputs)) summary(VAE) ## Model: &quot;model_5&quot; ## __________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================================================== ## input_4 (InputLayer) [(None, 80, 80, 3)] 0 ## ## conv2d_51 (Conv2D) (None, 76, 76, 32) 2432 ## ## conv2d_52 (Conv2D) (None, 36, 36, 32) 25632 ## ## conv2d_53 (Conv2D) (None, 32, 32, 64) 51264 ## ## conv2d_54 (Conv2D) (None, 14, 14, 64) 102464 ## ## conv2d_55 (Conv2D) (None, 8, 8, 128) 401536 ## ## flatten_22 (Flatten) (None, 8192) 0 ## ## dense_236 (Dense) (None, 65) 532545 ## ## multivariate_normal_tri_l (MultivariateNormalTriL ((None, 10), 0 ## ) (None, 10)) ## ## sequential_76 (Sequential) (None, 80, 80, 3) 1278464 ## ## ================================================================================================================== ## Total params: 2,394,337 ## Trainable params: 2,394,337 ## Non-trainable params: 0 ## __________________________________________________________________________________________________________________ Compile and train model: be = function(true, pred){ return(tf$losses$binary_crossentropy(true, pred) * 80.0 * 80.0) } VAE$compile(loss = be, optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.0003)) VAE$fit(x = train, y = train, epochs = 50L, shuffle = TRUE, batch_size = 20L) ## &lt;keras.callbacks.History&gt; dist = down_size_model(train[1:10,,,]) images = up_size_model( dist$sample()[1:5,] ) oldpar = par(mfrow = c(3, 1), mar = rep(1, 4)) scales::rescale(images[1,,,]$numpy(), to = c(0, 255)) %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() scales::rescale(images[2,,,]$numpy(), to = c(0, 255)) %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() scales::rescale(images[3,,,]$numpy(), to = c(0, 255)) %&gt;% image_to_array() %&gt;% `/`(., 255) %&gt;% as.raster() %&gt;% plot() par(oldpar) Task Go through the R examples on generative adversarial networks (7.2) and compare the flower example with the MNIST example - where are the differences - and why? Solution The MNIST example uses a “simple” deep neural network which is sufficient for a classification that easy. The flower example uses a much more expensive convolutional neural network to classify the images. "],["datasets.html", "8 Data sets 8.1 Titanic 8.2 Plant-pollinator Database 8.3 Wine 8.4 Nasa 8.5 Flower", " 8 Data sets You can download the data sets we use in the course here (ignore browser warnings) or by installing the EcoData package: devtools::install_github(repo = &quot;florianhartig/EcoData&quot;, subdir = &quot;EcoData&quot;, dependencies = TRUE, build_vignettes = FALSE) 8.1 Titanic The data set is a collection of Titanic passengers with information about their age, class, sex, and their survival status. The competition is simple here: Train a machine learning model and predict the survival probability. The Titanic data set is very well explored and serves as a stepping stone in many machine learning careers. For inspiration and data exploration notebooks, check out this kaggle competition. Response variable: “survived” A minimal working example: Load data set: library(EcoData) data(titanic_ml) titanic = titanic_ml summary(titanic) ## pclass survived name sex age sibsp ## Min. :1.000 Min. :0.0000 Length:1309 female:466 Min. : 0.1667 Min. :0.0000 ## 1st Qu.:2.000 1st Qu.:0.0000 Class :character male :843 1st Qu.:21.0000 1st Qu.:0.0000 ## Median :3.000 Median :0.0000 Mode :character Median :28.0000 Median :0.0000 ## Mean :2.295 Mean :0.3853 Mean :29.8811 Mean :0.4989 ## 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:39.0000 3rd Qu.:1.0000 ## Max. :3.000 Max. :1.0000 Max. :80.0000 Max. :8.0000 ## NA&#39;s :655 NA&#39;s :263 ## parch ticket fare cabin embarked boat body ## Min. :0.000 CA. 2343: 11 Min. : 0.000 :1014 : 2 :823 Min. : 1.0 ## 1st Qu.:0.000 1601 : 8 1st Qu.: 7.896 C23 C25 C27 : 6 C:270 13 : 39 1st Qu.: 72.0 ## Median :0.000 CA 2144 : 8 Median : 14.454 B57 B59 B63 B66: 5 Q:123 C : 38 Median :155.0 ## Mean :0.385 3101295 : 7 Mean : 33.295 G6 : 5 S:914 15 : 37 Mean :160.8 ## 3rd Qu.:0.000 347077 : 7 3rd Qu.: 31.275 B96 B98 : 4 14 : 33 3rd Qu.:256.0 ## Max. :9.000 347082 : 7 Max. :512.329 C22 C26 : 4 4 : 31 Max. :328.0 ## (Other) :1261 NA&#39;s :1 (Other) : 271 (Other):308 NA&#39;s :1188 ## home.dest ## :564 ## New York, NY : 64 ## London : 14 ## Montreal, PQ : 10 ## Cornwall / Akron, OH: 9 ## Paris, France : 9 ## (Other) :639 Impute missing values (not our response variable!): library(missRanger) library(dplyr) set.seed(123) titanic_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest) titanic_imputed = missRanger::missRanger(data = titanic_imputed %&gt;% select(-survived)) ## ## Missing value imputation by random forests ## ## Variables to impute: age, fare, body ## Variables used to impute: pclass, sex, age, sibsp, parch, fare, embarked, body ## iter 1: ... ## iter 2: ... ## iter 3: ... ## iter 4: ... titanic_imputed$survived = titanic$survived Split into training and test set: train = titanic_imputed[!is.na(titanic$survived), ] test = titanic_imputed[is.na(titanic$survived), ] Train model: model = glm(survived~., data = train, family = binomial()) Predictions: preds = predict(model, data = test, type = &quot;response&quot;) head(preds) ## 561 321 1177 1098 1252 1170 ## 0.79511615 0.29231652 0.01461978 0.12323274 0.14130063 0.11847391 Create submission csv: write.csv(data.frame(y = preds), file = &quot;glm.csv&quot;) And submit the csv on http://rhsbio7.uni-regensburg.de:8500. 8.2 Plant-pollinator Database The plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look here. Response variable: “interaction” A minimal working example: Load data set: library(EcoData) data(plantPollinator_df) plant_poll = plantPollinator_df summary(plant_poll) ## crop insect type season diameter ## Vaccinium_corymbosum: 256 Andrena_wilkella : 80 Length:20480 Length:20480 Min. : 2.00 ## Brassica_napus : 256 Andrena_barbilabris: 80 Class :character Class :character 1st Qu.: 5.00 ## Carum_carvi : 256 Andrena_cineraria : 80 Mode :character Mode :character Median : 19.00 ## Coriandrum_sativum : 256 Andrena_flavipes : 80 Mean : 27.03 ## Daucus_carota : 256 Andrena_gravida : 80 3rd Qu.: 25.00 ## Malus_domestica : 256 Andrena_haemorrhoa : 80 Max. :150.00 ## (Other) :18944 (Other) :20000 NA&#39;s :9472 ## corolla colour nectar b.system s.pollination ## Length:20480 Length:20480 Length:20480 Length:20480 Length:20480 ## Class :character Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## inflorescence composite guild tongue body sociality ## Length:20480 Length:20480 Length:20480 Min. : 2.000 Min. : 2.00 Length:20480 ## Class :character Class :character Class :character 1st Qu.: 4.800 1st Qu.: 8.00 Class :character ## Mode :character Mode :character Mode :character Median : 6.600 Median :10.50 Mode :character ## Mean : 8.104 Mean :10.66 ## 3rd Qu.:10.500 3rd Qu.:13.00 ## Max. :26.400 Max. :25.00 ## NA&#39;s :17040 NA&#39;s :6160 ## feeding interaction ## Length:20480 0 :14095 ## Class :character 1 : 595 ## Mode :character NA&#39;s: 5790 ## ## ## ## Impute missing values (not our response variable!) We will select only a few predictors here (you can work with all predictors of course). library(missRanger) library(dplyr) set.seed(123) plant_poll_imputed = plant_poll %&gt;% select(diameter, corolla, tongue, body, interaction) plant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %&gt;% select(-interaction)) ## ## Missing value imputation by random forests ## ## Variables to impute: diameter, corolla, tongue, body ## Variables used to impute: diameter, corolla, tongue, body ## iter 1: .... ## iter 2: .... ## iter 3: .... ## iter 4: .... plant_poll_imputed$interaction = plant_poll$interaction Split into training and test set: train = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ] test = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ] Train model: model = glm(interaction~., data = train, family = binomial()) Predictions: preds = predict(model, newdata = test, type = &quot;response&quot;) head(preds) ## 1 2 3 4 5 6 ## 0.02887581 0.04972722 0.03670465 0.03670465 0.02713261 0.03996870 Create submission csv: write.csv(data.frame(y = preds), file = &quot;glm.csv&quot;) 8.3 Wine The data set is a collection of wines of different quality. The aim is to predict the quality of the wine based on physiochemical predictors. For inspiration and data exploration notebooks, check out this kaggle competition. For instance, check out this very nice notebook which removes a few problems from the data. Response variable: “quality” We could theoretically use a regression model for this task but we will stick with a classification model. A minimal working example: Load data set: library(EcoData) data(wine) summary(wine) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide ## Min. : 4.600 Min. :0.1200 Min. :0.0000 Min. : 0.900 Min. :0.01200 Min. : 1.00 ## 1st Qu.: 7.100 1st Qu.:0.3900 1st Qu.:0.0900 1st Qu.: 1.900 1st Qu.:0.07000 1st Qu.: 7.00 ## Median : 7.900 Median :0.5200 Median :0.2600 Median : 2.200 Median :0.07900 Median :14.00 ## Mean : 8.335 Mean :0.5284 Mean :0.2705 Mean : 2.533 Mean :0.08747 Mean :15.83 ## 3rd Qu.: 9.300 3rd Qu.:0.6400 3rd Qu.:0.4200 3rd Qu.: 2.600 3rd Qu.:0.09000 3rd Qu.:21.00 ## Max. :15.900 Max. :1.5800 Max. :1.0000 Max. :15.500 Max. :0.61100 Max. :72.00 ## NA&#39;s :70 NA&#39;s :48 NA&#39;s :41 NA&#39;s :60 NA&#39;s :37 NA&#39;s :78 ## total.sulfur.dioxide density pH sulphates alcohol quality ## Min. : 6.00 Min. :0.9901 Min. :2.740 Min. :0.3300 Min. : 8.40 Min. :3.000 ## 1st Qu.: 22.00 1st Qu.:0.9956 1st Qu.:3.210 1st Qu.:0.5500 1st Qu.: 9.50 1st Qu.:5.000 ## Median : 38.00 Median :0.9968 Median :3.310 Median :0.6200 Median :10.20 Median :6.000 ## Mean : 46.23 Mean :0.9968 Mean :3.311 Mean :0.6572 Mean :10.42 Mean :5.596 ## 3rd Qu.: 62.00 3rd Qu.:0.9979 3rd Qu.:3.400 3rd Qu.:0.7300 3rd Qu.:11.10 3rd Qu.:6.000 ## Max. :289.00 Max. :1.0037 Max. :4.010 Max. :2.0000 Max. :14.90 Max. :8.000 ## NA&#39;s :78 NA&#39;s :78 NA&#39;s :25 NA&#39;s :51 NA&#39;s :905 Impute missing values (not our response variable!). library(missRanger) library(dplyr) set.seed(123) wine_imputed = missRanger::missRanger(data = wine %&gt;% select(-quality)) ## ## Missing value imputation by random forests ## ## Variables to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates ## Variables used to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol ## iter 1: .......... ## iter 2: .......... ## iter 3: .......... ## iter 4: .......... ## iter 5: .......... ## iter 6: .......... ## iter 7: .......... wine_imputed$quality = wine$quality Split into training and test set: train = wine_imputed[!is.na(wine$quality), ] test = wine_imputed[is.na(wine$quality), ] Train model: library(ranger) set.seed(123) rf = ranger(quality~., data = train, classification = TRUE) Predictions: preds = predict(rf, data = test)$predictions head(preds) ## [1] 6 5 5 7 6 6 Create submission csv: write.csv(data.frame(y = preds), file = &quot;rf.csv&quot;) 8.4 Nasa A collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. For inspiration and data exploration notebooks, check out this kaggle competition. Response variable: “Hazardous” Load data set: library(EcoData) data(nasa) summary(nasa) ## Neo.Reference.ID Name Absolute.Magnitude Est.Dia.in.KM.min. Est.Dia.in.KM.max. Est.Dia.in.M.min. ## Min. :2000433 Min. :2000433 Min. :11.16 Min. : 0.00101 Min. : 0.00226 Min. : 1.011 ## 1st Qu.:3102682 1st Qu.:3102683 1st Qu.:20.10 1st Qu.: 0.03346 1st Qu.: 0.07482 1st Qu.: 33.462 ## Median :3514800 Median :3514800 Median :21.90 Median : 0.11080 Median : 0.24777 Median : 110.804 ## Mean :3272675 Mean :3273113 Mean :22.27 Mean : 0.20523 Mean : 0.45754 Mean : 204.649 ## 3rd Qu.:3690987 3rd Qu.:3690385 3rd Qu.:24.50 3rd Qu.: 0.25384 3rd Qu.: 0.56760 3rd Qu.: 253.837 ## Max. :3781897 Max. :3781897 Max. :32.10 Max. :15.57955 Max. :34.83694 Max. :15579.552 ## NA&#39;s :53 NA&#39;s :57 NA&#39;s :36 NA&#39;s :60 NA&#39;s :23 NA&#39;s :29 ## Est.Dia.in.M.max. Est.Dia.in.Miles.min. Est.Dia.in.Miles.max. Est.Dia.in.Feet.min. Est.Dia.in.Feet.max. ## Min. : 2.26 Min. :0.00063 Min. : 0.00140 Min. : 3.32 Min. : 7.41 ## 1st Qu.: 74.82 1st Qu.:0.02079 1st Qu.: 0.04649 1st Qu.: 109.78 1st Qu.: 245.49 ## Median : 247.77 Median :0.06885 Median : 0.15395 Median : 363.53 Median : 812.88 ## Mean : 458.45 Mean :0.12734 Mean : 0.28486 Mean : 670.44 Mean : 1500.77 ## 3rd Qu.: 567.60 3rd Qu.:0.15773 3rd Qu.: 0.35269 3rd Qu.: 832.80 3rd Qu.: 1862.19 ## Max. :34836.94 Max. :9.68068 Max. :21.64666 Max. :51114.02 Max. :114294.42 ## NA&#39;s :46 NA&#39;s :42 NA&#39;s :50 NA&#39;s :21 NA&#39;s :46 ## Close.Approach.Date Epoch.Date.Close.Approach Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr ## 2016-07-22: 18 Min. :7.889e+11 Min. : 0.3355 Min. : 1208 ## 2015-01-15: 17 1st Qu.:1.016e+12 1st Qu.: 8.4497 1st Qu.: 30399 ## 2015-02-15: 16 Median :1.203e+12 Median :12.9370 Median : 46532 ## 2007-11-08: 15 Mean :1.180e+12 Mean :13.9848 Mean : 50298 ## 2012-01-15: 15 3rd Qu.:1.356e+12 3rd Qu.:18.0774 3rd Qu.: 65068 ## (Other) :4577 Max. :1.473e+12 Max. :44.6337 Max. :160681 ## NA&#39;s : 29 NA&#39;s :43 NA&#39;s :27 NA&#39;s :28 ## Miles.per.hour Miss.Dist..Astronomical. Miss.Dist..lunar. Miss.Dist..kilometers. Miss.Dist..miles. ## Min. : 750.5 Min. :0.00018 Min. : 0.06919 Min. : 26610 Min. : 16535 ## 1st Qu.:18846.7 1st Qu.:0.13341 1st Qu.: 51.89874 1st Qu.:19964907 1st Qu.:12454813 ## Median :28893.7 Median :0.26497 Median :103.19415 Median :39685408 Median :24662435 ## Mean :31228.0 Mean :0.25690 Mean : 99.91366 Mean :38436154 Mean :23885560 ## 3rd Qu.:40436.9 3rd Qu.:0.38506 3rd Qu.:149.59244 3rd Qu.:57540318 3rd Qu.:35714721 ## Max. :99841.2 Max. :0.49988 Max. :194.45491 Max. :74781600 Max. :46467132 ## NA&#39;s :38 NA&#39;s :60 NA&#39;s :30 NA&#39;s :56 NA&#39;s :27 ## Orbiting.Body Orbit.ID Orbit.Determination.Date Orbit.Uncertainity Minimum.Orbit.Intersection ## Earth:4665 Min. : 1.00 2017-06-21 06:17:20: 9 Min. :0.000 Min. :0.00000 ## NA&#39;s : 22 1st Qu.: 9.00 2017-04-06 08:57:13: 8 1st Qu.:0.000 1st Qu.:0.01435 ## Median : 16.00 2017-04-06 09:24:24: 8 Median :3.000 Median :0.04653 ## Mean : 28.34 2017-04-06 08:24:13: 7 Mean :3.521 Mean :0.08191 ## 3rd Qu.: 31.00 2017-04-06 08:26:19: 7 3rd Qu.:6.000 3rd Qu.:0.12150 ## Max. :611.00 (Other) :4622 Max. :9.000 Max. :0.47789 ## NA&#39;s :33 NA&#39;s : 26 NA&#39;s :49 NA&#39;s :137 ## Jupiter.Tisserand.Invariant Epoch.Osculation Eccentricity Semi.Major.Axis Inclination ## Min. :2.196 Min. :2450164 Min. :0.00752 Min. :0.6159 Min. : 0.01451 ## 1st Qu.:4.047 1st Qu.:2458000 1st Qu.:0.24086 1st Qu.:1.0012 1st Qu.: 4.93290 ## Median :5.071 Median :2458000 Median :0.37251 Median :1.2422 Median :10.27694 ## Mean :5.056 Mean :2457723 Mean :0.38267 Mean :1.4009 Mean :13.36159 ## 3rd Qu.:6.017 3rd Qu.:2458000 3rd Qu.:0.51256 3rd Qu.:1.6782 3rd Qu.:19.47848 ## Max. :9.025 Max. :2458020 Max. :0.96026 Max. :5.0720 Max. :75.40667 ## NA&#39;s :56 NA&#39;s :60 NA&#39;s :39 NA&#39;s :53 NA&#39;s :42 ## Asc.Node.Longitude Orbital.Period Perihelion.Distance Perihelion.Arg Aphelion.Dist Perihelion.Time ## Min. : 0.0019 Min. : 176.6 Min. :0.08074 Min. : 0.0069 Min. :0.8038 Min. :2450100 ## 1st Qu.: 83.1849 1st Qu.: 365.9 1st Qu.:0.63038 1st Qu.: 95.6430 1st Qu.:1.2661 1st Qu.:2457815 ## Median :172.6347 Median : 504.9 Median :0.83288 Median :189.7729 Median :1.6182 Median :2457972 ## Mean :172.1717 Mean : 635.5 Mean :0.81316 Mean :184.0185 Mean :1.9864 Mean :2457726 ## 3rd Qu.:254.8804 3rd Qu.: 793.1 3rd Qu.:0.99718 3rd Qu.:271.9535 3rd Qu.:2.4497 3rd Qu.:2458108 ## Max. :359.9059 Max. :4172.2 Max. :1.29983 Max. :359.9931 Max. :8.9839 Max. :2458839 ## NA&#39;s :60 NA&#39;s :46 NA&#39;s :22 NA&#39;s :48 NA&#39;s :38 NA&#39;s :59 ## Mean.Anomaly Mean.Motion Equinox Hazardous ## Min. : 0.0032 Min. :0.08628 J2000:4663 Min. :0.000 ## 1st Qu.: 87.0069 1st Qu.:0.45147 NA&#39;s : 24 1st Qu.:0.000 ## Median :186.0219 Median :0.71137 Median :0.000 ## Mean :181.2882 Mean :0.73732 Mean :0.176 ## 3rd Qu.:276.6418 3rd Qu.:0.98379 3rd Qu.:0.000 ## Max. :359.9180 Max. :2.03900 Max. :1.000 ## NA&#39;s :40 NA&#39;s :48 NA&#39;s :4187 Impute missing values (not our response variable!): library(missRanger) library(dplyr) set.seed(123) nasa_imputed = missRanger::missRanger(data = nasa %&gt;% select(-Hazardous), maxiter = 1, num.trees = 5L) ## ## Missing value imputation by random forests ## ## Variables to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox ## Variables used to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox ## iter 1: ....................................... nasa_imputed$Hazardous = nasa$Hazardous Split into training and test set: train = nasa_imputed[!is.na(nasa$Hazardous), ] test = nasa_imputed[is.na(nasa$Hazardous), ] Train model: library(ranger) set.seed(123) rf = ranger(Hazardous~., data = train, classification = TRUE, probability = TRUE) Predictions: preds = predict(rf, data = test)$predictions[,2] head(preds) ## [1] 0.6879548 0.7865111 0.0020000 0.7919698 0.1693063 0.1920595 Create submission csv: write.csv(data.frame(y = preds), file = &quot;rf.csv&quot;) 8.5 Flower A collection of over 4000 flower images of 5 plant species. The data set is from kaggle but we downsampled the images from \\(320*240\\) to \\(80*80\\) pixels. You can download the data set here. Notes: Check out convolutional neural network notebooks on kaggle (they are often written in Python but you can still copy the architectures), e.g. this one. Last year’s winners have used a transfer learning approach (they achieved around 70% accuracy), check out this notebook, see also the section about transfer learning 5.4.2. Response variable: “Plant species” Load data set: library(tensorflow) library(keras) set_random_seed(321L, disable_gpu = FALSE) # Already sets R&#39;s random seed. train = EcoData::dataset_flower()$train/255 test = EcoData::dataset_flower()$test/255 labels = EcoData::dataset_flower()$labels Let’s visualize a flower: train[100,,,] %&gt;% image_to_array() %&gt;% as.raster() %&gt;% plot() Build and train model: model = keras_model_sequential() model %&gt;% layer_conv_2d(filters = 4L, kernel_size = 2L, input_shape = list(80L, 80L, 3L)) %&gt;% layer_max_pooling_2d() %&gt;% layer_flatten() %&gt;% layer_dense(units = 5L, activation = &quot;softmax&quot;) ### Model fitting ### epochs = 50L batch_size = 25L steps = floor(dim(train)[1]/batch_size) generator = keras::flow_images_from_data(x = train, y = keras::k_one_hot(labels, 5L), batch_size = batch_size) optim = optimizer_adamax(learning_rate = 0.01) epoch_losses = c() for(e in 1:epochs){ epoch_loss = c() for(s in 1:steps){ batch = reticulate::iter_next(generator) with(tf$GradientTape() %as% tape, { pred = model(batch[[1]]) loss = keras::loss_categorical_crossentropy(batch[[2]], pred) loss = tf$reduce_mean(loss) } ) gradients = tape$gradient(target = loss, sources = model$weights) optim$apply_gradients(purrr::transpose(list(gradients, model$weights))) epoch_loss = c(epoch_loss, loss$numpy()) } epoch_losses = c(epoch_losses, epoch_loss) cat(&quot;Epoch: &quot;, e, &quot; Loss: &quot;, mean(epoch_losses), &quot; \\n&quot;) } Predictions: # Prediction on training data: pred = apply(model %&gt;% predict(train), 1, which.max) Metrics::accuracy(pred - 1L, labels) table(pred) # Prediction for the submission server: pred = model %&gt;% predict(test) %&gt;% apply(1, which.max) - 1L table(pred) Create submission csv: write.csv(data.frame(y = pred), file = &quot;cnn.csv&quot;) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
