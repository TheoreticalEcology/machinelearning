<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Fundamental principles and techniques | Machine Learning and AI in TensorFlow and R</title>
  <meta name="description" content="3 Fundamental principles and techniques | Machine Learning and AI in TensorFlow and R" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Fundamental principles and techniques | Machine Learning and AI in TensorFlow and R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="3 Fundamental principles and techniques | Machine Learning and AI in TensorFlow and R" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Fundamental principles and techniques | Machine Learning and AI in TensorFlow and R" />
  
  <meta name="twitter:description" content="3 Fundamental principles and techniques | Machine Learning and AI in TensorFlow and R" />
  

<meta name="author" content="Maximilian Pichler and Florian Hartig" />


<meta name="date" content="2021-05-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="Deep.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction to Machine Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>2.1</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="introduction.html"><a href="introduction.html#hierarchical-clustering"><i class="fa fa-check"></i><b>2.1.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction.html"><a href="introduction.html#k-means-clustering"><i class="fa fa-check"></i><b>2.1.2</b> k-means clustering</a></li>
<li class="chapter" data-level="2.1.3" data-path="introduction.html"><a href="introduction.html#density-based-clustering"><i class="fa fa-check"></i><b>2.1.3</b> Density-based clustering</a></li>
<li class="chapter" data-level="2.1.4" data-path="introduction.html"><a href="introduction.html#model-based-clustering"><i class="fa fa-check"></i><b>2.1.4</b> Model-based clustering</a></li>
<li class="chapter" data-level="2.1.5" data-path="introduction.html"><a href="introduction.html#ordination"><i class="fa fa-check"></i><b>2.1.5</b> Ordination</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#supervised-learning-regression-and-classification"><i class="fa fa-check"></i><b>2.2</b> Supervised learning: regression and classification</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction.html"><a href="introduction.html#supervised-regression-using-random-forest"><i class="fa fa-check"></i><b>2.2.1</b> Supervised regression using Random Forest</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction.html"><a href="introduction.html#supervised-classification-using-random-forest"><i class="fa fa-check"></i><b>2.2.2</b> Supervised classification using Random Forest</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#introduction-to-tensorflow"><i class="fa fa-check"></i><b>2.3</b> Introduction to Tensorflow</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction.html"><a href="introduction.html#tensorflow-data-containers"><i class="fa fa-check"></i><b>2.3.1</b> Tensorflow data containers</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction.html"><a href="introduction.html#tensorflow-data-types---good-practise-with-r-tf"><i class="fa fa-check"></i><b>2.3.2</b> Tensorflow data types - good practise with R-TF</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#introduction-to-pytorch"><i class="fa fa-check"></i><b>2.4</b> Introduction to PyTorch</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#pytorch-data-containers"><i class="fa fa-check"></i><b>2.4.1</b> PyTorch data containers</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction.html"><a href="introduction.html#torch-data-types---good-practise-with-r-tf"><i class="fa fa-check"></i><b>2.4.2</b> Torch data types - good practise with R-TF</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#first-steps-with-the-keras-framework"><i class="fa fa-check"></i><b>2.5</b> First steps with the keras framework</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introduction.html"><a href="introduction.html#example-workflow-in-keras"><i class="fa fa-check"></i><b>2.5.1</b> Example workflow in keras</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fund.html"><a href="fund.html"><i class="fa fa-check"></i><b>3</b> Fundamental principles and techniques</a>
<ul>
<li class="chapter" data-level="3.1" data-path="fund.html"><a href="fund.html#machine-learning-principles"><i class="fa fa-check"></i><b>3.1</b> Machine learning principles</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="fund.html"><a href="fund.html#optimization"><i class="fa fa-check"></i><b>3.1.1</b> Optimization</a></li>
<li class="chapter" data-level="3.1.2" data-path="fund.html"><a href="fund.html#regularization"><i class="fa fa-check"></i><b>3.1.2</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="fund.html"><a href="fund.html#tree-based-ml-algorithms"><i class="fa fa-check"></i><b>3.2</b> Tree-based ML algorithms</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="fund.html"><a href="fund.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>3.2.1</b> Classification and Regression Trees</a></li>
<li class="chapter" data-level="3.2.2" data-path="fund.html"><a href="fund.html#random-forest"><i class="fa fa-check"></i><b>3.2.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.2.3" data-path="fund.html"><a href="fund.html#boosted-regression-trees"><i class="fa fa-check"></i><b>3.2.3</b> Boosted regression trees</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="fund.html"><a href="fund.html#distance-based-algorithms"><i class="fa fa-check"></i><b>3.3</b> Distance-based algorithms</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="fund.html"><a href="fund.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>3.3.1</b> k-nearest-neighbor</a></li>
<li class="chapter" data-level="3.3.2" data-path="fund.html"><a href="fund.html#support-vector-machines-svm"><i class="fa fa-check"></i><b>3.3.2</b> Support Vector Machines (SVM)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="fund.html"><a href="fund.html#artificial-neural-networks"><i class="fa fa-check"></i><b>3.4</b> Artificial neural networks</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="fund.html"><a href="fund.html#data-cleaning"><i class="fa fa-check"></i><b>3.4.1</b> Data cleaning</a></li>
<li class="chapter" data-level="3.4.2" data-path="fund.html"><a href="fund.html#pre-processing-and-feature-selection"><i class="fa fa-check"></i><b>3.4.2</b> Pre-processing and feature selection</a></li>
<li class="chapter" data-level="3.4.3" data-path="fund.html"><a href="fund.html#split-data-for-training-and-testing"><i class="fa fa-check"></i><b>3.4.3</b> Split data for training and testing</a></li>
<li class="chapter" data-level="3.4.4" data-path="fund.html"><a href="fund.html#model-fitting"><i class="fa fa-check"></i><b>3.4.4</b> Model fitting</a></li>
<li class="chapter" data-level="3.4.5" data-path="fund.html"><a href="fund.html#model-evaluation"><i class="fa fa-check"></i><b>3.4.5</b> Model evaluation</a></li>
<li class="chapter" data-level="3.4.6" data-path="fund.html"><a href="fund.html#predictions-and-submission"><i class="fa fa-check"></i><b>3.4.6</b> Predictions and submission</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Deep.html"><a href="Deep.html"><i class="fa fa-check"></i><b>4</b> Deep learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Deep.html"><a href="Deep.html#deep-neural-networks"><i class="fa fa-check"></i><b>4.1</b> Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="Deep.html"><a href="Deep.html#dropout-and-early-stopping"><i class="fa fa-check"></i><b>4.1.1</b> Dropout and Early stopping</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="Deep.html"><a href="Deep.html#convolutional-neural-networks---mnist"><i class="fa fa-check"></i><b>4.2</b> Convolutional Neural Networks - MNIST</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="Deep.html"><a href="Deep.html#data-augmentation"><i class="fa fa-check"></i><b>4.2.1</b> Data Augmentation</a></li>
<li class="chapter" data-level="4.2.2" data-path="Deep.html"><a href="Deep.html#transfer"><i class="fa fa-check"></i><b>4.2.2</b> Transfer learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Deep.html"><a href="Deep.html#flower-dataset"><i class="fa fa-check"></i><b>4.3</b> Flower dataset</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="xAI.html"><a href="xAI.html"><i class="fa fa-check"></i><b>5</b> Explainable AI (xAI), NLP, and RNNs</a>
<ul>
<li class="chapter" data-level="5.1" data-path="xAI.html"><a href="xAI.html#xai-methods"><i class="fa fa-check"></i><b>5.1</b> xAI Methods</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="xAI.html"><a href="xAI.html#variable-importance"><i class="fa fa-check"></i><b>5.1.1</b> Variable Importance</a></li>
<li class="chapter" data-level="5.1.2" data-path="xAI.html"><a href="xAI.html#partial-dependencies"><i class="fa fa-check"></i><b>5.1.2</b> Partial dependencies</a></li>
<li class="chapter" data-level="5.1.3" data-path="xAI.html"><a href="xAI.html#accumulated-local-effects"><i class="fa fa-check"></i><b>5.1.3</b> Accumulated local effects</a></li>
<li class="chapter" data-level="5.1.4" data-path="xAI.html"><a href="xAI.html#friedmans-h-statistic"><i class="fa fa-check"></i><b>5.1.4</b> Friedmans H-statistic</a></li>
<li class="chapter" data-level="5.1.5" data-path="xAI.html"><a href="xAI.html#global-explainer---simplifying-the-ml-model"><i class="fa fa-check"></i><b>5.1.5</b> Global explainer - Simplifying the ML model</a></li>
<li class="chapter" data-level="5.1.6" data-path="xAI.html"><a href="xAI.html#local-explainer---lime-explaining-single-instances-observations"><i class="fa fa-check"></i><b>5.1.6</b> Local explainer - LIME explaining single instances (observations)</a></li>
<li class="chapter" data-level="5.1.7" data-path="xAI.html"><a href="xAI.html#local-explainer---shapley"><i class="fa fa-check"></i><b>5.1.7</b> Local explainer - Shapley</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="xAI.html"><a href="xAI.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>5.2</b> Natural Language Processing (NLP)</a></li>
<li class="chapter" data-level="5.3" data-path="xAI.html"><a href="xAI.html#recurrent-neural-networks-rnns"><i class="fa fa-check"></i><b>5.3</b> Recurrent neural networks (RNNs)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html"><i class="fa fa-check"></i><b>6</b> GANs, VAEs, and Reinforcement learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#generative-adversarial-network-gans"><i class="fa fa-check"></i><b>6.1</b> Generative adversarial network (GANs)</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#mnist---gan-based-on-dnns"><i class="fa fa-check"></i><b>6.1.1</b> MNIST - GAN based on DNNs</a></li>
<li class="chapter" data-level="6.1.2" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#flower---gan"><i class="fa fa-check"></i><b>6.1.2</b> Flower - GAN</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#autoencoder"><i class="fa fa-check"></i><b>6.2</b> Autoencoder</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#autoencoder---mnist-cnn"><i class="fa fa-check"></i><b>6.2.1</b> Autoencoder - MNIST CNN</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gans-vaes-and-reinforcement-learning.html"><a href="gans-vaes-and-reinforcement-learning.html#varational-autoencoder"><i class="fa fa-check"></i><b>6.3</b> Varational Autoencoder</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i><b>7</b> Datasets</a>
<ul>
<li class="chapter" data-level="7.1" data-path="datasets.html"><a href="datasets.html#titanic"><i class="fa fa-check"></i><b>7.1</b> Titanic</a></li>
<li class="chapter" data-level="7.2" data-path="datasets.html"><a href="datasets.html#plant-pollinator-database"><i class="fa fa-check"></i><b>7.2</b> Plant-pollinator database</a></li>
<li class="chapter" data-level="7.3" data-path="datasets.html"><a href="datasets.html#wine"><i class="fa fa-check"></i><b>7.3</b> Wine</a></li>
<li class="chapter" data-level="7.4" data-path="datasets.html"><a href="datasets.html#nasa"><i class="fa fa-check"></i><b>7.4</b> Nasa</a></li>
<li class="chapter" data-level="7.5" data-path="datasets.html"><a href="datasets.html#flower"><i class="fa fa-check"></i><b>7.5</b> Flower</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning and AI in TensorFlow and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fund" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Fundamental principles and techniques</h1>
<div id="machine-learning-principles" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Machine learning principles</h2>
<div id="optimization" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Optimization</h3>
<p>from wikipedia: " an optimization problem is the problem of finding the best solution from all feasible solutions"</p>
<p>Why do we need this “optimization”?</p>
<p>We need to somehow tell the algorithm what it should learn. To do so we have the so called loss-function, which expresses what our goal is. But we also need to somewhow find the configurations for which the loss function is
minimized. This is the job of the optimizer. Thus, an optimization consists of:</p>
<ul>
<li><p>A loss function (e.g. we tell in each training step the algorithm how many observations were miss-classified) guides the training of ML algorithms</p></li>
<li><p>The optimizer, which tries to update the weights of the ML algorithms in a way that the loss function is minimized</p></li>
</ul>
<p>Calculating analytically the global optima is a non-trivial problem and thus a bunch of diverse optimization algorithms evolved</p>
<p>Some optimization algorithms are inspired by biological systems e.g. Ants, Bee, or even slime algorithms. These optimizers are explained int the following video, have a look:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/X-iSQQgOd1A" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<div id="small-optimization-example" class="section level4" number="3.1.1.1">
<h4><span class="header-section-number">3.1.1.1</span> Small optimization example</h4>
<p>As an easy example for optimization we can think of a quadratic function:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="fund.html#cb136-1" aria-hidden="true" tabindex="-1"></a>func <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">return</span>(x<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>This function is so easy, we can randomly prob it and identify the optimum by plotting</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="fund.html#cb137-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb137-2"><a href="fund.html#cb137-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(a, <span class="fu">func</span>(a))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<p>The smallest value is at x = 0 (to be honest, we can calculate this for this simple case analytically)</p>
<p>We can also use an optimizer with the optim-function (the first argument is the starting value)</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="fund.html#cb138-1" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim</span>(<span class="fl">1.0</span>, func)</span></code></pre></div>
<pre><code>## Warning in optim(1, func): one-dimensional optimization by Nelder-Mead is unreliable:
## use &quot;Brent&quot; or optimize() directly</code></pre>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="fund.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(opt<span class="sc">$</span>par)</span></code></pre></div>
<pre><code>## [1] -8.881784e-16</code></pre>
<p>opt$par will return the best values found by the optimizer, which is really close to zeor :)</p>
</div>
<div id="advanced-optimization-example" class="section level4" number="3.1.1.2">
<h4><span class="header-section-number">3.1.1.2</span> Advanced optimization example</h4>
<p>Optimization is also done when fitting a linear regression model. Thereby, we optimize the weights (intercept and slope). But using lm (y~x) is too simple, we would like to do this by hand to also better understand what optimization is and how it works.</p>
<p>As an example we take the airquality data set. First, we have to be sure to have no NAs in there. Then we split into response (Ozone) and predictors (Month, Day, Solar.R, Wind, Temp).Additionally it is beneficial for the optimizer, when the different predictors have the same support, and thus we scale them.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="fund.html#cb142-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> airquality[<span class="fu">complete.cases</span>(airquality<span class="sc">$</span>Ozone) <span class="sc">&amp;</span> <span class="fu">complete.cases</span>(airquality<span class="sc">$</span>Solar.R),]</span>
<span id="cb142-2"><a href="fund.html#cb142-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb142-3"><a href="fund.html#cb142-3" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> data<span class="sc">$</span>Ozone</span></code></pre></div>
<p>The model we want to optimize: <span class="math inline">\(ozone = Solar.R*X1 + Wind*X2 + Temp*X3 + Month*X4 + Day*X5 + X6\)</span></p>
<p>As the we assume that the residuals are normally distributed, our loss function is the mean squared errors: mean(predicted ozone - true ozone)^2)</p>
<p>Our task is now to find the parameters X1-X6 for which this loss function is the smallest. Therefore, we implement a function, that takes parameters and returns the loss.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="fund.html#cb143-1" aria-hidden="true" tabindex="-1"></a>linear_regression <span class="ot">=</span> <span class="cf">function</span>(w) {</span>
<span id="cb143-2"><a href="fund.html#cb143-2" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> w[<span class="dv">1</span>]<span class="sc">*</span>X[,<span class="dv">1</span>] <span class="sc">+</span> <span class="co"># Solar.R</span></span>
<span id="cb143-3"><a href="fund.html#cb143-3" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">2</span>]<span class="sc">*</span>X[,<span class="dv">2</span>] <span class="sc">+</span> <span class="co"># Wind</span></span>
<span id="cb143-4"><a href="fund.html#cb143-4" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">3</span>]<span class="sc">*</span>X[,<span class="dv">3</span>] <span class="sc">+</span> <span class="co"># Temp</span></span>
<span id="cb143-5"><a href="fund.html#cb143-5" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">4</span>]<span class="sc">*</span>X[,<span class="dv">4</span>] <span class="sc">+</span> <span class="co"># Month</span></span>
<span id="cb143-6"><a href="fund.html#cb143-6" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">5</span>]<span class="sc">*</span>X[,<span class="dv">5</span>] <span class="sc">+</span></span>
<span id="cb143-7"><a href="fund.html#cb143-7" aria-hidden="true" tabindex="-1"></a>         w[<span class="dv">6</span>]         <span class="co"># or X %*% w[1:5] + w[6]</span></span>
<span id="cb143-8"><a href="fund.html#cb143-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># loss  = MSE, we want to find the optimal weights </span></span>
<span id="cb143-9"><a href="fund.html#cb143-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># to minimize the sum of squared residuals</span></span>
<span id="cb143-10"><a href="fund.html#cb143-10" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">mean</span>((pred <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb143-11"><a href="fund.html#cb143-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(loss)</span>
<span id="cb143-12"><a href="fund.html#cb143-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>For example we can sample some weights and see what the loss with this weights is.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="fund.html#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="fu">linear_regression</span>(<span class="fu">runif</span>(<span class="dv">6</span>))</span></code></pre></div>
<pre><code>## [1] 2807.74</code></pre>
<p>We can try to find the optimum bruteforce (which means we will use a random set of weights and see for which the loss function is smallest):</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="fund.html#cb146-1" aria-hidden="true" tabindex="-1"></a>random_search <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">6</span><span class="sc">*</span><span class="dv">5000</span>,<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>), <span class="dv">5000</span>, <span class="dv">6</span>)</span>
<span id="cb146-2"><a href="fund.html#cb146-2" aria-hidden="true" tabindex="-1"></a>losses <span class="ot">=</span> <span class="fu">apply</span>(random_search, <span class="dv">1</span>, linear_regression)</span>
<span id="cb146-3"><a href="fund.html#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(losses, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="fund.html#cb147-1" aria-hidden="true" tabindex="-1"></a>random_search[<span class="fu">which.min</span>(losses),]</span></code></pre></div>
<pre><code>## [1]  4.847733 -9.630324  6.326885 -2.982961  1.211288  9.963248</code></pre>
<p>Bruteforce isn’t a good approach, it might work well with only a few parameters, but with increasing complexity and more parameters it will take a long time.</p>
<p>In R the optim function helps to get faster to the optimum.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="fund.html#cb149-1" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">=</span> <span class="fu">optim</span>(<span class="fu">runif</span>(<span class="dv">6</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), linear_regression)</span>
<span id="cb149-2"><a href="fund.html#cb149-2" aria-hidden="true" tabindex="-1"></a>opt<span class="sc">$</span>par</span></code></pre></div>
<pre><code>## [1]   0.6473966 -20.0175388  21.7380624 -10.2651763  -8.4418507
## [6]  25.5586780</code></pre>
<p>By comparing the weights from the optimizer to the estimated weights of the lm() function, we see that our self-written code obtains the same weights as the lm.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="fund.html#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
</div>
</div>
<div id="regularization" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Regularization</h3>
<p>Regularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a ML model. The most important distinction is between shrinkage estimators and estimators based on model averaging.</p>
<p><strong>Shrikage estimators</strong> are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are <em>“shrunk”</em> to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penality is proportional to the absolute deviation (L1 penalty), and the Tikhonov regularization aka ridge regression, where the penalty is proportional to the squared distance from the reference (L2 penalty). Thus, the loss function that we optimize is thus given by</p>
<p><span class="math display">\[
loss = fit - \lambda \cdot d
\]</span>
where fit refers to the standard loss function, <span class="math inline">\(\lambda\)</span> is the strength of the regularization, and <span class="math inline">\(d\)</span> is the chosen metrics, e.g. L1 or L2. <span class="math inline">\(\lambda\)</span> and possibly d are typically optimized under cross-validation</p>
<p><strong>Model averaging</strong> refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it often more efficient to have many simpler models and average them, than to have one “super model”. The reasons are complicated, and explained in more detail in Dormann et al., 2018. A particular important application of averaging is boosting, where the principle is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap the data, and average the boot- strapped predictions.</p>
<p>To see how these techniques work in practice, let’s first focus on lasso and ridge regularization for weights in neural networks. We can imagine that the lasso and ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunk) to zero.</p>
<p>Lasso (penalty ~ abs(sum(Weights))) and ridge (penalty ~ (sum(Weights))^2) have slightly different properties, which are best understood if we express those as the effective prior preference that they create on the parameters:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-80-1.png" width="672" /></p>
<p>As you can see, the Lasso creates a very strong preferencet towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the ridge. For this reason, Lasso is often interpreted more as a model selection method.</p>
<p>The Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.</p>
<p>We can implement the linear regression also in keras, when we do not specify any hidden layers</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="fund.html#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb153-2"><a href="fund.html#cb153-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> airquality[<span class="fu">complete.cases</span>(airquality),]</span>
<span id="cb153-3"><a href="fund.html#cb153-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb153-4"><a href="fund.html#cb153-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> data<span class="sc">$</span>Ozone</span>
<span id="cb153-5"><a href="fund.html#cb153-5" aria-hidden="true" tabindex="-1"></a><span class="co"># l1/l2 on linear model</span></span>
<span id="cb153-6"><a href="fund.html#cb153-6" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb153-7"><a href="fund.html#cb153-7" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb153-8"><a href="fund.html#cb153-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="fu">dim</span>(X)[<span class="dv">2</span>]))</span>
<span id="cb153-9"><a href="fund.html#cb153-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_1&quot;
## _________________________________________________________________________
## Layer (type)                    Output Shape                  Param #    
## =========================================================================
## dense_4 (Dense)                 (None, 1)                     6          
## =========================================================================
## Total params: 6
## Trainable params: 6
## Non-trainable params: 0
## _________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="fund.html#cb155-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb155-2"><a href="fund.html#cb155-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(<span class="at">loss =</span> loss_mean_squared_error, <span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.5</span>))</span>
<span id="cb155-3"><a href="fund.html#cb155-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb155-4"><a href="fund.html#cb155-4" aria-hidden="true" tabindex="-1"></a> model <span class="sc">%&gt;%</span></span>
<span id="cb155-5"><a href="fund.html#cb155-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, <span class="at">epochs =</span> 50L, <span class="at">batch_size =</span> 20L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb155-6"><a href="fund.html#cb155-6" aria-hidden="true" tabindex="-1"></a>unconstrained <span class="ot">=</span> model<span class="sc">$</span><span class="fu">get_weights</span>()</span>
<span id="cb155-7"><a href="fund.html#cb155-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.014 -12.284  -3.302   8.454  95.348 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.980  21.264  &lt; 2e-16 ***
## XSolar.R       4.583      2.135   2.147   0.0341 *  
## XWind        -11.806      2.293  -5.149 1.23e-06 ***
## XTemp         18.067      2.610   6.922 3.66e-10 ***
## XMonth        -4.479      2.230  -2.009   0.0471 *  
## XDay           2.385      2.000   1.192   0.2358    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20.86 on 105 degrees of freedom
## Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 
## F-statistic: 34.99 on 5 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="fund.html#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
<p>But keras also allows use to use lasso and ridge on the weights.
Lets see what happens when we put a l1 (lasso) regularization on the weights:</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="fund.html#cb159-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb159-2"><a href="fund.html#cb159-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb159-3"><a href="fund.html#cb159-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(<span class="fu">dim</span>(X)[<span class="dv">2</span>]), </span>
<span id="cb159-4"><a href="fund.html#cb159-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(<span class="dv">10</span>), <span class="at">bias_regularizer =</span> <span class="fu">regularizer_l1</span>(<span class="dv">10</span>))</span>
<span id="cb159-5"><a href="fund.html#cb159-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_2&quot;
## _________________________________________________________________________
## Layer (type)                    Output Shape                  Param #    
## =========================================================================
## dense_5 (Dense)                 (None, 1)                     6          
## =========================================================================
## Total params: 6
## Trainable params: 6
## Non-trainable params: 0
## _________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="fund.html#cb161-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb161-2"><a href="fund.html#cb161-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_mean_squared_error, <span class="fu">optimizer_adamax</span>(<span class="at">lr =</span> <span class="fl">0.5</span>), <span class="at">metrics =</span> <span class="fu">c</span>(metric_mean_squared_error))</span>
<span id="cb161-3"><a href="fund.html#cb161-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb161-4"><a href="fund.html#cb161-4" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb161-5"><a href="fund.html#cb161-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> Y, <span class="at">epochs =</span> 30L, <span class="at">batch_size =</span> 20L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb161-6"><a href="fund.html#cb161-6" aria-hidden="true" tabindex="-1"></a>l1 <span class="ot">=</span> model<span class="sc">$</span><span class="fu">get_weights</span>()</span>
<span id="cb161-7"><a href="fund.html#cb161-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.014 -12.284  -3.302   8.454  95.348 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.980  21.264  &lt; 2e-16 ***
## XSolar.R       4.583      2.135   2.147   0.0341 *  
## XWind        -11.806      2.293  -5.149 1.23e-06 ***
## XTemp         18.067      2.610   6.922 3.66e-10 ***
## XMonth        -4.479      2.230  -2.009   0.0471 *  
## XDay           2.385      2.000   1.192   0.2358    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 20.86 on 105 degrees of freedom
## Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 
## F-statistic: 34.99 on 5 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="fund.html#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X))</span></code></pre></div>
<pre><code>## (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay 
##   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705</code></pre>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="fund.html#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(<span class="fu">unlist</span>(l1), <span class="fu">unlist</span>(unconstrained))</span></code></pre></div>
<pre><code>##             [,1]       [,2]
## [1,]  1.72543979   4.652931
## [2,] -8.79228878 -11.969406
## [3,] 13.00332832  17.423628
## [4,]  0.03124749  -4.100816
## [5,] -0.01484923   2.308330
## [6,] 33.83497238  41.373863</code></pre>
<p>One can clearly see that parameters are pulled towards zero because of the regularization.</p>
</div>
</div>
<div id="tree-based-ml-algorithms" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Tree-based ML algorithms</h2>
<p>Famous ML algorithms such as random Forest and gradient boosted trees are based on classification- and regression trees.</p>
<div id="classification-and-regression-trees" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Classification and Regression Trees</h3>
<p>Tree-based models in general use a series of if-then rules to generate predictions from one or more decision trees.
In this lecture, we will explore regression and classifaction trees at the example of the airquality data set. There is one important hyper-parameter for regression trees: minsplit</p>
<ul>
<li>it controls the depth of tree (see the help of rpart for a description)</li>
<li>it controls the complexity of the tree and thus also be seen as a regularization parameter</li>
</ul>
<p>We first prepare and visualize the data and afterwards fit a decision tree.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="fund.html#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;rpart&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dendextend&#39;:
## 
##     prune</code></pre>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="fund.html#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb170-2"><a href="fund.html#cb170-2" aria-hidden="true" tabindex="-1"></a>data<span class="ot">=</span>airquality[<span class="fu">complete.cases</span>(airquality),]</span></code></pre></div>
<p>Fit and visualize a regression tree:</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="fund.html#cb171-1" aria-hidden="true" tabindex="-1"></a>rt <span class="ot">=</span> <span class="fu">rpart</span>(Ozone<span class="sc">~</span>., <span class="at">data =</span> data,<span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">10</span>))</span>
<span id="cb171-2"><a href="fund.html#cb171-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(rt)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-84-1.png" width="672" /></p>
<p>Visualize the predictions:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="fund.html#cb172-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(rt, data)</span>
<span id="cb172-2"><a href="fund.html#cb172-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(data<span class="sc">$</span>Temp, data<span class="sc">$</span>Ozone)</span>
<span id="cb172-3"><a href="fund.html#cb172-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data<span class="sc">$</span>Temp[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], pred[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>The angular form of the prediction line is typical for regression trees and is a weakness of it.</p>
</div>
<div id="random-forest" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Random Forest</h3>
<p>To overcome this weakness, a random forest uses an ensemble of regression/classification trees. Thus, the random forest is in principle nothing else than a normal regression/classification tree, but it uses the idea of the “wisdom of the crowd”: By asking many people (regression/classification trees) one can make a more informed decision (prediction/classification). When you buy a new phone for example you would also no directly go into the shop, but search in the internet and ask your friends and family.</p>
<p>There are two randomization steps with the RF that are responsible for the success of RF:</p>
<ul>
<li>bootstrap sample for each tree (we will sample observations with replacement from the dataset, for the phone this is like that not everyone has experience about each phone)</li>
<li>at each split, we will sample a subset of predictors which are then considered as potential splitting criterion (for the phone this is like that not everyone has the same decision criteria).</li>
</ul>
<p>Applying the random forest follows the same principle as for the methods before: we visualize the data (we have already done this so often for the airquality data set, thus we skip it here), fit the algorithm and then plot the outcomes.</p>
<p>Fit a RF and visualize the predictions:</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="fund.html#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb173-2"><a href="fund.html#cb173-2" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">=</span> <span class="fu">randomForest</span>(Ozone<span class="sc">~</span>., <span class="at">data =</span> data)</span>
<span id="cb173-3"><a href="fund.html#cb173-3" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(rf, data)</span>
<span id="cb173-4"><a href="fund.html#cb173-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Ozone<span class="sc">~</span>Temp, <span class="at">data =</span> data)</span>
<span id="cb173-5"><a href="fund.html#cb173-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(data<span class="sc">$</span>Temp[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], pred[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<p>One advantage of RF is that we will get a variable importance. At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Thus the variable importance shows us how important a variable is averaged over all trees.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="fund.html#cb174-1" aria-hidden="true" tabindex="-1"></a>rf<span class="sc">$</span>importance</span></code></pre></div>
<pre><code>##         IncNodePurity
## Solar.R      18081.79
## Wind         31079.19
## Temp         35426.69
## Month        10975.86
## Day          14765.34</code></pre>
<p>There are several important hyperparameters in a random forest, that we can tune to get better results:</p>
<ul>
<li>Similar to the minsplit parameter in regression and classification trees, the hyper parameter nodesize controls for complexity -&gt; Minimum size of terminal nodes in the tree. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).</li>
<li>mtry - Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times.</li>
</ul>
</div>
<div id="boosted-regression-trees" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Boosted regression trees</h3>
<p>RF fits hundreds of trees independent of each other. Here, the idea of a boosted regression tree comes in. Maybe we could learn from the errors the previous weak learners make and thus enhance the performance of the algorithm.</p>
<p>Thus, a boosted regression tree (BRT) starts with a simple regression tree (weak learner) and then fits sequentially additional trees to improve the results.
There are two different strategies to do so:</p>
<ul>
<li>AdaBoost, wrong classified observations (by the previous tree) will get a higher weight and therefore the next trees will focus on difficult/missclassified observations.</li>
<li>Gradient boosting (state of the art), each sequential model will be fit on the residual errors of the previous model.</li>
</ul>
<p>We can fit a BRT using xgboost, but before we have to transform the data into a xgb.Dmatrix.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="fund.html#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb176-2"><a href="fund.html#cb176-2" aria-hidden="true" tabindex="-1"></a>data_xg <span class="ot">=</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])), <span class="at">label =</span> data<span class="sc">$</span>Ozone)</span>
<span id="cb176-3"><a href="fund.html#cb176-3" aria-hidden="true" tabindex="-1"></a>brt <span class="ot">=</span> <span class="fu">xgboost</span>(data_xg, <span class="at">nrounds =</span> 16L, <span class="at">nthreads =</span> 4L)</span></code></pre></div>
<pre><code>## [23:00:08] WARNING: amalgamation/../src/learner.cc:516: 
## Parameters: { nthreads } might not be used.
## 
##   This may not be accurate due to some parameters are only used in language bindings but
##   passed down to XGBoost core.  Or some parameters are not used but slip through this
##   verification. Please open an issue if you find above cases.
## 
## 
## [1]  train-rmse:39.724625 
## [2]  train-rmse:30.225761 
## [3]  train-rmse:23.134842 
## [4]  train-rmse:17.899178 
## [5]  train-rmse:14.097784 
## [6]  train-rmse:11.375458 
## [7]  train-rmse:9.391275 
## [8]  train-rmse:7.889690 
## [9]  train-rmse:6.646585 
## [10] train-rmse:5.804859 
## [11] train-rmse:5.128438 
## [12] train-rmse:4.456416 
## [13] train-rmse:4.069464 
## [14] train-rmse:3.674615 
## [15] train-rmse:3.424578 
## [16] train-rmse:3.191301</code></pre>
<p>The nrounds controls how many sequantial trees we fit, in our example this was 16. When we predict to new data, we can limit the number of trees used to prevent overfitting (remeber: each new tree tries to improve the predictions of the previous trees).</p>
<p>Let us visualize the predictions for different number of trees:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="fund.html#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb178-2"><a href="fund.html#cb178-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>){</span>
<span id="cb178-3"><a href="fund.html#cb178-3" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">predict</span>(brt, <span class="at">newdata =</span> data_xg, <span class="at">ntreelimit =</span> i)</span>
<span id="cb178-4"><a href="fund.html#cb178-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(data<span class="sc">$</span>Temp, data<span class="sc">$</span>Ozone, <span class="at">main =</span> i)</span>
<span id="cb178-5"><a href="fund.html#cb178-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(data<span class="sc">$</span>Temp[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], pred[<span class="fu">order</span>(data<span class="sc">$</span>Temp)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb178-6"><a href="fund.html#cb178-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="_main_files/figure-html/BRT2-1.png" width="672" />
There are also other ways to control for complexity of the BRT algorithm:</p>
<ul>
<li>max_depth, depth of each tree</li>
<li>shrinkage (each tree will get a weight and the weight will decrease with the number of trees)</li>
</ul>
<p>When having specified the final model, we can as for random forests get a variable importance:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="fund.html#cb179-1" aria-hidden="true" tabindex="-1"></a>xgboost<span class="sc">::</span><span class="fu">xgb.importance</span>(<span class="at">model =</span> brt)</span></code></pre></div>
<pre><code>##    Feature        Gain     Cover  Frequency
## 1:    Temp 0.570071875 0.2958229 0.24836601
## 2:    Wind 0.348230710 0.3419576 0.24183007
## 3: Solar.R 0.058795559 0.1571072 0.30718954
## 4:     Day 0.019530002 0.1779925 0.16993464
## 5:   Month 0.003371853 0.0271197 0.03267974</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="fund.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">mean</span>((data<span class="sc">$</span>Ozone <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># RMSE</span></span></code></pre></div>
<pre><code>## [1] 17.89918</code></pre>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="fund.html#cb183-1" aria-hidden="true" tabindex="-1"></a>data_xg <span class="ot">=</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(data[,<span class="sc">-</span><span class="dv">1</span>])), <span class="at">label =</span> data<span class="sc">$</span>Ozone)</span></code></pre></div>
<p>One important strength of xgboost is that we can directly do a cross-validation and specify its properties with nfold (the original dataset is randomly partitioned intonfoldequal size subsamples and each time one of these data sets is used for predictions to judge the performance):</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="fund.html#cb184-1" aria-hidden="true" tabindex="-1"></a>brt <span class="ot">=</span> <span class="fu">xgboost</span>(data_xg, <span class="at">nrounds =</span> 5L)</span></code></pre></div>
<pre><code>## [1]  train-rmse:39.724625 
## [2]  train-rmse:30.225760 
## [3]  train-rmse:23.134842 
## [4]  train-rmse:17.899178 
## [5]  train-rmse:14.097785</code></pre>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="fund.html#cb186-1" aria-hidden="true" tabindex="-1"></a>brt_cv <span class="ot">=</span> xgboost<span class="sc">::</span><span class="fu">xgb.cv</span>(<span class="at">data =</span> data_xg, <span class="at">nfold =</span> 3L, <span class="at">nrounds =</span> 3L, <span class="at">nthreads =</span> 4L)</span></code></pre></div>
<pre><code>## [1]  train-rmse:39.671415+3.285518   test-rmse:40.210448+7.533677 
## [2]  train-rmse:30.239256+2.645637   test-rmse:32.283635+7.998225 
## [3]  train-rmse:23.291644+2.050674   test-rmse:28.101882+7.670404</code></pre>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="fund.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(brt_cv)</span></code></pre></div>
<pre><code>## ##### xgb.cv 3-folds
##  iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std
##     1        39.67141       3.285518       40.21045      7.533677
##     2        30.23926       2.645637       32.28363      7.998225
##     3        23.29164       2.050674       28.10188      7.670404</code></pre>
<p>This now tells us how well the model performed.</p>
</div>
</div>
<div id="distance-based-algorithms" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Distance-based algorithms</h2>
<p>In this chapter, we introduce support-vector machines (SVMs) and other distance-based methods.</p>
<div id="k-nearest-neighbor" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> k-nearest-neighbor</h3>
<p>K Nearest Neighbour (kNN) is a simple algorithm that stores all the available cases and classifies the new data based on a similarity measure. It is mostly used to classifies a data point based on how its k nearest neighbours are classified.</p>
<p>Let us first see an example:</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="fund.html#cb190-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb190-2"><a href="fund.html#cb190-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> iris[,<span class="dv">5</span>]</span>
<span id="cb190-3"><a href="fund.html#cb190-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X[<span class="sc">-</span><span class="dv">100</span>,<span class="dv">1</span>], X[<span class="sc">-</span><span class="dv">100</span>,<span class="dv">3</span>], <span class="at">col =</span> Y)</span>
<span id="cb190-4"><a href="fund.html#cb190-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X[<span class="dv">100</span>,<span class="dv">1</span>], X[<span class="dv">100</span>,<span class="dv">3</span>], <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">18</span>, <span class="at">cex =</span> <span class="fl">1.3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>Which class would you decide for the blue point? What are the classes of the nearest points? Well this procedure is used by the kNN and thus there is actually no “real” learning in a kNN.</p>
<p>For applying a kNN, we first have to scale teh data set, because we deal with distances and a priori want the same influence of all predictors (image one variable has values from -10.000 to 10.000 and one from -1 to 1, then the influence of the first variable on the distance to the other points is stronger than the second variable). As in the iris-data set there are no real test, we also have to split the data into train and test. Then we will follow the usual pipeline.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="fund.html#cb191-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> iris</span>
<span id="cb191-2"><a href="fund.html#cb191-2" aria-hidden="true" tabindex="-1"></a>data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">=</span> <span class="fu">apply</span>(data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>],<span class="dv">2</span>, scale)</span>
<span id="cb191-3"><a href="fund.html#cb191-3" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(data), <span class="fl">0.7</span><span class="sc">*</span><span class="fu">nrow</span>(data))</span>
<span id="cb191-4"><a href="fund.html#cb191-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[indices,]</span>
<span id="cb191-5"><a href="fund.html#cb191-5" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data[<span class="sc">-</span>indices,]</span></code></pre></div>
<p>Fit model and create predictions:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="fund.html#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kknn)</span>
<span id="cb192-2"><a href="fund.html#cb192-2" aria-hidden="true" tabindex="-1"></a>knn <span class="ot">=</span> <span class="fu">kknn</span>(Species<span class="sc">~</span>., <span class="at">train =</span> train, <span class="at">test =</span> test)</span>
<span id="cb192-3"><a href="fund.html#cb192-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(knn)</span></code></pre></div>
<pre><code>## 
## Call:
## kknn(formula = Species ~ ., train = train, test = test)
## 
## Response: &quot;nominal&quot;
##           fit prob.setosa prob.versicolor prob.virginica
## 1      setosa   1.0000000      0.00000000      0.0000000
## 2      setosa   1.0000000      0.00000000      0.0000000
## 3      setosa   1.0000000      0.00000000      0.0000000
## 4      setosa   1.0000000      0.00000000      0.0000000
## 5      setosa   1.0000000      0.00000000      0.0000000
## 6      setosa   1.0000000      0.00000000      0.0000000
## 7      setosa   1.0000000      0.00000000      0.0000000
## 8      setosa   0.7200657      0.27993434      0.0000000
## 9      setosa   1.0000000      0.00000000      0.0000000
## 10     setosa   1.0000000      0.00000000      0.0000000
## 11     setosa   1.0000000      0.00000000      0.0000000
## 12 versicolor   0.0000000      1.00000000      0.0000000
## 13 versicolor   0.0000000      1.00000000      0.0000000
## 14 versicolor   0.0000000      1.00000000      0.0000000
## 15 versicolor   0.0000000      1.00000000      0.0000000
## 16 versicolor   0.0000000      1.00000000      0.0000000
## 17 versicolor   0.0000000      1.00000000      0.0000000
## 18 versicolor   0.0000000      1.00000000      0.0000000
## 19 versicolor   0.0000000      1.00000000      0.0000000
## 20 versicolor   0.0000000      1.00000000      0.0000000
## 21 versicolor   0.0000000      1.00000000      0.0000000
## 22 versicolor   0.0000000      1.00000000      0.0000000
## 23 versicolor   0.0000000      1.00000000      0.0000000
## 24  virginica   0.0000000      0.00000000      1.0000000
## 25  virginica   0.0000000      0.23111986      0.7688801
## 26  virginica   0.0000000      0.14963308      0.8503669
## 27  virginica   0.0000000      0.00000000      1.0000000
## 28  virginica   0.0000000      0.00000000      1.0000000
## 29  virginica   0.0000000      0.04881448      0.9511855
## 30  virginica   0.0000000      0.01569160      0.9843084
## 31  virginica   0.0000000      0.40570858      0.5942914
## 32 versicolor   0.0000000      0.85036692      0.1496331
## 33  virginica   0.0000000      0.44203121      0.5579688
## 34  virginica   0.0000000      0.00000000      1.0000000
## 35  virginica   0.0000000      0.38075294      0.6192471
## 36  virginica   0.0000000      0.04881448      0.9511855
## 37  virginica   0.0000000      0.00000000      1.0000000
## 38 versicolor   0.0000000      1.00000000      0.0000000
## 39  virginica   0.0000000      0.00000000      1.0000000
## 40  virginica   0.0000000      0.36506134      0.6349387
## 41  virginica   0.0000000      0.00000000      1.0000000
## 42  virginica   0.0000000      0.23111986      0.7688801
## 43  virginica   0.0000000      0.00000000      1.0000000
## 44  virginica   0.0000000      0.17288113      0.8271189
## 45  virginica   0.0000000      0.39948408      0.6005159</code></pre>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="fund.html#cb194-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(test<span class="sc">$</span>Species, <span class="fu">fitted</span>(knn))</span></code></pre></div>
<pre><code>##             
##              setosa versicolor virginica
##   setosa         11          0         0
##   versicolor      0         12         0
##   virginica       0          2        20</code></pre>
</div>
<div id="support-vector-machines-svm" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Support Vector Machines (SVM)</h3>
<p>Support vectors machines have a different approach. They try to divide the predictor space into spaces sectors for each class. To do so a SVM fits the parameters of a hyperplane (a n-1 dimensional subspace in a n-dimensional space) in the predictor space by optimizing the distance between the hyperlane and the nearest point from each class.</p>
<p>Fitting a SVM:</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="fund.html#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb196-2"><a href="fund.html#cb196-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> iris</span>
<span id="cb196-3"><a href="fund.html#cb196-3" aria-hidden="true" tabindex="-1"></a>data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">=</span> <span class="fu">apply</span>(data[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>],<span class="dv">2</span>, scale)</span>
<span id="cb196-4"><a href="fund.html#cb196-4" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(data), <span class="fl">0.7</span><span class="sc">*</span><span class="fu">nrow</span>(data))</span>
<span id="cb196-5"><a href="fund.html#cb196-5" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data[indices,]</span>
<span id="cb196-6"><a href="fund.html#cb196-6" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data[<span class="sc">-</span>indices,]</span>
<span id="cb196-7"><a href="fund.html#cb196-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb196-8"><a href="fund.html#cb196-8" aria-hidden="true" tabindex="-1"></a>sm <span class="ot">=</span> <span class="fu">svm</span>(Species<span class="sc">~</span>., <span class="at">data =</span> train, <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>)</span>
<span id="cb196-9"><a href="fund.html#cb196-9" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> <span class="fu">predict</span>(sm, <span class="at">newdata =</span> test)</span></code></pre></div>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="fund.html#cb197-1" aria-hidden="true" tabindex="-1"></a>oldpar <span class="ot">=</span> <span class="fu">par</span>()</span>
<span id="cb197-2"><a href="fund.html#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb197-3"><a href="fund.html#cb197-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(test<span class="sc">$</span>Sepal.Length, test<span class="sc">$</span>Petal.Length, <span class="at">col =</span>  pred, <span class="at">main =</span> <span class="st">&quot;predicted&quot;</span>)</span>
<span id="cb197-4"><a href="fund.html#cb197-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(test<span class="sc">$</span>Sepal.Length, test<span class="sc">$</span>Petal.Length, <span class="at">col =</span>  test<span class="sc">$</span>Species, <span class="at">main =</span> <span class="st">&quot;observed&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="fund.html#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(oldpar)</span></code></pre></div>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;cin&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;cra&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;csi&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;cxy&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;din&quot; cannot be set</code></pre>
<pre><code>## Warning in par(oldpar): graphical parameter &quot;page&quot; cannot be set</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="fund.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(pred<span class="sc">==</span>test<span class="sc">$</span>Species) <span class="co"># accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.9333333</code></pre>
<p>SVM can only work on linear separable problems (A problem is called linearly separable if there exists at least one line in the plane with all of the points of one class on one side of the hyperplane and all the points of the others classes on the other side).</p>
<p>If this is not possible, we however, can use the so called kernel trick, which maps the predictor space into a (higher dimensional) space in which the problem is linear separable. After having identified the boundaries in the higher-dimensional space, we can project them back into the original dimensions.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="fund.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb207-2"><a href="fund.html#cb207-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb207-3"><a href="fund.html#cb207-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb207-4"><a href="fund.html#cb207-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">expand.grid</span>(x1, x2)</span>
<span id="cb207-5"><a href="fund.html#cb207-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">apply</span>(X, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">exp</span>(<span class="sc">-</span>x[<span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> x[<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb207-6"><a href="fund.html#cb207-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>y)) <span class="sc">&lt;</span> <span class="fl">0.62</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb207-7"><a href="fund.html#cb207-7" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">matrix</span>(y, <span class="dv">100</span>, <span class="dv">100</span>))</span>
<span id="cb207-8"><a href="fund.html#cb207-8" aria-hidden="true" tabindex="-1"></a>animation<span class="sc">::</span><span class="fu">saveGIF</span>({</span>
<span id="cb207-9"><a href="fund.html#cb207-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">c</span>(<span class="st">&quot;truth&quot;</span>,<span class="st">&quot;linear&quot;</span>, <span class="st">&quot;radial&quot;</span>, <span class="st">&quot;sigmoid&quot;</span>)) {</span>
<span id="cb207-10"><a href="fund.html#cb207-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="st">&quot;truth&quot;</span>){</span>
<span id="cb207-11"><a href="fund.html#cb207-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">image</span>(<span class="fu">matrix</span>(y, <span class="dv">100</span>,<span class="dv">100</span>),<span class="at">main =</span> <span class="st">&quot;Ground truth&quot;</span>,<span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb207-12"><a href="fund.html#cb207-12" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb207-13"><a href="fund.html#cb207-13" aria-hidden="true" tabindex="-1"></a>      sv <span class="ot">=</span> e1071<span class="sc">::</span><span class="fu">svm</span>(<span class="at">x =</span> X, <span class="at">y =</span> <span class="fu">factor</span>(y), <span class="at">kernel =</span> i)</span>
<span id="cb207-14"><a href="fund.html#cb207-14" aria-hidden="true" tabindex="-1"></a>      <span class="fu">image</span>(<span class="fu">matrix</span>(<span class="fu">as.numeric</span>(<span class="fu">as.character</span>(<span class="fu">predict</span>(sv, X))), <span class="dv">100</span>,<span class="dv">100</span>),<span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Kernel: &quot;</span>, i),<span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb207-15"><a href="fund.html#cb207-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="at">labels =</span> <span class="fu">round</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="dv">1</span>))</span>
<span id="cb207-16"><a href="fund.html#cb207-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">axis</span>(<span class="dv">2</span>, <span class="at">at =</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="at">labels =</span> <span class="fu">round</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">10</span>), <span class="dv">1</span>), <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb207-17"><a href="fund.html#cb207-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb207-18"><a href="fund.html#cb207-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb207-19"><a href="fund.html#cb207-19" aria-hidden="true" tabindex="-1"></a>},<span class="at">movie.name =</span> <span class="st">&quot;svm.gif&quot;</span>, <span class="at">autobrowse =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="images/svm.gif" /><!-- --></p>
<p>As you have seen this does not work with each kernel. Thus, the problem is to find the actual correct kernel, which is again an optimization procedure and can thus be approximated.</p>
</div>
</div>
<div id="artificial-neural-networks" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Artificial neural networks</h2>
<p>Now, we will come to artificial neural networks (ANNs), for which the topic of regularization is also important. We can specify the regularization in each layer via the kernel_regularization argument.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="fund.html#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb208-2"><a href="fund.html#cb208-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> airquality</span>
<span id="cb208-3"><a href="fund.html#cb208-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      Ozone           Solar.R           Wind             Temp      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  
##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  
##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  
##  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  
##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##  NA&#39;s   :37       NA&#39;s   :7                                       
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0  
## </code></pre>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="fund.html#cb210-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> data[<span class="fu">complete.cases</span>(data),] <span class="co"># remove NAs</span></span>
<span id="cb210-2"><a href="fund.html#cb210-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      Ozone          Solar.R           Wind            Temp      
##  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  
##  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  
##  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  
##  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  
##  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  
##  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  
##      Month            Day       
##  Min.   :5.000   Min.   : 1.00  
##  1st Qu.:6.000   1st Qu.: 9.00  
##  Median :7.000   Median :16.00  
##  Mean   :7.216   Mean   :15.95  
##  3rd Qu.:9.000   3rd Qu.:22.50  
##  Max.   :9.000   Max.   :31.00</code></pre>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="fund.html#cb212-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">scale</span>(data[,<span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>])</span>
<span id="cb212-2"><a href="fund.html#cb212-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> data[,<span class="dv">1</span>]</span>
<span id="cb212-3"><a href="fund.html#cb212-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb212-4"><a href="fund.html#cb212-4" aria-hidden="true" tabindex="-1"></a>penalty <span class="ot">=</span> <span class="fl">0.01</span></span>
<span id="cb212-5"><a href="fund.html#cb212-5" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb212-6"><a href="fund.html#cb212-6" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">input_shape =</span> <span class="fu">list</span>(5L), <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty)) <span class="sc">%&gt;%</span></span>
<span id="cb212-7"><a href="fund.html#cb212-7" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty) ) <span class="sc">%&gt;%</span></span>
<span id="cb212-8"><a href="fund.html#cb212-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 100L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty)) <span class="sc">%&gt;%</span></span>
<span id="cb212-9"><a href="fund.html#cb212-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">layer_dense</span>(<span class="at">units =</span> 1L, <span class="at">activation =</span> <span class="st">&quot;linear&quot;</span>, <span class="at">kernel_regularizer =</span> <span class="fu">regularizer_l1</span>(penalty)) <span class="co"># one output dimension with a linear activation function</span></span>
<span id="cb212-10"><a href="fund.html#cb212-10" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_3&quot;
## _________________________________________________________________________
## Layer (type)                    Output Shape                  Param #    
## =========================================================================
## dense_6 (Dense)                 (None, 100)                   600        
## _________________________________________________________________________
## dense_7 (Dense)                 (None, 100)                   10100      
## _________________________________________________________________________
## dense_8 (Dense)                 (None, 100)                   10100      
## _________________________________________________________________________
## dense_9 (Dense)                 (None, 1)                     101        
## =========================================================================
## Total params: 20,901
## Trainable params: 20,901
## Non-trainable params: 0
## _________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="fund.html#cb214-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb214-2"><a href="fund.html#cb214-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">compile</span>(<span class="at">loss =</span> loss_mean_squared_error, keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="fl">0.1</span>))</span>
<span id="cb214-3"><a href="fund.html#cb214-3" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb214-4"><a href="fund.html#cb214-4" aria-hidden="true" tabindex="-1"></a> model <span class="sc">%&gt;%</span></span>
<span id="cb214-5"><a href="fund.html#cb214-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">fit</span>(<span class="at">x =</span> X, <span class="at">y =</span> <span class="fu">matrix</span>(Y, <span class="at">ncol =</span> 1L), <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 20L, <span class="at">shuffle =</span> <span class="cn">TRUE</span>, <span class="at">validation_split =</span> <span class="fl">0.2</span>)</span>
<span id="cb214-6"><a href="fund.html#cb214-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="fund.html#cb216-1" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">=</span> <span class="fu">lapply</span>(model<span class="sc">$</span>weights, <span class="cf">function</span>(w) w<span class="sc">$</span><span class="fu">numpy</span>() )</span>
<span id="cb216-2"><a href="fund.html#cb216-2" aria-hidden="true" tabindex="-1"></a>fields<span class="sc">::</span><span class="fu">image.plot</span>(weights[[<span class="dv">1</span>]])</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-95-2.png" width="672" /></p>
<p>Additionally to the usual l1 and l2 regularisation there is an additional regularisation: the so called dropout-layer (we will learn about this in more detail later).</p>
<p>Before we specialise on any tuning it is important to understand that ML always consists of a pipeline of actions.
## The standard ML pipeline at the example of the titanic dataset
The typical ML workflow consist of:</p>
<ul>
<li>Data cleaning and exploration (EDA=explorative data analysis) with tidyverse</li>
<li>Pre-processing and feature selection</li>
<li>Splitting dataset into train and test set for evaluation</li>
<li>Model fitting</li>
<li>Model evaluation</li>
<li>New predictions
Here is an (optional) video that explains the entire pipeline from a slightly different perspective</li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/nKW8Ndu7Mjw" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>In the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics can be found here</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/nRtp7wSEtJA" frameborder="0" allow="accelerometer; autoplay; encrypted-media;
  gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Another good reference is R for data science by Hadley <a href="https://r4ds.had.co.nz/"></a></p>
<p>For this lecture you need the titanic dataset provided by us. You can find it in GRIPS (datasets.RData in the dataset and submission section) or at <a href="http://rhsbio6.uni-regensburg.de:8500"></a>.</p>
<p>We have split the dataset already into training and testing datasets (the test split has one column less than the train split, as the result is not known a priori for the test)</p>
<div id="data-cleaning" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Data cleaning</h3>
<p>Load necessary libraries:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="fund.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb217-2"><a href="fund.html#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tensorflow)</span>
<span id="cb217-3"><a href="fund.html#cb217-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## Registered S3 method overwritten by &#39;cli&#39;:
##   method     from
##   print.tree tree</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────── tidyverse 1.3.0 ──</code></pre>
<pre><code>## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.0     ✓ stringr 1.4.0
## ✓ readr   1.4.0     ✓ forcats 0.5.0
## ✓ purrr   0.3.4</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::combine()       masks randomForest::combine()
## x dplyr::filter()        masks stats::filter()
## x dplyr::lag()           masks stats::lag()
## x purrr::map()           masks mclust::map()
## x randomForest::margin() masks ggplot2::margin()
## x dplyr::slice()         masks xgboost::slice()</code></pre>
<p>Load dataset:</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="fund.html#cb222-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;datasets.RData&quot;</span>)</span>
<span id="cb222-2"><a href="fund.html#cb222-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> titanic</span></code></pre></div>
<p>Standard summaries:</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="fund.html#cb223-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(data)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1309 obs. of  14 variables:
##  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...
##  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...
##  $ name     : chr  &quot;Sinkkonen, Miss. Anna&quot; &quot;Woolner, Mr. Hugh&quot; &quot;Sage, Mr. Douglas Bullen&quot; &quot;Palsson, Master. Paul Folke&quot; ...
##  $ sex      : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 2 2 1 1 1 ...
##  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...
##  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...
##  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...
##  $ ticket   : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 221 123 779 542 589 873 472 823 588 834 ...
##  $ fare     : num  13 35.5 69.55 21.07 8.05 ...
##  $ cabin    : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 1 94 1 1 1 1 1 1 1 1 ...
##  $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 4 4 4 4 4 4 2 4 2 ...
##  $ boat     : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 3 28 1 1 1 1 1 19 1 15 ...
##  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...
##  $ home.dest: Factor w/ 370 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 121 213 1 1 1 1 322 350 1 1 ...</code></pre>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="fund.html#cb225-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      pclass         survived          name               sex     
##  Min.   :1.000   Min.   :0.0000   Length:1309        female:466  
##  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  
##  Median :3.000   Median :0.0000   Mode  :character               
##  Mean   :2.295   Mean   :0.3853                                  
##  3rd Qu.:3.000   3rd Qu.:1.0000                                  
##  Max.   :3.000   Max.   :1.0000                                  
##                  NA&#39;s   :655                                     
##       age              sibsp            parch            ticket    
##  Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  
##  1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  
##  Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  
##  Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  
##  3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  
##  Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  
##  NA&#39;s   :263                                        (Other) :1261  
##       fare                     cabin      embarked      boat    
##  Min.   :  0.000                  :1014    :  2           :823  
##  1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  
##  Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  
##  Mean   : 33.295   G6             :   5   S:914    15     : 37  
##  3rd Qu.: 31.275   B96 B98        :   4            14     : 33  
##  Max.   :512.329   C22 C26        :   4            4      : 31  
##  NA&#39;s   :1         (Other)        : 271            (Other):308  
##       body                      home.dest  
##  Min.   :  1.0                       :564  
##  1st Qu.: 72.0   New York, NY        : 64  
##  Median :155.0   London              : 14  
##  Mean   :160.8   Montreal, PQ        : 10  
##  3rd Qu.:256.0   Cornwall / Akron, OH:  9  
##  Max.   :328.0   Paris, France       :  9  
##  NA&#39;s   :1188    (Other)             :639</code></pre>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="fund.html#cb227-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)</span></code></pre></div>
<pre><code>##      pclass survived                         name    sex  age sibsp
## 561       2        1        Sinkkonen, Miss. Anna female 30.0     0
## 321       1        1            Woolner, Mr. Hugh   male   NA     0
## 1177      3        0     Sage, Mr. Douglas Bullen   male   NA     8
## 1098      3        0  Palsson, Master. Paul Folke   male  6.0     3
## 1252      3        0   Tomlin, Mr. Ernest Portage   male 30.5     0
## 1170      3        0 Saether, Mr. Simon Sivertsen   male 38.5     0
##      parch             ticket   fare cabin embarked boat body
## 561      0             250648 13.000              S   10   NA
## 321      0              19947 35.500   C52        S    D   NA
## 1177     2           CA. 2343 69.550              S        NA
## 1098     1             349909 21.075              S        NA
## 1252     0             364499  8.050              S        50
## 1170     0 SOTON/O.Q. 3101262  7.250              S        32
##                     home.dest
## 561  Finland / Washington, DC
## 321           London, England
## 1177                         
## 1098                         
## 1252                         
## 1170</code></pre>
<p>The name variable consists of 1309 unique factors (there are 1309 observations…):</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="fund.html#cb229-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>name))</span></code></pre></div>
<pre><code>## [1] 1307</code></pre>
<p>However, there is a title in each name. Let’s extract the titles:</p>
<ol style="list-style-type: decimal">
<li>we will extract all names and split each name after each comma “,”</li>
<li>we will split the second split of the name after a point “.” and extract the titles</li>
</ol>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="fund.html#cb231-1" aria-hidden="true" tabindex="-1"></a>first_split <span class="ot">=</span> <span class="fu">sapply</span>(data<span class="sc">$</span>name, <span class="cf">function</span>(x) stringr<span class="sc">::</span><span class="fu">str_split</span>(x, <span class="at">pattern =</span> <span class="st">&quot;,&quot;</span>)[[<span class="dv">1</span>]][<span class="dv">2</span>])</span>
<span id="cb231-2"><a href="fund.html#cb231-2" aria-hidden="true" tabindex="-1"></a>titles <span class="ot">=</span> <span class="fu">sapply</span>(first_split, <span class="cf">function</span>(x) <span class="fu">strsplit</span>(x, <span class="st">&quot;.&quot;</span>,<span class="at">fixed =</span> <span class="cn">TRUE</span>)[[<span class="dv">1</span>]][<span class="dv">1</span>])</span></code></pre></div>
<p>We get 18 unique titles:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="fund.html#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(titles)</span></code></pre></div>
<pre><code>## titles
##          Capt           Col           Don          Dona            Dr 
##             1             4             1             1             8 
##      Jonkheer          Lady         Major        Master          Miss 
##             1             1             2            61           260 
##          Mlle           Mme            Mr           Mrs            Ms 
##             2             1           757           197             2 
##           Rev           Sir  the Countess 
##             8             1             1</code></pre>
<p>A few titles have a very low occurrence rate:</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="fund.html#cb234-1" aria-hidden="true" tabindex="-1"></a>titles <span class="ot">=</span> stringr<span class="sc">::</span><span class="fu">str_trim</span>((titles))</span>
<span id="cb234-2"><a href="fund.html#cb234-2" aria-hidden="true" tabindex="-1"></a>titles <span class="sc">%&gt;%</span></span>
<span id="cb234-3"><a href="fund.html#cb234-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">fct_count</span>()</span></code></pre></div>
<pre><code>## # A tibble: 18 x 2
##    f                n
##    &lt;fct&gt;        &lt;int&gt;
##  1 Capt             1
##  2 Col              4
##  3 Don              1
##  4 Dona             1
##  5 Dr               8
##  6 Jonkheer         1
##  7 Lady             1
##  8 Major            2
##  9 Master          61
## 10 Miss           260
## 11 Mlle             2
## 12 Mme              1
## 13 Mr             757
## 14 Mrs            197
## 15 Ms               2
## 16 Rev              8
## 17 Sir              1
## 18 the Countess     1</code></pre>
<p>We will collapse titles with low occurrences into one title, which we can easily do with the forcats package.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="fund.html#cb236-1" aria-hidden="true" tabindex="-1"></a>titles2 <span class="ot">=</span></span>
<span id="cb236-2"><a href="fund.html#cb236-2" aria-hidden="true" tabindex="-1"></a>  forcats<span class="sc">::</span><span class="fu">fct_collapse</span>(titles,</span>
<span id="cb236-3"><a href="fund.html#cb236-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">officer =</span> <span class="fu">c</span>(<span class="st">&quot;Capt&quot;</span>, <span class="st">&quot;Col&quot;</span>, <span class="st">&quot;Major&quot;</span>, <span class="st">&quot;Dr&quot;</span>, <span class="st">&quot;Rev&quot;</span>),</span>
<span id="cb236-4"><a href="fund.html#cb236-4" aria-hidden="true" tabindex="-1"></a>                        <span class="at">royal =</span> <span class="fu">c</span>(<span class="st">&quot;Jonkheer&quot;</span>, <span class="st">&quot;Don&quot;</span>, <span class="st">&quot;Sir&quot;</span>, <span class="st">&quot;the Countess&quot;</span>, <span class="st">&quot;Dona&quot;</span>, <span class="st">&quot;Lady&quot;</span>),</span>
<span id="cb236-5"><a href="fund.html#cb236-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">miss =</span> <span class="fu">c</span>(<span class="st">&quot;Miss&quot;</span>, <span class="st">&quot;Mlle&quot;</span>),</span>
<span id="cb236-6"><a href="fund.html#cb236-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mrs =</span> <span class="fu">c</span>(<span class="st">&quot;Mrs&quot;</span>, <span class="st">&quot;Mme&quot;</span>, <span class="st">&quot;Ms&quot;</span>)</span>
<span id="cb236-7"><a href="fund.html#cb236-7" aria-hidden="true" tabindex="-1"></a>                        )</span></code></pre></div>
<p>We can count titles again to see the new number of titles</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="fund.html#cb237-1" aria-hidden="true" tabindex="-1"></a>titles2 <span class="sc">%&gt;%</span>  </span>
<span id="cb237-2"><a href="fund.html#cb237-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">fct_count</span>()</span></code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   f           n
##   &lt;fct&gt;   &lt;int&gt;
## 1 officer    23
## 2 royal       6
## 3 Master     61
## 4 miss      262
## 5 mrs       200
## 6 Mr        757</code></pre>
<p>Add new title variable to dataset:</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="fund.html#cb239-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span></span>
<span id="cb239-2"><a href="fund.html#cb239-2" aria-hidden="true" tabindex="-1"></a>  data <span class="sc">%&gt;%</span></span>
<span id="cb239-3"><a href="fund.html#cb239-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">title =</span> titles2)</span></code></pre></div>
<p>As a second example, we will explore and clean the numeric “age” variable:</p>
<p>Explore the variable:</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="fund.html#cb240-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(data)</span></code></pre></div>
<pre><code>##      pclass         survived          name               sex     
##  Min.   :1.000   Min.   :0.0000   Length:1309        female:466  
##  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  
##  Median :3.000   Median :0.0000   Mode  :character               
##  Mean   :2.295   Mean   :0.3853                                  
##  3rd Qu.:3.000   3rd Qu.:1.0000                                  
##  Max.   :3.000   Max.   :1.0000                                  
##                  NA&#39;s   :655                                     
##       age              sibsp            parch            ticket    
##  Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  
##  1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  
##  Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  
##  Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  
##  3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  
##  Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  
##  NA&#39;s   :263                                        (Other) :1261  
##       fare                     cabin      embarked      boat    
##  Min.   :  0.000                  :1014    :  2           :823  
##  1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  
##  Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  
##  Mean   : 33.295   G6             :   5   S:914    15     : 37  
##  3rd Qu.: 31.275   B96 B98        :   4            14     : 33  
##  Max.   :512.329   C22 C26        :   4            4      : 31  
##  NA&#39;s   :1         (Other)        : 271            (Other):308  
##       body                      home.dest       title    
##  Min.   :  1.0                       :564   officer: 23  
##  1st Qu.: 72.0   New York, NY        : 64   royal  :  6  
##  Median :155.0   London              : 14   Master : 61  
##  Mean   :160.8   Montreal, PQ        : 10   miss   :262  
##  3rd Qu.:256.0   Cornwall / Akron, OH:  9   mrs    :200  
##  Max.   :328.0   Paris, France       :  9   Mr     :757  
##  NA&#39;s   :1188    (Other)             :639</code></pre>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="fund.html#cb242-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(data<span class="sc">$</span>age))<span class="sc">/</span><span class="fu">nrow</span>(data)</span></code></pre></div>
<pre><code>## [1] 0.2009167</code></pre>
<p>20% NAs!
Either we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups.
In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title).
After grouping, all operations (such as our median(age….)) will be done within the specified groups.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="fund.html#cb244-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span></span>
<span id="cb244-2"><a href="fund.html#cb244-2" aria-hidden="true" tabindex="-1"></a>  data <span class="sc">%&gt;%</span></span>
<span id="cb244-3"><a href="fund.html#cb244-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(sex, pclass, title) <span class="sc">%&gt;%</span></span>
<span id="cb244-4"><a href="fund.html#cb244-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">age2 =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(age), <span class="fu">median</span>(age, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), age)) <span class="sc">%&gt;%</span></span>
<span id="cb244-5"><a href="fund.html#cb244-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ungroup</span>()</span></code></pre></div>
</div>
<div id="pre-processing-and-feature-selection" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Pre-processing and feature selection</h3>
<p>We want to you keras in our example, but it cannot handle factors and requires scaled the data.</p>
<p>Normally, one would do this for all predictors, but as we here only showe the pipeline, we have sub-selected a bunch of predictors and do this only for them.</p>
<p>We first scale the numeric predictors abd change the factors with only two groups/levels into integer (this can be handled from keras)</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="fund.html#cb245-1" aria-hidden="true" tabindex="-1"></a>data_sub <span class="ot">=</span></span>
<span id="cb245-2"><a href="fund.html#cb245-2" aria-hidden="true" tabindex="-1"></a>  data <span class="sc">%&gt;%</span></span>
<span id="cb245-3"><a href="fund.html#cb245-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(survived, sex, age2, fare, title, pclass) <span class="sc">%&gt;%</span></span>
<span id="cb245-4"><a href="fund.html#cb245-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">age2 =</span> scales<span class="sc">::</span><span class="fu">rescale</span>(age2, <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)), <span class="at">fare =</span> scales<span class="sc">::</span><span class="fu">rescale</span>(fare, <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb245-5"><a href="fund.html#cb245-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">sex =</span> <span class="fu">as.integer</span>(sex) <span class="sc">-</span> 1L, <span class="at">title =</span> <span class="fu">as.integer</span>(title) <span class="sc">-</span> 1L, <span class="at">pclass =</span> <span class="fu">as.integer</span>(pclass <span class="sc">-</span> 1L))</span></code></pre></div>
<p>Factors with more than two levels, should be one hot encoded:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="fund.html#cb246-1" aria-hidden="true" tabindex="-1"></a>one_title <span class="ot">=</span> <span class="fu">k_one_hot</span>(data_sub<span class="sc">$</span>title, <span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>title)))<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb246-2"><a href="fund.html#cb246-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(one_title) <span class="ot">=</span> <span class="fu">levels</span>(data<span class="sc">$</span>title)</span>
<span id="cb246-3"><a href="fund.html#cb246-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb246-4"><a href="fund.html#cb246-4" aria-hidden="true" tabindex="-1"></a>one_sex <span class="ot">=</span> <span class="fu">k_one_hot</span>(data_sub<span class="sc">$</span>sex, <span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>sex)))<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb246-5"><a href="fund.html#cb246-5" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(one_sex) <span class="ot">=</span> <span class="fu">levels</span>(data<span class="sc">$</span>sex)</span>
<span id="cb246-6"><a href="fund.html#cb246-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb246-7"><a href="fund.html#cb246-7" aria-hidden="true" tabindex="-1"></a>one_pclass <span class="ot">=</span> <span class="fu">k_one_hot</span>(data_sub<span class="sc">$</span>pclass,  <span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>pclass)))<span class="sc">$</span><span class="fu">numpy</span>()</span>
<span id="cb246-8"><a href="fund.html#cb246-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(one_pclass) <span class="ot">=</span> <span class="fu">paste0</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(<span class="fu">unique</span>(data<span class="sc">$</span>pclass)), <span class="st">&quot;pclass&quot;</span>)</span></code></pre></div>
<p>And we have to add the dummy encoded variables to the dataset:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="fund.html#cb247-1" aria-hidden="true" tabindex="-1"></a>data_sub <span class="ot">=</span> <span class="fu">cbind</span>(<span class="fu">data.frame</span>(<span class="at">survived=</span> data_sub<span class="sc">$</span>survived), one_title, one_sex, <span class="at">age =</span> data_sub<span class="sc">$</span>age2, <span class="at">fare =</span> data_sub<span class="sc">$</span>fare, one_pclass)</span>
<span id="cb247-2"><a href="fund.html#cb247-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_sub)</span></code></pre></div>
<pre><code>##   survived officer royal Master miss mrs Mr female male        age
## 1        1       0     0      0    1   0  0      1    0 0.37369494
## 2        1       0     0      0    0   0  1      0    1 0.51774510
## 3        0       0     0      0    0   0  1      0    1 0.32359053
## 4        0       0     0      1    0   0  0      0    1 0.07306851
## 5        0       0     0      0    0   0  1      0    1 0.37995799
## 6        0       0     0      0    0   0  1      0    1 0.48016680
##         fare 1pclass 2pclass 3pclass
## 1 0.02537431       0       1       0
## 2 0.06929139       1       0       0
## 3 0.13575256       0       0       1
## 4 0.04113566       0       0       1
## 5 0.01571255       0       0       1
## 6 0.01415106       0       0       1</code></pre>
</div>
<div id="split-data-for-training-and-testing" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Split data for training and testing</h3>
<p>The splitting consists of two splits:</p>
<ul>
<li>an outer split (the original split, remember we got a train and test split without the response “survived”)</li>
<li>an inner split (we will split further the train dataset into another train and test split with known response)
The inner split is important because to assess the model’s performance and potential overfitting</li>
</ul>
<p>Outer split:</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="fund.html#cb249-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">=</span> data_sub[<span class="sc">!</span><span class="fu">is.na</span>(data_sub<span class="sc">$</span>survived),]</span>
<span id="cb249-2"><a href="fund.html#cb249-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">=</span> data_sub[<span class="fu">is.na</span>(data_sub<span class="sc">$</span>survived),]</span></code></pre></div>
<p>Inner split:</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="fund.html#cb250-1" aria-hidden="true" tabindex="-1"></a>indices <span class="ot">=</span> <span class="fu">sample.int</span>(<span class="fu">nrow</span>(train), <span class="fl">0.7</span><span class="sc">*</span><span class="fu">nrow</span>(train))</span>
<span id="cb250-2"><a href="fund.html#cb250-2" aria-hidden="true" tabindex="-1"></a>sub_train <span class="ot">=</span> train[indices,]</span>
<span id="cb250-3"><a href="fund.html#cb250-3" aria-hidden="true" tabindex="-1"></a>sub_test <span class="ot">=</span> train[<span class="sc">-</span>indices,]</span></code></pre></div>
<p>What is the difference between the two splits? (Tip: have a look at the variable survived)</p>
</div>
<div id="model-fitting" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Model fitting</h3>
<p>In the next step we will fit a keras model on the train data of the inner split:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="fund.html#cb251-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb251-2"><a href="fund.html#cb251-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb251-3"><a href="fund.html#cb251-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">input_shape =</span> <span class="fu">ncol</span>(sub_train) <span class="sc">-</span> 1L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb251-4"><a href="fund.html#cb251-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb251-5"><a href="fund.html#cb251-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 20L, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb251-6"><a href="fund.html#cb251-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> 2L, <span class="at">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb251-7"><a href="fund.html#cb251-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## Model: &quot;sequential_4&quot;
## _________________________________________________________________________
## Layer (type)                    Output Shape                  Param #    
## =========================================================================
## dense_10 (Dense)                (None, 20)                    280        
## _________________________________________________________________________
## dense_11 (Dense)                (None, 20)                    420        
## _________________________________________________________________________
## dense_12 (Dense)                (None, 20)                    420        
## _________________________________________________________________________
## dense_13 (Dense)                (None, 2)                     42         
## =========================================================================
## Total params: 1,162
## Trainable params: 1,162
## Non-trainable params: 0
## _________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="fund.html#cb253-1" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb253-2"><a href="fund.html#cb253-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span></span>
<span id="cb253-3"><a href="fund.html#cb253-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(<span class="at">loss =</span> loss_categorical_crossentropy, <span class="at">optimizer =</span> keras<span class="sc">::</span><span class="fu">optimizer_adamax</span>(<span class="fl">0.01</span>))</span>
<span id="cb253-4"><a href="fund.html#cb253-4" aria-hidden="true" tabindex="-1"></a>model_history <span class="ot">=</span></span>
<span id="cb253-5"><a href="fund.html#cb253-5" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb253-6"><a href="fund.html#cb253-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(sub_train[,<span class="sc">-</span><span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">to_categorical</span>(sub_train[,<span class="dv">1</span>],<span class="at">num_classes =</span> 2L), <span class="at">epochs =</span> 100L, <span class="at">batch_size =</span> 32L, <span class="at">validation_split =</span> <span class="fl">0.2</span>, <span class="at">shuffle =</span> <span class="cn">TRUE</span>)</span>
<span id="cb253-7"><a href="fund.html#cb253-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb253-8"><a href="fund.html#cb253-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model_history)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-115-1.png" width="672" /></p>
</div>
<div id="model-evaluation" class="section level3" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Model evaluation</h3>
<p>We will predict survived for the test data of the inner split and calculate the accuracy:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="fund.html#cb255-1" aria-hidden="true" tabindex="-1"></a>preds <span class="ot">=</span></span>
<span id="cb255-2"><a href="fund.html#cb255-2" aria-hidden="true" tabindex="-1"></a>  model <span class="sc">%&gt;%</span></span>
<span id="cb255-3"><a href="fund.html#cb255-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(sub_test[,<span class="sc">-</span><span class="dv">1</span>]))</span>
<span id="cb255-4"><a href="fund.html#cb255-4" aria-hidden="true" tabindex="-1"></a>predicted <span class="ot">=</span> <span class="fu">ifelse</span>(preds[,<span class="dv">2</span>] <span class="sc">&lt;</span> <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb255-5"><a href="fund.html#cb255-5" aria-hidden="true" tabindex="-1"></a>observed <span class="ot">=</span> sub_test[,<span class="dv">1</span>]</span>
<span id="cb255-6"><a href="fund.html#cb255-6" aria-hidden="true" tabindex="-1"></a>(<span class="at">accuracy =</span> <span class="fu">mean</span>(predicted <span class="sc">==</span> observed))</span></code></pre></div>
<pre><code>## [1] NA</code></pre>
</div>
<div id="predictions-and-submission" class="section level3" number="3.4.6">
<h3><span class="header-section-number">3.4.6</span> Predictions and submission</h3>
<p>When we are satisfied with the performance of our model in the inner split, we will create predictions for the test data of the outer split:</p>
<p>To do so, we select all observations that belong to the outer test split (use the filter function) and remove the survived (NAs) columns</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="fund.html#cb257-1" aria-hidden="true" tabindex="-1"></a>submit <span class="ot">=</span> </span>
<span id="cb257-2"><a href="fund.html#cb257-2" aria-hidden="true" tabindex="-1"></a>  test <span class="sc">%&gt;%</span> </span>
<span id="cb257-3"><a href="fund.html#cb257-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(<span class="sc">-</span>survived)</span></code></pre></div>
<p>We cannot assess the performance on the test split because the true survived ratio is unknown, however, we can now submit our predictions to the submission server at
<a href="http://rhsbio7.uni-regensburg.de:8500" class="uri">http://rhsbio7.uni-regensburg.de:8500</a>
To do so, we have to transform our survived probabilities into actual 0/1 predictions (probabilities are not allowed) and create a csv:</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="fund.html#cb258-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> model <span class="sc">%&gt;%</span> </span>
<span id="cb258-2"><a href="fund.html#cb258-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(<span class="fu">as.matrix</span>(submit))</span></code></pre></div>
<p>All values &gt; 0.5 will be set to 1 and values &lt; 0.5 to zero.
For the submission it is critical to change the predictions into a data.frame and save it with the write.csv function:</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="fund.html#cb259-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(<span class="fu">data.frame</span>(<span class="at">y=</span>pred), <span class="at">file =</span> <span class="st">&quot;Max_1.csv&quot;</span>)</span></code></pre></div>
<p>The file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Deep.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
