[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"","code":""},{"path":"index.html","id":"r-system","chapter":"1 Prerequisites","heading":"1.1 R System","text":"Make sure recent version R (>=3.6, ideally >=4.0) RStudio computers.","code":""},{"path":"index.html","id":"tensorflow-and-keras","chapter":"1 Prerequisites","heading":"1.2 TensorFlow and Keras","text":"want run code computers, also need install TensorFlow / Keras R. , following work people.Run R:work computers, particular software recent. Sometimes, however, things don’t work well, especially python distribution often makes problems. installation work , can look together. Also, provide virtual machines case computers / laptops old don’t manage install TensorFlow.Warning: need least TensorFlow version 2.6, otherwise, argument “learning_rate” must “lr!”","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"rstudio/tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow()\n\ninstall.packages(\"keras\", dependencies = T)\nkeras::install_keras()"},{"path":"index.html","id":"torch-for-r","chapter":"1 Prerequisites","heading":"1.3 Torch for R","text":"may also use Torch R. R frontend popular PyTorch framework. install Torch, type R:","code":"\ninstall.packages(\"torch\")\nlibrary(torch)"},{"path":"index.html","id":"ecodata","chapter":"1 Prerequisites","heading":"1.4 EcoData","text":"may sometimes use data sets EcoData package. install package, run:","code":"\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = TRUE, build_vignettes = TRUE)"},{"path":"index.html","id":"further-used-libraries","chapter":"1 Prerequisites","heading":"1.5 Further Used Libraries","text":"make huge use different libraries. take coffee two (take …) install following libraries.\nPlease given order unless know ’re , dependencies packages.","code":"\ninstall.packages(\"abind\")\ninstall.packages(\"animation\")\ninstall.packages(\"ape\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"Rgraphviz\", \"graph\", \"RBGL\"))\ninstall.packages(\"coro\")\ninstall.packages(\"dbscan\")\ninstall.packages(\"dendextend\")\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"e1071\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"fields\")\ninstall.packages(\"forcats\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"gym\")\ninstall.packages(\"kknn\")\ninstall.packages(\"knitr\")\ninstall.packages(\"iml\")\ninstall.packages(\"lavaan\")\ninstall.packages(\"lmtest\")\ninstall.packages(\"magick\")\ninstall.packages(\"mclust\")\ninstall.packages(\"Metrics\")\ninstall.packages(\"microbenchmark\")\ninstall.packages(\"missRanger\")\ninstall.packages(\"mlbench\")\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\ninstall.packages(\"mlr3measures\")\ninstall.packages(\"mlr3pipelines\")\ninstall.packages(\"mlr3tuning\")\ninstall.packages(\"paradox\")\ninstall.packages(\"partykit\")\ninstall.packages(\"pcalg\")\ninstall.packages(\"piecewiseSEM\")\ninstall.packages(\"purrr\")\ninstall.packages(\"randomForest\")\ninstall.packages(\"ranger\")\ninstall.packages(\"reticulate\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"scales\")\ninstall.packages(\"semPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tfprobability\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"xgboost\")\n\ndevtools::install_github(\"andrie/deepviz\", dependencies = TRUE,\n                         upgrade = \"always\")\ndevtools::install_github(repo = \"florianhartig/EcoData\", subdir = \"EcoData\",\n                         dependencies = TRUE, build_vignettes = FALSE)\ndevtools::install_github('skinner927/reprtree')\ndevtools::install_version(\"lavaanPlot\", version = \"0.6.0\")\n\nreticulate::conda_install(\"r-reticulate\", packages = \"scipy\", pip = TRUE)\n\ntorch::install_torch()"},{"path":"index.html","id":"linuxunix-systems-have-to-fulfill-some-further-dependencies","chapter":"1 Prerequisites","heading":"1.6 Linux/UNIX systems have to fulfill some further dependencies","text":"Debian based systemsFor Debian based systems, need:new installing packages Debian / Ubuntu, etc., type following:Authors:Maximilian Pichler: @_Max_PichlerFlorian Hartig: @florianhartigContributors:Johannes Oberpriller, Matthias Meier","code":"build-essential\ngfortran\nlibmagick++-dev\nr-base-devsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev"},{"path":"reminder.html","id":"reminder","chapter":"2 Reminders About Basic Operations in R","heading":"2 Reminders About Basic Operations in R","text":"","code":""},{"path":"reminder.html","id":"your-r-system","chapter":"2 Reminders About Basic Operations in R","heading":"2.1 Your R System","text":"course, work combination R + RStudio.R calculation engine performs computations.RStudio editor helps sending inputs R collect outputs.Make sure recent version R + RStudio installed computer.never used RStudio, introductory video.","code":""},{"path":"reminder.html","id":"data-types-in-r","chapter":"2 Reminders About Basic Operations in R","heading":"2.2 Data types in R","text":"following subchapters introduce / remind R data types.","code":""},{"path":"reminder.html","id":"test-your-knowledge","chapter":"2 Reminders About Basic Operations in R","heading":"2.2.1 Test Your Knowledge","text":"Discuss partner(s) - meaning / structure / properties following data types R:Atomic types (atomic types exist)listvectordata.framematrixarray","code":""},{"path":"reminder.html","id":"iris-data","chapter":"2 Reminders About Basic Operations in R","heading":"2.2.2 Iris Data","text":"data type iris data set? Explore using following commands:","code":"\niris\n\nclass(iris)\ndim(iris)\nstr(iris)"},{"path":"reminder.html","id":"dynamic-typing","chapter":"2 Reminders About Basic Operations in R","heading":"2.2.3 Dynamic typing","text":"R dynamically typed language, means type variables determined automatically depending values supply. Try :also works data set already exists, .e. assign different value, type automatically changed. Look happens assign character value previously numeric column data.frame:Note numeric values changed characters well. can try force back values numeric :look values iris$Sepal.Length.Note:\nactions operate local copy iris data set. don’t overwrite base data can use new R session reset “data(iris).”","code":"\nx = 1\nclass(x)\nx = \"dog\"\nclass(x)\niris$Sepal.Length[2] = \"dog\"\nstr(iris)\niris$Sepal.Length = as.numeric(iris$Sepal.Length)"},{"path":"reminder.html","id":"data-selection-slicing-and-subsetting","chapter":"2 Reminders About Basic Operations in R","heading":"2.3 Data selection, Slicing and Subsetting","text":"chapter, discuss data selection, slicing subsetting.","code":""},{"path":"reminder.html","id":"subsetting-and-slicing-for-single-data-types","chapter":"2 Reminders About Basic Operations in R","heading":"2.3.1 Subsetting and Slicing for Single Data Types","text":"often want select subset data. can generally subset data structures using indices TRUE/FALSE (T/F).\nvector:use TRUE/FALSE, must specify truth value every (!) position.list, ’s basically , except following points:Elements lists usually name, can also access via “list$name.”Lists accessed [] return list. want select single element, access via [[]], “list[[2]].”data.frames objects im > 2, true, except several indices.syntax “matrix[1,]” also called slicing, obvious reasons.Data.frames matrices, except , like lists vectors, can also access columns via names “data.frame$column.”","code":"\nvector[1] # First element.\nvector[1:3] # Elements 1,2,3.\nvector[c(1,5,6)] # Elements 1,5,6.\nvector[c(T,T,F,F,T)] # Elements 1,2,5.\nvector = c(1,2,3,4,5)\nvector[c(T,F)] # Does NOT work!\nmatrix[1,2] # Element in first row, second column.\nmatrix[1:2,] # First two rows, all columns.\nmatrix[,c(T,F,T)] # All rows, 1st and 3rd column."},{"path":"reminder.html","id":"logic-and-slicing","chapter":"2 Reminders About Basic Operations in R","heading":"2.3.2 Logic and Slicing","text":"Slicing powerful combine logical operators, “&” (logical ), “|” (logical ), “==” (equal), “!=” (equal), “<=,” “>,” etc. examples:Note identical following:can also combine several logical commands:Note works element-wise!","code":"\niris[iris$Species == \"virginica\", ]\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 101          6.3         3.3          6.0         2.5\n#> 102          5.8         2.7          5.1         1.9\n#> 103          7.1         3.0          5.9         2.1\n#> 104          6.3         2.9          5.6         1.8\n#> 105          6.5         3.0          5.8         2.2\n#> 106          7.6         3.0          6.6         2.1\n#> 107          4.9         2.5          4.5         1.7\n#> 108          7.3         2.9          6.3         1.8\n#> 109          6.7         2.5          5.8         1.8\n#> 110          7.2         3.6          6.1         2.5\n#> 111          6.5         3.2          5.1         2.0\n#> 112          6.4         2.7          5.3         1.9\n#> 113          6.8         3.0          5.5         2.1\n#> 114          5.7         2.5          5.0         2.0\n#> 115          5.8         2.8          5.1         2.4\n#> 116          6.4         3.2          5.3         2.3\n#> 117          6.5         3.0          5.5         1.8\n#> 118          7.7         3.8          6.7         2.2\n#> 119          7.7         2.6          6.9         2.3\n#> 120          6.0         2.2          5.0         1.5\n#> 121          6.9         3.2          5.7         2.3\n#> 122          5.6         2.8          4.9         2.0\n#> 123          7.7         2.8          6.7         2.0\n#> 124          6.3         2.7          4.9         1.8\n#> 125          6.7         3.3          5.7         2.1\n#> 126          7.2         3.2          6.0         1.8\n#> 127          6.2         2.8          4.8         1.8\n#> 128          6.1         3.0          4.9         1.8\n#> 129          6.4         2.8          5.6         2.1\n#> 130          7.2         3.0          5.8         1.6\n#> 131          7.4         2.8          6.1         1.9\n#> 132          7.9         3.8          6.4         2.0\n#> 133          6.4         2.8          5.6         2.2\n#> 134          6.3         2.8          5.1         1.5\n#> 135          6.1         2.6          5.6         1.4\n#> 136          7.7         3.0          6.1         2.3\n#> 137          6.3         3.4          5.6         2.4\n#> 138          6.4         3.1          5.5         1.8\n#> 139          6.0         3.0          4.8         1.8\n#> 140          6.9         3.1          5.4         2.1\n#> 141          6.7         3.1          5.6         2.4\n#> 142          6.9         3.1          5.1         2.3\n#> 143          5.8         2.7          5.1         1.9\n#> 144          6.8         3.2          5.9         2.3\n#> 145          6.7         3.3          5.7         2.5\n#> 146          6.7         3.0          5.2         2.3\n#> 147          6.3         2.5          5.0         1.9\n#> 148          6.5         3.0          5.2         2.0\n#> 149          6.2         3.4          5.4         2.3\n#> 150          5.9         3.0          5.1         1.8\n#>       Species\n#> 101 virginica\n#> 102 virginica\n#> 103 virginica\n#> 104 virginica\n#> 105 virginica\n#> 106 virginica\n#> 107 virginica\n#> 108 virginica\n#> 109 virginica\n#> 110 virginica\n#> 111 virginica\n#> 112 virginica\n#> 113 virginica\n#> 114 virginica\n#> 115 virginica\n#> 116 virginica\n#> 117 virginica\n#> 118 virginica\n#> 119 virginica\n#> 120 virginica\n#> 121 virginica\n#> 122 virginica\n#> 123 virginica\n#> 124 virginica\n#> 125 virginica\n#> 126 virginica\n#> 127 virginica\n#> 128 virginica\n#> 129 virginica\n#> 130 virginica\n#> 131 virginica\n#> 132 virginica\n#> 133 virginica\n#> 134 virginica\n#> 135 virginica\n#> 136 virginica\n#> 137 virginica\n#> 138 virginica\n#> 139 virginica\n#> 140 virginica\n#> 141 virginica\n#> 142 virginica\n#> 143 virginica\n#> 144 virginica\n#> 145 virginica\n#> 146 virginica\n#> 147 virginica\n#> 148 virginica\n#> 149 virginica\n#> 150 virginica\nsubset(iris, Species == \"virginica\") \n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 101          6.3         3.3          6.0         2.5\n#> 102          5.8         2.7          5.1         1.9\n#> 103          7.1         3.0          5.9         2.1\n#> 104          6.3         2.9          5.6         1.8\n#> 105          6.5         3.0          5.8         2.2\n#> 106          7.6         3.0          6.6         2.1\n#> 107          4.9         2.5          4.5         1.7\n#> 108          7.3         2.9          6.3         1.8\n#> 109          6.7         2.5          5.8         1.8\n#> 110          7.2         3.6          6.1         2.5\n#> 111          6.5         3.2          5.1         2.0\n#> 112          6.4         2.7          5.3         1.9\n#> 113          6.8         3.0          5.5         2.1\n#> 114          5.7         2.5          5.0         2.0\n#> 115          5.8         2.8          5.1         2.4\n#> 116          6.4         3.2          5.3         2.3\n#> 117          6.5         3.0          5.5         1.8\n#> 118          7.7         3.8          6.7         2.2\n#> 119          7.7         2.6          6.9         2.3\n#> 120          6.0         2.2          5.0         1.5\n#> 121          6.9         3.2          5.7         2.3\n#> 122          5.6         2.8          4.9         2.0\n#> 123          7.7         2.8          6.7         2.0\n#> 124          6.3         2.7          4.9         1.8\n#> 125          6.7         3.3          5.7         2.1\n#> 126          7.2         3.2          6.0         1.8\n#> 127          6.2         2.8          4.8         1.8\n#> 128          6.1         3.0          4.9         1.8\n#> 129          6.4         2.8          5.6         2.1\n#> 130          7.2         3.0          5.8         1.6\n#> 131          7.4         2.8          6.1         1.9\n#> 132          7.9         3.8          6.4         2.0\n#> 133          6.4         2.8          5.6         2.2\n#> 134          6.3         2.8          5.1         1.5\n#> 135          6.1         2.6          5.6         1.4\n#> 136          7.7         3.0          6.1         2.3\n#> 137          6.3         3.4          5.6         2.4\n#> 138          6.4         3.1          5.5         1.8\n#> 139          6.0         3.0          4.8         1.8\n#> 140          6.9         3.1          5.4         2.1\n#> 141          6.7         3.1          5.6         2.4\n#> 142          6.9         3.1          5.1         2.3\n#> 143          5.8         2.7          5.1         1.9\n#> 144          6.8         3.2          5.9         2.3\n#> 145          6.7         3.3          5.7         2.5\n#> 146          6.7         3.0          5.2         2.3\n#> 147          6.3         2.5          5.0         1.9\n#> 148          6.5         3.0          5.2         2.0\n#> 149          6.2         3.4          5.4         2.3\n#> 150          5.9         3.0          5.1         1.8\n#>       Species\n#> 101 virginica\n#> 102 virginica\n#> 103 virginica\n#> 104 virginica\n#> 105 virginica\n#> 106 virginica\n#> 107 virginica\n#> 108 virginica\n#> 109 virginica\n#> 110 virginica\n#> 111 virginica\n#> 112 virginica\n#> 113 virginica\n#> 114 virginica\n#> 115 virginica\n#> 116 virginica\n#> 117 virginica\n#> 118 virginica\n#> 119 virginica\n#> 120 virginica\n#> 121 virginica\n#> 122 virginica\n#> 123 virginica\n#> 124 virginica\n#> 125 virginica\n#> 126 virginica\n#> 127 virginica\n#> 128 virginica\n#> 129 virginica\n#> 130 virginica\n#> 131 virginica\n#> 132 virginica\n#> 133 virginica\n#> 134 virginica\n#> 135 virginica\n#> 136 virginica\n#> 137 virginica\n#> 138 virginica\n#> 139 virginica\n#> 140 virginica\n#> 141 virginica\n#> 142 virginica\n#> 143 virginica\n#> 144 virginica\n#> 145 virginica\n#> 146 virginica\n#> 147 virginica\n#> 148 virginica\n#> 149 virginica\n#> 150 virginica\niris[iris$Species == \"virginica\" & iris$Sepal.Length > 7, ]\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 103          7.1         3.0          5.9         2.1\n#> 106          7.6         3.0          6.6         2.1\n#> 108          7.3         2.9          6.3         1.8\n#> 110          7.2         3.6          6.1         2.5\n#> 118          7.7         3.8          6.7         2.2\n#> 119          7.7         2.6          6.9         2.3\n#> 123          7.7         2.8          6.7         2.0\n#> 126          7.2         3.2          6.0         1.8\n#> 130          7.2         3.0          5.8         1.6\n#> 131          7.4         2.8          6.1         1.9\n#> 132          7.9         3.8          6.4         2.0\n#> 136          7.7         3.0          6.1         2.3\n#>       Species\n#> 103 virginica\n#> 106 virginica\n#> 108 virginica\n#> 110 virginica\n#> 118 virginica\n#> 119 virginica\n#> 123 virginica\n#> 126 virginica\n#> 130 virginica\n#> 131 virginica\n#> 132 virginica\n#> 136 virginica"},{"path":"reminder.html","id":"applying-functions-and-aggregates-across-a-data-set","chapter":"2 Reminders About Basic Operations in R","heading":"2.4 Applying Functions and Aggregates Across a Data set","text":"chapter, discuss basic functions R calculating means, averages apply functions across data set.","code":""},{"path":"reminder.html","id":"functions","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.1 Functions","text":"Maybe good time remind functions. two basic options use R :Variables / data structures.Functions.already used variables / data structures. Variables name type name R, get values inside respective data structure.Functions algorithms called like:example, can :want know summary function , type “?summary,” put mouse function press “F1.”able work properly data, know define functions. works like following:Try happens type \"squareValue(2)\".Write function multiplying 2 values. Hint - start \"function(x1, x2)\".Change first line \"squareValue\" function \"function(x = 3)\" try following commands: \"squareValue(2)\", \"squareValue()\" - sense syntax?\n  1\ngiven value (3 example ) default value. value used automatically, value supplied respective variable.\nDefault values can specified variables, put end function definition.\nHint: R, always useful name parameters using functions.Look following example:","code":"function(variable)\nsummary(iris)\nsquareValue = function(x){\n  temp = x * x \n  return(temp)\n}\nmultiply = function(x1, x2){\n  return(x1 * x2)\n}\nsquareValue(2)\n#> [1] 4\nsquareValue = function(x = 3){\n  temp = x * x \n  return(temp)\n}\n\nsquareValue(2)\n#> [1] 4\n\nsquareValue()\n#> [1] 9\ntestFunction = function(a = 1, b, c = 3){\n  return(a * b + c)\n}\n\ntestFunction()\n#> Error in testFunction(): argument \"b\" is missing, with no default\n\ntestFunction(10)\n#> Error in testFunction(10): argument \"b\" is missing, with no default\n\ntestFunction(10, 20)\n#> [1] 203\n\ntestFunction(10, 20, 30)\n#> [1] 230\n\ntestFunction(b = 10, c = 20, a = 30)\n#> [1] 320"},{"path":"reminder.html","id":"the-apply-function","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.2 The apply() Function","text":"Now know functions, can introduce functions use functions. One important apply function. apply function applies function data structure, typically matrix data.frame.Try following:Check help apply understand .first result \"apply(iris[,1:4], 2, mean)\" NA? Check help mean understand .Try \"apply(iris[,1:4], 1, mean)\". Think changed .happen use \"iris\" instead \"iris[,1:4]\"?.\n  1\nRemember, done (run part separately, execute following lines ):Taking mean character sequence possible, result NA (Available, missing value(s)).can skip missing values option “na.rm = TRUE” “mean” function. use “apply” function, pass argument(s) .Arrays (thus matrices, data.frame(s), etc.) several dimensions. simple \\(2D\\) array (matrix), first dimension rows second dimension columns. second parameter “apply” function specifies dimension mean computed. use \\(1\\), demand row means (150), use \\(2\\), request column means (5, resp. 4).5th column “Species.” values numeric. whole data.frame taken data.frame full characters.Remark: “NULL” statement return value apply. “str” returns nothing (prints something ), returned vector (array, list, …) empty, just like:","code":"\napply(iris[,1:4], 2, mean)\n?apply\niris$Sepal.Length[2] = \"Hund\"\niris$Sepal.Length = as.numeric(iris$Sepal.Length)\n#> Warning: NAs introduced by coercion\napply(iris[,1:4], 2, mean)\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>           NA     3.057333     3.758000     1.199333\napply(iris[,1:4], 2, mean, na.rm = T)\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>     5.849664     3.057333     3.758000     1.199333\napply(iris[,1:4], 1, mean)\n#>   [1] 2.550    NA 2.350 2.350 2.550 2.850 2.425 2.525 2.225\n#>  [10] 2.400 2.700 2.500 2.325 2.125 2.800 3.000 2.750 2.575\n#>  [19] 2.875 2.675 2.675 2.675 2.350 2.650 2.575 2.450 2.600\n#>  [28] 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n#>  [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800\n#>  [46] 2.375 2.675 2.350 2.675 2.475 4.075 3.900 4.100 3.275\n#>  [55] 3.850 3.575 3.975 2.900 3.850 3.300 2.875 3.650 3.300\n#>  [64] 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n#>  [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200\n#>  [82] 3.150 3.400 3.850 3.600 3.875 4.000 3.575 3.500 3.325\n#>  [91] 3.425 3.775 3.400 2.900 3.450 3.525 3.525 3.675 2.925\n#> [100] 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n#> [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200\n#> [118] 5.100 4.875 3.675 4.525 3.825 4.800 3.925 4.450 4.550\n#> [127] 3.900 3.950 4.225 4.400 4.550 5.025 4.250 3.925 3.925\n#> [136] 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n#> [145] 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris, 2, mean)\n#> Warning in mean.default(newX[, i], ...): argument is not\n#> numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not\n#> numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not\n#> numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not\n#> numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not\n#> numeric or logical: returning NA\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>           NA           NA           NA           NA \n#>      Species \n#>           NA\napply(iris[,1:4], 2, str)\n#>  num [1:150] 5.1 NA 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#>  num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#>  num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#>  num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#> NULL\napply(iris, 2, str)\n#>  chr [1:150] \"5.1\" NA \"4.7\" \"4.6\" \"5.0\" \"5.4\" \"4.6\" ...\n#>  chr [1:150] \"3.5\" \"3.0\" \"3.2\" \"3.1\" \"3.6\" \"3.9\" \"3.4\" ...\n#>  chr [1:150] \"1.4\" \"1.4\" \"1.3\" \"1.5\" \"1.4\" \"1.7\" \"1.4\" ...\n#>  chr [1:150] \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.4\" \"0.3\" ...\n#>  chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n#> NULL\nc()\n#> NULL"},{"path":"reminder.html","id":"the-aggregate-function","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.3 The aggregate() Function","text":"aggregate() calculates function per grouping variable. Try example:Note max function get maximum value, nothing lecturer, spelled Max.dot general R syntax usually refers “use columns data set.”","code":"\naggregate(. ~ Species, data = iris, FUN = max)"},{"path":"reminder.html","id":"plotting","chapter":"2 Reminders About Basic Operations in R","heading":"2.5 Plotting","text":"following two commands identical:plot(iris$Sepal.Length, iris$Sepal.Width)plot(Sepal.Width ~ Sepal.Length, data = iris)second option preferable, allows subset data easier.plot command use standard plot depending type variable supplied. example, x axis factor, boxplot produced.can change color, size, shape etc. often useful visualization.plots R short video:","code":"\nplot(Sepal.Width ~ Sepal.Length, data = iris[iris$Species == \"versicolor\", ])\nplot(Sepal.Width ~ Species, data = iris)\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species,\n     cex = iris$Petal.Length)"},{"path":"reminder.html","id":"additional-resources","chapter":"2 Reminders About Basic Operations in R","heading":"2.6 Additional Resources","text":"additional R resources self-study, recommend:","code":""},{"path":"reminder.html","id":"books","chapter":"2 Reminders About Basic Operations in R","heading":"2.6.1 Books","text":"PDF introduction R.Introduction Statistical Learning - simplified version version classic machine learning textbook, free PDF download.Quick R - Good site reference code examples standard tasks.Ebook Hands Programming R.","code":""},{"path":"reminder.html","id":"instructional-videos","chapter":"2 Reminders About Basic Operations in R","heading":"2.6.2 Instructional videos","text":"YouTube - R Programming Tutorial - Learn Basics Statistical Computing (approx 2h, goes basics).YouTube - Statistics R, Tutorials MarinStatsLectures - Lots smaller videos particular topics.","code":""},{"path":"introduction.html","id":"introduction","chapter":"3 Introduction to Machine Learning","heading":"3 Introduction to Machine Learning","text":"three basic machine learning tasks:Supervised learningUnsupervised learningReinforcement learningIn supervised learning, train algorithms using labeled data, means already know correct answer part data (called training data).Unsupervised learning contrast technique, one need monitor model apply labels. Instead, allow model work discover information.Reinforcement learning technique emulates game-like situation. algorithm finds solution trial error gets either rewards penalties every action. games, goal maximize rewards. talk technique last day course.moment, focus first two tasks, supervised unsupervised learning. , begin small example. start code, video prepare class:","code":""},{"path":"introduction.html","id":"unsupervised-learning","chapter":"3 Introduction to Machine Learning","heading":"3.1 Unsupervised Learning","text":"unsupervised learning, want identify patterns data without examples (supervision) correct patterns / classes . example, consider iris data set. , 150 observations 4 floral traits:\nFigure 3.1: Trait distributions iris dataset\nobservations 3 species indeed species tend different traits, meaning observations form 3 clusters.\nFigure 3.2: Scatterplots trait-trait combinations.\nHowever, imagine don’t know species , basically situation people antique . people just noted plants different flowers others, decided give different names. kind process unsupervised learning .","code":"\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4]) \nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits, \n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])"},{"path":"introduction.html","id":"hierarchical-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.1 Hierarchical Clustering","text":"cluster refers collection data points aggregated together certain similarities.hierarchical clustering, hierarchy (tree) data points built.Agglomerative: Start data point cluster, merge hierarchically.Divisive: Start data points one cluster, split hierarchically.Merges / splits done according linkage criterion, measures distance (potential) clusters. Cut tree certain height get clusters.example\nFigure 3.3: Results hierarchical clustering. Red rectangle drawn around corresponding clusters.\nplot, colors true species identity\nFigure 3.4: Results hierarchical clustering. Colors correspond three species classes.\nCalculate confusion matrix. Note switching labels fits species.Table 3.1: Confusion matrix predicted observed species classes.Note results might change choose different agglomeration method, distance metric scale variables. Compare, e.g. example:\nFigure 3.5: Results hierarchical clustering. Colors correspond three species classes. Different agglomeration method\nTable 3.2: Confusion matrix predicted observed species classes.method best?might conclude ward.D2 works best . However, learn later, optimizing method without hold-testing implies model may overfitting. check using cross-validation.","code":"\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nd = dist(traits)\nhc = hclust(d, method = \"complete\")\n\nplot(hc, main=\"\")\nrect.hclust(hc, k = 3)  # Draw rectangles around the branches.\nlibrary(ape)\n\nplot(as.phylo(hc), \n     tip.color = colors[as.integer(species)], \n     direction = \"downwards\")\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntmp = hcRes3\ntmp[hcRes3 == 2] = 3\ntmp[hcRes3 == 3] = 2\nhcRes3 = tmp\ntable(hcRes3, species)\nhc = hclust(d, method = \"ward.D2\")\n\nplot(as.phylo(hc), \n     tip.color = colors[as.integer(species)], \n     direction = \"downwards\")\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntable(hcRes3, species)\nlibrary(dendextend)\nset.seed(123)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n             \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\nout = dendlist()   # Create a dendlist object from several dendrograms.\nfor(method in methods){\n  res = hclust(d, method = method)   \n  out = dendlist(out, as.dendrogram(res))\n}\nnames(out) = methods\nprint(out)\n#> $ward.D\n#> 'dendrogram' with 2 branches and 150 members total, at height 199.6205 \n#> \n#> $single\n#> 'dendrogram' with 2 branches and 150 members total, at height 1.640122 \n#> \n#> $complete\n#> 'dendrogram' with 2 branches and 150 members total, at height 7.085196 \n#> \n#> $average\n#> 'dendrogram' with 2 branches and 150 members total, at height 4.062683 \n#> \n#> $mcquitty\n#> 'dendrogram' with 2 branches and 150 members total, at height 4.497283 \n#> \n#> $median\n#> 'dendrogram' with 2 branches and 150 members total, at height 2.82744 \n#> \n#> $centroid\n#> 'dendrogram' with 2 branches and 150 members total, at height 2.994307 \n#> \n#> $ward.D2\n#> 'dendrogram' with 2 branches and 150 members total, at height 32.44761 \n#> \n#> attr(,\"class\")\n#> [1] \"dendlist\"\n\nget_ordered_3_clusters = function(dend){\n  # order.dendrogram function returns the order (index)\n  # or the \"label\" attribute for the leaves.\n  # cutree: Cut the tree (dendrogram) into groups of data.\n  cutree(dend, k = 3)[order.dendrogram(dend)]\n}\ndend_3_clusters = lapply(out, get_ordered_3_clusters)\n\n# Calculate Fowlkes-Mallows Index (determine the similarity between clusterings)\ncompare_clusters_to_iris = function(clus){\n  FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)\n}\n\nclusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\ndotchart(sort(clusters_performance), xlim = c(0.3, 1),\n         xlab = \"Fowlkes-Mallows index\",\n         main = \"Performance of linkage methods\n         in detecting the 3 species \\n in our example\",\n         pch = 19)"},{"path":"introduction.html","id":"k-means-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.2 K-means Clustering","text":"Another example unsupervised learning algorithm k-means clustering, one simplest popular unsupervised machine learning algorithms.start algorithm, first specify number clusters (example number species). cluster centroid, assumed real location representing center cluster (example average plant specific species look like). algorithm starts randomly putting centroids somewhere. Afterwards data point assigned respective cluster raises overall -cluster sum squares (variance) related distance centroid least . algorithm placed data points cluster centroids get updated. iterating procedure assignment doesn’t change longer, algorithm can find (locally) optimal centroids data points belonging cluster.\nNote results might differ according initial positions centroids. Thus several (locally) optimal solutions might found.“k” K-means refers number clusters ‘means’ refers averaging data-points find centroids.typical pipeline using k-means clustering looks algorithms. visualized data, fit model, visualize results look performance use confusion matrix. setting fixed seed, can ensure results reproducible.Visualizing results.\nColor codes true species identity, symbol shows cluster result.see discrepancies. Confusion matrix:want animate clustering process, runElbow technique determine probably best suited number clusters:Often, one interested sparse models. Furthermore, higher k necessary tends overfitting. kink picture, sum squares dropped enough k still low enough.\nkeep mind, rule thumb might wrong special cases.","code":"\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n#> K-means clustering with 3 clusters of sizes 50, 62, 38\n#> \n#> Cluster means:\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 1     5.006000    3.428000     1.462000    0.246000\n#> 2     5.901613    2.748387     4.393548    1.433871\n#> 3     6.850000    3.073684     5.742105    2.071053\n#> \n#> Clustering vector:\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [28] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2\n#>  [55] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n#>  [82] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3\n#> [109] 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3\n#> [136] 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2\n#> \n#> Within cluster sum of squares by cluster:\n#> [1] 15.15100 39.82097 23.87947\n#>  (between_SS / total_SS =  88.4 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"       \n#> [4] \"withinss\"     \"tot.withinss\" \"betweenss\"   \n#> [7] \"size\"         \"iter\"         \"ifault\"\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\ntable(iris$Species, kc$cluster)\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 48  2\n#>   virginica   0 14 36\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE, \n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))"},{"path":"introduction.html","id":"density-based-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.3 Density-based Clustering","text":"Determine affinity data point according affinity k nearest neighbors.\ngeneral description many ways .","code":"\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nlibrary(dbscan)\nset.seed(123)\n\nkNNdistplot(traits, k = 4)   # Calculate and plot k-nearest-neighbor distances.\nabline(h = 0.4, lty = 2)\n\ndc = dbscan(traits, eps = 0.4, minPts = 6)\nprint(dc)\n#> DBSCAN clustering for 150 objects.\n#> Parameters: eps = 0.4, minPts = 6\n#> The clustering contains 4 cluster(s) and 32 noise points.\n#> \n#>  0  1  2  3  4 \n#> 32 46 36 14 22 \n#> \n#> Available fields: cluster, eps, minPts\nlibrary(factoextra)\nfviz_cluster(dc, traits, geom = \"point\", ggtheme = theme_light())"},{"path":"introduction.html","id":"model-based-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.4 Model-based Clustering","text":"last class methods unsupervised clustering -called model-based clustering methods.Mclust automatically compares number candidate models (clusters, shape) according BIC (BIC criterion classifying algorithms depending prediction quality usage parameters). can look selected model via:see algorithm prefers 2 clusters. better comparability 2 methods, override setting:Result terms predicted densities 3 clustersPredicted clusters:Confusion matrix:","code":"\nlibrary(mclust)\nmb = Mclust(traits)\nmb$G # Two clusters.\n#> [1] 2\nmb$modelName # > Ellipsoidal, equal shape.\n#> [1] \"VEV\"\nmb3 = Mclust(traits, 3)\nplot(mb3, \"density\")\nplot(mb3, what=c(\"classification\"), add = T)\ntable(iris$Species, mb3$classification)\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 45  5\n#>   virginica   0  0 50"},{"path":"introduction.html","id":"ordination","chapter":"3 Introduction to Machine Learning","heading":"3.1.5 Ordination","text":"Ordination used explorative analysis compared clustering, similar objects ordered together.\nrelationship clustering ordination. PCA ordination iris data set.can cluster results ordination, ordinate clustering, superimpose one .","code":"\npcTraits = prcomp(traits, center = TRUE, scale. = TRUE)\nbiplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25))"},{"path":"introduction.html","id":"exercises","chapter":"3 Introduction to Machine Learning","heading":"3.1.6 Exercises","text":"Go 4(5) algorithms , check sensitive (.e. results change) scale input features (= predictors), instead using raw data. Discuss group: appropriate analysis /general: Scaling scaling?\n  Hierarchical Clustering\nseems scaling harmful hierarchical clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.seems scaling harmful K-means clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.seems scaling harmful density based clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.model based clustering, scaling matter.PCA ordination, scaling matters.\ninterested directions maximal variance, parameters scaled, one highest values might dominate others.","code":"\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)   \n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n  \n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n  \n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n  \n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n  \n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n  \n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n  \n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [28] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n#>  [55] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#>  [82] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n#> [109] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> [136] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> Levels: 1 2 3\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n  \n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n  \n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n  \n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n  \n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n#> Not scaled \n#> \n#> Confusion matrix:\n#>        labels\n#> correct noise  1  2  3  4\n#>       1     3 47  0  0  0\n#>       2     5  0 38  3  4\n#>       3    17  0  0 33  0\n#> \n#> Correct classified points:  118  /  150\n#> Sum of noise points:  25\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n#> Scaled \n#> \n#> Confusion matrix:\n#>        labels\n#> correct noise  1  2  3  4\n#>       1     9 41  0  0  0\n#>       2    14  0 36  0  0\n#>       3    36  0  1  4  9\n#> \n#> Correct classified points:  81  /  150\n#> Sum of noise points:  59\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n  \n  mb3 = Mclust(traits, 3)\n  \n  tbl = table(iris$Species, mb3$classification)\n  \n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n#> Not scaled \n#> \n#> Confusion matrix:\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 45  5\n#>   virginica   0  0 50\n#> \n#> Correct classified points:  145  /  150\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n#> Scaled \n#> \n#> Confusion matrix:\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 45  5\n#>   virginica   0  0 50\n#> \n#> Correct classified points:  145  /  150\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")"},{"path":"introduction.html","id":"supervised-learning-regression-and-classification","chapter":"3 Introduction to Machine Learning","heading":"3.2 Supervised Learning: Regression and Classification","text":"two prominent branches supervised learning regression classification. Fundamentally, classification predicting label regression predicting quantity. following video explains depth:","code":""},{"path":"introduction.html","id":"supervised-regression-using-random-forest","chapter":"3 Introduction to Machine Learning","heading":"3.2.1 Supervised Regression Using Random Forest","text":"random forest (RF) algorithm possibly widely used machine learning algorithm can used regression classification. talk algorithm tomorrow.moment, want go typical workflow supervised regression: First, visualize data. Next, fit model lastly visualize results. use iris data set used . goal now predict Sepal.Length based information variables (including species).Fitting model:Visualization results:understand structure random forest detail, can use package GitHub., one regression trees shown.","code":"\nlibrary(randomForest)\nset.seed(123)\nm1 = randomForest(Sepal.Length ~ ., data = iris)   # ~.: Against all others.\n# str(m1)\n# m1$type\n# predict(m1)\nprint(m1)\n#> \n#> Call:\n#>  randomForest(formula = Sepal.Length ~ ., data = iris) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 1\n#> \n#>           Mean of squared residuals: 0.1364625\n#>                     % Var explained: 79.97\noldpar = par(mfrow = c(1, 2))\nplot(predict(m1), iris$Sepal.Length, xlab = \"Predicted\", ylab = \"Observed\")\nabline(0, 1)\nvarImpPlot(m1)\npar(oldpar)\nreprtree:::plot.getTree(m1, iris)"},{"path":"introduction.html","id":"supervised-classification-using-random-forest","chapter":"3 Introduction to Machine Learning","heading":"3.2.2 Supervised Classification Using Random Forest","text":"random forest, can also classification. steps regression tasks, can additionally see well performed looking confusion matrix. row matrix contains instances predicted class column represents instances actual class. Thus diagonals correctly predicted classes -diagonal elements falsely classified elements.Fitting model:Visualizing one fitted models:Visualizing results ecologically:Confusion matrix:\n      \n        makeMultipleChoiceForm(\n         'Using random forest iris dataset, parameter important (remember function check ) predict Petal.Width?',\n          'radio',\n          [\n            {\n              'answer':'Species.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'Sepal.Width.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n          ],\n          ''\n        );\n      ","code":"\nset.seed(123)\n\nm1 = randomForest(Species ~ ., data = iris)\noldpar = par(mfrow = c(1, 2))\nreprtree:::plot.getTree(m1, iris)\npar(mfrow = c(1, 2))\nplot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = \"Observed\")\nplot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = \"Predicted\")\npar(oldpar)   #Reset par.\ntable(predict(m1), iris$Species)\n#>             \n#>              setosa versicolor virginica\n#>   setosa         50          0         0\n#>   versicolor      0         47         4\n#>   virginica       0          3        46"},{"path":"introduction.html","id":"basicMath","chapter":"3 Introduction to Machine Learning","heading":"3.3 Small Introduction Into the Underlying Mathematical Concepts of all Following Lessons - Optional","text":"yet familiar underlying concepts neural networks want know , suggested read / view following videos / sites. Consider Links videos descriptions parentheses optional bonus.might useful understand concepts depth.(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially animated graphic interesting).)(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially animated graphic interesting).)https://en.wikipedia.org/wiki/Gradient_descent#Descriptionhttps://en.wikipedia.org/wiki/Gradient_descent#DescriptionNeural networks (Backpropagation, etc.).Neural networks (Backpropagation, etc.).Activation functions detail (requires prerequisite).Activation functions detail (requires prerequisite).Videos topic:Gradient descent explained(Stochastic gradient descent explained)(Entropy explained)Short explanation entropy, cross entropy Kullback–Leibler divergenceDeep Learning (chapter 1)neural networks learn - Deep Learning (chapter 2)Backpropagation - Deep Learning (chapter 3)Another video backpropagation (extends previous one) - Deep Learning (chapter 4)","code":""},{"path":"introduction.html","id":"caveat-about-learning-rates-and-activation-functions","chapter":"3 Introduction to Machine Learning","heading":"3.3.1 Caveat About Learning Rates and Activation Functions","text":"Depending activation functions, might occur network won’t get updated, even high learning rates (called vanishing gradient, especially “sigmoid” functions).\nFurthermore, updates might overshoot (called exploding gradients) activation functions result many zeros (especially “relu,” dying relu).general, first layers network tend learn (much) slowly subsequent ones.","code":""},{"path":"introduction.html","id":"introduction-to-tensorflow","chapter":"3 Introduction to Machine Learning","heading":"3.4 Introduction to TensorFlow","text":"One commonly used frameworks machine learning TensorFlow. TensorFlow open source linear algebra library focus neural networks, published Google 2015. TensorFlow supports several interesting features, particular automatic differentiation, several gradient optimizers CPU GPU parallelization.advantages nicely explained following video:sum important points video:TensorFlow math library highly optimized neural networks.GPU available, computations can easily run GPU even CPU TensorFlow still fast.“backend” (.e. functions computations) written C++ CUDA (CUDA programming language NVIDIA GPUs).interface (part TensorFlow use) written Python also available R, means, can write code R/Python executed (compiled) C++ backend.operations TensorFlow written C++ highly optimized. don’t worry, don’t use C++ use TensorFlow several bindings languages. TensorFlow officially supports Python API, meanwhile several community carried APIs languages:RGoRustSwiftJavaScriptIn course use TensorFlow https://tensorflow.rstudio.com/ binding, developed published 2017 RStudio team. First, developed R package (reticulate) calling Python R. Actually, using Python TensorFlow module R (later).TensorFlow offers different levels API. implement neural network completely use Keras provided submodule TensorFlow. Keras powerful module building training neural networks. allows us building training neural networks lines codes. Since end 2018, Keras TensorFlow completly interoperable, allowing us utilize best . course, show can use Keras neural networks also can use TensorFlow’s automatic differenation using complex objective functions.Useful links:TensorFlow documentation (Python API, just replace “.” “$.”)Rstudio TensorFlow website","code":""},{"path":"introduction.html","id":"tensorflow-data-containers","chapter":"3 Introduction to Machine Learning","heading":"3.4.1 TensorFlow Data Containers","text":"TensorFlow two data containers (structures):constant (tf$constant): Creates constant (immutable) value computation graph.variable (tf$Variable): Creates mutable value computation graph (used parameter/weight models).get started TensorFlow, load library check installation worked.Don’t worry weird messages (appear start session).","code":"\nlibrary(tensorflow)\nlibrary(keras)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n#> [1] TRUE\n\nimmutable = tf$constant(5.0)\n#> Loaded Tensorflow version 2.9.1\nmutable = tf$constant(5.0)"},{"path":"introduction.html","id":"basic-operations","chapter":"3 Introduction to Machine Learning","heading":"3.4.2 Basic Operations","text":"now can define variables math :Normal R methods print() provided R package “tensorflow.”TensorFlow library (created RStudio team) built R methods common operations:operators also automatically transform R numbers constant tensors attempting add tensor R number:TensorFlow containers objects, means just simple variables type numeric (class(5)), instead called methods. Methods changing state class (purposes values object).\ninstance, method transform tensor object back R object:","code":"\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n#> tf.Tensor(5.0, shape=(), dtype=float32)\nprint(b)\n#> tf.Tensor(10.0, shape=(), dtype=float32)\nc = tf$add(a, b)\nprint(c)\n#> tf.Tensor(15.0, shape=(), dtype=float32)\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\nk_print_tensor(c) # Comes out of Keras!\n#> tf.Tensor(15.0, shape=(), dtype=float32)\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\nk_print_tensor(a+b)\n#> tf.Tensor(15.0, shape=(), dtype=float32)\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n#> tf.Tensor(20.0, shape=(), dtype=float32)\nclass(d)\n#> [1] \"tensorflow.tensor\"                               \n#> [2] \"tensorflow.python.framework.ops.EagerTensor\"     \n#> [3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n#> [4] \"tensorflow.python.framework.ops.Tensor\"          \n#> [5] \"tensorflow.python.types.internal.NativeObject\"   \n#> [6] \"tensorflow.python.types.core.Tensor\"             \n#> [7] \"python.builtin.object\"\nclass(d$numpy())\n#> [1] \"numeric\""},{"path":"introduction.html","id":"tensorflow-data-types---good-practice-with-r-tensorflow","chapter":"3 Introduction to Machine Learning","heading":"3.4.3 TensorFlow Data Types - Good Practice With R-TensorFlow","text":"R uses dynamic typing, means can assign number, character, function whatever variable type automatically inferred.\nlanguages state type explicitly, e.g. C:TensorFlow tries infer type dynamically, must often state explicitly.\nCommon important types:float32 (floating point number 32 bits, “single precision”)float64 (floating point number 64 bits, “double precision”)int8 (integer 8 bits)reason TensorFlow explicit types many GPUs (e.g. NVIDIA GeForces) can handle 32 bit numbers! (need high precision graphical modeling)let us see practice types specifcy :went wrong ? tried divide float32 float64 number, can divide numbers type!can also specify type object providing object e.g. tf$float64.TensorFlow, arguments often require exact/explicit data types:\nTensorFlow often expects integers arguments. R however integer normally saved float.\nThus, use “L” integer tell R interpreter treated integer:Skipping “L” one common errors using R-TensorFlow!","code":"int a = 5;\nfloat a = 5.0;\nchar a = \"a\";\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()"},{"path":"introduction.html","id":"exercises-1","chapter":"3 Introduction to Machine Learning","heading":"3.4.4 Exercises","text":"run TensorFlow R, note can access different mathematical operations TensorFlow via tf$…, e.g. tf$math$… common math operations tf$linalg$… different linear algebra operations.\nTip: type tf$ hit tab key list available options (sometimes directly console).example: get maximum value vector?Rewrite following expressions (g) TensorFlow:\nexercise compares speed R TensorFlow.\nfirst exercise rewrite following function TensorFlow:, provide skeleton TensorFlow function:can compare speed using Microbenchmark package:Try different matrix sizes test matrix compare speed.Tip: look tf.reduce_mean documentation “axis” argument.Compare following different matrix sizes:test = matrix(0.0, 1000L, 500L)testTF = tf$constant(test)Also try following:\nR faster (first time)?R functions used (apply, mean, “-”) also implemented C.\nR functions used (apply, mean, “-”) also implemented C.problem large enough TensorFlow overhead.\nproblem large enough TensorFlow overhead.Google find write following tasks TensorFlow:\nTensorFlow supports automatic differentiation (analytical numerical!).\nLet’s look function \\(f(x) = 5 x^2 + 3\\) derivative \\(f'(x) = 10x\\).\n\\(f'(5)\\) get \\(10\\).Let’s TensorFlow. Define function:want calculate derivative \\(x = 2.0\\):automatic differentiation, forward \\(x\\) function within tf$GradientTape() environment. also tell TensorFlow value “watch”:print gradient:can also calculate second order derivative \\(f''(x) = 10\\):happening ? Think discuss .advanced example: Linear regressionIn case first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows normal distribution == error).R following fit linear regression model:Let’s build model TensorFlow.\n, use now variable data container type (remember mutable need type weights (\\(\\boldsymbol{w}\\)) regression model). want model learn weights.input (predictors, independent variables features, \\(\\boldsymbol{X}\\)) observed (response, \\(\\boldsymbol{y}\\)) constant learned/optimized.Discuss code, go code line line try understand .Additional exercise:Play around simulation, increase/decrease number weights, add intercept (also need additional variable model).\n","code":"\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\nmax(y)  # Float!\ntf$math$reduce_max(y) # Float!\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n#> [1] 1\n\n# b)\nmean(x)\n#> [1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n#> [1] 1\n\n# d) \nwhich.min(x)\n#> [1] 100\n\n# e) Tip: Use Google! \norder(x)\n#>   [1] 100  99  98  97  96  95  94  93  92  91  90  89  88\n#>  [14]  87  86  85  84  83  82  81  80  79  78  77  76  75\n#>  [27]  74  73  72  71  70  69  68  67  66  65  64  63  62\n#>  [40]  61  60  59  58  57  56  55  54  53  52  51  50  49\n#>  [53]  48  47  46  45  44  43  42  41  40  39  38  37  36\n#>  [66]  35  34  33  32  31  30  29  28  27  26  25  24  23\n#>  [79]  22  21  20  19  18  17  16  15  14  13  12  11  10\n#>  [92]   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n#>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#>  [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431\n#>  [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022\n#>  [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593\n#>  [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144\n#>  [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674\n#>  [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181\n#>  [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666\n#>  [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127\n#>  [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565\n#> [10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977\n#>           [,7]     [,8]     [,9]    [,10]\n#>  [1,] 10.46880 10.45305 10.43705 10.42079\n#>  [2,] 10.45478 10.43910 10.42317 10.40699\n#>  [3,] 10.44057 10.42496 10.40910 10.39299\n#>  [4,] 10.42614 10.41061 10.39482 10.37879\n#>  [5,] 10.41151 10.39605 10.38034 10.36439\n#>  [6,] 10.39666 10.38127 10.36565 10.34977\n#>  [7,] 10.38158 10.36628 10.35073 10.33495\n#>  [8,] 10.36628 10.35105 10.33559 10.31989\n#>  [9,] 10.35073 10.33559 10.32022 10.30461\n#> [10,] 10.33495 10.31989 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n#> [1] TRUE\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n#> tf.Tensor(1, shape=(), dtype=int32)\ntf$math$reduce_min(y) # Float!\n#> tf.Tensor(1.0, shape=(), dtype=float32)\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n#> [1] 50.5\nmean(y)\n#> [1] 50.5\ntf$math$reduce_mean(x)  # Integer!\n#> tf.Tensor(50, shape=(), dtype=int32)\ntf$math$reduce_mean(y)  # Float!\n#> tf.Tensor(50.5, shape=(), dtype=float32)\n\n# c)    which.max(x)\ntf$argmax(x)\n#> tf.Tensor(0, shape=(), dtype=int64)\ntf$argmax(y)\n#> tf.Tensor(0, shape=(), dtype=int64)\n\n# d)    which.min(x)\ntf$argmin(x)\n#> tf.Tensor(99, shape=(), dtype=int64)\n\n# e)    order(x)\ntf$argsort(x)\n#> tf.Tensor(\n#> [99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n#>  75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n#>  51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n#>  27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n#>   3  2  1  0], shape=(100), dtype=int32)\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n#> tf.Tensor(\n#> [[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n#>   10.433674  10.103771   9.608109   8.582045 ]\n#>  [11.311237  11.200746  11.076511  10.934624  10.769221  10.570931\n#>   10.323348   9.993557   9.498147   8.473241 ]\n#>  [11.186988  11.076511  10.952296  10.810434  10.645067  10.446828\n#>   10.199324   9.869672   9.374583   8.351139 ]\n#>  [11.045079  10.934624  10.810434  10.668606  10.503284  10.305112\n#>   10.057709   9.728241   9.233569   8.212026 ]\n#>  [10.87965   10.769221  10.645067  10.503284  10.338025  10.139942\n#>    9.892679   9.563459   9.069353   8.0503845]\n#>  [10.68132   10.570931  10.446828  10.305112  10.139942   9.941987\n#>    9.694924   9.366061   8.872768   7.857481 ]\n#>  [10.433674  10.323348  10.199324  10.057709   9.892679   9.694924\n#>    9.448175   9.119869   8.62784    7.6182513]\n#>  [10.103771   9.993557   9.869672   9.728241   9.563459   9.366061\n#>    9.119869   8.79255    8.302762   7.30317  ]\n#>  [ 9.608109   9.498147   9.374583   9.233569   9.069353   8.872768\n#>    8.62784    8.302762   7.818028   6.8405466]\n#>  [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n#>    7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n#> tf.Tensor(True, shape=(), dtype=bool)\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n#> Unit: microseconds\n#>                   expr      min        lq      mean\n#>   do_something_R(test)  488.105  515.1465  578.6744\n#>  do_something_TF(test) 2348.819 2455.7905 2654.4844\n#>    median       uq       max neval cld\n#>   557.570  589.306  2631.497   100  a \n#>  2506.715 2582.320 12534.910   100   b\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n#> Unit: milliseconds\n#>                   expr      min       lq      mean\n#>   do_something_R(test) 8.807962 9.449509 12.806207\n#>  do_something_TF(test) 3.737344 4.251434  5.149071\n#>     median       uq      max neval cld\n#>  12.010741 15.20489 27.11427   100   b\n#>   4.695778  5.19169 14.26088   100  a\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n#> Unit: milliseconds\n#>                                     expr      min        lq\n#>  tf$matmul(testTF, tf$transpose(testTF)) 6.438021 16.264679\n#>                         test %*% t(test) 7.808996  9.460024\n#>      mean   median       uq       max neval cld\n#>  17.84026 17.90731 20.25642  33.91409   100   a\n#>  17.99433 12.18004 21.16537 230.14897   100   a\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n#>      [,1] [,2]       [,3]\n#> [1,]    1  0.0 -0.6666667\n#> [2,]   -1  0.5 -0.1666667\n#> [3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n#> [1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    0    2    0\n#> [3,]    0    0    3\n\n# l)\neigen(A)\n#> eigen() decomposition\n#> $values\n#> [1] 3 2 1\n#> \n#> $vectors\n#>           [,1] [,2]       [,3]\n#> [1,] 0.1400280    0  0.4472136\n#> [2,] 0.9801961    1 -0.8944272\n#> [3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n#> [1] 6\nlibrary(tensorflow)\nlibrary(keras)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n#> tf.Tensor(\n#> [[ 1.          0.         -0.66666667]\n#>  [-1.          0.5        -0.16666667]\n#>  [ 0.          0.          0.33333333]], shape=(3, 3), dtype=float64)\n\n# j)    diag(A)\ntf$linalg$diag_part(A)\n#> tf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n#> tf.Tensor(\n#> [[1. 0. 0.]\n#>  [0. 2. 0.]\n#>  [0. 0. 3.]], shape=(3, 3), dtype=float64)\n\n# l)    eigen(A)\ntf$linalg$eigh(A)\n#> [[1]]\n#> tf.Tensor([-0.56155281  3.          3.56155281], shape=(3), dtype=float64)\n#> \n#> [[2]]\n#> tf.Tensor(\n#> [[-0.78820544  0.         -0.61541221]\n#>  [ 0.61541221  0.         -0.78820544]\n#>  [ 0.          1.         -0.        ]], shape=(3, 3), dtype=float64)\n\n# m)    det(A)\ntf$linalg$det(A)\n#> tf.Tensor(6.0, shape=(), dtype=float64)\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\nx = tf$constant(2.0)\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n(tape$gradient(y, x))\n#> tf.Tensor(20.0, shape=(), dtype=float32)\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n#> tf.Tensor(10.0, shape=(), dtype=float32)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\nsummary(lm(y~x))\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.67893 -0.16399  0.00968  0.15058  0.51099 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.004865   0.027447   0.177     0.86    \n#> x1          2.191511   0.023243  94.287   <2e-16 ***\n#> x2          2.741690   0.025328 108.249   <2e-16 ***\n#> x3          1.179181   0.023644  49.872   <2e-16 ***\n#> x4          0.591873   0.025154  23.530   <2e-16 ***\n#> x5          2.302417   0.022575 101.991   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2645 on 94 degrees of freedom\n#> Multiple R-squared:  0.9974, Adjusted R-squared:  0.9972 \n#> F-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n#> <tf.Variable 'Variable:0' shape=(5, 1) dtype=float64, numpy=\n#> array([[2.19290567],\n#>        [2.74534135],\n#>        [1.1714656 ],\n#>        [0.58811305],\n#>        [2.30174942]])>\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  2.217 2.719 1.165 0.593 2.303\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n  grads = tape$gradient(loss, interceptTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(interceptTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n#> <tf.Variable 'Variable:0' shape=(3, 1) dtype=float64, numpy=\n#> array([[2.46391571],\n#>        [2.45852885],\n#>        [1.00566707]])>\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  2.47 2.465 1.003\nk_print_tensor(interceptTF, message = \"Resulting intercept:\\n\")\n#> <tf.Variable 'Variable:0' shape=(1, 1) dtype=float64, numpy=array([[4.22135068]])>\ncat(\"Original intercept: \", intercept, \"\\n\")\n#> Original intercept:  4.09"},{"path":"introduction.html","id":"introduction-to-pytorch","chapter":"3 Introduction to Machine Learning","heading":"3.5 Introduction to PyTorch","text":"PyTorch another famous library deep learning. Like TensorFlow, Torch written C++ API Python. 2020, RStudio team released R-Torch, R-TensorFlow calls Python API background, R-Torch API built directly C++ Torch library!Useful links:PyTorch documentation (Python API, bust just replace “.” “$.”)R-Torch websiteTo get started Torch, load library check installation worked.","code":"\nlibrary(torch)"},{"path":"introduction.html","id":"pytorch-data-containers","chapter":"3 Introduction to Machine Learning","heading":"3.5.1 PyTorch Data Containers","text":"Unlike TensorFlow, Torch doesn’t two data containers mutable immutable variables. variables initialized via torch_tensor function:mark variables mutable (track operations automatic differentiation) set argument ‘requires_grad’ true torch_tensor function:","code":"\na = torch_tensor(1.)\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)"},{"path":"introduction.html","id":"basic-operations-1","chapter":"3 Introduction to Machine Learning","heading":"3.5.2 Basic Operations","text":"now can define variables math :R-Torch package provides common R methods (advantage TensorFlow).operators also automatically transform R numbers tensors attempting add tensor R number:TensorFlow, explicitly transform tensors back R:","code":"\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n#> torch_tensor\n#>  5\n#> [ CPUFloatType{1} ]\nprint(b)\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ]\nc = a$add(b)\nprint(c)\n#> torch_tensor\n#>  15\n#> [ CPUFloatType{1} ]\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n#> torch_tensor\n#>  15\n#> [ CPUFloatType{1} ]\nprint(a/b)\n#> torch_tensor\n#>  0.5000\n#> [ CPUFloatType{1} ]\nprint(a*b)\n#> torch_tensor\n#>  50\n#> [ CPUFloatType{1} ]\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ]\nclass(d)\n#> [1] \"torch_tensor\" \"R7\"\nclass(as.numeric(d))\n#> [1] \"numeric\""},{"path":"introduction.html","id":"torch-data-types---good-practice-with-r-torch","chapter":"3 Introduction to Machine Learning","heading":"3.5.3 Torch Data Types - Good Practice With R-Torch","text":"Similar TensorFlow:’s difference! TensorFlow get error, R-Torch, m automatically casted double (float64). However, still bad practice!course try provide corresponding PyTorch code snippets Keras/TensorFlow examples.","code":"\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b "},{"path":"introduction.html","id":"exercises-2","chapter":"3 Introduction to Machine Learning","heading":"3.5.4 Exercises","text":"Rewrite following expressions (g) torch:\nexercise compares speed R torch\nfirst exercise rewrite following function torch:, provide skeleton TensorFlow function:can compare speed using Microbenchmark package:Try different matrix sizes test matrix compare speed.Tip: look torch_mean documentation “dim” argument.Compare following different matrix sizes:test = matrix(0.0, 1000L, 500L)testTorch = torch_tensor(test)Also try following:\nR faster (first time)?R functions used (apply, mean, “-”) also implemented C.\nR functions used (apply, mean, “-”) also implemented C.problem large enough torch overhead.\nproblem large enough torch overhead.Google find write following tasks torch:\nTorch supports automatic differentiation (analytical numerical!).\nLet’s look function \\(f(x) = 5 x^2 + 3\\) derivative \\(f'(x) = 10x\\).\n\\(f'(5)\\) get \\(10\\).Let’s torch Define function:want calculate derivative \\(x = 2.0\\):automatic differentiation, forward \\(x\\) function call $backward() method result:print gradient:can also calculate second order derivative \\(f''(x) = 10\\):happening ? Think discuss .advanced example: Linear regressionIn case first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows normal distribution == error).R following fit linear regression model:Let’s build model TensorFlow.\n, use now variable data container type (remember mutable need type weights (\\(\\boldsymbol{w}\\)) regression model). want model learn weights.input (predictors, independent variables features, \\(\\boldsymbol{X}\\)) observed (response, \\(\\boldsymbol{y}\\)) constant learned/optimized.Discuss code, go code line line try understand .Additional exercise:Play around simulation, increase/decrease number weights, add intercept (also need additional variable model).\n","code":"\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n#> [1] 1\n\n# b)\nmean(x)\n#> [1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n#> [1] 1\n\n# d) \nwhich.min(x)\n#> [1] 100\n\n# e) Tip: Use Google! \norder(x)\n#>   [1] 100  99  98  97  96  95  94  93  92  91  90  89  88\n#>  [14]  87  86  85  84  83  82  81  80  79  78  77  76  75\n#>  [27]  74  73  72  71  70  69  68  67  66  65  64  63  62\n#>  [40]  61  60  59  58  57  56  55  54  53  52  51  50  49\n#>  [53]  48  47  46  45  44  43  42  41  40  39  38  37  36\n#>  [66]  35  34  33  32  31  30  29  28  27  26  25  24  23\n#>  [79]  22  21  20  19  18  17  16  15  14  13  12  11  10\n#>  [92]   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n#>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#>  [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431\n#>  [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022\n#>  [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593\n#>  [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144\n#>  [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674\n#>  [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181\n#>  [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666\n#>  [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127\n#>  [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565\n#> [10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977\n#>           [,7]     [,8]     [,9]    [,10]\n#>  [1,] 10.46880 10.45305 10.43705 10.42079\n#>  [2,] 10.45478 10.43910 10.42317 10.40699\n#>  [3,] 10.44057 10.42496 10.40910 10.39299\n#>  [4,] 10.42614 10.41061 10.39482 10.37879\n#>  [5,] 10.41151 10.39605 10.38034 10.36439\n#>  [6,] 10.39666 10.38127 10.36565 10.34977\n#>  [7,] 10.38158 10.36628 10.35073 10.33495\n#>  [8,] 10.36628 10.35105 10.33559 10.31989\n#>  [9,] 10.35073 10.33559 10.32022 10.30461\n#> [10,] 10.33495 10.31989 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n#> [1] TRUE\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\ntorch_min(y) # Float!\n#> torch_tensor\n#> 1\n#> [ CPUFloatType{} ]\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n#> [1] 50.5\nmean(y)\n#> [1] 50.5\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n#> torch_tensor\n#> 50.5\n#> [ CPUFloatType{} ]\ntorch_mean(y)  # Float!\n#> torch_tensor\n#> 50.5\n#> [ CPUFloatType{} ]\n\n# c)    which.max(x)\ntorch_argmax(x)\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\ntorch_argmax(y)\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\n\n# d)    which.min(x)\ntorch_argmin(x)\n#> torch_tensor\n#> 100\n#> [ CPULongType{} ]\n\n# e)    order(x)\ntorch_argsort(x)\n#> torch_tensor\n#>  100\n#>   99\n#>   98\n#>   97\n#>   96\n#>   95\n#>   94\n#>   93\n#>   92\n#>   91\n#>   90\n#>   89\n#>   88\n#>   87\n#>   86\n#>   85\n#>   84\n#>   83\n#>   82\n#>   81\n#>   80\n#>   79\n#>   78\n#>   77\n#>   76\n#>   75\n#>   74\n#>   73\n#>   72\n#>   71\n#> ... [the output was truncated (use n=-1 to disable)]\n#> [ CPULongType{100} ]\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n#> torch_tensor\n#>  4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n#>  4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n#>  4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n#>  4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n#>  4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n#>  3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n#>  3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n#>  3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n#>  2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n#>  2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n#> [ CPUFloatType{10,10} ]\n\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n#> torch_tensor\n#>  1\n#> [ CPUBoolType{1} ]\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n#> Unit: microseconds\n#>                      expr     min       lq     mean\n#>      do_something_R(test) 469.494 501.0335 533.5923\n#>  do_something_torch(test) 288.284 303.1915 380.8973\n#>    median       uq      max neval cld\n#>  514.6685 526.7010 2597.007   100   b\n#>  326.8505 377.0325 3030.417   100  a\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n#> Unit: milliseconds\n#>                      expr      min       lq      mean\n#>      do_something_R(test) 8.441663 8.629451 10.594149\n#>  do_something_torch(test) 2.107049 2.322623  2.827194\n#>    median       uq      max neval cld\n#>  8.746467 12.70234 31.92455   100   b\n#>  2.582252  2.84635 10.64425   100  a\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n#> Unit: milliseconds\n#>                                    expr      min        lq\n#>  torch_matmul(testTorch, testTorch$t()) 3.857475  4.227933\n#>                        test %*% t(test) 8.401205 10.052670\n#>      mean    median       uq       max neval cld\n#>   7.60400  4.609556 11.47081  29.26671   100  a \n#>  19.64731 16.737911 20.81175 264.60567   100   b\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n#>      [,1] [,2]       [,3]\n#> [1,]    1  0.0 -0.6666667\n#> [2,]   -1  0.5 -0.1666667\n#> [3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n#> [1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    0    2    0\n#> [3,]    0    0    3\n\n# l)\neigen(A)\n#> eigen() decomposition\n#> $values\n#> [1] 3 2 1\n#> \n#> $vectors\n#>           [,1] [,2]       [,3]\n#> [1,] 0.1400280    0  0.4472136\n#> [2,] 0.9801961    1 -0.8944272\n#> [3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n#> [1] 6\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n#> torch_tensor\n#>  1.0000  0.0000 -0.6667\n#> -1.0000  0.5000 -0.1667\n#>  0.0000  0.0000  0.3333\n#> [ CPUFloatType{3,3} ]\n\n# j)    diag(A)\ntorch_diag(A)\n#> torch_tensor\n#>  1\n#>  2\n#>  3\n#> [ CPUFloatType{3} ]\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n#> tf.Tensor(\n#> [[1. 0. 0.]\n#>  [0. 2. 0.]\n#>  [0. 0. 3.]], shape=(3, 3), dtype=float64)\ntorch_diag(A)$diag()\n#> torch_tensor\n#>  1  0  0\n#>  0  2  0\n#>  0  0  3\n#> [ CPUFloatType{3,3} ]\n\n# l)    eigen(A)\nlinalg_eigh(A)\n#> [[1]]\n#> torch_tensor\n#> -0.5616\n#>  3.0000\n#>  3.5616\n#> [ CPUFloatType{3} ]\n#> \n#> [[2]]\n#> torch_tensor\n#> -0.7882  0.0000  0.6154\n#>  0.6154  0.0000  0.7882\n#>  0.0000  1.0000  0.0000\n#> [ CPUFloatType{3,3} ]\n\n# m)    det(A)\nlinalg_det(A)\n#> torch_tensor\n#> 6\n#> [ CPUFloatType{} ]\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ny$backward(retain_graph=TRUE )\nx$grad\n#> torch_tensor\n#>  20\n#> [ CPUFloatType{1} ]\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ][ grad_fn = <MulBackward0> ]\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\nsummary(lm(y~x))\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.67893 -0.16399  0.00968  0.15058  0.51099 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.004865   0.027447   0.177     0.86    \n#> x1          2.191511   0.023243  94.287   <2e-16 ***\n#> x2          2.741690   0.025328 108.249   <2e-16 ***\n#> x3          1.179181   0.023644  49.872   <2e-16 ***\n#> x4          0.591873   0.025154  23.530   <2e-16 ***\n#> x5          2.302417   0.022575 101.991   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2645 on 94 degrees of freedom\n#> Multiple R-squared:  0.9974, Adjusted R-squared:  0.9972 \n#> F-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n#> [1] \"Loss: 4.4065318107605\"\n#> [1] \"Loss: 2.37925982475281\"\n#> [1] \"Loss: 0.901207089424133\"\n#> [1] \"Loss: 0.403193950653076\"\n#> [1] \"Loss: 0.296265482902527\"\n#> [1] \"Loss: 0.268377900123596\"\n#> [1] \"Loss: 0.232994794845581\"\n#> [1] \"Loss: 0.219554618000984\"\n#> [1] \"Loss: 0.215328559279442\"\n#> [1] \"Loss: 0.213282063603401\"\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n#> Inferred weights:  0.701 3.089 1.801 1.123 3.452\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  0.67 3.085 1.787 1.121 3.455\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n#> [1] \"Loss: 3.51533484458923\"\n#> [1] \"Loss: 1.74870145320892\"\n#> [1] \"Loss: 0.414169400930405\"\n#> [1] \"Loss: 0.518697261810303\"\n#> [1] \"Loss: 0.293963462114334\"\n#> [1] \"Loss: 0.263338685035706\"\n#> [1] \"Loss: 0.258341372013092\"\n#> [1] \"Loss: 0.254723280668259\"\n#> [1] \"Loss: 0.252453774213791\"\n#> [1] \"Loss: 0.25116890668869\"\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n#> Inferred weights:  3.118 -0.349 2.107\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  3.131 -0.353 2.11\n\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n#> Inferred intercept:  2.836\ncat(\"Original intercept: \", intercept, \"\\n\")\n#> Original intercept:  2.832"},{"path":"introduction.html","id":"first-steps-with-the-keras-framework","chapter":"3 Introduction to Machine Learning","heading":"3.6 First Steps With the Keras Framework","text":"seen can use TensorFlow directly R, use knowledge implement neural network TensorFlow directly R. However, can quite cumbersome. simple problems, usually faster use higher-level API helps us implementing machine learning models TensorFlow. common Keras.Keras powerful framework building training neural networks lines codes. Since end 2018, Keras TensorFlow completely interoperable, allowing us utilize best .objective lesson familiarize Keras. installed TensorFlow, Keras can found within TensorFlow: tf.keras. However, RStudio team built R package top tf.keras, convenient use . load Keras package, type","code":"\nlibrary(keras)"},{"path":"introduction.html","id":"example-workflow-in-keras-and-torch","chapter":"3 Introduction to Machine Learning","heading":"3.6.1 Example Workflow in Keras and Torch","text":"show Keras works, now build small classifier predict three species iris data set. Load necessary packages data sets:neural networks, beneficial scale predictors (scaling = centering standardization, see ?scale).\nalso split data predictors (X) response (Y = three species).Additionally, Keras/TensorFlow handle factors create contrasts (one-hot encoding).\n, specify number categories. can tricky beginner, programming languages like Python C++, arrays start zero. Thus, specify 3 number classes three species, classes 0,1,2,3. Keep mind.prepared data, now see typical workflow specify model Keras/Torch.1. Initialize sequential model Keras:sequential Keras model higher order type model within Keras consists one input one output model.2. Add hidden layers model (learn hidden layers next days).specifying hidden layers, also specify shape called activation function.\ncan think activation function decision forwarded next neuron (learn later).\nwant know topic even depth, consider watching videos presented section 3.3.shape input number predictors (4) shape output number classes (3).KerastorchThe Torch syntax similar, give list layers “nn_sequential” function. , specify softmax activation function extra layer:softmax scales potential multidimensional vector interval \\((0, 1]\\) component. sum components equals 1. might useful example handling probabilities. Ensure ther labels start 0! Otherwise softmax function work well!3. Compile model loss function (: cross entropy) optimizer (: Adamax).learn options later, now, worry “learning_rate” (“lr” Torch earlier TensorFlow) argument, cross entropy optimizer.KerastorchSpecify optimizer parameters trained (case parameters network):4. Fit model 30 iterations (epochs)\nTorch, jump directly training loop, however, write training loop:Get batch data.\nPredict batch.\nCcalculate loss predictions true labels.\nBackpropagate error.\nUpdate weights.\nGo step 1 repeat.\nPredict batch.Ccalculate loss predictions true labels.Backpropagate error.Update weights.Go step 1 repeat.5. Plot training history:\n6. Create predictions:Get probabilities:plant, want know species got highest probability:\n7. Calculate Accuracy (often correct):8. Plot predictions, see done good job:see, building neural network easy Keras can already .\n      \n        makeMultipleChoiceForm(\n         'look following two textbooks machine learning (&apos;<href=\"https://www.statlearning.com/\" target=\"_blank\" rel=\"noopener\">Introduction Statistical Learning<\/>&apos; &apos;<href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\" target=\"_blank\" rel=\"noopener\">Elements Statistical Learning<\/>&apos;) - following statements true?',\n          'checkbox',\n          [\n            {\n              'answer':'books can downloaded free.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'elements Statistical Learning published earlier Introduction Statistical Learning.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'book \"Introduction Statistical Learning\" also includes online course videos different topics website.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'Higher model complexity always better predicting.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            }\n          ],\n          ''\n        );\n      \n        makeMultipleChoiceForm(\n          'following statements bias-variance trade-correct?',\n          'checkbox',\n          [\n            {\n              'answer':'goal considering bias-variance trade-get bias model small possible.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'goal considering bias-variance trade-realize increasing complexity typically leads flexibility (allowing reduce bias) cost uncertainty (variance) estimated parameters.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n             'answer':'bias-variance trade-, see model complexity also depends want optimize : bias, variance (rarely), total error model.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            }\n          ],\n          ''\n        );\n      now build regression airquality data set Keras. want predict variable “Ozone.”Load prepare data set:Explore data summary() plot():NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Build sequential Keras model.Build sequential Keras model.Add hidden layers (input output layer already specified, add hidden layers ):Add hidden layers (input output layer already specified, add hidden layers ):use 5L input shape?one output node “linear” activation layer?Compile model.“mean_squared_error” loss?Fit model:Tip: matrices accepted \\(\\boldsymbol{X}\\) \\(\\boldsymbol{y}\\) Keras. R often drops one column matrix vector (change back matrix!)Plot training history.Plot training history.Create predictions.Create predictions.Compare Keras model linear model:Compare Keras model linear model:\n1. NAs data, remove Keras handle NAs.2. Split data predictors response scale matrix.3. Build sequential Keras model.4. Add hidden layers (input output layer already specified, add hidden layers ).use 5L input shape, 5 predictors. Analogously, use 1L 1d response.\nwant compression, dilation nonlinear effects, use simple linear layer (equal activation function ). activation functions, look example . wait next days.\nmay also seen previously shown link activation functions detail.5. Compile model.mean_squared_error ordinary least squares approach regression analysis.6. Fit model.7. Plot training history.8. Create predictions.9. Compare Keras model linear model.Look slightly complex model compare loss plot accuracy contrast former.see, complex model works better, can learn coherences better.\nkeep overfitting problem mind!Look little change learning rates next 2 models compare loss plot accuracy contrast former.can see, higher learning rate yields little bit worse results. optimum jumped .can see, lower learning rate, optimum (compared run learning rate 0.05) yet reached (epochs gone ).\nalso , mind overfitting problem. many epochs, things might get worse!task airquality example, go code line line try understand .\nNote, TensorFlow intermingled Keras.Now change code iris data set.\nTip: tf$keras$losses$… can find various loss functions.\nRemarks:Mind different input output layer numbers.loss function increases randomly, different subsets data drawn.\ndownside stochastic gradient descent.positive thing stochastic gradient descent , local valleys hills may left global ones can found instead.","code":"\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(torch)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n#> Loaded Tensorflow version 2.9.1\n\ndata(iris)\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\nX = scale(iris[,1:4])\nY = iris[,5]\nY = to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    1    0    0\n#> [3,]    1    0    0\n#> [4,]    1    0    0\n#> [5,]    1    0    0\n#> [6,]    1    0    0\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 3L, activation = \"softmax\") \nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n#> Model: \"sequential\"\n#> ____________________________________________________________\n#>  Layer (type)              Output Shape            Param #  \n#> ============================================================\n#>  dense_3 (Dense)           (None, 20)              100      \n#>  dense_2 (Dense)           (None, 20)              420      \n#>  dense_1 (Dense)           (None, 20)              420      \n#>  dense (Dense)             (None, 3)               63       \n#> ============================================================\n#> Total params: 1,003\n#> Trainable params: 1,003\n#> Non-trainable params: 0\n#> ____________________________________________________________\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel_history =\n  model %>%\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\nplot(model_history)\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\npredictions = predict(model, X) # Probabilities for each class.\nhead(predictions) # Quasi-probabilities for each species.\n#>           [,1]       [,2]        [,3]\n#> [1,] 0.9819264 0.01476339 0.003310232\n#> [2,] 0.9563531 0.03986335 0.003783490\n#> [3,] 0.9830711 0.01501246 0.001916326\n#> [4,] 0.9789233 0.01915258 0.001923956\n#> [5,] 0.9871404 0.01057778 0.002281806\n#> [6,] 0.9808626 0.01525488 0.003882431\npreds = apply(predictions, 1, which.max) \nprint(preds)\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [28] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2\n#>  [55] 3 2 3 2 2 2 2 3 2 3 2 3 3 2 2 2 3 2 2 2 2 3 3 3 3 2 2\n#>  [82] 2 2 3 2 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3\n#> [109] 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> [136] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [28] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 3 2\n#>  [55] 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2\n#>  [82] 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3\n#> [109] 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2\n#> [136] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\nmean(preds_torch == as.integer(iris$Species))\n#> [1] 0.9333333\nmean(preds == as.integer(iris$Species))\n#> [1] 0.8666667\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\npar(oldpar)   # Reset par.\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\nsummary(data)\n#>      Ozone           Solar.R           Wind       \n#>  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700  \n#>  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400  \n#>  Median : 31.50   Median :205.0   Median : 9.700  \n#>  Mean   : 42.13   Mean   :185.9   Mean   : 9.958  \n#>  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500  \n#>  Max.   :168.00   Max.   :334.0   Max.   :20.700  \n#>  NA's   :37       NA's   :7                       \n#>       Temp           Month            Day      \n#>  Min.   :56.00   Min.   :5.000   Min.   : 1.0  \n#>  1st Qu.:72.00   1st Qu.:6.000   1st Qu.: 8.0  \n#>  Median :79.00   Median :7.000   Median :16.0  \n#>  Mean   :77.88   Mean   :6.993   Mean   :15.8  \n#>  3rd Qu.:85.00   3rd Qu.:8.000   3rd Qu.:23.0  \n#>  Max.   :97.00   Max.   :9.000   Max.   :31.0  \n#> \nplot(data)\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\nprint(rmse_keras)\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n#>      Ozone          Solar.R           Wind      \n#>  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30  \n#>  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40  \n#>  Median : 31.0   Median :207.0   Median : 9.70  \n#>  Mean   : 42.1   Mean   :184.8   Mean   : 9.94  \n#>  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50  \n#>  Max.   :168.0   Max.   :334.0   Max.   :20.70  \n#>       Temp           Month            Day       \n#>  Min.   :57.00   Min.   :5.000   Min.   : 1.00  \n#>  1st Qu.:71.00   1st Qu.:6.000   1st Qu.: 9.00  \n#>  Median :79.00   Median :7.000   Median :16.00  \n#>  Mean   :77.79   Mean   :7.216   Mean   :15.95  \n#>  3rd Qu.:84.50   3rd Qu.:9.000   3rd Qu.:22.50  \n#>  Max.   :97.00   Max.   :9.000   Max.   :31.00\nx = scale(data[,2:6])\ny = data[,1]\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 173.4729\npred_keras = predict(model, x)\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 9.621961\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 30L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 105.1682\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 7.798208\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 30L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 116.6115\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 8.247861\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 30L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.01))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 238.9949\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 11.58784\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n#> $bX\n#>         Solar.R        Wind        Temp      Month\n#> 87  -1.13877323 -0.37654514  0.44147123 -0.1467431\n#> 117  0.58361881 -1.83815816  0.33653910  0.5319436\n#> 129 -1.01809608  1.56290291  0.65133550  1.2106304\n#> 121  0.44100036 -2.14734553  1.70065685  0.5319436\n#> 91   0.74817856 -0.71384046  0.54640337 -0.1467431\n#> 137 -1.76410028  0.26993754 -0.71278225  1.2106304\n#> 21  -1.93963068 -0.06735777 -1.97196786 -1.5041165\n#> 141 -1.73118833  0.10128988 -0.18812157  1.2106304\n#> 78   0.97856221  0.10128988  0.44147123 -0.1467431\n#> 15  -1.31430363  0.91642022 -2.07689999 -1.5041165\n#> 38  -0.63412333 -0.06735777  0.44147123 -0.8254298\n#> 49  -1.62148183 -0.20789749 -1.34237505 -0.8254298\n#> 123  0.03508631 -1.02302783  1.70065685  0.5319436\n#> 136  0.58361881 -1.02302783 -0.08318944  1.2106304\n#> 120  0.19964606 -0.06735777  2.01545325  0.5319436\n#> 114 -1.63245248  1.22560759 -0.60785011  0.5319436\n#> 145 -1.87380678 -0.20789749 -0.71278225  1.2106304\n#> 140  0.43002971  1.08506788 -1.13251078  1.2106304\n#> 64   0.56167751 -0.20789749  0.33653910 -0.1467431\n#> 118  0.33129386 -0.54519280  0.86119977  0.5319436\n#> 128 -0.98518413 -0.71384046  0.96613190  1.2106304\n#> 62   0.92370896 -1.64140257  0.65133550 -0.1467431\n#> 125  0.13382216 -1.36032314  1.49079258  1.2106304\n#> 4    1.40641756  0.43858520 -1.65717146 -1.5041165\n#> 79   1.09923936 -1.02302783  0.65133550 -0.1467431\n#> 82  -1.95060133 -0.85438017 -0.39798584 -0.1467431\n#> 149  0.08993956 -0.85438017 -0.81771438  1.2106304\n#> 17   1.34059366  0.57912491 -1.23744292 -1.5041165\n#> 48   1.08826871  3.02451593 -0.60785011 -0.8254298\n#> 130  0.73720791  0.26993754  0.23160696  1.2106304\n#> 132  0.49585361  0.26993754 -0.29305371  1.2106304\n#> 30   0.41905906 -1.19167548  0.12667483 -1.5041165\n#>            Day\n#> 87   1.1546835\n#> 117  1.0398360\n#> 129 -1.1422676\n#> 121  1.4992262\n#> 91   1.6140738\n#> 137 -0.2234871\n#> 21   0.5804458\n#> 141  0.2359031\n#> 78   0.1210555\n#> 15  -0.1086396\n#> 38  -1.0274200\n#> 49   0.2359031\n#> 123  1.7289213\n#> 136 -0.3383347\n#> 120  1.3843787\n#> 114  0.6952933\n#> 145  0.6952933\n#> 140  0.1210555\n#> 64  -1.4868103\n#> 118  1.1546835\n#> 128 -1.2571152\n#> 62  -1.7165054\n#> 125 -1.6016578\n#> 4   -1.3719627\n#> 79   0.2359031\n#> 82   0.5804458\n#> 149  1.1546835\n#> 17   0.1210555\n#> 48   0.1210555\n#> 130 -1.0274200\n#> 132 -0.7977249\n#> 30   1.6140738\n#> \n#> $bY\n#>  [1]  20 168  32 118  64   9   1  13  35  18  29  20  85  28\n#> [15]  76   9  23  18  32  73  47 135  78  18  61  16  30  34\n#> [29]  37  20  21 115\n\n\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n#> Loss:  645.1247 \n#> Loss:  257.9622 \n#> Loss:  257.9248 \n#> Loss:  424.3474 \n#> Loss:  132.1914 \n#> Loss:  201.8619 \n#> Loss:  225.3891 \n#> Loss:  111.7508 \n#> Loss:  343.3166 \n#> Loss:  255.3797 \n#> Loss:  245.1779 \n#> Loss:  227.4517 \n#> Loss:  222.4553 \n#> Loss:  348.0878 \n#> Loss:  365.9766 \n#> Loss:  178.8896 \n#> Loss:  220.2557 \n#> Loss:  344.3786 \n#> Loss:  238.2619 \n#> Loss:  324.3969\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n#> Loss:  0.002592299 \n#> Loss:  0.0004166169 \n#> Loss:  0.0005878718 \n#> Loss:  0.0001814893 \n#> Loss:  0.0003771982 \n#> Loss:  0.0006105618 \n#> Loss:  0.0003895268 \n#> Loss:  0.0001912034 \n#> Loss:  0.0002297373 \n#> Loss:  0.0001141062 \n#> Loss:  0.0002618438 \n#> Loss:  0.0001288175 \n#> Loss:  5.752431e-05 \n#> Loss:  0.000256366 \n#> Loss:  0.0002148773 \n#> Loss:  0.0001434388 \n#> Loss:  0.0001920019 \n#> Loss:  0.0001954518 \n#> Loss:  7.47276e-05 \n#> Loss:  2.274193e-05 \n#> Loss:  0.000115741 \n#> Loss:  2.059802e-05 \n#> Loss:  7.065996e-05 \n#> Loss:  1.295879e-05 \n#> Loss:  6.738321e-05 \n#> Loss:  2.543455e-05"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
