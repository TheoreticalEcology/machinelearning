[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"","code":""},{"path":"index.html","id":"r-system","chapter":"1 Prerequisites","heading":"1.1 R System","text":"Make sure recent version R (>=3.6, ideally >=4.0) RStudio computers.","code":""},{"path":"index.html","id":"tensorflow-and-keras","chapter":"1 Prerequisites","heading":"1.2 TensorFlow and Keras","text":"want run code computers, also need install TensorFlow / Keras R. , following work people.Run R:work computers, particular software recent. Sometimes, however, things don’t work well, especially python distribution often makes problems. installation work , can look together. Also, provide virtual machines case computers / laptops old don’t manage install TensorFlow.Warning: need least TensorFlow version 2.6, otherwise, argument “learning_rate” must “lr!”","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"rstudio/tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow()\n\ninstall.packages(\"keras\", dependencies = T)\nkeras::install_keras()"},{"path":"index.html","id":"torch-for-r","chapter":"1 Prerequisites","heading":"1.3 Torch for R","text":"may also use Torch R. R frontend popular PyTorch framework. install Torch, type R:","code":"\ninstall.packages(\"torch\")\nlibrary(torch)"},{"path":"index.html","id":"ecodata","chapter":"1 Prerequisites","heading":"1.4 EcoData","text":"may sometimes use data sets EcoData package. install package, run:","code":"\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = TRUE, build_vignettes = TRUE)"},{"path":"index.html","id":"further-used-libraries","chapter":"1 Prerequisites","heading":"1.5 Further Used Libraries","text":"make huge use different libraries. take coffee two (take …) install following libraries.\nPlease given order unless know ’re , dependencies packages.","code":"\ninstall.packages(\"abind\")\ninstall.packages(\"animation\")\ninstall.packages(\"ape\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"Rgraphviz\", \"graph\", \"RBGL\"))\ninstall.packages(\"coro\")\ninstall.packages(\"dbscan\")\ninstall.packages(\"dendextend\")\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"e1071\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"fields\")\ninstall.packages(\"forcats\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"gym\")\ninstall.packages(\"kknn\")\ninstall.packages(\"knitr\")\ninstall.packages(\"iml\")\ninstall.packages(\"lavaan\")\ninstall.packages(\"lmtest\")\ninstall.packages(\"magick\")\ninstall.packages(\"mclust\")\ninstall.packages(\"Metrics\")\ninstall.packages(\"microbenchmark\")\ninstall.packages(\"missRanger\")\ninstall.packages(\"mlbench\")\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\ninstall.packages(\"mlr3measures\")\ninstall.packages(\"mlr3pipelines\")\ninstall.packages(\"mlr3tuning\")\ninstall.packages(\"paradox\")\ninstall.packages(\"partykit\")\ninstall.packages(\"pcalg\")\ninstall.packages(\"piecewiseSEM\")\ninstall.packages(\"purrr\")\ninstall.packages(\"randomForest\")\ninstall.packages(\"ranger\")\ninstall.packages(\"reticulate\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"scales\")\ninstall.packages(\"semPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tfprobability\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"xgboost\")\n\ndevtools::install_github(\"andrie/deepviz\", dependencies = TRUE,\n                         upgrade = \"always\")\ndevtools::install_github(repo = \"florianhartig/EcoData\", subdir = \"EcoData\",\n                         dependencies = TRUE, build_vignettes = FALSE)\ndevtools::install_github('skinner927/reprtree')\ndevtools::install_version(\"lavaanPlot\", version = \"0.6.0\")\n\nreticulate::conda_install(\"r-reticulate\", packages = \"scipy\", pip = TRUE)\n\ntorch::install_torch()"},{"path":"index.html","id":"linuxunix-systems-have-to-fulfill-some-further-dependencies","chapter":"1 Prerequisites","heading":"1.6 Linux/UNIX systems have to fulfill some further dependencies","text":"Debian based systemsFor Debian based systems, need:new installing packages Debian / Ubuntu, etc., type following:Authors:Maximilian Pichler: @_Max_PichlerFlorian Hartig: @florianhartigContributors:Johannes Oberpriller, Matthias Meier","code":"build-essential\ngfortran\nlibmagick++-dev\nr-base-devsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev"},{"path":"reminder.html","id":"reminder","chapter":"2 Reminders About Basic Operations in R","heading":"2 Reminders About Basic Operations in R","text":"","code":""},{"path":"reminder.html","id":"your-r-system","chapter":"2 Reminders About Basic Operations in R","heading":"2.1 Your R System","text":"course, work combination R + RStudio.R calculation engine performs computations.RStudio editor helps sending inputs R collect outputs.Make sure recent version R + RStudio installed computer.never used RStudio, introductory video.","code":""},{"path":"reminder.html","id":"data-types-in-r","chapter":"2 Reminders About Basic Operations in R","heading":"2.2 Data types in R","text":"following subchapters introduce / remind R data types.","code":""},{"path":"reminder.html","id":"test-your-knowledge","chapter":"2 Reminders About Basic Operations in R","heading":"2.2.1 Test Your Knowledge","text":"Discuss partner(s) - meaning / structure / properties following data types R:Atomic types (atomic types exist)listvectordata.framematrixarray","code":""},{"path":"reminder.html","id":"iris-data","chapter":"2 Reminders About Basic Operations in R","heading":"2.2.2 Iris Data","text":"data type iris data set? Explore using following commands:","code":"\niris\n\nclass(iris)\ndim(iris)\nstr(iris)"},{"path":"reminder.html","id":"dynamic-typing","chapter":"2 Reminders About Basic Operations in R","heading":"2.2.3 Dynamic typing","text":"R dynamically typed language, means type variables determined automatically depending values supply. Try :also works data set already exists, .e. assign different value, type automatically changed. Look happens assign character value previously numeric column data.frame:Note numeric values changed characters well. can try force back values numeric :look values iris$Sepal.Length.Note:\nactions operate local copy iris data set. don’t overwrite base data can use new R session reset “data(iris).”","code":"\nx = 1\nclass(x)\nx = \"dog\"\nclass(x)\niris$Sepal.Length[2] = \"dog\"\nstr(iris)\niris$Sepal.Length = as.numeric(iris$Sepal.Length)"},{"path":"reminder.html","id":"data-selection-slicing-and-subsetting","chapter":"2 Reminders About Basic Operations in R","heading":"2.3 Data selection, Slicing and Subsetting","text":"chapter, discuss data selection, slicing subsetting.","code":""},{"path":"reminder.html","id":"subsetting-and-slicing-for-single-data-types","chapter":"2 Reminders About Basic Operations in R","heading":"2.3.1 Subsetting and Slicing for Single Data Types","text":"often want select subset data. can generally subset data structures using indices TRUE/FALSE (T/F).\nvector:use TRUE/FALSE, must specify truth value every (!) position.list, ’s basically , except following points:Elements lists usually name, can also access via “list$name.”Lists accessed [] return list. want select single element, access via [[]], “list[[2]].”data.frames objects im > 2, true, except several indices.syntax “matrix[1,]” also called slicing, obvious reasons.Data.frames matrices, except , like lists vectors, can also access columns via names “data.frame$column.”","code":"\nvector[1] # First element.\nvector[1:3] # Elements 1,2,3.\nvector[c(1,5,6)] # Elements 1,5,6.\nvector[c(T,T,F,F,T)] # Elements 1,2,5.\nvector = c(1,2,3,4,5)\nvector[c(T,F)] # Does NOT work!\nmatrix[1,2] # Element in first row, second column.\nmatrix[1:2,] # First two rows, all columns.\nmatrix[,c(T,F,T)] # All rows, 1st and 3rd column."},{"path":"reminder.html","id":"logic-and-slicing","chapter":"2 Reminders About Basic Operations in R","heading":"2.3.2 Logic and Slicing","text":"Slicing powerful combine logical operators, “&” (logical ), “|” (logical ), “==” (equal), “!=” (equal), “<=,” “>,” etc. examples:Note identical following:can also combine several logical commands:Note works element-wise!","code":"\nhead(iris[iris$Species == \"virginica\", ])\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n#> 101          6.3         3.3          6.0         2.5 virginica\n#> 102          5.8         2.7          5.1         1.9 virginica\n#> 103          7.1         3.0          5.9         2.1 virginica\n#> 104          6.3         2.9          5.6         1.8 virginica\n#> 105          6.5         3.0          5.8         2.2 virginica\n#> 106          7.6         3.0          6.6         2.1 virginica\nsubset(iris, Species == \"virginica\") \n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n#> 101          6.3         3.3          6.0         2.5 virginica\n#> 102          5.8         2.7          5.1         1.9 virginica\n#> 103          7.1         3.0          5.9         2.1 virginica\n#> 104          6.3         2.9          5.6         1.8 virginica\n#> 105          6.5         3.0          5.8         2.2 virginica\n#> 106          7.6         3.0          6.6         2.1 virginica\n#> 107          4.9         2.5          4.5         1.7 virginica\n#> 108          7.3         2.9          6.3         1.8 virginica\n#> 109          6.7         2.5          5.8         1.8 virginica\n#> 110          7.2         3.6          6.1         2.5 virginica\n#> 111          6.5         3.2          5.1         2.0 virginica\n#> 112          6.4         2.7          5.3         1.9 virginica\n#> 113          6.8         3.0          5.5         2.1 virginica\n#> 114          5.7         2.5          5.0         2.0 virginica\n#> 115          5.8         2.8          5.1         2.4 virginica\n#> 116          6.4         3.2          5.3         2.3 virginica\n#> 117          6.5         3.0          5.5         1.8 virginica\n#> 118          7.7         3.8          6.7         2.2 virginica\n#> 119          7.7         2.6          6.9         2.3 virginica\n#> 120          6.0         2.2          5.0         1.5 virginica\n#> 121          6.9         3.2          5.7         2.3 virginica\n#> 122          5.6         2.8          4.9         2.0 virginica\n#> 123          7.7         2.8          6.7         2.0 virginica\n#> 124          6.3         2.7          4.9         1.8 virginica\n#> 125          6.7         3.3          5.7         2.1 virginica\n#> 126          7.2         3.2          6.0         1.8 virginica\n#> 127          6.2         2.8          4.8         1.8 virginica\n#> 128          6.1         3.0          4.9         1.8 virginica\n#> 129          6.4         2.8          5.6         2.1 virginica\n#> 130          7.2         3.0          5.8         1.6 virginica\n#> 131          7.4         2.8          6.1         1.9 virginica\n#> 132          7.9         3.8          6.4         2.0 virginica\n#> 133          6.4         2.8          5.6         2.2 virginica\n#> 134          6.3         2.8          5.1         1.5 virginica\n#> 135          6.1         2.6          5.6         1.4 virginica\n#> 136          7.7         3.0          6.1         2.3 virginica\n#> 137          6.3         3.4          5.6         2.4 virginica\n#> 138          6.4         3.1          5.5         1.8 virginica\n#> 139          6.0         3.0          4.8         1.8 virginica\n#> 140          6.9         3.1          5.4         2.1 virginica\n#> 141          6.7         3.1          5.6         2.4 virginica\n#> 142          6.9         3.1          5.1         2.3 virginica\n#> 143          5.8         2.7          5.1         1.9 virginica\n#> 144          6.8         3.2          5.9         2.3 virginica\n#> 145          6.7         3.3          5.7         2.5 virginica\n#> 146          6.7         3.0          5.2         2.3 virginica\n#> 147          6.3         2.5          5.0         1.9 virginica\n#> 148          6.5         3.0          5.2         2.0 virginica\n#> 149          6.2         3.4          5.4         2.3 virginica\n#> 150          5.9         3.0          5.1         1.8 virginica\nhead(iris[iris$Species == \"virginica\" & iris$Sepal.Length > 7, ])\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n#> 103          7.1         3.0          5.9         2.1 virginica\n#> 106          7.6         3.0          6.6         2.1 virginica\n#> 108          7.3         2.9          6.3         1.8 virginica\n#> 110          7.2         3.6          6.1         2.5 virginica\n#> 118          7.7         3.8          6.7         2.2 virginica\n#> 119          7.7         2.6          6.9         2.3 virginica"},{"path":"reminder.html","id":"applying-functions-and-aggregates-across-a-data-set","chapter":"2 Reminders About Basic Operations in R","heading":"2.4 Applying Functions and Aggregates Across a Data set","text":"chapter, discuss basic functions R calculating means, averages apply functions across data set.","code":""},{"path":"reminder.html","id":"functions","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.1 Functions","text":"Maybe good time remind functions. two basic options use R :Variables / data structures.Functions.already used variables / data structures. Variables name type name R, get values inside respective data structure.Functions algorithms called like:example, can :want know summary function , type “?summary,” put mouse function press “F1.”able work properly data, know define functions. works like following:","code":"function(variable)\nsummary(iris)\nsquareValue = function(x){\n  temp = x * x \n  return(temp)\n}"},{"path":"reminder.html","id":"exercise","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.2 Exercise","text":"Try happens type \"squareValue(2)\".Write function multiplying 2 values. Hint - start \"function(x1, x2)\".Change first line \"squareValue\" function \"function(x = 3)\" try following commands: \"squareValue(2)\", \"squareValue()\" - sense syntax?\n  1\ngiven value (3 example ) default value. value used automatically, value supplied respective variable.\nDefault values can specified variables, put end function definition.\nHint: R, always useful name parameters using functions.Look following example:","code":"\nmultiply = function(x1, x2){\n  return(x1 * x2)\n}\nsquareValue(2)\n#> [1] 4\nsquareValue = function(x = 3){\n  temp = x * x \n  return(temp)\n}\n\nsquareValue(2)\n#> [1] 4\n\nsquareValue()\n#> [1] 9\ntestFunction = function(a = 1, b, c = 3){\n  return(a * b + c)\n}\n\ntestFunction()\n#> Error in testFunction(): argument \"b\" is missing, with no default\n\ntestFunction(10)\n#> Error in testFunction(10): argument \"b\" is missing, with no default\n\ntestFunction(10, 20)\n#> [1] 203\n\ntestFunction(10, 20, 30)\n#> [1] 230\n\ntestFunction(b = 10, c = 20, a = 30)\n#> [1] 320"},{"path":"reminder.html","id":"the-apply-function","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.3 The apply() Function","text":"Now know functions, can introduce functions use functions. One important apply function. apply function applies function data structure, typically matrix data.frame.Try following:","code":"\napply(iris[,1:4], 2, mean)"},{"path":"reminder.html","id":"exercise-1","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.4 Exercise","text":"Check help apply understand .first result \"apply(iris[,1:4], 2, mean)\" NA? Check help mean understand .Try \"apply(iris[,1:4], 1, mean)\". Think changed .happen use \"iris\" instead \"iris[,1:4]\"?.\n  1\nRemember, done (run part separately, execute following lines ):Taking mean character sequence possible, result NA (Available, missing value(s)).can skip missing values option “na.rm = TRUE” “mean” function. use “apply” function, pass argument(s) .Arrays (thus matrices, data.frame(s), etc.) several dimensions. simple \\(2D\\) array (matrix), first dimension rows second dimension columns. second parameter “apply” function specifies dimension mean computed. use \\(1\\), demand row means (150), use \\(2\\), request column means (5, resp. 4).5th column “Species.” values numeric. whole data.frame taken data.frame full characters.Remark: “NULL” statement return value apply. “str” returns nothing (prints something ), returned vector (array, list, …) empty, just like:","code":"\n?apply\niris$Sepal.Length[2] = \"Hund\"\niris$Sepal.Length = as.numeric(iris$Sepal.Length)\n#> Warning: NAs introduced by coercion\napply(iris[,1:4], 2, mean)\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>           NA     3.057333     3.758000     1.199333\napply(iris[,1:4], 2, mean, na.rm = T)\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n#>     5.849664     3.057333     3.758000     1.199333\napply(iris[,1:4], 1, mean)\n#>   [1] 2.550    NA 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500 2.325 2.125\n#>  [15] 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650 2.575 2.450 2.600 2.600\n#>  [29] 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400 2.625 2.500 2.225 2.550 2.525 2.100\n#>  [43] 2.275 2.675 2.800 2.375 2.675 2.350 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575\n#>  [57] 3.975 2.900 3.850 3.300 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275\n#>  [71] 3.925 3.550 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n#>  [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525 3.525 3.675\n#>  [99] 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575 4.200 4.850 4.200 4.075\n#> [113] 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675 4.525 3.825 4.800 3.925 4.450 4.550\n#> [127] 3.900 3.950 4.225 4.400 4.550 5.025 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375\n#> [141] 4.450 4.350 3.875 4.550 4.550 4.300 3.925 4.175 4.325 3.950\napply(iris, 2, mean)\n#> Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA\n\n#> Warning in mean.default(newX[, i], ...): argument is not numeric or logical: returning NA\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>           NA           NA           NA           NA           NA\napply(iris[,1:4], 2, str)\n#>  num [1:150] 5.1 NA 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#>  num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#>  num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#>  num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#> NULL\napply(iris, 2, str)\n#>  chr [1:150] \"5.1\" NA \"4.7\" \"4.6\" \"5.0\" \"5.4\" \"4.6\" \"5.0\" \"4.4\" \"4.9\" \"5.4\" \"4.8\" ...\n#>  chr [1:150] \"3.5\" \"3.0\" \"3.2\" \"3.1\" \"3.6\" \"3.9\" \"3.4\" \"3.4\" \"2.9\" \"3.1\" \"3.7\" \"3.4\" ...\n#>  chr [1:150] \"1.4\" \"1.4\" \"1.3\" \"1.5\" \"1.4\" \"1.7\" \"1.4\" \"1.5\" \"1.4\" \"1.5\" \"1.5\" \"1.6\" ...\n#>  chr [1:150] \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.4\" \"0.3\" \"0.2\" \"0.2\" \"0.1\" \"0.2\" \"0.2\" ...\n#>  chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n#> NULL\nc()\n#> NULL"},{"path":"reminder.html","id":"the-aggregate-function","chapter":"2 Reminders About Basic Operations in R","heading":"2.4.5 The aggregate() Function","text":"aggregate() calculates function per grouping variable. Try example:Note max function get maximum value, nothing lecturer, spelled Max.dot general R syntax usually refers “use columns data set.”","code":"\naggregate(. ~ Species, data = iris, FUN = max)"},{"path":"reminder.html","id":"plotting","chapter":"2 Reminders About Basic Operations in R","heading":"2.5 Plotting","text":"following two commands identical:plot(iris$Sepal.Length, iris$Sepal.Width)plot(Sepal.Width ~ Sepal.Length, data = iris)second option preferable, allows subset data easier.plot command use standard plot depending type variable supplied. example, x axis factor, boxplot produced.can change color, size, shape etc. often useful visualization.plots R short video:","code":"\nplot(Sepal.Width ~ Sepal.Length, data = iris[iris$Species == \"versicolor\", ])\nplot(Sepal.Width ~ Species, data = iris)\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species,\n     cex = iris$Petal.Length)"},{"path":"reminder.html","id":"additional-resources","chapter":"2 Reminders About Basic Operations in R","heading":"2.6 Additional Resources","text":"additional R resources self-study, recommend:","code":""},{"path":"reminder.html","id":"books","chapter":"2 Reminders About Basic Operations in R","heading":"2.6.1 Books","text":"PDF introduction R.Introduction Statistical Learning - simplified version version classic machine learning textbook, free PDF download.Quick R - Good site reference code examples standard tasks.Ebook Hands Programming R.","code":""},{"path":"reminder.html","id":"instructional-videos","chapter":"2 Reminders About Basic Operations in R","heading":"2.6.2 Instructional videos","text":"YouTube - R Programming Tutorial - Learn Basics Statistical Computing (approx 2h, goes basics).YouTube - Statistics R, Tutorials MarinStatsLectures - Lots smaller videos particular topics.","code":""},{"path":"introduction.html","id":"introduction","chapter":"3 Introduction to Machine Learning","heading":"3 Introduction to Machine Learning","text":"three basic machine learning tasks:Supervised learningUnsupervised learningReinforcement learningIn supervised learning, train algorithms using labeled data, means already know correct answer part data (called training data).Unsupervised learning contrast technique, one need monitor model apply labels. Instead, allow model work discover information.Reinforcement learning technique emulates game-like situation. algorithm finds solution trial error gets either rewards penalties every action. games, goal maximize rewards. talk technique last day course.moment, focus first two tasks, supervised unsupervised learning. , begin small example. start code, video prepare class:","code":""},{"path":"introduction.html","id":"unsupervised-learning","chapter":"3 Introduction to Machine Learning","heading":"3.1 Unsupervised Learning","text":"unsupervised learning, want identify patterns data without examples (supervision) correct patterns / classes . example, consider iris data set. , 150 observations 4 floral traits:\nFigure 3.1: Trait distributions iris dataset\nobservations 3 species indeed species tend different traits, meaning observations form 3 clusters.\nFigure 3.2: Scatterplots trait-trait combinations.\nHowever, imagine don’t know species , basically situation people antique . people just noted plants different flowers others, decided give different names. kind process unsupervised learning .","code":"\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4]) \nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits, \n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])"},{"path":"introduction.html","id":"hierarchical-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.1 Hierarchical Clustering","text":"cluster refers collection data points aggregated together certain similarities.hierarchical clustering, hierarchy (tree) data points built.Agglomerative: Start data point cluster, merge hierarchically.Divisive: Start data points one cluster, split hierarchically.Merges / splits done according linkage criterion, measures distance (potential) clusters. Cut tree certain height get clusters.example\nFigure 3.3: Results hierarchical clustering. Red rectangle drawn around corresponding clusters.\nplot, colors true species identity\nFigure 3.4: Results hierarchical clustering. Colors correspond three species classes.\nCalculate confusion matrix. Note switching labels fits species.Table 3.1: Confusion matrix predicted observed species classes.Note results might change choose different agglomeration method, distance metric scale variables. Compare, e.g. example:\nFigure 3.5: Results hierarchical clustering. Colors correspond three species classes. Different agglomeration method\nTable 3.2: Confusion matrix predicted observed species classes.method best?might conclude ward.D2 works best . However, learn later, optimizing method without hold-testing implies model may overfitting. check using cross-validation.","code":"\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nd = dist(traits)\nhc = hclust(d, method = \"complete\")\n\nplot(hc, main=\"\")\nrect.hclust(hc, k = 3)  # Draw rectangles around the branches.\nlibrary(ape)\n\nplot(as.phylo(hc), \n     tip.color = colors[as.integer(species)], \n     direction = \"downwards\")\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntmp = hcRes3\ntmp[hcRes3 == 2] = 3\ntmp[hcRes3 == 3] = 2\nhcRes3 = tmp\ntable(hcRes3, species)\nhc = hclust(d, method = \"ward.D2\")\n\nplot(as.phylo(hc), \n     tip.color = colors[as.integer(species)], \n     direction = \"downwards\")\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntable(hcRes3, species)\nlibrary(dendextend)\nset.seed(123)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n             \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\nout = dendlist()   # Create a dendlist object from several dendrograms.\nfor(method in methods){\n  res = hclust(d, method = method)   \n  out = dendlist(out, as.dendrogram(res))\n}\nnames(out) = methods\nprint(out)\n#> $ward.D\n#> 'dendrogram' with 2 branches and 150 members total, at height 199.6205 \n#> \n#> $single\n#> 'dendrogram' with 2 branches and 150 members total, at height 1.640122 \n#> \n#> $complete\n#> 'dendrogram' with 2 branches and 150 members total, at height 7.085196 \n#> \n#> $average\n#> 'dendrogram' with 2 branches and 150 members total, at height 4.062683 \n#> \n#> $mcquitty\n#> 'dendrogram' with 2 branches and 150 members total, at height 4.497283 \n#> \n#> $median\n#> 'dendrogram' with 2 branches and 150 members total, at height 2.82744 \n#> \n#> $centroid\n#> 'dendrogram' with 2 branches and 150 members total, at height 2.994307 \n#> \n#> $ward.D2\n#> 'dendrogram' with 2 branches and 150 members total, at height 32.44761 \n#> \n#> attr(,\"class\")\n#> [1] \"dendlist\"\n\nget_ordered_3_clusters = function(dend){\n  # order.dendrogram function returns the order (index)\n  # or the \"label\" attribute for the leaves.\n  # cutree: Cut the tree (dendrogram) into groups of data.\n  cutree(dend, k = 3)[order.dendrogram(dend)]\n}\ndend_3_clusters = lapply(out, get_ordered_3_clusters)\n\n# Calculate Fowlkes-Mallows Index (determine the similarity between clusterings)\ncompare_clusters_to_iris = function(clus){\n  FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)\n}\n\nclusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\ndotchart(sort(clusters_performance), xlim = c(0.3, 1),\n         xlab = \"Fowlkes-Mallows index\",\n         main = \"Performance of linkage methods\n         in detecting the 3 species \\n in our example\",\n         pch = 19)"},{"path":"introduction.html","id":"k-means-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.2 K-means Clustering","text":"Another example unsupervised learning algorithm k-means clustering, one simplest popular unsupervised machine learning algorithms.start algorithm, first specify number clusters (example number species). cluster centroid, assumed real location representing center cluster (example average plant specific species look like). algorithm starts randomly putting centroids somewhere. Afterwards data point assigned respective cluster raises overall -cluster sum squares (variance) related distance centroid least . algorithm placed data points cluster centroids get updated. iterating procedure assignment doesn’t change longer, algorithm can find (locally) optimal centroids data points belonging cluster.\nNote results might differ according initial positions centroids. Thus several (locally) optimal solutions might found.“k” K-means refers number clusters ‘means’ refers averaging data-points find centroids.typical pipeline using k-means clustering looks algorithms. visualized data, fit model, visualize results look performance use confusion matrix. setting fixed seed, can ensure results reproducible.Visualizing results.\nColor codes true species identity, symbol shows cluster result.see discrepancies. Confusion matrix:want animate clustering process, runElbow technique determine probably best suited number clusters:Often, one interested sparse models. Furthermore, higher k necessary tends overfitting. kink picture, sum squares dropped enough k still low enough.\nkeep mind, rule thumb might wrong special cases.","code":"\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n#> K-means clustering with 3 clusters of sizes 50, 62, 38\n#> \n#> Cluster means:\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 1     5.006000    3.428000     1.462000    0.246000\n#> 2     5.901613    2.748387     4.393548    1.433871\n#> 3     6.850000    3.073684     5.742105    2.071053\n#> \n#> Clustering vector:\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2\n#>  [85] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3\n#> [127] 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2\n#> \n#> Within cluster sum of squares by cluster:\n#> [1] 15.15100 39.82097 23.87947\n#>  (between_SS / total_SS =  88.4 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\ntable(iris$Species, kc$cluster)\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 48  2\n#>   virginica   0 14 36\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE, \n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))"},{"path":"introduction.html","id":"density-based-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.3 Density-based Clustering","text":"Determine affinity data point according affinity k nearest neighbors.\ngeneral description many ways .","code":"\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nlibrary(dbscan)\nset.seed(123)\n\nkNNdistplot(traits, k = 4)   # Calculate and plot k-nearest-neighbor distances.\nabline(h = 0.4, lty = 2)\n\ndc = dbscan(traits, eps = 0.4, minPts = 6)\nprint(dc)\n#> DBSCAN clustering for 150 objects.\n#> Parameters: eps = 0.4, minPts = 6\n#> The clustering contains 4 cluster(s) and 32 noise points.\n#> \n#>  0  1  2  3  4 \n#> 32 46 36 14 22 \n#> \n#> Available fields: cluster, eps, minPts\nlibrary(factoextra)\nfviz_cluster(dc, traits, geom = \"point\", ggtheme = theme_light())"},{"path":"introduction.html","id":"model-based-clustering","chapter":"3 Introduction to Machine Learning","heading":"3.1.4 Model-based Clustering","text":"last class methods unsupervised clustering -called model-based clustering methods.Mclust automatically compares number candidate models (clusters, shape) according BIC (BIC criterion classifying algorithms depending prediction quality usage parameters). can look selected model via:see algorithm prefers 2 clusters. better comparability 2 methods, override setting:Result terms predicted densities 3 clustersPredicted clusters:Confusion matrix:","code":"\nlibrary(mclust)\n#> Package 'mclust' version 5.4.10\n#> Type 'citation(\"mclust\")' for citing this R package in publications.\nmb = Mclust(traits)\nmb$G # Two clusters.\n#> [1] 2\nmb$modelName # > Ellipsoidal, equal shape.\n#> [1] \"VEV\"\nmb3 = Mclust(traits, 3)\nplot(mb3, \"density\")\nplot(mb3, what=c(\"classification\"), add = T)\ntable(iris$Species, mb3$classification)"},{"path":"introduction.html","id":"ordination","chapter":"3 Introduction to Machine Learning","heading":"3.1.5 Ordination","text":"Ordination used explorative analysis compared clustering, similar objects ordered together.\nrelationship clustering ordination. PCA ordination iris data set.can cluster results ordination, ordinate clustering, superimpose one .","code":"\npcTraits = prcomp(traits, center = TRUE, scale. = TRUE)\nbiplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25))"},{"path":"introduction.html","id":"exercise-2","chapter":"3 Introduction to Machine Learning","heading":"3.1.6 Exercise","text":"Go 4(5) algorithms , check sensitive (.e. results change) scale input features (= predictors), instead using raw data. Discuss group: appropriate analysis /general: Scaling scaling?\n  Hierarchical Clustering\nseems scaling harmful hierarchical clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.seems scaling harmful K-means clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.seems scaling harmful density based clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.model based clustering, scaling matter.PCA ordination, scaling matters.\ninterested directions maximal variance, parameters scaled, one highest values might dominate others.","code":"\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)   \n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n  \n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n  \n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n  \n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n  \n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n  \n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n  \n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#>  [85] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> [127] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> Levels: 1 2 3\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n  \n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n  \n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n  \n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n  \n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n#> Not scaled \n#> \n#> Confusion matrix:\n#>        labels\n#> correct noise  1  2  3  4\n#>       1     3 47  0  0  0\n#>       2     5  0 38  3  4\n#>       3    17  0  0 33  0\n#> \n#> Correct classified points:  118  /  150\n#> Sum of noise points:  25\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n#> Scaled \n#> \n#> Confusion matrix:\n#>        labels\n#> correct noise  1  2  3  4\n#>       1     9 41  0  0  0\n#>       2    14  0 36  0  0\n#>       3    36  0  1  4  9\n#> \n#> Correct classified points:  81  /  150\n#> Sum of noise points:  59\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n  \n  mb3 = Mclust(traits, 3)\n  \n  tbl = table(iris$Species, mb3$classification)\n  \n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n#> Not scaled \n#> \n#> Confusion matrix:\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 45  5\n#>   virginica   0  0 50\n#> \n#> Correct classified points:  145  /  150\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n#> Scaled \n#> \n#> Confusion matrix:\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 45  5\n#>   virginica   0  0 50\n#> \n#> Correct classified points:  145  /  150\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")"},{"path":"introduction.html","id":"supervised-learning","chapter":"3 Introduction to Machine Learning","heading":"3.2 Supervised Learning","text":"two prominent branches supervised learning regression classification. Fundamentally, classification predicting label regression predicting quantity. following video explains depth:","code":""},{"path":"introduction.html","id":"regression","chapter":"3 Introduction to Machine Learning","heading":"3.2.1 Regression","text":"random forest (RF) algorithm possibly widely used machine learning algorithm can used regression classification. talk algorithm tomorrow.moment, want go typical workflow supervised regression: First, visualize data. Next, fit model lastly visualize results. use iris data set used . goal now predict Sepal.Length based information variables (including species).Fitting model:Visualization results:understand structure random forest detail, can use package GitHub., one regression trees shown.","code":"\nlibrary(randomForest)\nset.seed(123)\nm1 = randomForest(Sepal.Length ~ ., data = iris)   # ~.: Against all others.\n# str(m1)\n# m1$type\n# predict(m1)\nprint(m1)\n#> \n#> Call:\n#>  randomForest(formula = Sepal.Length ~ ., data = iris) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 1\n#> \n#>           Mean of squared residuals: 0.1364625\n#>                     % Var explained: 79.97\noldpar = par(mfrow = c(1, 2))\nplot(predict(m1), iris$Sepal.Length, xlab = \"Predicted\", ylab = \"Observed\")\nabline(0, 1)\nvarImpPlot(m1)\npar(oldpar)\nreprtree:::plot.getTree(m1, iris)"},{"path":"introduction.html","id":"classification","chapter":"3 Introduction to Machine Learning","heading":"3.2.2 Classification","text":"random forest, can also classification. steps regression tasks, can additionally see well performed looking confusion matrix. row matrix contains instances predicted class column represents instances actual class. Thus diagonals correctly predicted classes -diagonal elements falsely classified elements.Fitting model:Visualizing one fitted models:Visualizing results ecologically:Confusion matrix:","code":"\nset.seed(123)\n\nm1 = randomForest(Species ~ ., data = iris)\noldpar = par(mfrow = c(1, 2))\nreprtree:::plot.getTree(m1, iris)\npar(mfrow = c(1, 2))\nplot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = \"Observed\")\nplot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = \"Predicted\")\npar(oldpar)   #Reset par.\ntable(predict(m1), iris$Species)"},{"path":"introduction.html","id":"questions","chapter":"3 Introduction to Machine Learning","heading":"3.2.3 Questions","text":"\n      \n        makeMultipleChoiceForm(\n         'Using random forest iris dataset, parameter important (remember function check ) predict Petal.Width?',\n          'radio',\n          [\n            {\n              'answer':'Species.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'Sepal.Width.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n          ],\n          ''\n        );\n      ","code":""},{"path":"tensorflowintro.html","id":"tensorflowintro","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4 Introduction to TensorFlow, Keras, and Torch","text":"","code":""},{"path":"tensorflowintro.html","id":"introduction-to-tensorflow","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.1 Introduction to TensorFlow","text":"One commonly used frameworks machine learning TensorFlow. TensorFlow open source linear algebra library focus neural networks, published Google 2015. TensorFlow supports several interesting features, particular automatic differentiation, several gradient optimizers CPU GPU parallelization.advantages nicely explained following video:sum important points video:TensorFlow math library highly optimized neural networks.GPU available, computations can easily run GPU even CPU TensorFlow still fast.“backend” (.e. functions computations) written C++ CUDA (CUDA programming language NVIDIA GPUs).interface (part TensorFlow use) written Python also available R, means, can write code R/Python executed (compiled) C++ backend.operations TensorFlow written C++ highly optimized. don’t worry, don’t use C++ use TensorFlow several bindings languages. TensorFlow officially supports Python API, meanwhile several community carried APIs languages:RGoRustSwiftJavaScriptIn course use TensorFlow https://tensorflow.rstudio.com/ binding, developed published 2017 RStudio team. First, developed R package (reticulate) calling Python R. Actually, using Python TensorFlow module R (later).TensorFlow offers different levels API. implement neural network completely use Keras provided submodule TensorFlow. Keras powerful module building training neural networks. allows us building training neural networks lines codes. Since end 2018, Keras TensorFlow completly interoperable, allowing us utilize best . course, show can use Keras neural networks also can use TensorFlow’s automatic differenation using complex objective functions.Useful links:TensorFlow documentation (Python API, just replace “.” “$.”)Rstudio TensorFlow website","code":""},{"path":"tensorflowintro.html","id":"data-containers","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.1.1 Data Containers","text":"TensorFlow two data containers (structures):constant (tf$constant): Creates constant (immutable) value computation graph.variable (tf$Variable): Creates mutable value computation graph (used parameter/weight models).get started TensorFlow, load library check installation worked.Don’t worry weird messages (appear start session).","code":"\nlibrary(tensorflow)\nlibrary(keras)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n#> [1] TRUE\n\nimmutable = tf$constant(5.0)\n#> Loaded Tensorflow version 2.9.1\nmutable = tf$constant(5.0)"},{"path":"tensorflowintro.html","id":"basic-operations","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.1.2 Basic Operations","text":"now can define variables math :Normal R methods print() provided R package “tensorflow.”TensorFlow library (created RStudio team) built R methods common operations:operators also automatically transform R numbers constant tensors attempting add tensor R number:TensorFlow containers objects, means just simple variables type numeric (class(5)), instead called methods. Methods changing state class (purposes values object).\ninstance, method transform tensor object back R object:","code":"\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n#> tf.Tensor(5.0, shape=(), dtype=float32)\nprint(b)\n#> tf.Tensor(10.0, shape=(), dtype=float32)\nc = tf$add(a, b)\nprint(c)\n#> tf.Tensor(15.0, shape=(), dtype=float32)\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\nk_print_tensor(c) # Comes out of Keras!\n#> tf.Tensor(15.0, shape=(), dtype=float32)\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\nk_print_tensor(a+b)\n#> tf.Tensor(15.0, shape=(), dtype=float32)\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n#> tf.Tensor(20.0, shape=(), dtype=float32)\nclass(d)\n#> [1] \"tensorflow.tensor\"                               \n#> [2] \"tensorflow.python.framework.ops.EagerTensor\"     \n#> [3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n#> [4] \"tensorflow.python.framework.ops.Tensor\"          \n#> [5] \"tensorflow.python.types.internal.NativeObject\"   \n#> [6] \"tensorflow.python.types.core.Tensor\"             \n#> [7] \"python.builtin.object\"\nclass(d$numpy())\n#> [1] \"numeric\""},{"path":"tensorflowintro.html","id":"data-types","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.1.3 Data Types","text":"R uses dynamic typing, means can assign number, character, function whatever variable type automatically inferred.\nlanguages state type explicitly, e.g. C:TensorFlow tries infer type dynamically, must often state explicitly.\nCommon important types:float32 (floating point number 32 bits, “single precision”)float64 (floating point number 64 bits, “double precision”)int8 (integer 8 bits)reason TensorFlow explicit types many GPUs (e.g. NVIDIA GeForces) can handle 32 bit numbers! (need high precision graphical modeling)let us see practice types specifcy :went wrong ? tried divide float32 float64 number, can divide numbers type!can also specify type object providing object e.g. tf$float64.TensorFlow, arguments often require exact/explicit data types:\nTensorFlow often expects integers arguments. R however integer normally saved float.\nThus, use “L” integer tell R interpreter treated integer:Skipping “L” one common errors using R-TensorFlow!","code":"int a = 5;\nfloat a = 5.0;\nchar a = \"a\";\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()"},{"path":"tensorflowintro.html","id":"exercises","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.1.4 Exercises","text":"run TensorFlow R, note can access different mathematical operations TensorFlow via tf$…, e.g. tf$math$… common math operations tf$linalg$… different linear algebra operations.\nTip: type tf$ hit tab key list available options (sometimes directly console).example: get maximum value vector?Rewrite following expressions (g) TensorFlow:\nexercise compares speed R TensorFlow.\nfirst exercise rewrite following function TensorFlow:, provide skeleton TensorFlow function:can compare speed using Microbenchmark package:Try different matrix sizes test matrix compare speed.Tip: look tf.reduce_mean documentation “axis” argument.Compare following different matrix sizes:test = matrix(0.0, 1000L, 500L)testTF = tf$constant(test)Also try following:\nR faster (first time)?R functions used (apply, mean, “-”) also implemented C.\nR functions used (apply, mean, “-”) also implemented C.problem large enough TensorFlow overhead.\nproblem large enough TensorFlow overhead.Google find write following tasks TensorFlow:\nTensorFlow supports automatic differentiation (analytical numerical!).\nLet’s look function \\(f(x) = 5 x^2 + 3\\) derivative \\(f'(x) = 10x\\).\n\\(f'(5)\\) get \\(10\\).Let’s TensorFlow. Define function:want calculate derivative \\(x = 2.0\\):automatic differentiation, forward \\(x\\) function within tf$GradientTape() environment. also tell TensorFlow value “watch”:print gradient:can also calculate second order derivative \\(f''(x) = 10\\):happening ? Think discuss .advanced example: Linear regressionIn case first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows normal distribution == error).R following fit linear regression model:Let’s build model TensorFlow.\n, use now variable data container type (remember mutable need type weights (\\(\\boldsymbol{w}\\)) regression model). want model learn weights.input (predictors, independent variables features, \\(\\boldsymbol{X}\\)) observed (response, \\(\\boldsymbol{y}\\)) constant learned/optimized.Discuss code, go code line line try understand .Additional exercise:Play around simulation, increase/decrease number weights, add intercept (also need additional variable model).\n","code":"\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\nmax(y)  # Float!\ntf$math$reduce_max(y) # Float!\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n#> [1] 1\n\n# b)\nmean(x)\n#> [1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n#> [1] 1\n\n# d) \nwhich.min(x)\n#> [1] 100\n\n# e) Tip: Use Google! \norder(x)\n#>   [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83  82  81  80\n#>  [22]  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65  64  63  62  61  60  59\n#>  [43]  58  57  56  55  54  53  52  51  50  49  48  47  46  45  44  43  42  41  40  39  38\n#>  [64]  37  36  35  34  33  32  31  30  29  28  27  26  25  24  23  22  21  20  19  18  17\n#>  [85]  16  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n#>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]\n#>  [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305 10.43705\n#>  [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910 10.42317\n#>  [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496 10.40910\n#>  [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061 10.39482\n#>  [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605 10.38034\n#>  [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127 10.36565\n#>  [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628 10.35073\n#>  [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105 10.33559\n#>  [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559 10.32022\n#> [10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989 10.30461\n#>          [,10]\n#>  [1,] 10.42079\n#>  [2,] 10.40699\n#>  [3,] 10.39299\n#>  [4,] 10.37879\n#>  [5,] 10.36439\n#>  [6,] 10.34977\n#>  [7,] 10.33495\n#>  [8,] 10.31989\n#>  [9,] 10.30461\n#> [10,] 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n#> [1] TRUE\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n#> tf.Tensor(1, shape=(), dtype=int32)\ntf$math$reduce_min(y) # Float!\n#> tf.Tensor(1.0, shape=(), dtype=float32)\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n#> [1] 50.5\nmean(y)\n#> [1] 50.5\ntf$math$reduce_mean(x)  # Integer!\n#> tf.Tensor(50, shape=(), dtype=int32)\ntf$math$reduce_mean(y)  # Float!\n#> tf.Tensor(50.5, shape=(), dtype=float32)\n\n# c)    which.max(x)\ntf$argmax(x)\n#> tf.Tensor(0, shape=(), dtype=int64)\ntf$argmax(y)\n#> tf.Tensor(0, shape=(), dtype=int64)\n\n# d)    which.min(x)\ntf$argmin(x)\n#> tf.Tensor(99, shape=(), dtype=int64)\n\n# e)    order(x)\ntf$argsort(x)\n#> tf.Tensor(\n#> [99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n#>  75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n#>  51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n#>  27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n#>   3  2  1  0], shape=(100), dtype=int32)\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n#> tf.Tensor(\n#> [[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n#>   10.433674  10.103771   9.608109   8.582045 ]\n#>  [11.311237  11.200746  11.076511  10.934624  10.769221  10.570931\n#>   10.323348   9.993557   9.498147   8.473241 ]\n#>  [11.186988  11.076511  10.952296  10.810434  10.645067  10.446828\n#>   10.199324   9.869672   9.374583   8.351139 ]\n#>  [11.045079  10.934624  10.810434  10.668606  10.503284  10.305112\n#>   10.057709   9.728241   9.233569   8.212026 ]\n#>  [10.87965   10.769221  10.645067  10.503284  10.338025  10.139942\n#>    9.892679   9.563459   9.069353   8.0503845]\n#>  [10.68132   10.570931  10.446828  10.305112  10.139942   9.941987\n#>    9.694924   9.366061   8.872768   7.857481 ]\n#>  [10.433674  10.323348  10.199324  10.057709   9.892679   9.694924\n#>    9.448175   9.119869   8.62784    7.6182513]\n#>  [10.103771   9.993557   9.869672   9.728241   9.563459   9.366061\n#>    9.119869   8.79255    8.302762   7.30317  ]\n#>  [ 9.608109   9.498147   9.374583   9.233569   9.069353   8.872768\n#>    8.62784    8.302762   7.818028   6.8405466]\n#>  [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n#>    7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n#> tf.Tensor(True, shape=(), dtype=bool)\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n#> Unit: microseconds\n#>                   expr      min       lq      mean    median       uq      max neval cld\n#>   do_something_R(test)  483.742  539.373  668.0696  686.3185  761.968 2328.809   100  a \n#>  do_something_TF(test) 2308.671 2400.477 2734.3863 2557.0600 2891.570 9770.898   100   b\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n#> Unit: milliseconds\n#>                   expr      min       lq      mean    median        uq      max neval cld\n#>   do_something_R(test) 8.866225 9.294396 11.552799 11.508769 13.077309 23.97977   100   b\n#>  do_something_TF(test) 3.809307 4.488089  5.009379  4.720541  5.300979  9.75693   100  a\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n#> Unit: milliseconds\n#>                                     expr      min       lq     mean   median       uq\n#>  tf$matmul(testTF, tf$transpose(testTF)) 6.268791 12.70258 15.63513 15.65174 18.49433\n#>                         test %*% t(test) 8.343718 10.64110 19.27335 16.04783 20.01532\n#>        max neval cld\n#>   33.42693   100   a\n#>  223.54858   100   a\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n#>      [,1] [,2]       [,3]\n#> [1,]    1  0.0 -0.6666667\n#> [2,]   -1  0.5 -0.1666667\n#> [3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n#> [1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    0    2    0\n#> [3,]    0    0    3\n\n# l)\neigen(A)\n#> eigen() decomposition\n#> $values\n#> [1] 3 2 1\n#> \n#> $vectors\n#>           [,1] [,2]       [,3]\n#> [1,] 0.1400280    0  0.4472136\n#> [2,] 0.9801961    1 -0.8944272\n#> [3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n#> [1] 6\nlibrary(tensorflow)\nlibrary(keras)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n#> tf.Tensor(\n#> [[ 1.          0.         -0.66666667]\n#>  [-1.          0.5        -0.16666667]\n#>  [ 0.          0.          0.33333333]], shape=(3, 3), dtype=float64)\n\n# j)    diag(A)\ntf$linalg$diag_part(A)\n#> tf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n#> tf.Tensor(\n#> [[1. 0. 0.]\n#>  [0. 2. 0.]\n#>  [0. 0. 3.]], shape=(3, 3), dtype=float64)\n\n# l)    eigen(A)\ntf$linalg$eigh(A)\n#> [[1]]\n#> tf.Tensor([-0.56155281  3.          3.56155281], shape=(3), dtype=float64)\n#> \n#> [[2]]\n#> tf.Tensor(\n#> [[-0.78820544  0.         -0.61541221]\n#>  [ 0.61541221  0.         -0.78820544]\n#>  [ 0.          1.         -0.        ]], shape=(3, 3), dtype=float64)\n\n# m)    det(A)\ntf$linalg$det(A)\n#> tf.Tensor(6.0, shape=(), dtype=float64)\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\nx = tf$constant(2.0)\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n(tape$gradient(y, x))\n#> tf.Tensor(20.0, shape=(), dtype=float32)\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n#> tf.Tensor(10.0, shape=(), dtype=float32)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\nsummary(lm(y~x))\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.67893 -0.16399  0.00968  0.15058  0.51099 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.004865   0.027447   0.177     0.86    \n#> x1          2.191511   0.023243  94.287   <2e-16 ***\n#> x2          2.741690   0.025328 108.249   <2e-16 ***\n#> x3          1.179181   0.023644  49.872   <2e-16 ***\n#> x4          0.591873   0.025154  23.530   <2e-16 ***\n#> x5          2.302417   0.022575 101.991   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2645 on 94 degrees of freedom\n#> Multiple R-squared:  0.9974, Adjusted R-squared:  0.9972 \n#> F-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n#> <tf.Variable 'Variable:0' shape=(5, 1) dtype=float64, numpy=\n#> array([[2.19290567],\n#>        [2.74534135],\n#>        [1.1714656 ],\n#>        [0.58811305],\n#>        [2.30174942]])>\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  2.217 2.719 1.165 0.593 2.303\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n  grads = tape$gradient(loss, interceptTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(interceptTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n#> <tf.Variable 'Variable:0' shape=(3, 1) dtype=float64, numpy=\n#> array([[2.46391571],\n#>        [2.45852885],\n#>        [1.00566707]])>\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  2.47 2.465 1.003\nk_print_tensor(interceptTF, message = \"Resulting intercept:\\n\")\n#> <tf.Variable 'Variable:0' shape=(1, 1) dtype=float64, numpy=array([[4.22135068]])>\ncat(\"Original intercept: \", intercept, \"\\n\")\n#> Original intercept:  4.09"},{"path":"tensorflowintro.html","id":"introduction-to-pytorch","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.2 Introduction to PyTorch","text":"PyTorch another famous library deep learning. Like TensorFlow, Torch written C++ API Python. 2020, RStudio team released R-Torch, R-TensorFlow calls Python API background, R-Torch API built directly C++ Torch library!Useful links:PyTorch documentation (Python API, bust just replace “.” “$.”)R-Torch websiteTo get started Torch, load library check installation worked.","code":"\nlibrary(torch)"},{"path":"tensorflowintro.html","id":"data-containers-1","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.2.1 Data Containers","text":"Unlike TensorFlow, Torch doesn’t two data containers mutable immutable variables. variables initialized via torch_tensor function:mark variables mutable (track operations automatic differentiation) set argument ‘requires_grad’ true torch_tensor function:","code":"\na = torch_tensor(1.)\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)"},{"path":"tensorflowintro.html","id":"basic-operations-1","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.2.2 Basic Operations","text":"now can define variables math :R-Torch package provides common R methods (advantage TensorFlow).operators also automatically transform R numbers tensors attempting add tensor R number:TensorFlow, explicitly transform tensors back R:","code":"\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n#> torch_tensor\n#>  5\n#> [ CPUFloatType{1} ]\nprint(b)\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ]\nc = a$add(b)\nprint(c)\n#> torch_tensor\n#>  15\n#> [ CPUFloatType{1} ]\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n#> torch_tensor\n#>  15\n#> [ CPUFloatType{1} ]\nprint(a/b)\n#> torch_tensor\n#>  0.5000\n#> [ CPUFloatType{1} ]\nprint(a*b)\n#> torch_tensor\n#>  50\n#> [ CPUFloatType{1} ]\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ]\nclass(d)\n#> [1] \"torch_tensor\" \"R7\"\nclass(as.numeric(d))\n#> [1] \"numeric\""},{"path":"tensorflowintro.html","id":"data-types-1","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.2.3 Data Types","text":"Similar TensorFlow:’s difference! TensorFlow get error, R-Torch, m automatically casted double (float64). However, still bad practice!course try provide corresponding PyTorch code snippets Keras/TensorFlow examples.","code":"\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b "},{"path":"tensorflowintro.html","id":"exercises-1","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.2.4 Exercises","text":"Rewrite following expressions (g) torch:\nexercise compares speed R torch\nfirst exercise rewrite following function torch:, provide skeleton TensorFlow function:can compare speed using Microbenchmark package:Try different matrix sizes test matrix compare speed.Tip: look torch_mean documentation “dim” argument.Compare following different matrix sizes:test = matrix(0.0, 1000L, 500L)testTorch = torch_tensor(test)Also try following:\nR faster (first time)?R functions used (apply, mean, “-”) also implemented C.\nR functions used (apply, mean, “-”) also implemented C.problem large enough torch overhead.\nproblem large enough torch overhead.Google find write following tasks torch:\nTorch supports automatic differentiation (analytical numerical!).\nLet’s look function \\(f(x) = 5 x^2 + 3\\) derivative \\(f'(x) = 10x\\).\n\\(f'(5)\\) get \\(10\\).Let’s torch Define function:want calculate derivative \\(x = 2.0\\):automatic differentiation, forward \\(x\\) function call $backward() method result:print gradient:can also calculate second order derivative \\(f''(x) = 10\\):happening ? Think discuss .advanced example: Linear regressionIn case first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows normal distribution == error).R following fit linear regression model:Let’s build model TensorFlow.\n, use now variable data container type (remember mutable need type weights (\\(\\boldsymbol{w}\\)) regression model). want model learn weights.input (predictors, independent variables features, \\(\\boldsymbol{X}\\)) observed (response, \\(\\boldsymbol{y}\\)) constant learned/optimized.Discuss code, go code line line try understand .Additional exercise:Play around simulation, increase/decrease number weights, add intercept (also need additional variable model).\n","code":"\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n#> [1] 1\n\n# b)\nmean(x)\n#> [1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n#> [1] 1\n\n# d) \nwhich.min(x)\n#> [1] 100\n\n# e) Tip: Use Google! \norder(x)\n#>   [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83  82  81  80\n#>  [22]  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65  64  63  62  61  60  59\n#>  [43]  58  57  56  55  54  53  52  51  50  49  48  47  46  45  44  43  42  41  40  39  38\n#>  [64]  37  36  35  34  33  32  31  30  29  28  27  26  25  24  23  22  21  20  19  18  17\n#>  [85]  16  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n#>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]\n#>  [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305 10.43705\n#>  [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910 10.42317\n#>  [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496 10.40910\n#>  [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061 10.39482\n#>  [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605 10.38034\n#>  [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127 10.36565\n#>  [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628 10.35073\n#>  [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105 10.33559\n#>  [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559 10.32022\n#> [10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989 10.30461\n#>          [,10]\n#>  [1,] 10.42079\n#>  [2,] 10.40699\n#>  [3,] 10.39299\n#>  [4,] 10.37879\n#>  [5,] 10.36439\n#>  [6,] 10.34977\n#>  [7,] 10.33495\n#>  [8,] 10.31989\n#>  [9,] 10.30461\n#> [10,] 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n#> [1] TRUE\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\ntorch_min(y) # Float!\n#> torch_tensor\n#> 1\n#> [ CPUFloatType{} ]\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n#> [1] 50.5\nmean(y)\n#> [1] 50.5\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n#> torch_tensor\n#> 50.5\n#> [ CPUFloatType{} ]\ntorch_mean(y)  # Float!\n#> torch_tensor\n#> 50.5\n#> [ CPUFloatType{} ]\n\n# c)    which.max(x)\ntorch_argmax(x)\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\ntorch_argmax(y)\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\n\n# d)    which.min(x)\ntorch_argmin(x)\n#> torch_tensor\n#> 100\n#> [ CPULongType{} ]\n\n# e)    order(x)\ntorch_argsort(x)\n#> torch_tensor\n#>  100\n#>   99\n#>   98\n#>   97\n#>   96\n#>   95\n#>   94\n#>   93\n#>   92\n#>   91\n#>   90\n#>   89\n#>   88\n#>   87\n#>   86\n#>   85\n#>   84\n#>   83\n#>   82\n#>   81\n#>   80\n#>   79\n#>   78\n#>   77\n#>   76\n#>   75\n#>   74\n#>   73\n#>   72\n#>   71\n#> ... [the output was truncated (use n=-1 to disable)]\n#> [ CPULongType{100} ]\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n#> torch_tensor\n#>  4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n#>  4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n#>  4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n#>  4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n#>  4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n#>  3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n#>  3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n#>  3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n#>  2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n#>  2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n#> [ CPUFloatType{10,10} ]\n\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n#> torch_tensor\n#>  1\n#> [ CPUBoolType{1} ]\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n#> Unit: microseconds\n#>                      expr     min      lq     mean  median       uq      max neval cld\n#>      do_something_R(test) 501.259 531.134 567.2356 546.484 562.4595 2375.494   100   b\n#>  do_something_torch(test) 276.280 296.105 366.4662 347.216 367.9885 2726.235   100  a\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n#> Unit: milliseconds\n#>                      expr      min       lq      mean   median        uq       max neval\n#>      do_something_R(test) 8.548734 9.017760 11.119547 9.738920 13.213963 16.234261   100\n#>  do_something_torch(test) 2.120753 2.423858  2.802547 2.641256  2.905044  6.337406   100\n#>  cld\n#>    b\n#>   a\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n#> Unit: milliseconds\n#>                                    expr      min       lq     mean    median        uq\n#>  torch_matmul(testTorch, testTorch$t()) 3.721221 4.004901  6.65570  4.350977  5.779028\n#>                        test %*% t(test) 8.452588 9.909395 19.75338 15.986848 19.232337\n#>        max neval cld\n#>   30.62088   100  a \n#>  224.83009   100   b\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n#>      [,1] [,2]       [,3]\n#> [1,]    1  0.0 -0.6666667\n#> [2,]   -1  0.5 -0.1666667\n#> [3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n#> [1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    0    2    0\n#> [3,]    0    0    3\n\n# l)\neigen(A)\n#> eigen() decomposition\n#> $values\n#> [1] 3 2 1\n#> \n#> $vectors\n#>           [,1] [,2]       [,3]\n#> [1,] 0.1400280    0  0.4472136\n#> [2,] 0.9801961    1 -0.8944272\n#> [3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n#> [1] 6\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n#> torch_tensor\n#>  1.0000  0.0000 -0.6667\n#> -1.0000  0.5000 -0.1667\n#>  0.0000  0.0000  0.3333\n#> [ CPUFloatType{3,3} ]\n\n# j)    diag(A)\ntorch_diag(A)\n#> torch_tensor\n#>  1\n#>  2\n#>  3\n#> [ CPUFloatType{3} ]\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n#> tf.Tensor(\n#> [[1. 0. 0.]\n#>  [0. 2. 0.]\n#>  [0. 0. 3.]], shape=(3, 3), dtype=float64)\ntorch_diag(A)$diag()\n#> torch_tensor\n#>  1  0  0\n#>  0  2  0\n#>  0  0  3\n#> [ CPUFloatType{3,3} ]\n\n# l)    eigen(A)\nlinalg_eigh(A)\n#> [[1]]\n#> torch_tensor\n#> -0.5616\n#>  3.0000\n#>  3.5616\n#> [ CPUFloatType{3} ]\n#> \n#> [[2]]\n#> torch_tensor\n#> -0.7882  0.0000  0.6154\n#>  0.6154  0.0000  0.7882\n#>  0.0000  1.0000  0.0000\n#> [ CPUFloatType{3,3} ]\n\n# m)    det(A)\nlinalg_det(A)\n#> torch_tensor\n#> 6\n#> [ CPUFloatType{} ]\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ny$backward(retain_graph=TRUE )\nx$grad\n#> torch_tensor\n#>  20\n#> [ CPUFloatType{1} ]\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ][ grad_fn = <MulBackward0> ]\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\nsummary(lm(y~x))\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.67893 -0.16399  0.00968  0.15058  0.51099 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.004865   0.027447   0.177     0.86    \n#> x1          2.191511   0.023243  94.287   <2e-16 ***\n#> x2          2.741690   0.025328 108.249   <2e-16 ***\n#> x3          1.179181   0.023644  49.872   <2e-16 ***\n#> x4          0.591873   0.025154  23.530   <2e-16 ***\n#> x5          2.302417   0.022575 101.991   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2645 on 94 degrees of freedom\n#> Multiple R-squared:  0.9974, Adjusted R-squared:  0.9972 \n#> F-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n#> [1] \"Loss: 4.4065318107605\"\n#> [1] \"Loss: 2.37925982475281\"\n#> [1] \"Loss: 0.901207089424133\"\n#> [1] \"Loss: 0.403193950653076\"\n#> [1] \"Loss: 0.296265482902527\"\n#> [1] \"Loss: 0.268377900123596\"\n#> [1] \"Loss: 0.232994794845581\"\n#> [1] \"Loss: 0.219554618000984\"\n#> [1] \"Loss: 0.215328559279442\"\n#> [1] \"Loss: 0.213282063603401\"\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n#> Inferred weights:  0.701 3.089 1.801 1.123 3.452\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  0.67 3.085 1.787 1.121 3.455\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n#> [1] \"Loss: 3.51533484458923\"\n#> [1] \"Loss: 1.74870145320892\"\n#> [1] \"Loss: 0.414169400930405\"\n#> [1] \"Loss: 0.518697261810303\"\n#> [1] \"Loss: 0.293963462114334\"\n#> [1] \"Loss: 0.263338685035706\"\n#> [1] \"Loss: 0.258341372013092\"\n#> [1] \"Loss: 0.254723280668259\"\n#> [1] \"Loss: 0.252453774213791\"\n#> [1] \"Loss: 0.25116890668869\"\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n#> Inferred weights:  3.118 -0.349 2.107\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  3.131 -0.353 2.11\n\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n#> Inferred intercept:  2.836\ncat(\"Original intercept: \", intercept, \"\\n\")\n#> Original intercept:  2.832"},{"path":"tensorflowintro.html","id":"keras-framework","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.3 Keras Framework","text":"seen can use TensorFlow directly R, use knowledge implement neural network TensorFlow directly R. However, can quite cumbersome. simple problems, usually faster use higher-level API helps us implementing machine learning models TensorFlow. common Keras.Keras powerful framework building training neural networks lines codes. Since end 2018, Keras TensorFlow completely interoperable, allowing us utilize best .objective lesson familiarize Keras. installed TensorFlow, Keras can found within TensorFlow: tf.keras. However, RStudio team built R package top tf.keras, convenient use . load Keras package, type","code":"\nlibrary(keras)"},{"path":"tensorflowintro.html","id":"example-workflow-in-keras","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.3.1 Example workflow in Keras","text":"show Keras works, now build small classifier predict three species iris data set. Load necessary packages data sets:neural networks, beneficial scale predictors (scaling = centering standardization, see ?scale).\nalso split data predictors (X) response (Y = three species).Additionally, Keras/TensorFlow handle factors create contrasts (one-hot encoding).\n, specify number categories. can tricky beginner, programming languages like Python C++, arrays start zero. Thus, specify 3 number classes three species, classes 0,1,2,3. Keep mind.prepared data, now see typical workflow specify model Keras/Torch.1. Initialize sequential model Keras:sequential Keras model higher order type model within Keras consists one input one output model.2. Add hidden layers model (learn hidden layers next days).specifying hidden layers, also specify shape called activation function.\ncan think activation function decision forwarded next neuron (learn later).\nwant know topic even depth, consider watching videos presented section 4.4.shape input number predictors (4) shape output number classes (3).KerasTorchThe Torch syntax similar, give list layers “nn_sequential” function. , specify softmax activation function extra layer:softmax scales potential multidimensional vector interval \\((0, 1]\\) component. sum components equals 1. might useful example handling probabilities. Ensure ther labels start 0! Otherwise softmax function work well!3. Compile model loss function (: cross entropy) optimizer (: Adamax).learn options later, now, worry “learning_rate” (“lr” Torch earlier TensorFlow) argument, cross entropy optimizer.KerasTorchSpecify optimizer parameters trained (case parameters network):4. Fit model 30 iterations (epochs)KerasTorchIn Torch, jump directly training loop, however, write training loop:Get batch data.Predict batch.Ccalculate loss predictions true labels.Backpropagate error.Update weights.Go step 1 repeat.5. Plot training history:KerasTorch6. Create predictions:KerasGet probabilities:plant, want know species got highest probability:Torch7. Calculate Accuracy (often correct):KerasTorch8. Plot predictions, see done good job:see, building neural network easy Keras Torch can already .","code":"\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(torch)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata(iris)\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\nX = scale(iris[,1:4])\nY = iris[,5]\nY = to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    1    0    0\n#> [3,]    1    0    0\n#> [4,]    1    0    0\n#> [5,]    1    0    0\n#> [6,]    1    0    0\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 3L, activation = \"softmax\") \nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n#> Model: \"sequential\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_3 (Dense)                        (None, 20)                          100           \n#>  dense_2 (Dense)                        (None, 20)                          420           \n#>  dense_1 (Dense)                        (None, 20)                          420           \n#>  dense (Dense)                          (None, 3)                           63            \n#> ==========================================================================================\n#> Total params: 1,003\n#> Trainable params: 1,003\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel_history =\n  model %>%\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\nplot(model_history)\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\npredictions = predict(model, X) # Probabilities for each class.\nhead(predictions) # Quasi-probabilities for each species.\n#>           [,1]       [,2]        [,3]\n#> [1,] 0.9819264 0.01476339 0.003310232\n#> [2,] 0.9563531 0.03986335 0.003783490\n#> [3,] 0.9830711 0.01501246 0.001916326\n#> [4,] 0.9789233 0.01915258 0.001923956\n#> [5,] 0.9871404 0.01057778 0.002281806\n#> [6,] 0.9808626 0.01525488 0.003882431\npreds = apply(predictions, 1, which.max) \nprint(preds)\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 3 3 3 2 3 2 3 2 2 2 2 3 2 3 2 3 3 2 2 2 3 2 2 2 2 3 3 3 3 2 2 2 2 3\n#>  [85] 2 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3\n#> [127] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3\n#>  [85] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> [127] 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\nmean(preds == as.integer(iris$Species))\n#> [1] 0.8666667\nmean(preds_torch == as.integer(iris$Species))\n#> [1] 0.98\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\npar(oldpar)   # Reset par."},{"path":"tensorflowintro.html","id":"questions-1","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.3.2 Questions","text":"\n      \n        makeMultipleChoiceForm(\n         'look following two textbooks machine learning (&apos;<href=\"https://www.statlearning.com/\" target=\"_blank\" rel=\"noopener\">Introduction Statistical Learning<\/>&apos; &apos;<href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\" target=\"_blank\" rel=\"noopener\">Elements Statistical Learning<\/>&apos;) - following statements true?',\n          'checkbox',\n          [\n            {\n              'answer':'books can downloaded free.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'elements Statistical Learning published earlier Introduction Statistical Learning.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'book \"Introduction Statistical Learning\" also includes online course videos different topics website.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'Higher model complexity always better predicting.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            }\n          ],\n          ''\n        );\n      \n        makeMultipleChoiceForm(\n          'following statements bias-variance trade-correct?',\n          'checkbox',\n          [\n            {\n              'answer':'goal considering bias-variance trade-get bias model small possible.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'goal considering bias-variance trade-realize increasing complexity typically leads flexibility (allowing reduce bias) cost uncertainty (variance) estimated parameters.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n             'answer':'bias-variance trade-, see model complexity also depends want optimize : bias, variance (rarely), total error model.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            }\n          ],\n          ''\n        );\n      ","code":""},{"path":"tensorflowintro.html","id":"exercises-2","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.3.3 Exercises","text":"now build regression airquality data set Keras. want predict variable “Ozone.”Load prepare data set:Explore data summary() plot():NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Build sequential Keras model.Build sequential Keras model.Add hidden layers (input output layer already specified, add hidden layers ):Add hidden layers (input output layer already specified, add hidden layers ):use 5L input shape?one output node “linear” activation layer?Compile model.“mean_squared_error” loss?Fit model:Tip: matrices accepted \\(\\boldsymbol{X}\\) \\(\\boldsymbol{y}\\) Keras. R often drops one column matrix vector (change back matrix!)Plot training history.Plot training history.Create predictions.Create predictions.Compare Keras model linear model:Compare Keras model linear model:\n1. NAs data, remove Keras handle NAs.2. Split data predictors response scale matrix.3. Build sequential Keras model.4. Add hidden layers (input output layer already specified, add hidden layers ).use 5L input shape, 5 predictors. Analogously, use 1L 1d response.\nwant compression, dilation nonlinear effects, use simple linear layer (equal activation function ). activation functions, look example . wait next days.\nmay also seen previously shown link activation functions detail.5. Compile model.mean_squared_error ordinary least squares approach regression analysis.6. Fit model.7. Plot training history.8. Create predictions.9. Compare Keras model linear model.Look slightly complex model compare loss plot accuracy contrast former.see, complex model works better, can learn coherences better.\nkeep overfitting problem mind!Look little change learning rates next 2 models compare loss plot accuracy contrast former.can see, higher learning rate yields little bit worse results. optimum jumped .can see, lower learning rate, optimum (compared run learning rate 0.05) yet reached (epochs gone ).\nalso , mind overfitting problem. many epochs, things might get worse!task airquality example, go code line line try understand .\nNote, TensorFlow intermingled Keras.Now change code iris data set.\nTip: tf$keras$losses$… can find various loss functions.\nRemarks:Mind different input output layer numbers.loss function increases randomly, different subsets data drawn.\ndownside stochastic gradient descent.positive thing stochastic gradient descent , local valleys hills may left global ones can found instead.","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\nsummary(data)\n#>      Ozone           Solar.R           Wind             Temp           Month      \n#>  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   Min.   :5.000  \n#>  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   1st Qu.:6.000  \n#>  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   Median :7.000  \n#>  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   Mean   :6.993  \n#>  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   3rd Qu.:8.000  \n#>  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   Max.   :9.000  \n#>  NA's   :37       NA's   :7                                                       \n#>       Day      \n#>  Min.   : 1.0  \n#>  1st Qu.: 8.0  \n#>  Median :16.0  \n#>  Mean   :15.8  \n#>  3rd Qu.:23.0  \n#>  Max.   :31.0  \n#> \nplot(data)\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\nprint(rmse_keras)\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n#>      Ozone          Solar.R           Wind            Temp           Month      \n#>  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00   Min.   :5.000  \n#>  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00   1st Qu.:6.000  \n#>  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00   Median :7.000  \n#>  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79   Mean   :7.216  \n#>  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50   3rd Qu.:9.000  \n#>  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00   Max.   :9.000  \n#>       Day       \n#>  Min.   : 1.00  \n#>  1st Qu.: 9.00  \n#>  Median :16.00  \n#>  Mean   :15.95  \n#>  3rd Qu.:22.50  \n#>  Max.   :31.00\nx = scale(data[,2:6])\ny = data[,1]\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 173.4729\npred_keras = predict(model, x)\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 9.621961\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 30L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 105.1682\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 7.798208\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 30L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 116.6115\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 8.247861\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 30L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.01))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 238.9949\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 11.58784\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n#> $bX\n#>         Solar.R        Wind        Temp      Month        Day\n#> 87  -1.13877323 -0.37654514  0.44147123 -0.1467431  1.1546835\n#> 117  0.58361881 -1.83815816  0.33653910  0.5319436  1.0398360\n#> 129 -1.01809608  1.56290291  0.65133550  1.2106304 -1.1422676\n#> 121  0.44100036 -2.14734553  1.70065685  0.5319436  1.4992262\n#> 91   0.74817856 -0.71384046  0.54640337 -0.1467431  1.6140738\n#> 137 -1.76410028  0.26993754 -0.71278225  1.2106304 -0.2234871\n#> 21  -1.93963068 -0.06735777 -1.97196786 -1.5041165  0.5804458\n#> 141 -1.73118833  0.10128988 -0.18812157  1.2106304  0.2359031\n#> 78   0.97856221  0.10128988  0.44147123 -0.1467431  0.1210555\n#> 15  -1.31430363  0.91642022 -2.07689999 -1.5041165 -0.1086396\n#> 38  -0.63412333 -0.06735777  0.44147123 -0.8254298 -1.0274200\n#> 49  -1.62148183 -0.20789749 -1.34237505 -0.8254298  0.2359031\n#> 123  0.03508631 -1.02302783  1.70065685  0.5319436  1.7289213\n#> 136  0.58361881 -1.02302783 -0.08318944  1.2106304 -0.3383347\n#> 120  0.19964606 -0.06735777  2.01545325  0.5319436  1.3843787\n#> 114 -1.63245248  1.22560759 -0.60785011  0.5319436  0.6952933\n#> 145 -1.87380678 -0.20789749 -0.71278225  1.2106304  0.6952933\n#> 140  0.43002971  1.08506788 -1.13251078  1.2106304  0.1210555\n#> 64   0.56167751 -0.20789749  0.33653910 -0.1467431 -1.4868103\n#> 118  0.33129386 -0.54519280  0.86119977  0.5319436  1.1546835\n#> 128 -0.98518413 -0.71384046  0.96613190  1.2106304 -1.2571152\n#> 62   0.92370896 -1.64140257  0.65133550 -0.1467431 -1.7165054\n#> 125  0.13382216 -1.36032314  1.49079258  1.2106304 -1.6016578\n#> 4    1.40641756  0.43858520 -1.65717146 -1.5041165 -1.3719627\n#> 79   1.09923936 -1.02302783  0.65133550 -0.1467431  0.2359031\n#> 82  -1.95060133 -0.85438017 -0.39798584 -0.1467431  0.5804458\n#> 149  0.08993956 -0.85438017 -0.81771438  1.2106304  1.1546835\n#> 17   1.34059366  0.57912491 -1.23744292 -1.5041165  0.1210555\n#> 48   1.08826871  3.02451593 -0.60785011 -0.8254298  0.1210555\n#> 130  0.73720791  0.26993754  0.23160696  1.2106304 -1.0274200\n#> 132  0.49585361  0.26993754 -0.29305371  1.2106304 -0.7977249\n#> 30   0.41905906 -1.19167548  0.12667483 -1.5041165  1.6140738\n#> \n#> $bY\n#>  [1]  20 168  32 118  64   9   1  13  35  18  29  20  85  28  76   9  23  18  32  73  47\n#> [22] 135  78  18  61  16  30  34  37  20  21 115\n\n\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n#> Loss:  645.1247 \n#> Loss:  257.9622 \n#> Loss:  257.9248 \n#> Loss:  424.3474 \n#> Loss:  132.1914 \n#> Loss:  201.8619 \n#> Loss:  225.3891 \n#> Loss:  111.7508 \n#> Loss:  343.3166 \n#> Loss:  255.3797 \n#> Loss:  245.1779 \n#> Loss:  227.4517 \n#> Loss:  222.4553 \n#> Loss:  348.0878 \n#> Loss:  365.9766 \n#> Loss:  178.8896 \n#> Loss:  220.2557 \n#> Loss:  344.3786 \n#> Loss:  238.2619 \n#> Loss:  324.3969\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n#> Loss:  0.002592299 \n#> Loss:  0.0004166169 \n#> Loss:  0.0005878718 \n#> Loss:  0.0001814893 \n#> Loss:  0.0003771982 \n#> Loss:  0.0006105618 \n#> Loss:  0.0003895268 \n#> Loss:  0.0001912034 \n#> Loss:  0.0002297373 \n#> Loss:  0.0001141062 \n#> Loss:  0.0002618438 \n#> Loss:  0.0001288175 \n#> Loss:  5.752431e-05 \n#> Loss:  0.000256366 \n#> Loss:  0.0002148773 \n#> Loss:  0.0001434388 \n#> Loss:  0.0001920019 \n#> Loss:  0.0001954518 \n#> Loss:  7.47276e-05 \n#> Loss:  2.274193e-05 \n#> Loss:  0.000115741 \n#> Loss:  2.059802e-05 \n#> Loss:  7.065996e-05 \n#> Loss:  1.295879e-05 \n#> Loss:  6.738321e-05 \n#> Loss:  2.543455e-05"},{"path":"tensorflowintro.html","id":"basicMath","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.4 Underlying mathematical concepts - optional","text":"yet familiar underlying concepts neural networks want know , suggested read / view following videos / sites. Consider Links videos descriptions parentheses optional bonus.might useful understand concepts depth.(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially animated graphic interesting).)(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially animated graphic interesting).)https://en.wikipedia.org/wiki/Gradient_descent#Descriptionhttps://en.wikipedia.org/wiki/Gradient_descent#DescriptionNeural networks (Backpropagation, etc.).Neural networks (Backpropagation, etc.).Activation functions detail (requires prerequisite).Activation functions detail (requires prerequisite).Videos topic:Gradient descent explained(Stochastic gradient descent explained)(Entropy explained)Short explanation entropy, cross entropy Kullback–Leibler divergenceDeep Learning (chapter 1)neural networks learn - Deep Learning (chapter 2)Backpropagation - Deep Learning (chapter 3)Another video backpropagation (extends previous one) - Deep Learning (chapter 4)","code":""},{"path":"tensorflowintro.html","id":"caveats-of-neural-network-optimization","chapter":"4 Introduction to TensorFlow, Keras, and Torch","heading":"4.4.1 Caveats of neural network optimization","text":"Depending activation functions, might occur network won’t get updated, even high learning rates (called vanishing gradient, especially “sigmoid” functions).\nFurthermore, updates might overshoot (called exploding gradients) activation functions result many zeros (especially “relu,” dying relu).general, first layers network tend learn (much) slowly subsequent ones.","code":""},{"path":"fundamental.html","id":"fundamental","chapter":"5 Common Machine Learning algorithms","heading":"5 Common Machine Learning algorithms","text":"","code":""},{"path":"fundamental.html","id":"machine-learning-principles","chapter":"5 Common Machine Learning algorithms","heading":"5.1 Machine Learning Principles","text":"","code":""},{"path":"fundamental.html","id":"optimization","chapter":"5 Common Machine Learning algorithms","heading":"5.1.1 Optimization","text":"Wikipedia: “optimization problem problem finding best solution feasible solutions.”need “optimization?”Somehow, need tell algorithm learn. called loss function, expresses goal . also need find configurations loss function attains minimum. job optimizer. Thus, optimization consists :loss function (e.g. tell algorithm training step many observations misclassified) guides training machine learning algorithms.loss function (e.g. tell algorithm training step many observations misclassified) guides training machine learning algorithms.optimizer, tries update weights machine learning algorithms way loss function minimized.optimizer, tries update weights machine learning algorithms way loss function minimized.Calculating global optimum analytically non-trivial problem thus bunch diverse optimization algorithms evolved.optimization algorithms inspired biological systems e.g. ants, bees even slimes. optimizers explained following video, look:","code":""},{"path":"fundamental.html","id":"questions-2","chapter":"5 Common Machine Learning algorithms","heading":"5.1.2 Questions","text":"\n      \n        makeMultipleChoiceForm(\n                'lecture, said , training, machine learning parameters optimised get good fit (loss function) training data. following statements loss functions correct?',\n                'checkbox',\n                [\n                    {\n                        'answer':'loss function measures difference (current) machine learning model prediction data.',\n                        'correct':true,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n                    },\n                    {\n                        'answer':'specify simple line machine learning model, loss functions lead line.',\n                        'correct':false,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n                    },\n                    {\n                        'answer':'Cross-Entropy Kullback–Leibler divergence common loss functions.',\n                        'correct':true,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n                    },\n                    {\n                        'answer':'regression, one sensible loss function, mean squared error.',\n                        'correct':false,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n            }\n          ],\n          ''\n        );\n      ","code":""},{"path":"fundamental.html","id":"small-optimization-example","chapter":"5 Common Machine Learning algorithms","heading":"5.1.2.1 Small Optimization Example","text":"easy example optimization can think quadratic function:function easy, can randomly probe identify optimum plotting.minimal value \\(x = 0\\) (honest, can calculate analytically simple case).can also use optimizer optim-function (first argument starting value):opt$par return best values found optimizer, really close zero :)","code":"\nfunc = function(x){ return(x^2) }\nset.seed(123)\n\na = rnorm(100)\nplot(a, func(a))\nset.seed(123)\n\nopt = optim(1.0, func, method = \"Brent\", lower = -100, upper = 100)\nprint(opt$par)\n#> [1] -3.552714e-15"},{"path":"fundamental.html","id":"advanced-optimization-example","chapter":"5 Common Machine Learning algorithms","heading":"5.1.3 Advanced Optimization Example","text":"Optimization also done fitting linear regression model. Thereby, optimize weights (intercept slope). Just using lm(y~x) simple. want hand also better understand optimization works.example take airquality data set. First, sure NAs . split response (Ozone) predictors (Month, Day, Solar.R, Wind, Temp). Additionally beneficial optimizer, different predictors support, thus scale .model want optimize: \\(Ozone = Solar.R \\cdot X1 + Wind \\cdot X2 + Temp \\cdot X3 + Month \\cdot X4 + Day \\cdot X5 + X6\\)assume residuals normally distributed, loss function mean squared error: \\(\\mathrm{mean}(predicted~ozone - true~ozone)^{2}\\)task now find parameters \\(X1,\\dots,X6\\) loss function minimal. Therefore, implement function, takes parameters returns loss.example can sample weights see loss changes weights.can try find optimum bruteforce (means use random set weights see loss function minimal):cases, bruteforce isn’t good approach. might work well parameters, increasing complexity parameters take long time. Furthermore guaranteed finds stable solution continuous data.R optim function helps computing optimum faster.background, mostly gradient descent methods used.comparing weights optimizer estimated weights lm() function, see self-written code obtains weights lm. Keep mind, simple method uses random numbers thus results might differ run (without setting seed).","code":"\ndata = airquality[complete.cases(airquality$Ozone) & complete.cases(airquality$Solar.R),]\nX = scale(data[,-1])\nY = data$Ozone\nlinear_regression = function(w){\n  pred = w[1]*X[,1] + # Solar.R\n         w[2]*X[,2] + # Wind\n         w[3]*X[,3] + # Temp\n         w[4]*X[,4] + # Month\n         w[5]*X[,5] +\n         w[6]         # or X * w[1:5]^T + w[6]\n  # loss  = MSE, we want to find the optimal weights \n  # to minimize the sum of squared residuals.\n  loss = mean((pred - Y)^2)\n  return(loss)\n}\nset.seed(123)\n\nlinear_regression(runif(6))\n#> [1] 2866.355\nset.seed(123)\n\nrandom_search = matrix(runif(6*5000, -10, 10), 5000, 6)\nlosses = apply(random_search, 1, linear_regression)\nplot(losses, type = \"l\")\nrandom_search[which.min(losses),]\n#> [1]  7.411631 -7.018960  9.376949  6.490659  5.432706  9.460573\nopt = optim(runif(6, -1, 1), linear_regression)\nopt$par\n#> [1]   1.631666 -17.272902  11.645608  -7.371417   1.754860  42.577956\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705"},{"path":"fundamental.html","id":"regularization","chapter":"5 Common Machine Learning algorithms","heading":"5.1.4 Regularization","text":"Regularization means adding information structure system order solve ill-posed optimization problem prevent overfitting. many ways regularizing machine learning model. important distinction shrinkage estimators estimators based model averaging.Shrikage estimators based idea adding penalty loss function penalizes deviations model parameters particular value (typically 0). way, estimates “shrunk” specified default value. practice, important penalties least absolute shrinkage selection operator; also Lasso LASSO, penalty proportional sum absolute deviations (\\(L1\\) penalty), Tikhonov regularization aka Ridge regression, penalty proportional sum squared distances reference (\\(L2\\) penalty). Thus, loss function optimize given \\[\nloss = fit - \\lambda \\cdot d\n\\]fit refers standard loss function, \\(\\lambda\\) strength regularization, \\(d\\) chosen metric, e.g. \\(L1\\) \\(L2\\):\\[\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n\\]\n\\[\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n\\]\\(\\lambda\\) possibly d typically optimized cross-validation. \\(L1\\) \\(L2\\) can also combined called elastic net (see Zou Hastie (2005)).Model averaging refers entire set techniques, including boosting, bagging averaging techniques. general principle predictions made combining (= averaging) several models. based insight often efficient many simpler models average , one “super model.” reasons complicated, explained detail Dormann et al. (2018).particular important application averaging boosting, idea many weak learners combined model average, resulting strong learner. Another related method bootstrap aggregating, also called bagging. Idea boostrap (use random sampling replacement ) data, average bootstrapped predictions.see techniques work practice, let’s first focus LASSO Ridge regularization weights neural networks. can imagine LASSO Ridge act similar rubber band weights pulls zero data strongly push away zero. leads important weights, supported data, estimated different zero, whereas unimportant model structures reduced (shrunken) zero.LASSO \\(\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)\\) Ridge \\(\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)\\) slightly different properties. best understood express effective prior preference create parameters:can see, LASSO creates strong preference towards exactly zero, falls less strongly towards tails. means parameters tend estimated either exactly zero, , , free Ridge. reason, LASSO often interpreted model selection method.Ridge, hand, certain area around zero relatively indifferent deviations zero, thus rarely leading exactly zero values. However, create stronger shrinkage values deviate significantly zero.can implement linear regression also Keras, specify hidden layers:KerasTorchTensorFlow thus Keras also allow use using LASSO Ridge weights.\nLets see happens put \\(L1\\) (LASSO) regularization weights:KerasOne can clearly see parameters pulled towards zero regularization.TorchIn Torch, specify regularization calculating loss.","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n#> Loaded Tensorflow version 2.9.1\n\ndata = airquality[complete.cases(airquality),]\nX = scale(data[,-1])\nY = data$Ozone\n# L1/L2 on linear model.\n\nmodel = keras_model_sequential()\nmodel %>%\n layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(X)[2]))\nsummary(model)\n#> Model: \"sequential\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense (Dense)                          (None, 1)                           6             \n#> ==========================================================================================\n#> Total params: 6\n#> Trainable params: 6\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel %>%\n compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5),\n         metrics = c(metric_mean_squared_error))\n\nmodel_history =\n model %>%\n fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE)\n\nunconstrained = model$get_weights()\nsummary(lm(Y~X))\n#> \n#> Call:\n#> lm(formula = Y ~ X)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> XSolar.R       4.583      2.135   2.147   0.0341 *  \n#> XWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> XTemp         18.067      2.610   6.922 3.66e-10 ***\n#> XMonth        -4.479      2.230  -2.009   0.0471 *  \n#> XDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 1L)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())\nfor(i in 1:500){\n  indices = sample.int(nrow(X), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  loss$sum()$backward()\n  opt$step()\n}\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\nmodel_torch$parameters\n#> $`0.weight`\n#> torch_tensor\n#>   4.1083 -10.1831  18.2815  -4.3478   1.2937\n#> [ CPUFloatType{1,5} ][ requires_grad = TRUE ]\n#> \n#> $`0.bias`\n#> torch_tensor\n#>  42.0958\n#> [ CPUFloatType{1} ][ requires_grad = TRUE ]\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>% # Remind the penalty lambda that is set to 10 here.\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(X)[2]), \n              kernel_regularizer = regularizer_l1(10),\n              bias_regularizer = regularizer_l1(10))\nsummary(model)\n#> Model: \"sequential_1\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_1 (Dense)                        (None, 1)                           6             \n#> ==========================================================================================\n#> Total params: 6\n#> Trainable params: 6\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5),\n          metrics = c(metric_mean_squared_error))\n\nmodel_history =\n  model %>%\n  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)\n\nl1 = model$get_weights()\nsummary(lm(Y~X))\n#> \n#> Call:\n#> lm(formula = Y ~ X)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> XSolar.R       4.583      2.135   2.147   0.0341 *  \n#> XWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> XTemp         18.067      2.610   6.922 3.66e-10 ***\n#> XMonth        -4.479      2.230  -2.009   0.0471 *  \n#> XDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\ncbind(unlist(l1), unlist(unconstrained))\n#>             [,1]       [,2]\n#> [1,]  1.69559026   4.576884\n#> [2,] -8.43734646 -11.771938\n#> [3,] 12.91758442  18.096060\n#> [4,]  0.01775982  -4.407187\n#> [5,]  0.01594419   2.432310\n#> [6,] 33.05084229  42.205029\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 1L)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())\nfor(i in 1:500){\n  indices = sample.int(nrow(X), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  # Add L1:\n  for(i in 1:length(model_torch$parameters)){\n    # Remind the penalty lambda that is set to 10 here.\n    loss = loss + model_torch$parameters[[i]]$abs()$sum()*10.0\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\nmodel_torch$parameters\n#> $`0.weight`\n#> torch_tensor\n#>   0.7787  -6.5937  14.6648  -0.0670   0.0475\n#> [ CPUFloatType{1,5} ][ requires_grad = TRUE ]\n#> \n#> $`0.bias`\n#> torch_tensor\n#>  37.2661\n#> [ CPUFloatType{1} ][ requires_grad = TRUE ]"},{"path":"fundamental.html","id":"exerexer","chapter":"5 Common Machine Learning algorithms","heading":"5.1.5 Exercise","text":"high learning rates optima might jumped .low learning rates might land local optima might take long.KerasTorch\nKerasTorchPlay around parameters !","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n#> Loaded Tensorflow version 2.9.1\n\niris = datasets::iris\nX = scale(iris[,1:4])\nY = iris[,5]\nY = keras::k_one_hot(as.integer(Y)-1L, 3)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %>%\n  layer_dense(units = 20L, activation = \"relu\", ) %>%\n  layer_dense(units = 20L, activation = \"relu\", ) %>%\n  layer_dense(units = 3L, activation = \"softmax\")\n# Softmax scales to (0, 1]; 3 output nodes for 3 response classes/labels.\n# The labels MUST start at 0!\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.5),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history =\n  model %>%\n  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>           0.04419739           0.98666668\n\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\niris = datasets::iris\nX = scale(iris[,1:4])\nY = iris[,5]\nY = as.integer(Y)\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 20L),\n  nn_relu(),\n  nn_linear(in_features = 20L, out_features = 20L, bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 20L, out_features = 20L, bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 20L, out_features = 3L, bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nbatch_size = 20L\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(Y, dtype = torch_long())\nlosses = rep(NA, 500)\nfor(i in 1:500){\n  indices = sample.int(nrow(X), batch_size)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  losses[[i]] = as.numeric(loss)\n  loss$sum()$backward()\n  opt$step()\n}\n\nplot(losses, main = \"Torch training history\", xlab = \"Epoch\", ylab = \"Loss\")\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.005),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history2 =\n  model %>%\n  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>           0.04000897           0.98666668\n\nplot(model_history2)\n\n##########  -> (very) Low learning rate: May take (very) long \n#           (and may need very many epochs) and get stuck in local optima.\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.00001),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history3 =\n  model %>%\n  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            0.0399975            0.9866667\n\nplot(model_history3)\n\nkeras::reset_states(model)\n\n# Try higher epoch number\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.00001),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history4 =\n  model %>%\n  fit(x = X, y = Y, epochs = 200L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>           0.03996202           0.98666668\n\nplot(model_history4)\n\n##########  -> (very) High learning rate (may skip optimum).\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 3),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history5 =\n  model %>%\n  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.3998153            0.9666666\n\nplot(model_history5)\n\n##########  -> Higher epoch number (possibly better fitting, maybe overfitting).\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 3),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history6 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            7.6132493            0.3333333\n\nplot(model_history6)\n\n##########\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.5),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history7 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.2877126            0.3333333\n\nplot(model_history7)\n\n##########\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.00001),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history8 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.1118358            0.3333333\n\nplot(model_history8)\n\n##########  -> Lower batch size.\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 3),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history9 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 5L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.2418699            0.3333333\n\nplot(model_history9)\n\n##########\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.5),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history10 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 5L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.1917901            0.3333333\n\nplot(model_history10)\n\n##########\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.00001),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history11 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 5L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.1628621            0.3333333\n\nplot(model_history11)\n\n##########  -> Higher batch size (faster but less accurate).\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 3),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history12 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 50L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.1135705            0.3333333\n\nplot(model_history12)\n\n##########\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.5),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history13 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 50L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.1114935            0.3333333\n\nplot(model_history13)\n\n##########\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.00001),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history14 =\n  model %>%\n  fit(x = X, y = Y, epochs = 100L, batch_size = 50L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>            1.1036538            0.3333333\n\nplot(model_history14)\n\n####################\n####################\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.05),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history15 =\n  model %>%\n  fit(x = X, y = Y, epochs = 150L, batch_size = 50L, shuffle = TRUE)\n\nplot(model_history15)\n\n##########  -> shuffle = FALSE (some kind of overfitting)\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.05),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history16 =\n  model %>%\n  fit(x = X, y = Y, epochs = 150L, batch_size = 50L, shuffle = FALSE)\n\nplot(model_history16)\n\n##########  -> shuffle = FALSE + lower batch size\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.05),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history17 =\n  model %>%\n  fit(x = X, y = Y, epochs = 150L, batch_size = 5L, shuffle = FALSE)\n\nplot(model_history17)\n\n##########  -> shuffle = FALSE + higher batch size\n#           (Many samples are taken at once, so no \"hopping\" any longer.)\n\nkeras::reset_states(model)\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.05),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history18 =\n  model %>%\n  fit(x = X, y = Y, epochs = 150L, batch_size = 75L, shuffle = FALSE)\n\nplot(model_history18)\n\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nbatch_size = 20L\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(Y, dtype = torch_long())\nlosses = rep(NA, 500)\nfor(i in 1:500){\n  indices = sample.int(nrow(X), batch_size)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  losses[[i]] = as.numeric(loss)\n  loss$sum()$backward()\n  opt$step()\n}\n\nplot(losses, main = \"Torch training history\", xlab = \"Epoch\", ylab = \"Loss\")\n\n##########\n\n# reset parameters\n\n.n = lapply(model_torch$children, function(layer) { if(!is.null(layer$parameters)) layer$reset_parameters() } )\nopt = optim_adam(params = model_torch$parameters, lr = 5.5)\n\nbatch_size = 20L\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(Y, dtype = torch_long())\nlosses = rep(NA, 500)\nfor(i in 1:500){\n  indices = sample.int(nrow(X), batch_size)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  losses[[i]] = as.numeric(loss)\n  loss$sum()$backward()\n  opt$step()\n}\n\nplot(losses, main = \"Torch training history\", xlab = \"Epoch\", ylab = \"Loss\")\n\n##########\n\n# reset parameters\n\n.n = lapply(model_torch$children, function(layer) { if(!is.null(layer$parameters)) layer$reset_parameters() } )\nopt = optim_adam(params = model_torch$parameters, lr = 15.5)\n\nbatch_size = 20L\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(Y, dtype = torch_long())\nlosses = rep(NA, 500)\nfor(i in 1:500){\n  indices = sample.int(nrow(X), batch_size)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  losses[[i]] = as.numeric(loss)\n  loss$sum()$backward()\n  opt$step()\n}\n\nplot(losses, main = \"Torch training history\", xlab = \"Epoch\", ylab = \"Loss\")"},{"path":"fundamental.html","id":"artificial-neural-networks","chapter":"5 Common Machine Learning algorithms","heading":"5.2 Artificial Neural Networks","text":"KerasNow, come artificial neural networks (ANNs), topic regularization important. can specify regularization layer via kernel_regularization (/bias_regularization) argument.TorchAgain, regularization Torch:Let’s visualize first (input) layer:Additionally usual \\(L1\\) \\(L2\\) regularization another regularization: called dropout-layer (learn detail later).","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\nsummary(data)\n#>      Ozone           Solar.R           Wind             Temp           Month      \n#>  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   Min.   :5.000  \n#>  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   1st Qu.:6.000  \n#>  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   Median :7.000  \n#>  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   Mean   :6.993  \n#>  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   3rd Qu.:8.000  \n#>  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   Max.   :9.000  \n#>  NA's   :37       NA's   :7                                                       \n#>       Day      \n#>  Min.   : 1.0  \n#>  1st Qu.: 8.0  \n#>  Median :16.0  \n#>  Mean   :15.8  \n#>  3rd Qu.:23.0  \n#>  Max.   :31.0  \n#> \ndata = data[complete.cases(data),] # Remove NAs.\nsummary(data)\n#>      Ozone          Solar.R           Wind            Temp           Month      \n#>  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00   Min.   :5.000  \n#>  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00   1st Qu.:6.000  \n#>  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00   Median :7.000  \n#>  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79   Mean   :7.216  \n#>  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50   3rd Qu.:9.000  \n#>  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00   Max.   :9.000  \n#>       Day       \n#>  Min.   : 1.00  \n#>  1st Qu.: 9.00  \n#>  Median :16.00  \n#>  Mean   :15.95  \n#>  3rd Qu.:22.50  \n#>  Max.   :31.00\n\nX = scale(data[,2:6])\nY = data[,1]\n\nmodel = keras_model_sequential()\npenalty = 0.1\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             input_shape = list(5L),\n             kernel_regularizer = regularizer_l1(penalty)) %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             kernel_regularizer = regularizer_l1(penalty) ) %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             kernel_regularizer = regularizer_l1(penalty)) %>%\n # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\",\n             kernel_regularizer = regularizer_l1(penalty))\n\nmodel %>%\n compile(\n   loss = loss_mean_squared_error,\n   keras::optimizer_adamax(learning_rate = 0.1)\n  )\n\nmodel_history =\n model %>%\n fit(x = X, y = matrix(Y, ncol = 1L), epochs = 100L,\n     batch_size = 20L, shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nweights = lapply(model$weights, function(w) w$numpy() )\nfields::image.plot(weights[[1]])\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 100L),\n  nn_relu(),\n  nn_linear(100L, 100L),\n  nn_relu(),\n  nn_linear(100L, 100L),\n  nn_relu(),\n  nn_linear(100L, 1L),\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.1)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())\nfor(i in 1:500){\n  indices = sample.int(nrow(X), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  # Add L1 (only on the 'kernel weights'):\n  for(i in seq(1, 8, by = 2)){\n    loss = loss + model_torch$parameters[[i]]$abs()$sum()*0.1\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\nfields::image.plot(as.matrix(model_torch$parameters$`0.weight`))"},{"path":"fundamental.html","id":"exercise-3","chapter":"5 Common Machine Learning algorithms","heading":"5.2.1 Exercise","text":"following code working example next exercises:KerasTorchWhat happens change regularization \\(L1\\) \\(L2\\)?\nKerasTorchWeights 4 5 strongly pushed towards zero \\(L1\\) regularization, \\(L2\\) regularization shrinks general.\nNote: weights similar linear model! (often, keep sign.)Try different regularization strengths, try push weights zero. strategy push parameters zero?\nNow less regularization:KerasTorchAnd regularization:KerasTorchFor pushing weights towards zero, \\(L1\\) regularization used rather \\(L2\\).\nHigher regularization leads smaller parameters (maybe combination smaller learning rates).Play around ! Ask questions .Use combination \\(L1\\) \\(L2\\) regularization (Keras function ). kind regularization called advantage approach?\nkind regularization called Elastic net. combination LASSO (\\(L1\\)) Ridge. flexible \\(L1\\) less flexible \\(L2\\) higher computational cost. Elastic net shrinkage well, separate highly correlated parameters much.Keras can tell model keep specific percentage data holdout (validation_split argument fit function):Run code view loss train validation (test) set viewer panel. happens validation loss? ?\ntraining loss keeps decreasing, validation loss increases time. increase validation loss due overfitting.Add \\(L1\\) / \\(L2\\) regularization neural network try keep test loss close training loss. Try little higher epoch number!Explain strategy helps achieve .\nAdding regularization (\\(L1\\) \\(L2\\)):Adding higher regularization (\\(L1\\) \\(L2\\)):Keep mind: higher regularization, training validation loss keep relatively high!Adding normal regularization (\\(L1\\) \\(L2\\)) use larger learning rate:Adding normal regularization (\\(L1\\) \\(L2\\)) use low learning rate:Look constantly lower validation loss.Adding low regularization (\\(L1\\) \\(L2\\)) use low learning rate:good example early stopping.Taking example , without shuffling:Play around regularization kind (\\(L1\\), \\(L2\\), \\(L1,L2\\)) strength learning rate also !","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l1(10),\n             bias_regularizer = regularizer_l1(10))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nl1 = model$get_weights()\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\n# The last parameter is the intercept (first parameter in lm).\ncat(\"L1:         \", round(c(unlist(l1)[length(unlist(l1))],\n                            unlist(l1)[1:(length(unlist(l1)) - 1 )]), 3))\n#> L1:          36.766 1.193 -8.446 13.394 -0.108 0.034\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(matrix(y, ncol = 1L), dtype = torch_float32())\nlambda = torch_tensor(10.)\nfor(i in 1:500){\n  indices = sample.int(nrow(x), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  ## add L1 regularization\n  \n  for(i in 1:length(model_torch$parameters)) {\n     # if(stringr::str_detect(names(model_torch$parameters)[[i]], \"weight\")) {\n      loss = loss + lambda*torch_norm(model_torch$parameters[[i]], 1)\n     # }\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\n\nl1_torch = model_torch$parameters\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\ncat(\"L1:         \", round(c(as.numeric( l1_torch$`0.bias` ),as.numeric( l1_torch$`0.weight` )), 3))\n#> L1:          37.266 0.779 -6.594 14.665 -0.067 0.048\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l2(10),\n             bias_regularizer = regularizer_l2(10))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nl2 = model$get_weights()#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\n#> L1:          36.766 1.193 -8.446 13.394 -0.108 0.034\n#> L2:          3.756 0.976 -1.721 1.956 0.402 -0.112\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(matrix(y, ncol = 1L), dtype = torch_float32())\nlambda = torch_tensor(10.)\nfor(i in 1:500){\n  indices = sample.int(nrow(x), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  ## add L1 regularization\n  \n  for(i in 1:length(model_torch$parameters)) {\n     # if(stringr::str_detect(names(model_torch$parameters)[[i]], \"weight\")) {\n      loss = loss + lambda*torch_norm(model_torch$parameters[[i]], 2)\n     # }\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\n\nl2_torch = model_torch$parameters\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\ncat(\"L1:         \", round(c(as.numeric( l1_torch$`0.bias` ),as.numeric( l1_torch$`0.weight` )), 3))\n#> L1:          37.266 0.779 -6.594 14.665 -0.067 0.048\ncat(\"L2:         \", round(c(as.numeric( l2_torch$`0.bias` ),as.numeric( l2_torch$`0.weight` )), 3))\n#> L2:          37.334 4.262 -8.593 14.167 -2.081 0.365\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l1(1),\n             bias_regularizer = regularizer_l1(1))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nl1_lesser = model$get_weights()#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\n#> L1:          36.766 1.193 -8.446 13.394 -0.108 0.034\n#> L1, lesser:  41.087 4.164 -11.617 17.043 -3.571 1.986\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l2(1),\n             bias_regularizer = regularizer_l2(1))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nl2_lesser = model$get_weights()#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\n#> L2:          3.756 0.976 -1.721 1.956 0.402 -0.112\n#> L2, lesser:  20.999 3.726 -7.585 9.065 -0.065 0.669\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(matrix(y, ncol = 1L), dtype = torch_float32())\nlambda = torch_tensor(1.)\nfor(i in 1:500){\n  indices = sample.int(nrow(x), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  ## add L1 regularization\n  \n  for(i in 1:length(model_torch$parameters)) {\n     # if(stringr::str_detect(names(model_torch$parameters)[[i]], \"weight\")) {\n      loss = loss + lambda*torch_norm(model_torch$parameters[[i]], 1)\n     # }\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\n\nl1_torch_lesser = model_torch$parameters\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\ncat(\"L1:         \", round(c(as.numeric( l1_torch$`0.bias` ),as.numeric( l1_torch$`0.weight` )), 3))\n#> L1:          37.266 0.779 -6.594 14.665 -0.067 0.048\ncat(\"L1 lesser:         \", round(c(as.numeric( l1_torch_lesser$`0.bias` ),as.numeric( l1_torch_lesser$`0.weight` )), 3))\n#> L1 lesser:          41.631 3.814 -9.777 17.704 -3.5 0.993\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(matrix(y, ncol = 1L), dtype = torch_float32())\nlambda = torch_tensor(1.)\nfor(i in 1:500){\n  indices = sample.int(nrow(x), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  ## add L1 regularization\n  \n  for(i in 1:length(model_torch$parameters)) {\n     # if(stringr::str_detect(names(model_torch$parameters)[[i]], \"weight\")) {\n      loss = loss + lambda*torch_norm(model_torch$parameters[[i]], 2)\n     # }\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\n\nl2_torch_lesser = model_torch$parameters\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\ncat(\"L2:         \", round(c(as.numeric( l2_torch$`0.bias` ),as.numeric( l2_torch$`0.weight` )), 3))\n#> L2:          37.334 4.262 -8.593 14.167 -2.081 0.365\ncat(\"L2 lesser:         \", round(c(as.numeric( l2_torch_lesser$`0.bias` ),as.numeric( l2_torch_lesser$`0.weight` )), 3))\n#> L2 lesser:          41.624 4.172 -10.056 17.798 -4.058 1.171\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l1(25),\n             bias_regularizer = regularizer_l1(25))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nl1_higher = model$get_weights()#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\n#> L1:          36.766 1.193 -8.446 13.394 -0.108 0.034\n#> L1, lesser:  41.087 4.164 -11.617 17.043 -3.571 1.986\n#> L1, higher:  29.367 0.112 -3.354 8.732 0.058 0.03\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l2(25),\n             bias_regularizer = regularizer_l2(25))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nl2_higher = model$get_weights()#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\n#> L2:          3.756 0.976 -1.721 1.956 0.402 -0.112\n#> L2, lesser:  20.999 3.726 -7.585 9.065 -0.065 0.669\n#> L2, higher:  1.564 0.414 -0.797 0.83 0.165 0.048\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(matrix(y, ncol = 1L), dtype = torch_float32())\nlambda = torch_tensor(25.)\nfor(i in 1:500){\n  indices = sample.int(nrow(x), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  ## add L1 regularization\n  \n  for(i in 1:length(model_torch$parameters)) {\n     # if(stringr::str_detect(names(model_torch$parameters)[[i]], \"weight\")) {\n      loss = loss + lambda*torch_norm(model_torch$parameters[[i]], 1)\n     # }\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\n\nl1_torch_higher = model_torch$parameters\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\ncat(\"L1:         \", round(c(as.numeric( l1_torch$`0.bias` ),as.numeric( l1_torch$`0.weight` )), 3))\n#> L1:          37.266 0.779 -6.594 14.665 -0.067 0.048\ncat(\"L1 lesser:         \", round(c(as.numeric( l1_torch_lesser$`0.bias` ),as.numeric( l1_torch_lesser$`0.weight` )), 3))\n#> L1 lesser:          41.631 3.814 -9.777 17.704 -3.5 0.993\ncat(\"L1 higher:         \", round(c(as.numeric( l1_torch_higher$`0.bias` ),as.numeric( l1_torch_higher$`0.weight` )), 3))\n#> L1 higher:          29.636 0.123 -1.257 10.496 -0.013 0.032\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(matrix(y, ncol = 1L), dtype = torch_float32())\nlambda = torch_tensor(25.)\nfor(i in 1:500){\n  indices = sample.int(nrow(x), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  ## add L1 regularization\n  \n  for(i in 1:length(model_torch$parameters)) {\n     # if(stringr::str_detect(names(model_torch$parameters)[[i]], \"weight\")) {\n      loss = loss + lambda*torch_norm(model_torch$parameters[[i]], 2)\n     # }\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\n\nl2_torch_higher = model_torch$parameters\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\ncat(\"L2:         \", round(c(as.numeric( l2_torch$`0.bias` ),as.numeric( l2_torch$`0.weight` )), 3))\n#> L2:          37.334 4.262 -8.593 14.167 -2.081 0.365\ncat(\"L2 lesser:         \", round(c(as.numeric( l2_torch_lesser$`0.bias` ),as.numeric( l2_torch_lesser$`0.weight` )), 3))\n#> L2 lesser:          41.624 4.172 -10.056 17.798 -4.058 1.171\ncat(\"L2 higher:         \", round(c(as.numeric( l2_torch_higher$`0.bias` ),as.numeric( l2_torch_higher$`0.weight` )), 3))\n#> L2 higher:          30.026 3.192 -5.988 9.622 -0.353 -0.043\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l1_l2(10),\n             bias_regularizer = regularizer_l1_l2(10))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L) %>%\n  layer_dense(units = 100L) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 50L, batch_size = 20L,\n        shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L,\n        shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(15),\n              bias_regularizer = regularizer_l1(2)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(15),\n              bias_regularizer = regularizer_l1(2)) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L,\n        shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.2))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L,\n        shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L,\n        shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L,\n        shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  layer_dense(units = 100L, kernel_regularizer = regularizer_l1_l2(.5),\n              bias_regularizer = regularizer_l1(2)) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 150L, batch_size = 20L,\n        shuffle = FALSE, validation_split = 0.2)\n\nplot(model_history)"},{"path":"fundamental.html","id":"tree-based-machine-learning-algorithms","chapter":"5 Common Machine Learning algorithms","heading":"5.3 Tree-based Machine Learning Algorithms","text":"Famous machine learning algorithms Random Forest Gradient Boosted trees based classification regression trees.\nHint: Tree-based algorithms distance based thus need scaling.want know little bit concepts described following - like decision, classification regression trees well random forests - might watch following videos:Decision treesRegression trees\n\nRegression trees\n\nRandom forests\n\nRandom forests\n\nPruning trees\n\nPruning trees\n\nwatching videos, know different hyperparameters prevent trees / forests something don’t want.","code":""},{"path":"fundamental.html","id":"classification-and-regression-trees","chapter":"5 Common Machine Learning algorithms","heading":"5.3.1 Classification and Regression Trees","text":"Tree-based models general use series -rules generate predictions one decision trees.\nlecture, explore regression classification trees example airquality data set. one important hyperparameter regression trees: “minsplit.”controls depth tree (see help rpart description).controls complexity tree can thus also seen regularization parameter.first prepare visualize data afterwards fit decision tree.Fit visualize one(!) regression tree:Visualize predictions:angular form prediction line typical regression trees weakness .","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\n\ndata = airquality[complete.cases(airquality),]\nrt = rpart(Ozone~., data = data, control = rpart.control(minsplit = 10))\nrpart.plot(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")"},{"path":"fundamental.html","id":"random-forest","chapter":"5 Common Machine Learning algorithms","heading":"5.3.2 Random Forest","text":"overcome weakness, random forest uses ensemble regression/classification trees. Thus, random forest principle nothing else normal regression/classification tree, uses idea “wisdom crowd” : asking many people (regression/classification trees) one can make informed decision (prediction/classification). want buy new phone example also wouldn’t go directly shop, search internet ask friends family.two randomization steps random forest responsible success:Bootstrap samples tree (sample observations replacement data set. phone like everyone experience phone).split, sample subset predictors considered potential splitting criterion (phone like everyone decision criteria).\nAnnotation: building decision tree (random forests consist many decision trees), one splits data point according features. example females males, big small people crowd, con split crowd gender size size gender build decision tree.Applying random forest follows principle methods : visualize data (already done often airquality data set, thus skip ), fit algorithm plot outcomes.Fit random forest visualize predictions:One advantage random forests get importance variables. split tree, improvement split-criterion importance measure attributed splitting variable, accumulated trees forest separately variable. Thus variable importance shows us important variable averaged trees.several important hyperparameters random forest can tune get better results:Similar minsplit parameter regression classification trees, hyperparameter “nodesize” controls complexity \\(\\rightarrow\\) Minimum size terminal nodes tree. Setting number larger causes smaller trees grown (thus take less time). Note default values different classification (1) regression (5).mtry: Number features randomly sampled candidates split.","code":"\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\npred = predict(rf, data)\nplot(Ozone~Temp, data = data)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nrf$importance\n#>         IncNodePurity\n#> Solar.R      17969.59\n#> Wind         31978.36\n#> Temp         34176.71\n#> Month        10753.73\n#> Day          15436.47"},{"path":"fundamental.html","id":"boosted-regression-trees","chapter":"5 Common Machine Learning algorithms","heading":"5.3.3 Boosted Regression Trees","text":"Random forests fit hundreds trees independent . , idea boosted regression tree comes . Maybe learn errors previous weak learners made thus enhance performance algorithm.boosted regression tree (BRT) starts simple regression tree (weak learner) sequentially fits additional trees improve results.\ntwo different strategies :AdaBoost: Wrong classified observations (previous tree) get higher weight therefore next trees focus difficult/missclassified observations.Gradient boosting (state art): sequential model fit residual errors previous model.can fit boosted regression tree using xgboost, transform data xgb.Dmatrix.parameter “nrounds” controls many sequential trees fit, example 16. predict new data, can limit number trees used prevent overfitting (remember: new tree tries improve predictions previous trees).Let us visualize predictions different numbers trees:also ways control complexity boosted regression tree algorithm:max_depth: Maximum depth tree.shrinkage (tree get weight weight decrease number trees).specified final model, can obtain importance variables like random forests:One important strength xgboost can directly cross-validation (independent boosted regression tree !) specify properties parameter “n-fold”:Annotation: original data set randomly partitioned \\(n\\) equal sized subsamples. time, model trained \\(n - 1\\) subsets (training set) tested left set (test set) judge performance.three-folded cross-validation, actually fit three different boosted regression tree models (xgboost models) \\(\\approx 67\\%\\) data points. Afterwards, judge performance respective holdout. now tells us well model performed.use following code snippet see influence mincut trees.Try different mincut parameters see happens.\n(Compare root mean squared error different mincut parameters explain see.\nCompare predictions different mincut parameters explain happens.)\nwrong snippet ?\n\nApproximately mincut = 15, prediction best (mind overfitting). mincut = 56, prediction information RMSE stays constant.Mind complete cases airquality data set, error.use following code snippet explore random forest:Try different values nodesize mtry describe predictions depend parameters.\nHigher numbers mtry smooth prediction curve yield less overfitting. holds nodesize.\nwords: bigger nodesize, smaller trees bias/less variance.Run code play different parameters xgboost (especially parameters control complexity) describe see!Tip: look boosting.gif.\npossibilities scale extremely, must vary parameters .\nmay use example following parameters: “eta,” “gamma,” “max_depth,” “min_child_weight,” “subsample,” “colsample_bytree,” “num_parallel_tree,” “monotone_constraints” “interaction_constraints.” look documentation:Just examples:implemented simple boosted regression tree using R just fun.\nGo code line line try understand . Ask, questions solve.\n","code":"\nlibrary(xgboost)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\nbrt = xgboost(data_xg, nrounds = 16L)\n#> [1]  train-rmse:39.724624 \n#> [2]  train-rmse:30.225761 \n#> [3]  train-rmse:23.134840 \n#> [4]  train-rmse:17.899179 \n#> [5]  train-rmse:14.097785 \n#> [6]  train-rmse:11.375457 \n#> [7]  train-rmse:9.391276 \n#> [8]  train-rmse:7.889690 \n#> [9]  train-rmse:6.646586 \n#> [10] train-rmse:5.804859 \n#> [11] train-rmse:5.128437 \n#> [12] train-rmse:4.456416 \n#> [13] train-rmse:4.069464 \n#> [14] train-rmse:3.674615 \n#> [15] train-rmse:3.424578 \n#> [16] train-rmse:3.191301\noldpar = par(mfrow = c(2, 2))\nfor(i in 1:4){\n  pred = predict(brt, newdata = data_xg, ntreelimit = i)\n  plot(data$Temp, data$Ozone, main = i)\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n#> [09:24:50] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n#> [09:24:50] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n#> [09:24:50] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n#> [09:24:50] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\npar(oldpar)\nxgboost::xgb.importance(model = brt)\n#>    Feature        Gain     Cover  Frequency\n#> 1:    Temp 0.570071903 0.2958229 0.24836601\n#> 2:    Wind 0.348230710 0.3419576 0.24183007\n#> 3: Solar.R 0.058795542 0.1571072 0.30718954\n#> 4:     Day 0.019529993 0.1779925 0.16993464\n#> 5:   Month 0.003371853 0.0271197 0.03267974\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n#> [1] 17.89918\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\nset.seed(123)\n\nbrt = xgboost(data_xg, nrounds = 5L)\n#> [1]  train-rmse:39.724624 \n#> [2]  train-rmse:30.225761 \n#> [3]  train-rmse:23.134840 \n#> [4]  train-rmse:17.899179 \n#> [5]  train-rmse:14.097785\nbrt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L,\n                         nrounds = 3L, nthreads = 4L)\n#> [1]  train-rmse:39.895106+2.127355   test-rmse:40.685477+5.745327 \n#> [2]  train-rmse:30.367660+1.728788   test-rmse:32.255812+5.572963 \n#> [3]  train-rmse:23.446237+1.366757   test-rmse:27.282435+5.746244\nprint(brt_cv)\n#> ##### xgb.cv 3-folds\n#>  iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n#>     1        39.89511       2.127355       40.68548      5.745327\n#>     2        30.36766       1.728788       32.25581      5.572963\n#>     3        23.44624       1.366757       27.28244      5.746244\nlibrary(tree)\nset.seed(123)\n\ndata = airquality\nrt = tree(Ozone~., data = data,\n          control = tree.control(mincut = 1L, nobs = nrow(data)))\n\nplot(rt)\ntext(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\nlibrary(tree)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\ndoTask = function(mincut){\n  rt = tree(Ozone~., data = data,\n            control = tree.control(mincut = mincut, nobs = nrow(data)))\n\n  pred = predict(rt, data)\n  plot(data$Temp, data$Ozone,\n       main = paste0(\n         \"mincut: \", mincut,\n         \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n  )\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\nfor(i in c(1, 2, 3, 5, 10, 15, 25, 50, 54, 55, 56, 57, 75, 100)){ doTask(i) }\nlibrary(randomForest)\nset.seed(123)\n\nairquality[complete.cases(airquality),]\n#>     Ozone Solar.R Wind Temp Month Day\n#> 1      41     190  7.4   67     5   1\n#> 2      36     118  8.0   72     5   2\n#> 3      12     149 12.6   74     5   3\n#> 4      18     313 11.5   62     5   4\n#> 7      23     299  8.6   65     5   7\n#> 8      19      99 13.8   59     5   8\n#> 9       8      19 20.1   61     5   9\n#> 12     16     256  9.7   69     5  12\n#> 13     11     290  9.2   66     5  13\n#> 14     14     274 10.9   68     5  14\n#> 15     18      65 13.2   58     5  15\n#> 16     14     334 11.5   64     5  16\n#> 17     34     307 12.0   66     5  17\n#> 18      6      78 18.4   57     5  18\n#> 19     30     322 11.5   68     5  19\n#> 20     11      44  9.7   62     5  20\n#> 21      1       8  9.7   59     5  21\n#> 22     11     320 16.6   73     5  22\n#> 23      4      25  9.7   61     5  23\n#> 24     32      92 12.0   61     5  24\n#> 28     23      13 12.0   67     5  28\n#> 29     45     252 14.9   81     5  29\n#> 30    115     223  5.7   79     5  30\n#> 31     37     279  7.4   76     5  31\n#> 38     29     127  9.7   82     6   7\n#> 40     71     291 13.8   90     6   9\n#> 41     39     323 11.5   87     6  10\n#> 44     23     148  8.0   82     6  13\n#> 47     21     191 14.9   77     6  16\n#> 48     37     284 20.7   72     6  17\n#> 49     20      37  9.2   65     6  18\n#> 50     12     120 11.5   73     6  19\n#> 51     13     137 10.3   76     6  20\n#> 62    135     269  4.1   84     7   1\n#> 63     49     248  9.2   85     7   2\n#> 64     32     236  9.2   81     7   3\n#> 66     64     175  4.6   83     7   5\n#> 67     40     314 10.9   83     7   6\n#> 68     77     276  5.1   88     7   7\n#> 69     97     267  6.3   92     7   8\n#> 70     97     272  5.7   92     7   9\n#> 71     85     175  7.4   89     7  10\n#> 73     10     264 14.3   73     7  12\n#> 74     27     175 14.9   81     7  13\n#> 76      7      48 14.3   80     7  15\n#> 77     48     260  6.9   81     7  16\n#> 78     35     274 10.3   82     7  17\n#> 79     61     285  6.3   84     7  18\n#> 80     79     187  5.1   87     7  19\n#> 81     63     220 11.5   85     7  20\n#> 82     16       7  6.9   74     7  21\n#> 85     80     294  8.6   86     7  24\n#> 86    108     223  8.0   85     7  25\n#> 87     20      81  8.6   82     7  26\n#> 88     52      82 12.0   86     7  27\n#> 89     82     213  7.4   88     7  28\n#> 90     50     275  7.4   86     7  29\n#> 91     64     253  7.4   83     7  30\n#> 92     59     254  9.2   81     7  31\n#> 93     39      83  6.9   81     8   1\n#> 94      9      24 13.8   81     8   2\n#> 95     16      77  7.4   82     8   3\n#> 99    122     255  4.0   89     8   7\n#> 100    89     229 10.3   90     8   8\n#> 101   110     207  8.0   90     8   9\n#> 104    44     192 11.5   86     8  12\n#> 105    28     273 11.5   82     8  13\n#> 106    65     157  9.7   80     8  14\n#> 108    22      71 10.3   77     8  16\n#> 109    59      51  6.3   79     8  17\n#> 110    23     115  7.4   76     8  18\n#> 111    31     244 10.9   78     8  19\n#> 112    44     190 10.3   78     8  20\n#> 113    21     259 15.5   77     8  21\n#> 114     9      36 14.3   72     8  22\n#> 116    45     212  9.7   79     8  24\n#> 117   168     238  3.4   81     8  25\n#> 118    73     215  8.0   86     8  26\n#> 120    76     203  9.7   97     8  28\n#> 121   118     225  2.3   94     8  29\n#> 122    84     237  6.3   96     8  30\n#> 123    85     188  6.3   94     8  31\n#> 124    96     167  6.9   91     9   1\n#> 125    78     197  5.1   92     9   2\n#> 126    73     183  2.8   93     9   3\n#> 127    91     189  4.6   93     9   4\n#> 128    47      95  7.4   87     9   5\n#> 129    32      92 15.5   84     9   6\n#> 130    20     252 10.9   80     9   7\n#> 131    23     220 10.3   78     9   8\n#> 132    21     230 10.9   75     9   9\n#> 133    24     259  9.7   73     9  10\n#> 134    44     236 14.9   81     9  11\n#> 135    21     259 15.5   76     9  12\n#> 136    28     238  6.3   77     9  13\n#> 137     9      24 10.9   71     9  14\n#> 138    13     112 11.5   71     9  15\n#> 139    46     237  6.9   78     9  16\n#> 140    18     224 13.8   67     9  17\n#> 141    13      27 10.3   76     9  18\n#> 142    24     238 10.3   68     9  19\n#> 143    16     201  8.0   82     9  20\n#> 144    13     238 12.6   64     9  21\n#> 145    23      14  9.2   71     9  22\n#> 146    36     139 10.3   81     9  23\n#> 147     7      49 10.3   69     9  24\n#> 148    14      20 16.6   63     9  25\n#> 149    30     193  6.9   70     9  26\n#> 151    14     191 14.3   75     9  28\n#> 152    18     131  8.0   76     9  29\n#> 153    20     223 11.5   68     9  30\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n#>         IncNodePurity\n#> Solar.R      17969.59\n#> Wind         31978.36\n#> Temp         34176.71\n#> Month        10753.73\n#> Day          15436.47\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n#> RMSE:  9.507848\n\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\nfor(nodesize in c(1, 5, 15, 50, 100)){\n  for(mtry in c(1, 3, 5)){\n    rf = randomForest(Ozone~., data = data, mtry = mtry, nodesize = nodesize)\n    \n    pred = predict(rf, data)\n    \n    plot(data$Temp, data$Ozone, main = paste0(\n        \"mtry: \", mtry, \"    nodesize: \", nodesize,\n        \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n    )\n    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n  }\n}\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\n\nmodel = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                         nrounds = 500L, verbose = 0L)\npred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n               ntreelimit = 10L)\n\nsaveGIF(\n  {\n    for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n      pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                     ntreelimit = i)\n      image(matrix(pred, 100, 100), main = paste0(\"Trees: \", i),\n            axes = FALSE, las = 2)\n      axis(1, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1))\n      axis(2, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n    }\n  },\n  movie.name = \"boosting.gif\", autobrowse = FALSE\n)\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\nfor(eta in c(.1, .3, .5, .7, .9)){\n  for(max_depth in c(3, 6, 10, 20)){\n    model = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                             max_depth = max_depth, eta = eta,\n                             nrounds = 500, verbose = 0L)\n  \n    saveGIF(\n      {\n        for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n          pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                         ntreelimit = i)\n          image(matrix(pred, 100, 100),\n                main = paste0(\"eta: \", eta,\n                              \"    max_depth: \", max_depth,\n                              \"    Trees: \", i),\n                axes = FALSE, las = 2)\n          axis(1, at = seq(0, 1, length.out = 10),\n               labels = round(seq(-3, 3, length.out = 10), 1))\n          axis(2, at = seq(0, 1, length.out = 10),\n               labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n        }\n      },\n      movie.name = paste0(\"boosting_\", max_depth, \"_\", eta, \".gif\"),\n      autobrowse = FALSE\n    )\n  }\n}\n?xgboost::xgboost\nlibrary(tree)\nset.seed(123)\n\ndepth = 1L\n\n#### Simulate Data\nx = runif(1000, -5, 5)\ny = x * sin(x) * 2 + rnorm(1000, 0, cos(x) + 1.8)\ndata = data.frame(x = x, y = y)\nplot(y~x)\n\n#### Helper function for single tree fit.\nget_model = function(x, y){\n  control = tree.control(nobs = length(x), mincut = 20L)\n  model = tree(y~x, data.frame(x = x, y = y), control = control)\n  pred = predict(model, data.frame(x = x, y = y))\n  return(list(model = model, pred = pred))\n}\n\n#### Boost function.\nget_boosting_model = function(depth){\n  pred = NULL\n  m_list = list()\n  for(i in 1:depth){\n    if(i == 1){\n      m = get_model(x, y)\n      pred = m$pred\n    }else{\n      y_res = y - pred\n      m = get_model(x, y_res)\n      pred = pred + m$pred\n    }\n    m_list[[i]] = m$model\n  }\n  model_list <<- m_list  # This writes outside function scope!\n  return(pred)\n}\n\n### Main.\npred = get_boosting_model(10L)[order(data$x)]\n\nlength(model_list)\n#> [1] 10\nplot(model_list[[1]])\n\nplot(y~x)\nlines(x = data$x[order(data$x)], get_boosting_model(1L)[order(data$x)],\n      col = 'red', lwd = 2)\nlines(x = data$x[order(data$x)], get_boosting_model(100L)[order(data$x)],\n      col = 'green', lwd = 2)"},{"path":"fundamental.html","id":"distance-based-algorithms","chapter":"5 Common Machine Learning algorithms","heading":"5.4 Distance-based Algorithms","text":"chapter, introduce support-vector machines (SVMs) distance-based methods\nHint: Distance-based models need scaling!","code":""},{"path":"fundamental.html","id":"k-nearest-neighbor","chapter":"5 Common Machine Learning algorithms","heading":"5.4.1 K-Nearest-Neighbor","text":"K-nearest-neighbor (kNN) simple algorithm stores available cases classifies new data based similarity measure. mostly used classify data point based \\(k\\) nearest neighbors classified.Let us first see example:class decide blue point? classes nearest points? Well, procedure used k-nearest-neighbors classifier thus actually “real” learning k-nearest-neighbors classification.applying k-nearest-neighbors classification, first scale data set, deal distances want influence predictors. Imagine one variable values -10.000 10.000 another -1 1. influence first variable distance points much stronger influence second variable.\niris data set, split data training test set . follow usual pipeline.Fit model create predictions:","code":"\nx = scale(iris[,1:4])\ny = iris[,5]\nplot(x[-100,1], x[-100, 3], col = y)\npoints(x[100,1], x[100, 3], col = \"blue\", pch = 18, cex = 1.3)\ndata = iris\ndata[,1:4] = apply(data[,1:4],2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\nlibrary(kknn)\nset.seed(123)\n\nknn = kknn(Species~., train = train, test = test)\nsummary(knn)\n#> \n#> Call:\n#> kknn(formula = Species ~ ., train = train, test = test)\n#> \n#> Response: \"nominal\"\n#>           fit prob.setosa prob.versicolor prob.virginica\n#> 1      setosa           1      0.00000000      0.0000000\n#> 2      setosa           1      0.00000000      0.0000000\n#> 3      setosa           1      0.00000000      0.0000000\n#> 4      setosa           1      0.00000000      0.0000000\n#> 5      setosa           1      0.00000000      0.0000000\n#> 6      setosa           1      0.00000000      0.0000000\n#> 7      setosa           1      0.00000000      0.0000000\n#> 8      setosa           1      0.00000000      0.0000000\n#> 9      setosa           1      0.00000000      0.0000000\n#> 10     setosa           1      0.00000000      0.0000000\n#> 11     setosa           1      0.00000000      0.0000000\n#> 12     setosa           1      0.00000000      0.0000000\n#> 13     setosa           1      0.00000000      0.0000000\n#> 14 versicolor           0      0.86605852      0.1339415\n#> 15 versicolor           0      0.74027417      0.2597258\n#> 16 versicolor           0      0.91487300      0.0851270\n#> 17 versicolor           0      0.98430840      0.0156916\n#> 18 versicolor           0      0.91487300      0.0851270\n#> 19 versicolor           0      1.00000000      0.0000000\n#> 20 versicolor           0      1.00000000      0.0000000\n#> 21 versicolor           0      1.00000000      0.0000000\n#> 22 versicolor           0      1.00000000      0.0000000\n#> 23 versicolor           0      1.00000000      0.0000000\n#> 24 versicolor           0      1.00000000      0.0000000\n#> 25 versicolor           0      1.00000000      0.0000000\n#> 26 versicolor           0      1.00000000      0.0000000\n#> 27 versicolor           0      0.86605852      0.1339415\n#> 28 versicolor           0      1.00000000      0.0000000\n#> 29  virginica           0      0.00000000      1.0000000\n#> 30  virginica           0      0.00000000      1.0000000\n#> 31  virginica           0      0.00000000      1.0000000\n#> 32  virginica           0      0.00000000      1.0000000\n#> 33  virginica           0      0.08512700      0.9148730\n#> 34  virginica           0      0.22169561      0.7783044\n#> 35  virginica           0      0.00000000      1.0000000\n#> 36  virginica           0      0.23111986      0.7688801\n#> 37 versicolor           0      1.00000000      0.0000000\n#> 38  virginica           0      0.04881448      0.9511855\n#> 39 versicolor           0      0.64309579      0.3569042\n#> 40 versicolor           0      0.67748579      0.3225142\n#> 41  virginica           0      0.17288113      0.8271189\n#> 42  virginica           0      0.00000000      1.0000000\n#> 43  virginica           0      0.00000000      1.0000000\n#> 44  virginica           0      0.00000000      1.0000000\n#> 45  virginica           0      0.35690421      0.6430958\ntable(test$Species, fitted(knn))\n#>             \n#>              setosa versicolor virginica\n#>   setosa         13          0         0\n#>   versicolor      0         15         0\n#>   virginica       0          3        14"},{"path":"fundamental.html","id":"support-vector-machines-svms","chapter":"5 Common Machine Learning algorithms","heading":"5.4.2 Support Vector Machines (SVMs)","text":"Support vectors machines different approach. try divide predictor space sectors class. , support-vector machine fits parameters hyperplane (\\(n-1\\) dimensional subspace \\(n\\)-dimensional space) predictor space optimizing distance hyperplane nearest point class.Fitting support-vector machine:Support-vector machines can work linearly separable problems. (problem called linearly separable exists least one line plane points one class one side hyperplane points others classes side).possible, however, can use called kernel trick, maps predictor space (higher dimensional) space problem linear separable. identified boundaries higher-dimensional space, can project back original dimensions.seen, work every kernel. Hence, problem find actual correct kernel, optimization procedure can thus approximated.use Sonar data set explore support-vector machines k-neartest-neighbor classifier.Split Sonar data set mlbench library training- testset 50% group. useful split?\nresponse variable “class.” trying classify class.\nstrong reasons , 50/50 really good decision. waste data/power.\nforget scaling!Fit standard k-nearest-neighbor classifier support vector machine linear kernel (check help), report fitted better.\nK-nearest neighbor fitted (slightly) better.Calculate accuracies algorithms.\nFit different kernels compare accuracies.\nTry fit different seed training test set generation.\n","code":"\nlibrary(e1071)\n\ndata = iris\ndata[,1:4] = apply(data[,1:4], 2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\nsm = svm(Species~., data = train, kernel = \"linear\")\npred = predict(sm, newdata = test)\noldpar = par(mfrow = c(1, 2))\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  pred, main = \"predicted\")\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  test$Species, main = \"observed\")\npar(oldpar)\n\nmean(pred == test$Species) # Accuracy.\n#> [1] 0.9777778\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(X, 1, function(t) exp(-t[1]^2 - t[2]^2))\ny = ifelse(1/(1+exp(-y)) < 0.62, 0, 1)\n\nimage(matrix(y, 100, 100))\nanimation::saveGIF(\n  {\n    for(i in c(\"truth\", \"linear\", \"radial\", \"sigmoid\")){\n      if(i == \"truth\"){\n        image(matrix(y, 100,100),\n        main = \"Ground truth\", axes = FALSE, las = 2)\n      }else{\n        sv = e1071::svm(x = x, y = factor(y), kernel = i)\n        image(matrix(as.numeric(as.character(predict(sv, x))), 100, 100),\n        main = paste0(\"Kernel: \", i), axes = FALSE, las = 2)\n        axis(1, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    }\n  },\n  movie.name = \"svm.gif\", autobrowse = FALSE\n)\nlibrary(mlbench)\nset.seed(123)\n\ndata(Sonar)\ndata = Sonar\nindices = sample.int(nrow(Sonar), 0.5 * nrow(Sonar))\nlibrary(mlbench)\nset.seed(123)\n\ndata(Sonar)\ndata = Sonar\n#str(data)\n\n# Do not forget scaling! This may be done implicitly by most functions.\n# Here, it's done explicitly for teaching purposes.\ndata = cbind.data.frame(\n  scale(data[,-length(data)]),\n  \"class\" = data[,length(data)]\n)\n\nn = length(data[,1])\nindicesTrain = sample.int(n, (n+1) %/% 2) # Take (at least) 50 % of the data.\n\ntrain = data[indicesTrain,]\ntest = data[-indicesTrain,]\n\nlabelsTrain = train[,length(train)]\nlabelsTest = test[,length(test)]\nlibrary(e1071)\nlibrary(kknn)\n\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"rectangular\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"linear\")\npredSVM = predict(sm, newdata = test)#> K-nearest-neighbor, standard (rectangular) kernel:\n#>        labelsTest\n#> predKNN  M  R\n#>       M 46 29\n#>       R  8 21\n#> Correctly classified:  67  /  104#> Support-vector machine, linear kernel:\n#>        labelsTest\n#> predSVM  M  R\n#>       M 41 15\n#>       R 13 35\n#> Correctly classified:  76  /  104\n(accKNN = mean(predKNN == labelsTest))\n#> [1] 0.6442308\n(accSVM = mean(predSVM == labelsTest))\n#> [1] 0.7307692\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"optimal\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"radial\")\npredSVM = predict(sm, newdata = test)\n\n(accKNN = mean(predKNN == labelsTest))\n#> [1] 0.75\n(accSVM = mean(predSVM == labelsTest))\n#> [1] 0.8076923\nset.seed(42)\n\ndata = Sonar\ndata = cbind.data.frame(\n  scale(data[,-length(data)]),\n  \"class\" = data[,length(data)]\n)\n\nn = length(data[,1])\nindicesTrain = sample.int(n, (n+1) %/% 2)\n\ntrain = data[indicesTrain,]\ntest = data[-indicesTrain,]\n\nlabelsTrain = train[,length(train)]\nlabelsTest = test[,length(test)]\n\n#####\n\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"rectangular\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"linear\")\npredSVM = predict(sm, newdata = test)\n\n(accKNN = mean(predKNN == labelsTest))\n#> [1] 0.7115385\n(accSVM = mean(predSVM == labelsTest))\n#> [1] 0.75\n\n#####\n\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"optimal\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"radial\")\npredSVM = predict(sm, newdata = test)\n\n(accKNN = mean(predKNN == labelsTest))\n#> [1] 0.8557692\n(accSVM = mean(predSVM == labelsTest))\n#> [1] 0.8365385"},{"path":"workflow.html","id":"workflow","chapter":"6 Machine Learning workflow","heading":"6 Machine Learning workflow","text":"","code":""},{"path":"workflow.html","id":"the-standard-machine-learning-pipeline-at-the-eexample-of-the-titanic-data-set","chapter":"6 Machine Learning workflow","heading":"6.1 The Standard Machine Learning Pipeline at the Eexample of the Titanic Data set","text":"specialize tuning, important understand machine learning always consists pipeline actions.typical machine learning workflow consist :Data cleaning exploration (EDA = explorative data analysis) example tidyverse.Preprocessing feature selection.Splitting data set training test set evaluation.Model fitting.Model evaluation.New predictions.(optional) video explains entire pipeline slightly different perspective:following example, use tidyverse, collection R packages data science / data manipulation mainly developed Hadley Wickham. video explains basics can found :Another good reference “R data science” Hadley Wickham: .lecture need Titanic data set provided us. can find GRIPS (datasets.RData data set submission section) http://rhsbio6.uni-regensburg.de:8500.split data set already training test/prediction data sets (test/prediction split one column less train split, result known priori).","code":""},{"path":"workflow.html","id":"data-cleaning","chapter":"6 Machine Learning workflow","heading":"6.1.1 Data Cleaning","text":"Load necessary libraries:Load data set:Standard summaries:name variable consists 1309 unique factors (1309 observations…):However, title name. Let’s extract titles:extract names split name comma “,”split second split name point “.” extract titles.get 18 unique titles:titles low occurrence rate:combine titles low occurrences one title, can easily forcats package.can count titles see new number titles:Add new title variable data set:second example, explore clean numeric “age” variable.Explore variable:20% NAs!\nEither remove observations NAs, impute (fill) missing values, e.g. median age. However, age might depend variables sex, class title. want fill NAs median age groups.\ntidyverse can easily “group” data, .e. nest observations (: group_by sex, pclass title).\ngrouping, operations (median(age….)) done within specified groups.","code":"\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(tidyverse)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\nlibrary(EcoData)\ndata(titanic_ml)\ndata = titanic_ml\nstr(data)\n#> 'data.frame':    1309 obs. of  14 variables:\n#>  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n#>  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n#>  $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n#>  $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n#>  $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n#>  $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\nsummary(data)\n#>      pclass         survived          name               sex           age         \n#>  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667  \n#>  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000  \n#>  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000  \n#>  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811  \n#>  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000  \n#>  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000  \n#>                  NA's   :655                                      NA's   :263      \n#>      sibsp            parch            ticket          fare        \n#>  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896  \n#>  Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454  \n#>  Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295  \n#>  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275  \n#>  Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329  \n#>                                   (Other) :1261   NA's   :1        \n#>              cabin      embarked      boat          body                      home.dest  \n#>                 :1014    :  2           :823   Min.   :  1.0                       :564  \n#>  C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0   New York, NY        : 64  \n#>  B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0   London              : 14  \n#>  G6             :   5   S:914    15     : 37   Mean   :160.8   Montreal, PQ        : 10  \n#>  B96 B98        :   4            14     : 33   3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n#>  C22 C26        :   4            4      : 31   Max.   :328.0   Paris, France       :  9  \n#>  (Other)        : 271            (Other):308   NA's   :1188    (Other)             :639\nhead(data)\n#>      pclass survived                         name    sex  age sibsp parch\n#> 561       2        1        Sinkkonen, Miss. Anna female 30.0     0     0\n#> 321       1        1            Woolner, Mr. Hugh   male   NA     0     0\n#> 1177      3        0     Sage, Mr. Douglas Bullen   male   NA     8     2\n#> 1098      3        0  Palsson, Master. Paul Folke   male  6.0     3     1\n#> 1252      3        0   Tomlin, Mr. Ernest Portage   male 30.5     0     0\n#> 1170      3        0 Saether, Mr. Simon Sivertsen   male 38.5     0     0\n#>                  ticket   fare cabin embarked boat body                home.dest\n#> 561              250648 13.000              S   10   NA Finland / Washington, DC\n#> 321               19947 35.500   C52        S    D   NA          London, England\n#> 1177           CA. 2343 69.550              S        NA                         \n#> 1098             349909 21.075              S        NA                         \n#> 1252             364499  8.050              S        50                         \n#> 1170 SOTON/O.Q. 3101262  7.250              S        32\nlength(unique(data$name))\n#> [1] 1307\nfirst_split = sapply(data$name,\n                     function(x) stringr::str_split(x, pattern = \",\")[[1]][2])\ntitles = sapply(first_split,\n                function(x) strsplit(x, \".\",fixed = TRUE)[[1]][1])\ntable(titles)\n#> titles\n#>          Capt           Col           Don          Dona            Dr      Jonkheer \n#>             1             4             1             1             8             1 \n#>          Lady         Major        Master          Miss          Mlle           Mme \n#>             1             2            61           260             2             1 \n#>            Mr           Mrs            Ms           Rev           Sir  the Countess \n#>           757           197             2             8             1             1\ntitles = stringr::str_trim((titles))\ntitles %>%\n fct_count()\n#> # A tibble: 18 × 2\n#>    f                n\n#>    <fct>        <int>\n#>  1 Capt             1\n#>  2 Col              4\n#>  3 Don              1\n#>  4 Dona             1\n#>  5 Dr               8\n#>  6 Jonkheer         1\n#>  7 Lady             1\n#>  8 Major            2\n#>  9 Master          61\n#> 10 Miss           260\n#> 11 Mlle             2\n#> 12 Mme              1\n#> 13 Mr             757\n#> 14 Mrs            197\n#> 15 Ms               2\n#> 16 Rev              8\n#> 17 Sir              1\n#> 18 the Countess     1\ntitles2 =\n  forcats::fct_collapse(titles,\n                        officer = c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\"),\n                        royal = c(\"Jonkheer\", \"Don\", \"Sir\",\n                                  \"the Countess\", \"Dona\", \"Lady\"),\n                        miss = c(\"Miss\", \"Mlle\"),\n                        mrs = c(\"Mrs\", \"Mme\", \"Ms\")\n                        )\ntitles2 %>%  \n   fct_count()\n#> # A tibble: 6 × 2\n#>   f           n\n#>   <fct>   <int>\n#> 1 officer    23\n#> 2 royal       6\n#> 3 Master     61\n#> 4 miss      262\n#> 5 mrs       200\n#> 6 Mr        757\ndata =\n  data %>%\n    mutate(title = titles2)\nsummary(data)\n#>      pclass         survived          name               sex           age         \n#>  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667  \n#>  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000  \n#>  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000  \n#>  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811  \n#>  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000  \n#>  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000  \n#>                  NA's   :655                                      NA's   :263      \n#>      sibsp            parch            ticket          fare        \n#>  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896  \n#>  Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454  \n#>  Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295  \n#>  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275  \n#>  Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329  \n#>                                   (Other) :1261   NA's   :1        \n#>              cabin      embarked      boat          body                      home.dest  \n#>                 :1014    :  2           :823   Min.   :  1.0                       :564  \n#>  C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0   New York, NY        : 64  \n#>  B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0   London              : 14  \n#>  G6             :   5   S:914    15     : 37   Mean   :160.8   Montreal, PQ        : 10  \n#>  B96 B98        :   4            14     : 33   3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n#>  C22 C26        :   4            4      : 31   Max.   :328.0   Paris, France       :  9  \n#>  (Other)        : 271            (Other):308   NA's   :1188    (Other)             :639  \n#>      title    \n#>  officer: 23  \n#>  royal  :  6  \n#>  Master : 61  \n#>  miss   :262  \n#>  mrs    :200  \n#>  Mr     :757  \n#> \nsum(is.na(data$age)) / nrow(data)\n#> [1] 0.2009167\ndata =\n  data %>%\n    group_by(sex, pclass, title) %>%\n    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %>%\n    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %>%\n    ungroup()"},{"path":"workflow.html","id":"preprocessing-and-feature-selection","chapter":"6 Machine Learning workflow","heading":"6.1.2 Preprocessing and Feature Selection","text":"want use Keras example, handle factors requires data scaled.Normally, one predictors, show pipeline , sub-selected bunch predictors .\nfirst scale numeric predictors change factors two groups/levels integers (can handled Keras).Factors two levels one hot encoded (Make columns every different factor level write 1 respective column every taken feature value 0 else. example: \\(\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}\\)):add dummy encoded variables data set:","code":"\ndata_sub =\n  data %>%\n    select(survived, sex, age2, fare2, title, pclass) %>%\n    mutate(age2 = scales::rescale(age2, c(0, 1)),\n           fare2 = scales::rescale(fare2, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           title = as.integer(title) - 1L, pclass = as.integer(pclass - 1L))\none_title = k_one_hot(data_sub$title, length(unique(data$title)))$numpy()\ncolnames(one_title) = levels(data$title)\n\none_sex = k_one_hot(data_sub$sex, length(unique(data$sex)))$numpy()\ncolnames(one_sex) = levels(data$sex)\n\none_pclass = k_one_hot(data_sub$pclass,  length(unique(data$pclass)))$numpy()\ncolnames(one_pclass) = paste0(1:length(unique(data$pclass)), \"pclass\")\ndata_sub = cbind(data.frame(survived= data_sub$survived),\n                 one_title, one_sex, age = data_sub$age2,\n                 fare = data_sub$fare2, one_pclass)\nhead(data_sub)\n#>   survived officer royal Master miss mrs Mr female male        age       fare 1pclass\n#> 1        1       0     0      0    1   0  0      1    0 0.37369494 0.02537431       0\n#> 2        1       0     0      0    0   0  1      0    1 0.51774510 0.06929139       1\n#> 3        0       0     0      0    0   0  1      0    1 0.32359053 0.13575256       0\n#> 4        0       0     0      1    0   0  0      0    1 0.07306851 0.04113566       0\n#> 5        0       0     0      0    0   0  1      0    1 0.37995799 0.01571255       0\n#> 6        0       0     0      0    0   0  1      0    1 0.48016680 0.01415106       0\n#>   2pclass 3pclass\n#> 1       1       0\n#> 2       0       0\n#> 3       0       1\n#> 4       0       1\n#> 5       0       1\n#> 6       0       1"},{"path":"workflow.html","id":"split-data","chapter":"6 Machine Learning workflow","heading":"6.1.3 Split Data","text":"splitting consists two splits:outer split (original split, remember got training test split without response “survived”).inner split (split training data set another training test split known response).\ninner split important assess model’s performance potential overfitting.Outer split:Inner split:difference two splits? (Tip: look variable survived.)","code":"\ntrain = data_sub[!is.na(data_sub$survived),]\ntest = data_sub[is.na(data_sub$survived),]\nindices = sample.int(nrow(train), 0.7 * nrow(train))\nsub_train = train[indices,]\nsub_test = train[-indices,]"},{"path":"workflow.html","id":"training","chapter":"6 Machine Learning workflow","heading":"6.1.4 Training","text":"next step fit Keras model training data inner split:\nNote: “nnf_cross_entropy” expects predictions scale linear predictors (loss function apply softmax!).","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, input_shape = ncol(sub_train) - 1L,\n              activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  # Output layer consists of the 1-hot encoded variable \"survived\" -> 2 units.\n  layer_dense(units = 2L, activation = \"softmax\")\n\nsummary(model)\n#> Model: \"sequential\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_3 (Dense)                        (None, 20)                          280           \n#>  dense_2 (Dense)                        (None, 20)                          420           \n#>  dense_1 (Dense)                        (None, 20)                          420           \n#>  dense (Dense)                          (None, 2)                           42            \n#> ==========================================================================================\n#> Total params: 1,162\n#> Trainable params: 1,162\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel_history =\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer = keras::optimizer_adamax(learning_rate = 0.01))\n\nmodel_history =\n  model %>%\n    fit(x = as.matrix(sub_train[,-1]),\n        y = to_categorical(sub_train[,1], num_classes = 2L),\n        epochs = 100L, batch_size = 32L,\n        validation_split = 0.2,   #Again a test set used by the algorithm.\n        shuffle = TRUE)\n\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(sub_train[,-1])[2], out_features = 20L),\n  nn_relu(),\n  nn_linear(20L, 20L),\n  nn_relu(),\n  nn_linear(20L, 2L)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nX_torch = torch_tensor(as.matrix(sub_train[,-1])) \nY_torch = torch_tensor(sub_train[,1]+1, dtype = torch_long())\nfor(i in 1:500){\n  indices = sample.int(nrow(sub_train), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices], reduction = \"mean\")\n  print(loss)\n  loss$backward()\n  opt$step()\n}\n#> torch_tensor\n#> 0.707869\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.686436\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.659611\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.654673\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.627494\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.68805\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.690998\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.652452\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.542723\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.553087\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.664908\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.708627\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.537437\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.43455\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.667144\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.496684\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.570908\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.597058\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.468032\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.649413\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.555161\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.657658\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.587866\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.5941\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.447766\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.604198\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.554323\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.643397\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.536915\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.674608\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.420315\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.32588\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.464963\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.596521\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.530618\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.234835\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.291255\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.929026\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.625346\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.278064\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.726598\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.62555\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.481452\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.673818\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.576146\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.475387\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.457026\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.50311\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.414785\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.385994\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.395334\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.676871\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.393855\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.710248\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.516789\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.530883\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.728959\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.470274\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.51207\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.453122\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.905312\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.4599\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.570826\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.445953\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.468076\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.565009\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.469997\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.600031\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.426152\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.700782\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.352638\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.400375\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.459513\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.568515\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.497609\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.600834\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.798421\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.327998\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.387632\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.63566\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.304985\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.526177\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.592815\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.346981\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.529488\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.623992\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.437405\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.471444\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.481523\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.595865\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.409776\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.408433\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.469692\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.652108\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.457667\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.535109\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.487944\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.588671\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.519266\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.4564\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.420434\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.564046\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.546535\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.655195\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.539748\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.451041\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.488228\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.443963\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.379549\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.592209\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.39709\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.403911\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.286926\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.746202\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.557143\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.343428\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.694007\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.418217\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.512508\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.605517\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.505859\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.57816\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.56861\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.371636\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.584577\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.542919\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.458773\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.673591\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.616572\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.54045\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.482451\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.413941\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.474845\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.419692\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.493641\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.499719\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.47251\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.759404\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.478613\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.446379\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.484713\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.532843\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.320036\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.546959\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.631938\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.367877\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.696314\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.858233\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.694158\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.68525\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.474375\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.547824\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.436111\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.538776\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.505266\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.47963\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.478373\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.517894\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.431919\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.558722\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.459529\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.465366\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.578217\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.603758\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.506026\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.383555\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.419733\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.515685\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.377787\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.251414\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.487529\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.595669\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.421177\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.35105\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.423871\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.474373\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.268141\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.518056\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.31674\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.46961\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.431192\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.329849\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.329375\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.609598\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.52314\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.402313\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.42145\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.634766\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.635958\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.46861\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.277993\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.482526\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.783625\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.481016\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.453809\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.532373\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.614475\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.429902\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.466269\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.52095\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.515713\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.594814\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.620781\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.496087\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.62666\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.574774\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.709346\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.696121\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.466134\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.390352\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.379508\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.518273\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.49647\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.409525\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.46741\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.58821\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.354203\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.54532\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.347564\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.399711\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.689771\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.68454\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.476687\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.44997\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.455688\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.403596\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.388859\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.532802\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.675519\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.496137\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.605388\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.465515\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.41161\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.349016\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.844552\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.45283\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.356554\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.324578\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.501251\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.740512\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.45971\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.4173\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.462109\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.505691\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.461158\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.457387\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.619157\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.540819\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.617918\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.46706\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.355819\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.883221\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.518348\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.44704\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.559336\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.297896\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.412469\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.502253\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.632805\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.374276\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.529211\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.361833\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.480561\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.390684\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.59659\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.2792\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.546276\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.366373\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.431727\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.511097\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.393672\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.510827\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.379485\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.236956\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.453161\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.576676\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.38702\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.511233\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.421102\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.671416\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.449251\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.421628\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.467304\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.471103\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.326304\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.330048\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.355706\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.37242\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.380477\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.445888\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.451437\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.44118\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.545506\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.454936\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.559109\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.554526\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.413272\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.433541\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.35778\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.640296\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.671153\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.271047\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.458804\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.298442\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.422499\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.379167\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.37151\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.29079\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.343601\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.309687\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.640103\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.374724\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.44974\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.349522\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.569582\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.55222\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.75569\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.382098\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.441822\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.608224\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.408617\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.399517\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.3563\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.582843\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.421269\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.458882\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.543267\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.497163\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.482459\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.561153\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.403645\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.417156\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.317258\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.543135\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.518725\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.43794\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.283879\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.341411\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.560608\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.700514\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.652319\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.370861\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.500019\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.701887\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.598595\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.446417\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.415801\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.555131\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.505287\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.663954\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.609121\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.438152\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.531093\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.558249\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.47065\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.476193\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.487094\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.398958\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.370758\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.474866\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.397224\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.429614\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.334949\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.71336\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.414789\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.332019\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.477892\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.317209\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.327091\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.419255\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.273266\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.573711\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.45249\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.346723\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.495111\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.381574\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.244849\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.424235\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.269414\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.607508\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.567444\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.606521\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.357238\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.60519\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.448005\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.650992\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.35432\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.724178\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.449799\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.421666\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.579403\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.945731\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.507651\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.624709\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.477855\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.297618\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.317562\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.578056\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.469425\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.694902\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.502019\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.633915\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.969901\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.416737\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.592856\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.394041\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.392264\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.353905\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.595177\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.359545\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.57412\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.364393\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.584685\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.583028\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.481971\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.383393\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.376592\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.586399\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.413973\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.342848\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.514671\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.406365\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.374424\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.290655\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.425498\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.420754\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.648903\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.859324\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.69948\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.432994\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.366822\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.50001\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.454298\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.475173\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.383777\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.445096\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.415456\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.461693\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.453002\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.519292\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.351406\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.608089\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.560487\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.416949\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.488941\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.562287\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.451058\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.501812\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.530042\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.382543\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.627286\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.569558\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.433292\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.392974\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.483538\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.493791\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.509549\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.519145\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.251333\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.35272\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.542718\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.506634\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.455043\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.526545\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.286775\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.445055\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.462968\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.420297\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.426609\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.22259\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.435372\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.484453\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.410166\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.297978\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.613961\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.479021\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.620836\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.625761\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.464297\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.276816\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.57245\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.380823\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.571787\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.349566\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.430171\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.569651\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.529671\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.428989\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.494857\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.603523\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.595237\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.60492\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.517747\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.492854\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.60878\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.534464\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.49266\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.463699\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.453637\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]\n#> torch_tensor\n#> 0.497914\n#> [ CPUFloatType{} ][ grad_fn = <NllLossBackward0> ]"},{"path":"workflow.html","id":"evaluation","chapter":"6 Machine Learning workflow","heading":"6.1.5 Evaluation","text":"predict variable “survived” test set inner split calculate accuracy:\n","code":"\npred =\n  model %>%\n    predict(x = as.matrix(sub_test[,-1]))\n\npredicted = ifelse(pred[,2] < 0.5, 0, 1) # Ternary operator.\nobserved = sub_test[,1]\n(accuracy = mean(predicted == observed))  # (...): Show output.\n#> [1] 0.8121827\nmodel_torch$eval()\npreds_torch = nnf_softmax(model_torch(torch_tensor(as.matrix(sub_test[,-1]))),\n                          dim = 2L)\npreds_torch = as.matrix(preds_torch)\npreds_torch = apply(preds_torch, 1, which.max)\n(accuracy = mean(preds_torch - 1 == observed))\n#> [1] 0.7817259Now we have to use the softmax function."},{"path":"workflow.html","id":"predictions-and-submission","chapter":"6 Machine Learning workflow","heading":"6.1.6 Predictions and Submission","text":"satisfied performance model inner split, create predictions test data outer split.\n, take observations belong outer test split (use filter function) remove survived (NAs) columns:assess performance test split true survival ratio unknown, however, can now submit predictions submission server http://rhsbio7.uni-regensburg.de:8500.\n, transform survived probabilities actual 0/1 predictions (probabilities allowed) create .csv file:values > 0.5 set 1 values < 0.5 zero.\nsubmission critical change predictions data.frame, select second column (probability survive), save write.csv function:file name used ID submission server, change whatever want long can identify .Annotation: AUC (always) higher probabilities 0/1 data (depending implementation definition AUC). expect upload 0/1 data (usage scenarios, theoretical ones)! Hint cheaters (just forget conversion): upload converted 0/1 data according ifelse(… < 0.5, 0, 1).tasks follow section machine learning pipelines. use titanic_ml data set EcoData package see pipeline. goal predict passenger survives .First, look data feature engineering / selection. Give try! (Ideas can found .)\nBuild neural network make predictions check performance hold-data.\nPlay around model parameters, optimizer(learning_rate = …), epochs = …, number hidden nodes layers: units = …, regularization: kernel_regularizer = …, bias_regularizer = … - Try maximize model’s accuracy hold-data.Hint: lot different activation functions like “linear,” “softmax,” “relu,” “leaky_relu,” “gelu,” “selu,” “elu,” “exponential,” “sigmoid,” “tanh,” “softplus,” “softsign,” etc. careful, might computation-intensive (especially “gelu,” tanh” “softmax”).\nEvery activation function properties, requirements (!), advantages disadvantages. Choose wisely!get accuracy least 90%. (looking solution…) Try better solution.\nNow try solution (exactly one!) another seed check overfitting (seed!) procedure.\nMake predictions submit via submission server.\n","code":"\nsubmit = \n  test %>% \n      select(-survived)\npred = model %>% \n  predict(as.matrix(submit))\nwrite.csv(data.frame(y = ifelse(pred[,2] < 0.5, 0, 1)), file = \"Max_1.csv\")\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(tidyverse)\nlibrary(randomForest)\nlibrary(rpart)\nlibrary(EcoData)\nset_random_seed(54321L, disable_gpu = FALSE)    # Already sets R's random seed.\n\ndata(titanic_ml)\ndata = titanic_ml\nstr(data)\n#> 'data.frame':    1309 obs. of  14 variables:\n#>  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n#>  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n#>  $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n#>  $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n#>  $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n#>  $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\nindicesPredict = which(is.na(titanic_ml$survived))\nindicesTrain = which(!is.na(titanic_ml$survived))\nsubset = sample(length(indicesTrain), length(indicesTrain) * 0.7, replace = F)\nindicesTest = sort(indicesTrain[-subset]) # Mind order of instructions!\nindicesTrain = sort(indicesTrain[subset])\n\n# Determine labels. As they are fixed from now on, samples must not be left!\n# Impute when necessary. \nlabelsTrain = data$survived[indicesTrain]\nlabelsTest = data$survived[indicesTest]\n\n# Parameter \"body\" has nearly no information).\n# Parameter \"name\" was already processed above.\n# Parameter \"ticket\" may include (other) information, but leave it out for now.\n# Parameter \"survived\" is the target variable.\ndata = data %>% select(-body, -name, -ticket, -survived)\n\ndata$pclass = as.factor(data$pclass)  # \"pclass\" makes only sense as a factor.\n\nfor(i in 1:3){\n  print(sum(data$cabin[data$pclass == i] == \"\") / sum(data$pclass == i))\n  }\n#> [1] 0.2074303\n#> [1] 0.9169675\n#> [1] 0.977433\n\n# Most people have no cabin (21% in 1st, 92% in 2nd and 98% in 3rd class).\n# Dummy code the availability of a cabin.\n# This is NOT one-hot encoding! Dummy coded variables use n-1 variables!\ndata$cabin = (data$cabin != \"\") * 1\n\n# Impute values for parameter \"embarked\":\n  # Leave parameters with too many levels or causal inverse ones (assumed).\n  tmp = data %>% select(-boat, -home.dest)\n  tmp = data.frame(tmp[complete.cases(tmp),]) # Leave samples with missing values.\n  \n  missingIndices = which(tmp$embarked == \"\")\n  toPredict = tmp[missingIndices, -8]\n  tmp = tmp[-missingIndices,] # Leave samples that should be predicted.\n  tmp$embarked = droplevels(tmp$embarked) # Remove unused levels (\"\").\n  \n  # Random forests and simple regression trees don't need scaling.\n  # Use a simple regression tree instead of a random forest here,\n  # we have only 2 missing values.\n  # And a simple regression tree is easy to visualize.\n  regressionTree = rpart(embarked ~ ., data = tmp,\n                         control = rpart.control(minsplit = 10))\n  rpart.plot::rpart.plot(regressionTree)\n  prediction = predict(regressionTree, toPredict)\n  \n  for(i in 1:nrow(prediction)){\n    index = which(rownames(data) == as.integer(rownames(prediction))[i])\n    data$embarked[index] = colnames(prediction)[which.max(prediction[i,])]\n  }\n  \n  data$embarked = droplevels(data$embarked) # Remove unused levels (\"\").\n\ntable(data$pclass)\n#> \n#>   1   2   3 \n#> 323 277 709\nsum(table(data$pclass))\n#> [1] 1309\nsum(table(data$home.dest))\n#> [1] 1309\nsum(is.na(data$home.dest))\n#> [1] 0\n# \"pclass\", \"sex\", \"sibsp\", \"parch\", \"boat\" and \"home.dest\" have no missing values.\n\nsummary(data$fare)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   0.000   7.896  14.454  33.295  31.275 512.329       1\n# Parameter \"fare\" has 1 NA entry. Impute the mean.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\n\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Impute values for \"age\":\n  tmp = data %>% select(-home.dest) # Leave parameters with too many levels.\n  \n  missingIndices = which(is.na(tmp$age))\n  toPredict = tmp[missingIndices, -3]\n  tmp = tmp[-missingIndices,] # Leave samples that should be predicted.\n  \n  forest = randomForest(x = tmp[,-3], y = tmp$age)\n  prediction = predict(forest, toPredict)\n  \n  for(i in 1:length(prediction)){\n    index = which(rownames(data) == as.integer(names(prediction))[i])\n    data$age[index] = prediction[i]\n  }\n\nstr(data)\n#> 'data.frame':    1309 obs. of  10 variables:\n#>  $ pclass   : Factor w/ 3 levels \"1\",\"2\",\"3\": 2 1 3 3 3 3 3 1 3 1 ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 36.7 13.9 6 30.5 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : num  0 1 0 0 0 0 0 0 0 0 ...\n#>  $ embarked : Factor w/ 3 levels \"C\",\"Q\",\"S\": 3 3 3 3 3 3 3 1 3 1 ...\n#>  $ boat     : Factor w/ 28 levels \"none\",\"1\",\"10\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ home.dest: Factor w/ 370 levels \"unknown\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\ndata = as.data.frame(data)\n\n# One-hot encoding of \"pclass\", \"sex\", \"cabin\", \"embarked\", \"boat\" and \"home.dest\":\n  for(element in c(\"pclass\", \"sex\", \"cabin\", \"embarked\", \"boat\", \"home.dest\")){\n    # Build integer representation:\n    # This MUST start at 0, otherwise, the encoding is wrong!!\n      integers = as.integer(data[[element]])\n      integers = integers - min(integers)\n    \n    # Determine number of classes:\n    num_classes = length(levels(as.factor(data[[element]])))\n    \n    # Encode:\n    encoded = k_one_hot(integers, num_classes)$numpy()\n    \n    # Copy factor names.\n    colnames(encoded) = paste0(element, \"_\", levels(as.factor(data[[element]])))\n    \n    data = data %>% select(-all_of(element))  # Remove original column.\n    data = cbind.data.frame(data, encoded)  # Plug in new (encoded) columns.\n  }\n\n# Scale parameters (including one-hot encoded):\ndata = scale(as.matrix(data))\n\n# Split into training, test and prediction set:\n# Be careful! You need matrices, no data.frames!\ntrain = as.matrix(data[indicesTrain,])\ntest = as.matrix(data[indicesTest,])\npredict = as.matrix(data[indicesPredict,])#>           labelsTest\n#> prediction   0   1\n#>          0 106  14\n#>          1  15  62\n#> Accuracy:\n#> [1] 0.8527919#>           labelsTest\n#> prediction   0   1\n#>          0 120   5\n#>          1   1  71\n#> Accuracy:\n#> [1] 0.9695431#>           labelsTest\n#> prediction   0   1\n#>          0 112   5\n#>          1   9  71\n#> Accuracy:\n#> [1] 0.928934\nprediction =\n  model %>%\n    predict(x = predict)\n\n# Take label with highest probability:\nprediction = (prediction[,1] < prediction[,2]) * 1\n\nwrite.csv(data.frame(y = prediction), file = \"submission_NN.csv\")"},{"path":"workflow.html","id":"mlr","chapter":"6 Machine Learning workflow","heading":"6.2 Bonus - Machine Learning Pipelines with mlr3","text":"seen today, many machine learning algorithms distributed several packages general machine learning pipeline similar models: feature engineering, feature selection, hyperparameter tuning cross-validation.idea mlr3 framework now provide general machine learning interface can use build reproducible automatic machine learning pipelines. key features mlr3 :common machine learning packages integrated mlr3, can easily switch different machine learning algorithms.common ‘language’/workflow specify machine learning pipelines.Support different cross-validation strategies.Hyperparameter tuning supported machine learning algorithms.Ensemble models.Useful links:mlr3-book (still work)mlr3 websitemlr3 cheatsheet","code":""},{"path":"workflow.html","id":"mlr3---the-basic-workflow","chapter":"6 Machine Learning workflow","heading":"6.2.1 mlr3 - The Basic Workflow","text":"mlr3 package actually consists several packages different tasks (e.g. mlr3tuning hyperparameter tuning, mlr3pipelines data preparation pipes). let’s start basic workflow:Let’s drop time, name ID variable create classification task:Create generic pipeline data transformation (imputation \\(\\rightarrow\\) scaling \\(\\rightarrow\\) encoding categorical variables):can even visualize preprocessing graph:Now, test model (random forest) 10-fold cross-validated, :Specify missing target rows validation ignored.Specify cross-validation, learner (machine learning model want use), measurement (AUC).Run (benchmark) model.cool! Preprocessing + 10-fold cross-validation model evaluation lines code!Let’s create final predictions:now submit predictions .still happy results, let’s hyperparameter tuning!","code":"\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n#> 'data.frame':    4687 obs. of  40 variables:\n#>  $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n#>  $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n#>  $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n#>  $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n#>  $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n#>  $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n#>  $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n#>  $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n#>  $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n#>  $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n#>  $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n#>  $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n#>  $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n#>  $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n#>  $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n#>  $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n#>  $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n#>  $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n#>  $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n#>  $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n#>  $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n#>  $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n#>  $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n#>  $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n#>  $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n#>  $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n#>  $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n#>  $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n#>  $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n#>  $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n#>  $ Orbital.Period              : num  419 1040 682 427 503 ...\n#>  $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n#>  $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n#>  $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n#>  $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n#>  $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n#>  $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n#>  $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n#>  $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\ndata = nasa %>% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n#>                    Hazardous           Absolute.Magnitude                Aphelion.Dist \n#>                         4187                            0                            0 \n#>           Asc.Node.Longitude                 Eccentricity    Epoch.Date.Close.Approach \n#>                            0                            0                            0 \n#>             Epoch.Osculation         Est.Dia.in.Feet.max.         Est.Dia.in.Feet.min. \n#>                            0                            0                            0 \n#>           Est.Dia.in.KM.max.           Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n#>                            0                            0                            0 \n#>            Est.Dia.in.M.min.        Est.Dia.in.Miles.max.        Est.Dia.in.Miles.min. \n#>                            0                            0                            0 \n#>                  Inclination  Jupiter.Tisserand.Invariant                 Mean.Anomaly \n#>                            0                            0                            0 \n#>                  Mean.Motion               Miles.per.hour   Minimum.Orbit.Intersection \n#>                            0                            0                            0 \n#>     Miss.Dist..Astronomical.       Miss.Dist..kilometers.            Miss.Dist..lunar. \n#>                            0                            0                            0 \n#>            Miss.Dist..miles.                     Orbit.ID           Orbit.Uncertainity \n#>                            0                            0                            0 \n#>               Orbital.Period               Perihelion.Arg          Perihelion.Distance \n#>                            0                            0                            0 \n#>              Perihelion.Time  Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec \n#>                            0                            0                            0 \n#>              Semi.Major.Axis                Equinox.J2000             Equinox..MISSING \n#>                            0                            0                            0 \n#>          Orbiting.Body.Earth       Orbiting.Body..MISSING \n#>                            0                            0\npreprocessing$plot()\nset.seed(123)\n\ntransformed_task$data()\n#>       Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity\n#>    1:         0        -0.81322649   -0.38042005       -1.140837452 -0.315605975\n#>    2:         0         0.02110348    0.94306517       -1.380254611  0.744287645\n#>    3:         0         0.68365964    0.10199889        0.044905370 -0.068280074\n#>    4:         1        -0.10159210   -0.38415066        1.606769281 -0.392030729\n#>    5:         1        -0.15067034   -0.29632490        0.151458877 -0.516897963\n#>   ---                                                                           \n#> 4683:      <NA>        -0.32244415    0.69173184       -0.171022906  1.043608082\n#> 4684:      <NA>         0.46280759   -0.24203066       -0.009803808 -0.006429588\n#> 4685:      <NA>         1.51798962   -0.56422744        1.514551982 -1.045386877\n#> 4686:      <NA>         0.16833819    0.14193044       -1.080452287  0.017146757\n#> 4687:      <NA>        -0.05251387   -0.08643345       -0.013006704 -0.579210554\n#>       Epoch.Date.Close.Approach Epoch.Osculation Est.Dia.in.Feet.max.\n#>    1:                -4.7929881       0.14026773          0.271417899\n#>    2:                 1.1058704      -0.26325244          0.032130074\n#>    3:                 0.1591740      -7.76281014         -0.012841645\n#>    4:                -0.7630231       0.24229559          0.048493723\n#>    5:                -0.6305034       0.24229559          0.056169717\n#>   ---                                                                \n#> 4683:                 1.3635097       0.24229559          0.089353662\n#> 4684:                 1.3635097       0.05711503         -0.003481174\n#> 4685:                 1.3635097       0.24229559         -0.027260163\n#> 4686:                 1.3635097       0.24229559          0.016872584\n#> 4687:                 1.3635097       0.24229559          0.041493133\n#>       Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max.\n#>    1:          0.313407647        0.300713440        0.256568684       0.271095311\n#>    2:         -0.029173486       -0.020055639        0.057560696       0.031844946\n#>    3:         -0.093558135       -0.080340934        0.020159164      -0.013119734\n#>    4:         -0.005746146        0.001880088        0.071169817       0.048206033\n#>    5:          0.005243343        0.012169879        0.077553695       0.055880826\n#>   ---                                                                             \n#> 4683:          0.052751793        0.056653478        0.105151714       0.089059576\n#> 4684:         -0.080157032       -0.067793075        0.027943967      -0.003760728\n#> 4685:         -0.114200690       -0.099669182        0.008167747      -0.027535994\n#> 4686:         -0.051017172       -0.040508543        0.044871533       0.016589844\n#> 4687:         -0.015768679       -0.007504312        0.065347651       0.041206539\n#>       Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n#>    1:       0.291624502          2.620443e-01           0.258651038   0.5442288\n#>    2:     -12.143577263          4.153888e-02           0.030928225  -0.5925952\n#>    3:      -0.060269734          9.711407e-05         -10.258220292  -0.5164818\n#>    4:       0.015659335          5.661810e-02           0.046501003   0.8225188\n#>    5:       0.025161701          6.369158e-02           0.053806009  -0.6568722\n#>   ---                                                                          \n#> 4683:       0.066241198          9.427082e-02           0.085386142   0.8222493\n#> 4684:      -0.048682099          8.722856e-03          -0.002961897   1.9818623\n#> 4685:      -0.078118891         -1.318965e-02          -0.025591624  -0.5220442\n#> 4686:      -0.023485512          2.747899e-02           0.016408144  -0.5912988\n#> 4687:       0.006993074          5.016700e-02           0.039838758   0.6181969\n#>       Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n#>    1:                   0.3840868  -1.02876096  0.31939530   -0.254130552\n#>    2:                  -0.7801632  -4.55056211 -0.71151122    0.009333354\n#>    3:                  -0.2872777  -4.55056211 -0.34600512   -0.866997591\n#>    4:                   0.3403535   1.02239674  0.28551117    0.039031045\n#>    5:                  -6.2415005   1.13265516  0.03164827   -0.995720084\n#>   ---                                                                    \n#> 4683:                  -0.6412806   0.01560046 -0.51852041    1.403775544\n#> 4684:                   0.1346891   1.08051799  0.17477591    0.970963141\n#> 4685:                   0.4810091   0.89998250  0.36895738   -1.150527134\n#> 4686:                  -0.3061894   0.22720275 -0.35895074   -0.705980518\n#> 4687:                  -0.2665930   0.22740438 -0.31462613   -0.239696213\n#>       Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers.\n#>    1:                -5.45911858               -7.0769260             0.25122963\n#>    2:                 0.07077092               -0.6830928            -1.08492125\n#>    3:                -0.11099960               -0.9035573            -1.40898698\n#>    4:                -5.45911858               -0.7188386            -4.48402327\n#>    5:                -0.02962490               -0.8013948            -1.25881601\n#>   ---                                                                           \n#> 4683:                 0.30711241               -0.2728622            -0.48191427\n#> 4684:                -0.05962478               -0.7879458            -1.23904708\n#> 4685:                -0.10766868               -0.9303542            -1.44837625\n#> 4686:                 0.08529226               -0.7077555            -1.12117355\n#> 4687:                 0.50904764                0.1075071             0.07719897\n#>       Miss.Dist..lunar. Miss.Dist..miles.   Orbit.ID Orbit.Uncertainity Orbital.Period\n#>    1:         0.2398625        0.23810770 -9.6514722         -1.0070872     -0.3013135\n#>    2:        -1.1742128       -1.18860632 -0.2412680          1.3770116      0.7811097\n#>    3:        -4.7878719       -1.53463694 -0.1803606          0.7809869      0.1566040\n#>    4:        -1.2298206       -1.24471124 -0.1803606         -1.0070872     -0.2866969\n#>    5:        -1.3582490       -1.37428752  1.0225620         -1.0070872     -0.1552813\n#>   ---                                                                                 \n#> 4683:        -0.5360384       -0.54472804 -0.1194531         -0.7090748      0.3873214\n#> 4684:        -1.3373272       -1.35317867 -0.3021755          1.3770116     -0.2345610\n#> 4685:        -1.5588644       -1.57669598 -0.3326292          0.7809869     -0.3216884\n#> 4686:        -1.2125793       -1.22731578 -0.1042262          0.7809869      0.1712806\n#> 4687:         0.0556823        0.05228143 -0.2717218          0.4829746      0.1224733\n#>       Perihelion.Arg Perihelion.Distance Perihelion.Time Relative.Velocity.km.per.hr\n#>    1:   -1.170536399         -0.01831583      0.10526107                 -0.28167821\n#>    2:    1.549452700          0.20604472     -0.28203779                 -0.00604459\n#>    3:    1.470307933          0.61816146      0.20313227                 -0.92285430\n#>    4:    0.769006449          0.09005898     -7.86832915                  0.02502487\n#>    5:    0.006829799          0.52730977      0.26755741                 -1.05752264\n#>   ---                                                                               \n#> 4683:   -0.580282684         -0.65810123      0.03734532                  1.45280854\n#> 4684:    0.839430173         -0.18350549      0.09156633                  1.00000402\n#> 4685:   -1.168210857          0.62646993      0.27629790                 -1.21948041\n#> 4686:    0.824836889          0.52899080      0.37994517                 -0.75439966\n#> 4687:    0.016358127          1.22720096      0.37399573                 -0.26657713\n#>       Relative.Velocity.km.per.sec Semi.Major.Axis Equinox.J2000 Equinox..MISSING\n#>    1:                 -0.284140684      -0.2791037             1                0\n#>    2:                 -0.008343348      -7.3370940             1                0\n#>    3:                 -0.925697621       0.2204883             0                1\n#>    4:                  0.022744569      -0.2617714             1                0\n#>    5:                 -1.060445948      -0.1106954             1                0\n#>   ---                                                                            \n#> 4683:                  1.451376301       0.4468886             1                0\n#> 4684:                  0.998302826      -0.2008499             1                0\n#> 4685:                 -1.222499918      -0.3034586             1                0\n#> 4686:                 -0.757142920       0.2353030             1                0\n#> 4687:                 -0.269030636       0.1857979             1                0\n#>       Orbiting.Body.Earth Orbiting.Body..MISSING\n#>    1:                   1                      0\n#>    2:                   1                      0\n#>    3:                   1                      0\n#>    4:                   1                      0\n#>    5:                   1                      0\n#>   ---                                           \n#> 4683:                   1                      0\n#> 4684:                   1                      0\n#> 4685:                   1                      0\n#> 4686:                   1                      0\n#> 4687:                   1                      0\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\nresult = mlr3::resample(transformed_task,\n                        rf, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)"},{"path":"workflow.html","id":"mlr3---hyperparameter-tuning","chapter":"6 Machine Learning workflow","heading":"6.2.2 mlr3 - Hyperparameter Tuning","text":"Machine learning algorithms varying number hyperparameters can (!) high impact predictive performance. list hyperparameters:Random ForestmtryMinimal node sizeK-nearest-neighbors classificationKernelNumber neighborsDistance metricBoosted Regression TreenroundsMaximum depthalphaboosteretagammalambdaWith mlr3, can easily extend example hyperparameter tuning within nested cross-validation (tuning inner cross-validation).Print hyperparameter space random forest learner:Define hyperparameter space random forest:set tuning pipeline need:Inner cross-validation resampling object.Tuning criterion (e.g. AUC).Tuning method (e.g. random block search).Tuning terminator (stop tuning? E.g. \\(n\\) iterations).Now can wrap normally 10-fold cross-validated setup done previously:Yeah, able improve performance!Let’s create final predictions:","code":"\nrf$param_set\n#> <ParamSet>\n#>                               id    class lower upper nlevels        default    parents\n#>  1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5           \n#>  2:       always.split.variables ParamUty    NA    NA     Inf <NoDefault[3]>           \n#>  3:                class.weights ParamUty    NA    NA     Inf                          \n#>  4:                      holdout ParamLgl    NA    NA       2          FALSE           \n#>  5:                   importance ParamFct    NA    NA       4 <NoDefault[3]>           \n#>  6:                   keep.inbag ParamLgl    NA    NA       2          FALSE           \n#>  7:                    max.depth ParamInt     0   Inf     Inf                          \n#>  8:                min.node.size ParamInt     1   Inf     Inf                          \n#>  9:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1           \n#> 10:                      minprop ParamDbl  -Inf   Inf     Inf            0.1           \n#> 11:                         mtry ParamInt     1   Inf     Inf <NoDefault[3]>           \n#> 12:                   mtry.ratio ParamDbl     0     1     Inf <NoDefault[3]>           \n#> 13:            num.random.splits ParamInt     1   Inf     Inf              1  splitrule\n#> 14:                  num.threads ParamInt     1   Inf     Inf              1           \n#> 15:                    num.trees ParamInt     1   Inf     Inf            500           \n#> 16:                    oob.error ParamLgl    NA    NA       2           TRUE           \n#> 17:        regularization.factor ParamUty    NA    NA     Inf              1           \n#> 18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE           \n#> 19:                      replace ParamLgl    NA    NA       2           TRUE           \n#> 20:    respect.unordered.factors ParamFct    NA    NA       3         ignore           \n#> 21:              sample.fraction ParamDbl     0     1     Inf <NoDefault[3]>           \n#> 22:                  save.memory ParamLgl    NA    NA       2          FALSE           \n#> 23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE importance\n#> 24:                    se.method ParamFct    NA    NA       2        infjack           \n#> 25:                         seed ParamInt  -Inf   Inf     Inf                          \n#> 26:         split.select.weights ParamUty    NA    NA     Inf                          \n#> 27:                    splitrule ParamFct    NA    NA       3           gini           \n#> 28:                      verbose ParamLgl    NA    NA       2           TRUE           \n#> 29:                 write.forest ParamLgl    NA    NA       2           TRUE           \n#>                               id    class lower upper nlevels        default    parents\n#>     value\n#>  1:      \n#>  2:      \n#>  3:      \n#>  4:      \n#>  5:      \n#>  6:      \n#>  7:      \n#>  8:      \n#>  9:      \n#> 10:      \n#> 11:      \n#> 12:      \n#> 13:      \n#> 14:     1\n#> 15:      \n#> 16:      \n#> 17:      \n#> 18:      \n#> 19:      \n#> 20:      \n#> 21:      \n#> 22:      \n#> 23:      \n#> 24:      \n#> 25:      \n#> 26:      \n#> 27:      \n#> 28:      \n#> 29:      \n#>     value\nlibrary(paradox)\n\nrf_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamInt$new(\"min.node.size\", lower = 1, upper = 30L),\n           paradox::ParamInt$new(\"mtry\", lower = 1, upper = 30L),\n           paradox::ParamLgl$new(\"regularization.usedepth\", default = TRUE)))\nprint(rf_pars)\n#> <ParamSet>\n#>                         id    class lower upper nlevels        default value\n#> 1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n#> 2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n#> 3: regularization.usedepth ParamLgl    NA    NA       2           TRUE\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = rf, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = rf_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n#> <AutoTuner:classif.ranger.tuned>\n#> * Model: -\n#> * Search Space:\n#> <ParamSet>\n#>                         id    class lower upper nlevels        default value\n#> 1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n#> 2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n#> 3: regularization.usedepth ParamLgl    NA    NA       2           TRUE      \n#> * Packages: mlr3, mlr3tuning, mlr3learners, ranger\n#> * Predict Type: prob\n#> * Feature Types: logical, integer, numeric, character, factor, ordered\n#> * Properties: hotstart_backward, importance, multiclass, oob_error, twoclass,\n#>   weights\nset.seed(123)\n\nouter3 = mlr3::rsmp(\"cv\", folds = 3L)\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = outer3, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)"},{"path":"workflow.html","id":"mlr3---hyperparameter-tuning-with-oversampling","chapter":"6 Machine Learning workflow","heading":"6.2.3 mlr3 - Hyperparameter Tuning with Oversampling","text":"Let’s go one step back, maybe noticed classes unbalanced:Many machine learning algorithms problems unbalanced data imbalance strong cheaper algorithm focus one class (e.g. predicting 0s 1s). need keep mind machine learning algorithms greedy main focus minimize loss function.techniques correct imbalance:Oversampling (oversample undersampled class).Undersampling (undersample oversampled class).SMOTE Synthetic Minority -sampling Technique (briefly, use k-nearest-neighbors classification create new samples around undersampled class)., use oversampling can extending random forest learner:learner now new feature space:can also tune oversampling rate!5 iterations hyperspace much…Let’s create final predictions:reading chapter mlr package, try transfer titanic data set (use titanic_ml data set, already NAs values predict).\nAlternatively, can also use data sets challenge (e.g. plant-pollinator data set, see data set chapter ??).\n","code":"\ntable(data$Hazardous)\n#> \n#>   0   1 \n#> 412  88\nset.seed(123)\n\nrf_over = po(\"classbalancing\", id = \"over\", adjust = \"minor\") %>>% rf\n\n# However rf_over is now a \"graph\",\n# but we can easily transform it back into a learner:\nrf_over_learner = GraphLearner$new(rf_over)\nprint(rf_over_learner)\n#> <GraphLearner:over.classif.ranger>\n#> * Model: -\n#> * Parameters: over.ratio=1, over.reference=all, over.adjust=minor,\n#>   over.shuffle=TRUE, classif.ranger.num.threads=1\n#> * Packages: mlr3, mlr3pipelines, mlr3learners, ranger\n#> * Predict Type: prob\n#> * Feature types: logical, integer, numeric, character, factor, ordered, POSIXct\n#> * Properties: featureless, hotstart_backward, hotstart_forward, importance,\n#>   loglik, missings, multiclass, oob_error, selected_features, twoclass, weights\nrf_over_learner$param_set\n#> <ParamSetCollection>\n#>                                              id    class lower upper nlevels\n#>  1:                        classif.ranger.alpha ParamDbl  -Inf   Inf     Inf\n#>  2:       classif.ranger.always.split.variables ParamUty    NA    NA     Inf\n#>  3:                classif.ranger.class.weights ParamUty    NA    NA     Inf\n#>  4:                      classif.ranger.holdout ParamLgl    NA    NA       2\n#>  5:                   classif.ranger.importance ParamFct    NA    NA       4\n#>  6:                   classif.ranger.keep.inbag ParamLgl    NA    NA       2\n#>  7:                    classif.ranger.max.depth ParamInt     0   Inf     Inf\n#>  8:                classif.ranger.min.node.size ParamInt     1   Inf     Inf\n#>  9:                     classif.ranger.min.prop ParamDbl  -Inf   Inf     Inf\n#> 10:                      classif.ranger.minprop ParamDbl  -Inf   Inf     Inf\n#> 11:                         classif.ranger.mtry ParamInt     1   Inf     Inf\n#> 12:                   classif.ranger.mtry.ratio ParamDbl     0     1     Inf\n#> 13:            classif.ranger.num.random.splits ParamInt     1   Inf     Inf\n#> 14:                  classif.ranger.num.threads ParamInt     1   Inf     Inf\n#> 15:                    classif.ranger.num.trees ParamInt     1   Inf     Inf\n#> 16:                    classif.ranger.oob.error ParamLgl    NA    NA       2\n#> 17:        classif.ranger.regularization.factor ParamUty    NA    NA     Inf\n#> 18:      classif.ranger.regularization.usedepth ParamLgl    NA    NA       2\n#> 19:                      classif.ranger.replace ParamLgl    NA    NA       2\n#> 20:    classif.ranger.respect.unordered.factors ParamFct    NA    NA       3\n#> 21:              classif.ranger.sample.fraction ParamDbl     0     1     Inf\n#> 22:                  classif.ranger.save.memory ParamLgl    NA    NA       2\n#> 23: classif.ranger.scale.permutation.importance ParamLgl    NA    NA       2\n#> 24:                    classif.ranger.se.method ParamFct    NA    NA       2\n#> 25:                         classif.ranger.seed ParamInt  -Inf   Inf     Inf\n#> 26:         classif.ranger.split.select.weights ParamUty    NA    NA     Inf\n#> 27:                    classif.ranger.splitrule ParamFct    NA    NA       3\n#> 28:                      classif.ranger.verbose ParamLgl    NA    NA       2\n#> 29:                 classif.ranger.write.forest ParamLgl    NA    NA       2\n#> 30:                                 over.adjust ParamFct    NA    NA       7\n#> 31:                                  over.ratio ParamDbl     0   Inf     Inf\n#> 32:                              over.reference ParamFct    NA    NA       6\n#> 33:                                over.shuffle ParamLgl    NA    NA       2\n#>                                              id    class lower upper nlevels\n#>            default                   parents value\n#>  1:            0.5                                \n#>  2: <NoDefault[3]>                                \n#>  3:                                               \n#>  4:          FALSE                                \n#>  5: <NoDefault[3]>                                \n#>  6:          FALSE                                \n#>  7:                                               \n#>  8:                                               \n#>  9:            0.1                                \n#> 10:            0.1                                \n#> 11: <NoDefault[3]>                                \n#> 12: <NoDefault[3]>                                \n#> 13:              1  classif.ranger.splitrule      \n#> 14:              1                               1\n#> 15:            500                                \n#> 16:           TRUE                                \n#> 17:              1                                \n#> 18:          FALSE                                \n#> 19:           TRUE                                \n#> 20:         ignore                                \n#> 21: <NoDefault[3]>                                \n#> 22:          FALSE                                \n#> 23:          FALSE classif.ranger.importance      \n#> 24:        infjack                                \n#> 25:                                               \n#> 26:                                               \n#> 27:           gini                                \n#> 28:           TRUE                                \n#> 29:           TRUE                                \n#> 30: <NoDefault[3]>                           minor\n#> 31: <NoDefault[3]>                               1\n#> 32: <NoDefault[3]>                             all\n#> 33: <NoDefault[3]>                            TRUE\n#>            default                   parents value\nset.seed(123)\n\nrf_pars_over = \n    paradox::ParamSet$new(\n      list(paradox::ParamInt$new(\"over.ratio\", lower = 1, upper = 7L),\n           paradox::ParamInt$new(\"classif.ranger.min.node.size\",\n                                 lower = 1, upper = 30L),\n           paradox::ParamInt$new(\"classif.ranger.mtry\", lower = 1,\n                                 upper = 30L),\n           paradox::ParamLgl$new(\"classif.ranger.regularization.usedepth\",\n                                 default = TRUE)))\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\n\nlearner_tuner_over = AutoTuner$new(learner = rf_over_learner, \n                                   measure = measurement, \n                                   tuner = tuner, \n                                   terminator = terminator,\n                                   search_space = rf_pars_over,\n                                   resampling = inner3)\nprint(learner_tuner)\n#> <AutoTuner:classif.ranger.tuned>\n#> * Model: -\n#> * Search Space:\n#> <ParamSet>\n#>                         id    class lower upper nlevels        default value\n#> 1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n#> 2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n#> 3: regularization.usedepth ParamLgl    NA    NA       2           TRUE      \n#> * Packages: mlr3, mlr3tuning, mlr3learners, ranger\n#> * Predict Type: prob\n#> * Feature Types: logical, integer, numeric, character, factor, ordered\n#> * Properties: hotstart_backward, importance, multiclass, oob_error, twoclass,\n#>   weights\nset.seed(123)\n\nouter3 = mlr3::rsmp(\"cv\", folds = 3L)\nresult = mlr3::resample(transformed_task, learner_tuner_over,\n                        resampling = outer3, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\nset.seed(123)\n\ndata(titanic_ml)\nstr(titanic_ml)\n#> 'data.frame':    1309 obs. of  14 variables:\n#>  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n#>  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n#>  $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n#>  $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n#>  $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n#>  $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\ndata = titanic_ml %>% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n#>  survived       age      boat     cabin  embarked      fare home.dest     parch    pclass \n#>       655       263         0         0         0         0         0         0         0 \n#>       sex     sibsp \n#>         0         0\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n\n# result = mlr3::resample(transformed_task, rf,\n#                         resampling = cv10, store_models = TRUE)\n# \n# # Calculate the average AUC of the holdouts.\n# result$aggregate(measurement)\n# \n# pred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\n# row_ids = (1:nrow(data))[is.na(data$survived)])$data$prob[, \"1\", drop = FALSE])\n# \n# dim(pred)\n# predictions = round(apply(pred, 1, mean))\n# \n# write.csv(data.frame(y = predictions), file = \"submission_RF.csv\")"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
