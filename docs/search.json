[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"","code":""},{"path":"index.html","id":"r-system","chapter":"1 Prerequisites","heading":"1.1 R System","text":"Make sure recent version R (>=3.6, ideally >=4.0) RStudio computers.","code":""},{"path":"index.html","id":"tensorflow-and-keras","chapter":"1 Prerequisites","heading":"1.2 TensorFlow and Keras","text":"want run code computers, also need install TensorFlow / Keras R. , following work people.Run R:work computers, particular software recent. Sometimes, however, things don’t work well, especially python distribution often makes problems. installation work , can look together. Also, provide virtual machines case computers / laptops old don’t manage install TensorFlow.Warning: need least TensorFlow version 2.6, otherwise, argument “learning_rate” must “lr!”","code":"\ninstall.packages(\"keras\", dependencies = T)\nkeras::install_keras()"},{"path":"index.html","id":"torch-for-r","chapter":"1 Prerequisites","heading":"1.3 Torch for R","text":"may also use Torch R. R frontend popular PyTorch framework. install Torch, type R:","code":"\ninstall.packages(\"torch\")\nlibrary(torch)\ntorch::install_torch()"},{"path":"index.html","id":"ecodata","chapter":"1 Prerequisites","heading":"1.4 EcoData","text":"may sometimes use data sets EcoData package. install package, run:default installation install number packages useful statistics. Especially Linux, may take time install. hurry want data, can also run","code":"\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = TRUE, build_vignettes = TRUE)\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = FALSE, build_vignettes = FALSE)"},{"path":"index.html","id":"further-used-libraries","chapter":"1 Prerequisites","heading":"1.5 Further Used Libraries","text":"make huge use different libraries. take coffee two (take …) install following libraries.\nPlease given order unless know ’re , dependencies packages.","code":"\ninstall.packages(\"abind\")\ninstall.packages(\"animation\")\ninstall.packages(\"ape\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"Rgraphviz\", \"graph\", \"RBGL\"))\ninstall.packages(\"coro\")\ninstall.packages(\"dbscan\")\ninstall.packages(\"dendextend\")\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"e1071\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"fields\")\ninstall.packages(\"forcats\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"gym\")\ninstall.packages(\"kknn\")\ninstall.packages(\"knitr\")\ninstall.packages(\"iml\")\ninstall.packages(\"lavaan\")\ninstall.packages(\"lmtest\")\ninstall.packages(\"magick\")\ninstall.packages(\"mclust\")\ninstall.packages(\"Metrics\")\ninstall.packages(\"microbenchmark\")\ninstall.packages(\"missRanger\")\ninstall.packages(\"mlbench\")\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\ninstall.packages(\"mlr3measures\")\ninstall.packages(\"mlr3pipelines\")\ninstall.packages(\"mlr3tuning\")\ninstall.packages(\"paradox\")\ninstall.packages(\"partykit\")\ninstall.packages(\"pcalg\")\ninstall.packages(\"piecewiseSEM\")\ninstall.packages(\"purrr\")\ninstall.packages(\"randomForest\")\ninstall.packages(\"ranger\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"scales\")\ninstall.packages(\"semPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tfprobability\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"xgboost\")\n\ndevtools::install_github(\"andrie/deepviz\", dependencies = TRUE,\n                         upgrade = \"always\")\ndevtools::install_github('skinner927/reprtree')\ndevtools::install_version(\"lavaanPlot\", version = \"0.6.0\")\n\nreticulate::conda_install(\"r-reticulate\", packages = \"scipy\", pip = TRUE)"},{"path":"index.html","id":"linuxunix-systems-have-to-fulfill-some-further-dependencies","chapter":"1 Prerequisites","heading":"1.6 Linux/UNIX systems have to fulfill some further dependencies","text":"Debian based systemsFor Debian based systems, need:new installing packages Debian / Ubuntu, etc., type following:","code":"build-essential\ngfortran\nlibmagick++-dev\nr-base-devsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev"},{"path":"index.html","id":"apple-m1m2-users","chapter":"1 Prerequisites","heading":"1.7 Apple M1/M2 users","text":"using Apple’s new M1/M2 processors may encounter difficulties trying install TensorFlow Torch time. Also, install instructions TensorFlow Torch differ depending version R:Run R-terminal:“x86_64” means using x86_64 intel R version. x86_64, can install torch via ´install.packages(‘torch’)´. can’t install keras tensorflow! , course, either always use torch (worries, provided torch code chunks everywhere alternative), use one virtual machine (can use R via rstudio-server browser)“x86_64” means using x86_64 intel R version. x86_64, can install torch via ´install.packages(‘torch’)´. can’t install keras tensorflow! , course, either always use torch (worries, provided torch code chunks everywhere alternative), use one virtual machine (can use R via rstudio-server browser)“aarch64” means using (native) ARM R version. , difficult possible install Keras TensorFlow, impossible install torch. normally install first R package keras via ´install.packages(‘torch’)´ run ´kras::install_keras()´. install_keras function downloads installs python dependencies background. However using R-ARM version need install python dependencies . ’s guide.“aarch64” means using (native) ARM R version. , difficult possible install Keras TensorFlow, impossible install torch. normally install first R package keras via ´install.packages(‘torch’)´ run ´kras::install_keras()´. install_keras function downloads installs python dependencies background. However using R-ARM version need install python dependencies . ’s guide.summary, difficult M1/M2 users get Keras TensorFlow work. recommend either using x86_64 version (since Torch installation easy) using one virtual machines successfully run Torch TensorFlow/Keras.","code":"\nsessionInfo()$R.version$arch\n#> [1] \"x86_64\""},{"path":"index.html","id":"reminders-about-basic-operations-in-r","chapter":"1 Prerequisites","heading":"1.8 Reminders About Basic Operations in R","text":"Basic advanced knowledge R required successfully participate course. like refresh knowledge R, can review chapter ‘Reminder: R Basics’ advanced statistic course.Authors:Maximilian Pichler: @_Max_PichlerFlorian Hartig: @florianhartigContributors:Johannes Oberpriller, Matthias Meier","code":""},{"path":"introduction.html","id":"introduction","chapter":"2 Introduction to Machine Learning","heading":"2 Introduction to Machine Learning","text":"","code":""},{"path":"introduction.html","id":"principles-of-machine-learning","chapter":"2 Introduction to Machine Learning","heading":"2.1 Principles of Machine Learning","text":"three basic machine learning tasks:Supervised learningUnsupervised learningReinforcement learningIn supervised learning, train algorithms using labeled data, means already know correct answer part data (called training data).Unsupervised learning contrast technique, one need monitor model apply labels. Instead, allow model work discover information.Reinforcement learning technique emulates game-like situation. algorithm finds solution trial error gets either rewards penalties every action. games, goal maximize rewards. talk technique last day course.moment, focus first two tasks, supervised unsupervised learning. , begin small example. start code, video prepare class:","code":""},{"path":"introduction.html","id":"questions","chapter":"2 Introduction to Machine Learning","heading":"2.1.1 Questions","text":"\n      \n        makeMultipleChoiceForm(\n         'look two textbooks ML (Elements statistical learning introduction statistical learning) readings end GRIPS course - following statements true?',\n          'checkbox',\n          [\n            {\n              'answer':'books can downloaded free.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'elements statistical learning published earlier introduction statistical learning.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'\"introduction statistical learning\" also includes online course videos different topics website.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'Higher model complexity always better predicting.',\n              'correct':false,\n              'explanationIfSelected':'! Bias-variance tradeoff!',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n          ],\n          ''\n        );\n      \n      \n        makeMultipleChoiceForm(\n         'following statements bias-variance trade-correct? (see figure )',\n          'checkbox',\n          [\n            {\n              'answer':'goal considering bias-variance trade-get bias model small possible.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'goal considering bias-variance trade-realize increasing complexity typically leads flexibility (allowing reduce bias) cost uncertainty (variance) estimated parameters.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'bias-variance trade-, see model complexity also depends want optimize : bias, variance (rarely), total error model.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n          ],\n          ''\n        );\n      ","code":""},{"path":"introduction.html","id":"unsupervised-learning","chapter":"2 Introduction to Machine Learning","heading":"2.2 Unsupervised Learning","text":"unsupervised learning, want identify patterns data without examples (supervision) correct patterns / classes . example, consider iris data set. , 150 observations 4 floral traits:\nFigure 2.1: Trait distributions iris dataset\nobservations 3 species indeed species tend different traits, meaning observations form 3 clusters.\nFigure 2.2: Scatterplots trait-trait combinations.\nHowever, imagine don’t know species , basically situation people antique . people just noted plants different flowers others, decided give different names. kind process unsupervised learning .","code":"\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4]) \nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits, \n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])"},{"path":"introduction.html","id":"hierarchical-clustering","chapter":"2 Introduction to Machine Learning","heading":"2.2.1 Hierarchical Clustering","text":"cluster refers collection data points aggregated together certain similarities.hierarchical clustering, hierarchy (tree) data points built.Agglomerative: Start data point cluster, merge hierarchically.Divisive: Start data points one cluster, split hierarchically.Merges / splits done according linkage criterion, measures distance (potential) clusters. Cut tree certain height get clusters.example\nFigure 2.3: Results hierarchical clustering. Red rectangle drawn around corresponding clusters.\nplot, colors true species identity\nFigure 2.4: Results hierarchical clustering. Colors correspond three species classes.\nCalculate confusion matrix. Note switching labels fits species.Table 2.1: Confusion matrix predicted observed species classes.Note results might change choose different agglomeration method, distance metric scale variables. Compare, e.g. example:\nFigure 2.5: Results hierarchical clustering. Colors correspond three species classes. Different agglomeration method\nTable 2.2: Confusion matrix predicted observed species classes.method best?might conclude ward.D2 works best . However, learn later, optimizing method without hold-testing implies model may overfitting. check using cross-validation.","code":"\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nd = dist(traits)\nhc = hclust(d, method = \"complete\")\n\nplot(hc, main=\"\")\nrect.hclust(hc, k = 3)  # Draw rectangles around the branches.\nlibrary(ape)\n\nplot(as.phylo(hc), \n     tip.color = colors[as.integer(species)], \n     direction = \"downwards\")\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntmp = hcRes3\ntmp[hcRes3 == 2] = 3\ntmp[hcRes3 == 3] = 2\nhcRes3 = tmp\ntable(hcRes3, species)\nhc = hclust(d, method = \"ward.D2\")\n\nplot(as.phylo(hc), \n     tip.color = colors[as.integer(species)], \n     direction = \"downwards\")\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntable(hcRes3, species)\nlibrary(dendextend)\nset.seed(123)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n             \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\nout = dendlist()   # Create a dendlist object from several dendrograms.\nfor(method in methods){\n  res = hclust(d, method = method)   \n  out = dendlist(out, as.dendrogram(res))\n}\nnames(out) = methods\nprint(out)\n#> $ward.D\n#> 'dendrogram' with 2 branches and 150 members total, at height 199.6205 \n#> \n#> $single\n#> 'dendrogram' with 2 branches and 150 members total, at height 1.640122 \n#> \n#> $complete\n#> 'dendrogram' with 2 branches and 150 members total, at height 7.085196 \n#> \n#> $average\n#> 'dendrogram' with 2 branches and 150 members total, at height 4.062683 \n#> \n#> $mcquitty\n#> 'dendrogram' with 2 branches and 150 members total, at height 4.497283 \n#> \n#> $median\n#> 'dendrogram' with 2 branches and 150 members total, at height 2.82744 \n#> \n#> $centroid\n#> 'dendrogram' with 2 branches and 150 members total, at height 2.994307 \n#> \n#> $ward.D2\n#> 'dendrogram' with 2 branches and 150 members total, at height 32.44761 \n#> \n#> attr(,\"class\")\n#> [1] \"dendlist\"\n\nget_ordered_3_clusters = function(dend){\n  # order.dendrogram function returns the order (index)\n  # or the \"label\" attribute for the leaves.\n  # cutree: Cut the tree (dendrogram) into groups of data.\n  cutree(dend, k = 3)[order.dendrogram(dend)]\n}\ndend_3_clusters = lapply(out, get_ordered_3_clusters)\n\n# Calculate Fowlkes-Mallows Index (determine the similarity between clusterings)\ncompare_clusters_to_iris = function(clus){\n  FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)\n}\n\nclusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\ndotchart(sort(clusters_performance), xlim = c(0.3, 1),\n         xlab = \"Fowlkes-Mallows index\",\n         main = \"Performance of linkage methods\n         in detecting the 3 species \\n in our example\",\n         pch = 19)"},{"path":"introduction.html","id":"k-means-clustering","chapter":"2 Introduction to Machine Learning","heading":"2.2.2 K-means Clustering","text":"Another example unsupervised learning algorithm k-means clustering, one simplest popular unsupervised machine learning algorithms.start algorithm, first specify number clusters (example number species). cluster centroid, assumed real location representing center cluster (example average plant specific species look like). algorithm starts randomly putting centroids somewhere. Afterwards data point assigned respective cluster raises overall -cluster sum squares (variance) related distance centroid least . algorithm placed data points cluster centroids get updated. iterating procedure assignment doesn’t change longer, algorithm can find (locally) optimal centroids data points belonging cluster.\nNote results might differ according initial positions centroids. Thus several (locally) optimal solutions might found.“k” K-means refers number clusters ‘means’ refers averaging data-points find centroids.typical pipeline using k-means clustering looks algorithms. visualized data, fit model, visualize results look performance use confusion matrix. setting fixed seed, can ensure results reproducible.Visualizing results.\nColor codes true species identity, symbol shows cluster result.see discrepancies. Confusion matrix:want animate clustering process, runElbow technique determine probably best suited number clusters:Often, one interested sparse models. Furthermore, higher k necessary tends overfitting. kink picture, sum squares dropped enough k still low enough.\nkeep mind, rule thumb might wrong special cases.","code":"\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n#> K-means clustering with 3 clusters of sizes 50, 62, 38\n#> \n#> Cluster means:\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 1     5.006000    3.428000     1.462000    0.246000\n#> 2     5.901613    2.748387     4.393548    1.433871\n#> 3     6.850000    3.073684     5.742105    2.071053\n#> \n#> Clustering vector:\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2\n#>  [85] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3\n#> [127] 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2\n#> \n#> Within cluster sum of squares by cluster:\n#> [1] 15.15100 39.82097 23.87947\n#>  (between_SS / total_SS =  88.4 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\ntable(iris$Species, kc$cluster)\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 48  2\n#>   virginica   0 14 36\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE, \n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))"},{"path":"introduction.html","id":"density-based-clustering","chapter":"2 Introduction to Machine Learning","heading":"2.2.3 Density-based Clustering","text":"Determine affinity data point according affinity k nearest neighbors.\ngeneral description many ways .","code":"\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nlibrary(dbscan)\nset.seed(123)\n\nkNNdistplot(traits, k = 4)   # Calculate and plot k-nearest-neighbor distances.\nabline(h = 0.4, lty = 2)\n\ndc = dbscan(traits, eps = 0.4, minPts = 6)\nprint(dc)\n#> DBSCAN clustering for 150 objects.\n#> Parameters: eps = 0.4, minPts = 6\n#> The clustering contains 4 cluster(s) and 32 noise points.\n#> \n#>  0  1  2  3  4 \n#> 32 46 36 14 22 \n#> \n#> Available fields: cluster, eps, minPts\nlibrary(factoextra)\nfviz_cluster(dc, traits, geom = \"point\", ggtheme = theme_light())"},{"path":"introduction.html","id":"model-based-clustering","chapter":"2 Introduction to Machine Learning","heading":"2.2.4 Model-based Clustering","text":"last class methods unsupervised clustering -called model-based clustering methods.Mclust automatically compares number candidate models (clusters, shape) according BIC (BIC criterion classifying algorithms depending prediction quality usage parameters). can look selected model via:see algorithm prefers 2 clusters. better comparability 2 methods, override setting:Result terms predicted densities 3 clustersPredicted clusters:Confusion matrix:","code":"\nlibrary(mclust)\n#> Package 'mclust' version 5.4.10\n#> Type 'citation(\"mclust\")' for citing this R package in publications.\nmb = Mclust(traits)\nmb$G # Two clusters.\n#> [1] 2\nmb$modelName # > Ellipsoidal, equal shape.\n#> [1] \"VEV\"\nmb3 = Mclust(traits, 3)\nplot(mb3, \"density\")\nplot(mb3, what=c(\"classification\"), add = T)\ntable(iris$Species, mb3$classification)"},{"path":"introduction.html","id":"ordination","chapter":"2 Introduction to Machine Learning","heading":"2.2.5 Ordination","text":"Ordination used explorative analysis compared clustering, similar objects ordered together.\nrelationship clustering ordination. PCA ordination iris data set.can cluster results ordination, ordinate clustering, superimpose one .","code":"\npcTraits = prcomp(traits, center = TRUE, scale. = TRUE)\nbiplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25))"},{"path":"introduction.html","id":"exercise","chapter":"2 Introduction to Machine Learning","heading":"2.2.6 Exercise","text":"Go 4(5) algorithms , check sensitive (.e. results change) scale input features (= predictors), instead using raw data. Discuss group: appropriate analysis /general: Scaling scaling?\n  Hierarchical Clustering\nseems scaling harmful hierarchical clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.seems scaling harmful K-means clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.seems scaling harmful density based clustering. might deception.\ncareful: data different units magnitudes, scaling definitely useful! Otherwise variables higher values get higher influence.model based clustering, scaling matter.PCA ordination, scaling matters.\ninterested directions maximal variance, parameters scaled, one highest values might dominate others.","code":"\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)   \n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n  \n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n  \n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n  \n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n  \n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n  \n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n  \n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#>  [85] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> [127] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> Levels: 1 2 3\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n  \n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n  \n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n  \n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n  \n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n#> Not scaled \n#> \n#> Confusion matrix:\n#>        labels\n#> correct noise  1  2  3  4\n#>       1     3 47  0  0  0\n#>       2     5  0 38  3  4\n#>       3    17  0  0 33  0\n#> \n#> Correct classified points:  118  /  150\n#> Sum of noise points:  25\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n#> Scaled \n#> \n#> Confusion matrix:\n#>        labels\n#> correct noise  1  2  3  4\n#>       1     9 41  0  0  0\n#>       2    14  0 36  0  0\n#>       3    36  0  1  4  9\n#> \n#> Correct classified points:  81  /  150\n#> Sum of noise points:  59\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  \n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n  \n  mb3 = Mclust(traits, 3)\n  \n  tbl = table(iris$Species, mb3$classification)\n  \n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n#> Not scaled \n#> \n#> Confusion matrix:\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 45  5\n#>   virginica   0  0 50\n#> \n#> Correct classified points:  145  /  150\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n#> Scaled \n#> \n#> Confusion matrix:\n#>             \n#>               1  2  3\n#>   setosa     50  0  0\n#>   versicolor  0 45  5\n#>   virginica   0  0 50\n#> \n#> Correct classified points:  145  /  150\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")"},{"path":"introduction.html","id":"supervised-learning","chapter":"2 Introduction to Machine Learning","heading":"2.3 Supervised Learning","text":"two prominent branches supervised learning regression classification. Fundamentally, classification predicting label regression predicting quantity. following video explains depth:","code":""},{"path":"introduction.html","id":"regression","chapter":"2 Introduction to Machine Learning","heading":"2.3.1 Regression","text":"random forest (RF) algorithm possibly widely used machine learning algorithm can used regression classification. talk algorithm tomorrow.moment, want go typical workflow supervised regression: First, visualize data. Next, fit model lastly visualize results. use iris data set used . goal now predict Sepal.Length based information variables (including species).Fitting model:Visualization results:understand structure random forest detail, can use package GitHub., one regression trees shown.","code":"\nlibrary(randomForest)\nset.seed(123)\nm1 = randomForest(Sepal.Length ~ ., data = iris)   # ~.: Against all others.\n# str(m1)\n# m1$type\n# predict(m1)\nprint(m1)\n#> \n#> Call:\n#>  randomForest(formula = Sepal.Length ~ ., data = iris) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 1\n#> \n#>           Mean of squared residuals: 0.1364625\n#>                     % Var explained: 79.97\noldpar = par(mfrow = c(1, 2))\nplot(predict(m1), iris$Sepal.Length, xlab = \"Predicted\", ylab = \"Observed\")\nabline(0, 1)\nvarImpPlot(m1)\npar(oldpar)\nreprtree:::plot.getTree(m1, iris)"},{"path":"introduction.html","id":"classification","chapter":"2 Introduction to Machine Learning","heading":"2.3.2 Classification","text":"random forest, can also classification. steps regression tasks, can additionally see well performed looking confusion matrix. row matrix contains instances predicted class column represents instances actual class. Thus diagonals correctly predicted classes -diagonal elements falsely classified elements.Fitting model:Visualizing one fitted models:Visualizing results ecologically:Confusion matrix:","code":"\nset.seed(123)\nlibrary(randomForest)\nm1 = randomForest(Species ~ ., data = iris)\nprint(m1)\n#> \n#> Call:\n#>  randomForest(formula = Species ~ ., data = iris) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 2\n#> \n#>         OOB estimate of  error rate: 4.67%\n#> Confusion matrix:\n#>            setosa versicolor virginica class.error\n#> setosa         50          0         0        0.00\n#> versicolor      0         47         3        0.06\n#> virginica       0          4        46        0.08\n\nvarImpPlot(m1)\noldpar = par(mfrow = c(1, 2))\nreprtree:::plot.getTree(m1, iris)\npar(mfrow = c(1, 2))\nplot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = \"Observed\")\nplot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = \"Predicted\")\npar(oldpar)   #Reset par.\ntable(predict(m1), iris$Species)"},{"path":"introduction.html","id":"questions-1","chapter":"2 Introduction to Machine Learning","heading":"2.3.3 Questions","text":"\n      \n        makeMultipleChoiceForm(\n         'Using random forest iris dataset, parameter important (remember function check ) predict Petal.Width?',\n          'radio',\n          [\n            {\n              'answer':'Species.',\n              'correct':true,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n            {\n              'answer':'Sepal.Width.',\n              'correct':false,\n              'explanationIfSelected':'',\n              'explanationIfNotSelected':'',\n              'explanationGeneral':''\n            },\n          ],\n          ''\n        );\n      ","code":""},{"path":"tensorflowintro.html","id":"tensorflowintro","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3 Introduction to TensorFlow, Keras, and Torch","text":"","code":""},{"path":"tensorflowintro.html","id":"introduction-to-tensorflow","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.1 Introduction to TensorFlow","text":"One commonly used frameworks machine learning TensorFlow. TensorFlow open source linear algebra library focus neural networks, published Google 2015. TensorFlow supports several interesting features, particular automatic differentiation, several gradient optimizers CPU GPU parallelization.advantages nicely explained following video:sum important points video:TensorFlow math library highly optimized neural networks.GPU available, computations can easily run GPU even CPU TensorFlow still fast.“backend” (.e. functions computations) written C++ CUDA (CUDA programming language NVIDIA GPUs).interface (part TensorFlow use) written Python also available R, means, can write code R/Python executed (compiled) C++ backend.operations TensorFlow written C++ highly optimized. don’t worry, don’t use C++ use TensorFlow several bindings languages. TensorFlow officially supports Python API, meanwhile several community carried APIs languages:RGoRustSwiftJavaScriptIn course use TensorFlow https://tensorflow.rstudio.com/ binding, developed published 2017 RStudio team. First, developed R package (reticulate) calling Python R. Actually, using Python TensorFlow module R (later).TensorFlow offers different levels API. implement neural network completely use Keras provided submodule TensorFlow. Keras powerful module building training neural networks. allows us building training neural networks lines codes. Since end 2018, Keras TensorFlow completly interoperable, allowing us utilize best . course, show can use Keras neural networks also can use TensorFlow’s automatic differenation using complex objective functions.Useful links:TensorFlow documentation (Python API, just replace “.” “$.”)Rstudio TensorFlow website","code":""},{"path":"tensorflowintro.html","id":"data-containers","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.1.1 Data Containers","text":"TensorFlow two data containers (structures):constant (tf$constant): Creates constant (immutable) value computation graph.variable (tf$Variable): Creates mutable value computation graph (used parameter/weight models).get started TensorFlow, load library check installation worked.Don’t worry weird messages (appear start session).","code":"\nlibrary(tensorflow)\nlibrary(keras)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n#> [1] TRUE\n\nimmutable = tf$constant(5.0)\n#> Loaded Tensorflow version 2.9.1\nmutable = tf$constant(5.0)"},{"path":"tensorflowintro.html","id":"basic-operations","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.1.2 Basic Operations","text":"now can define variables math :Normal R methods print() provided R package “tensorflow.”TensorFlow library (created RStudio team) built R methods common operations:operators also automatically transform R numbers constant tensors attempting add tensor R number:TensorFlow containers objects, means just simple variables type numeric (class(5)), instead called methods. Methods changing state class (purposes values object).\ninstance, method transform tensor object back R object:","code":"\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n#> tf.Tensor(5.0, shape=(), dtype=float32)\nprint(b)\n#> tf.Tensor(10.0, shape=(), dtype=float32)\nc = tf$add(a, b)\nprint(c)\n#> tf.Tensor(15.0, shape=(), dtype=float32)\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\nk_print_tensor(c) # Comes out of Keras!\n#> tf.Tensor(15.0, shape=(), dtype=float32)\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\nk_print_tensor(a+b)\n#> tf.Tensor(15.0, shape=(), dtype=float32)\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n#> tf.Tensor(20.0, shape=(), dtype=float32)\nclass(d)\n#> [1] \"tensorflow.tensor\"                               \n#> [2] \"tensorflow.python.framework.ops.EagerTensor\"     \n#> [3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n#> [4] \"tensorflow.python.framework.ops.Tensor\"          \n#> [5] \"tensorflow.python.types.internal.NativeObject\"   \n#> [6] \"tensorflow.python.types.core.Tensor\"             \n#> [7] \"python.builtin.object\"\nclass(d$numpy())\n#> [1] \"numeric\""},{"path":"tensorflowintro.html","id":"data-types","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.1.3 Data Types","text":"R uses dynamic typing, means can assign number, character, function whatever variable type automatically inferred.\nlanguages state type explicitly, e.g. C:TensorFlow tries infer type dynamically, must often state explicitly.\nCommon important types:float32 (floating point number 32 bits, “single precision”)float64 (floating point number 64 bits, “double precision”)int8 (integer 8 bits)reason TensorFlow explicit types many GPUs (e.g. NVIDIA GeForces) can handle 32 bit numbers! (need high precision graphical modeling)let us see practice types specifcy :went wrong ? tried divide float32 float64 number, can divide numbers type!can also specify type object providing object e.g. tf$float64.TensorFlow, arguments often require exact/explicit data types:\nTensorFlow often expects integers arguments. R however integer normally saved float.\nThus, use “L” integer tell R interpreter treated integer:Skipping “L” one common errors using R-TensorFlow!","code":"int a = 5;\nfloat a = 5.0;\nchar a = \"a\";\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()"},{"path":"tensorflowintro.html","id":"exercises","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.1.4 Exercises","text":"run TensorFlow R, note can access different mathematical operations TensorFlow via tf$…, e.g. tf$math$… common math operations tf$linalg$… different linear algebra operations.\nTip: type tf$ hit tab key list available options (sometimes directly console).example: get maximum value vector?Rewrite following expressions (g) TensorFlow:\nexercise compares speed R TensorFlow.\nfirst exercise rewrite following function TensorFlow:, provide skeleton TensorFlow function:can compare speed using Microbenchmark package:Try different matrix sizes test matrix compare speed.Tip: look tf.reduce_mean documentation “axis” argument.Compare following different matrix sizes:test = matrix(0.0, 1000L, 500L)testTF = tf$constant(test)Also try following:\nR faster (first time)?R functions used (apply, mean, “-”) also implemented C.\nR functions used (apply, mean, “-”) also implemented C.problem large enough TensorFlow overhead.\nproblem large enough TensorFlow overhead.Google find write following tasks TensorFlow:\nTensorFlow supports automatic differentiation (analytical numerical!).\nLet’s look function \\(f(x) = 5 x^2 + 3\\) derivative \\(f'(x) = 10x\\).\n\\(f'(5)\\) get \\(10\\).Let’s TensorFlow. Define function:want calculate derivative \\(x = 2.0\\):automatic differentiation, forward \\(x\\) function within tf$GradientTape() environment. also tell TensorFlow value “watch”:print gradient:can also calculate second order derivative \\(f''(x) = 10\\):happening ? Think discuss .advanced example: Linear regressionIn case first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows normal distribution == error).R following fit linear regression model:Let’s build model TensorFlow.\n, use now variable data container type (remember mutable need type weights (\\(\\boldsymbol{w}\\)) regression model). want model learn weights.input (predictors, independent variables features, \\(\\boldsymbol{X}\\)) observed (response, \\(\\boldsymbol{y}\\)) constant learned/optimized.Discuss code, go code line line try understand .Additional exercise:Play around simulation, increase/decrease number weights, add intercept (also need additional variable model).\n","code":"\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\nmax(y)  # Float!\ntf$math$reduce_max(y) # Float!\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n#> [1] 1\n\n# b)\nmean(x)\n#> [1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n#> [1] 1\n\n# d) \nwhich.min(x)\n#> [1] 100\n\n# e) Tip: Use Google! \norder(x)\n#>   [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83  82  81  80\n#>  [22]  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65  64  63  62  61  60  59\n#>  [43]  58  57  56  55  54  53  52  51  50  49  48  47  46  45  44  43  42  41  40  39  38\n#>  [64]  37  36  35  34  33  32  31  30  29  28  27  26  25  24  23  22  21  20  19  18  17\n#>  [85]  16  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n#>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]\n#>  [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305 10.43705\n#>  [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910 10.42317\n#>  [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496 10.40910\n#>  [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061 10.39482\n#>  [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605 10.38034\n#>  [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127 10.36565\n#>  [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628 10.35073\n#>  [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105 10.33559\n#>  [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559 10.32022\n#> [10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989 10.30461\n#>          [,10]\n#>  [1,] 10.42079\n#>  [2,] 10.40699\n#>  [3,] 10.39299\n#>  [4,] 10.37879\n#>  [5,] 10.36439\n#>  [6,] 10.34977\n#>  [7,] 10.33495\n#>  [8,] 10.31989\n#>  [9,] 10.30461\n#> [10,] 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n#> [1] TRUE\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n#> tf.Tensor(1, shape=(), dtype=int32)\ntf$math$reduce_min(y) # Float!\n#> tf.Tensor(1.0, shape=(), dtype=float32)\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n#> [1] 50.5\nmean(y)\n#> [1] 50.5\ntf$math$reduce_mean(x)  # Integer!\n#> tf.Tensor(50, shape=(), dtype=int32)\ntf$math$reduce_mean(y)  # Float!\n#> tf.Tensor(50.5, shape=(), dtype=float32)\n\n# c)    which.max(x)\ntf$argmax(x)\n#> tf.Tensor(0, shape=(), dtype=int64)\ntf$argmax(y)\n#> tf.Tensor(0, shape=(), dtype=int64)\n\n# d)    which.min(x)\ntf$argmin(x)\n#> tf.Tensor(99, shape=(), dtype=int64)\n\n# e)    order(x)\ntf$argsort(x)\n#> tf.Tensor(\n#> [99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n#>  75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n#>  51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n#>  27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n#>   3  2  1  0], shape=(100), dtype=int32)\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n#> tf.Tensor(\n#> [[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n#>   10.433674  10.103771   9.608109   8.582045 ]\n#>  [11.311237  11.200746  11.076511  10.934624  10.769221  10.570931\n#>   10.323348   9.993557   9.498147   8.473241 ]\n#>  [11.186988  11.076511  10.952296  10.810434  10.645067  10.446828\n#>   10.199324   9.869672   9.374583   8.351139 ]\n#>  [11.045079  10.934624  10.810434  10.668606  10.503284  10.305112\n#>   10.057709   9.728241   9.233569   8.212026 ]\n#>  [10.87965   10.769221  10.645067  10.503284  10.338025  10.139942\n#>    9.892679   9.563459   9.069353   8.0503845]\n#>  [10.68132   10.570931  10.446828  10.305112  10.139942   9.941987\n#>    9.694924   9.366061   8.872768   7.857481 ]\n#>  [10.433674  10.323348  10.199324  10.057709   9.892679   9.694924\n#>    9.448175   9.119869   8.62784    7.6182513]\n#>  [10.103771   9.993557   9.869672   9.728241   9.563459   9.366061\n#>    9.119869   8.79255    8.302762   7.30317  ]\n#>  [ 9.608109   9.498147   9.374583   9.233569   9.069353   8.872768\n#>    8.62784    8.302762   7.818028   6.8405466]\n#>  [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n#>    7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n#> tf.Tensor(True, shape=(), dtype=bool)\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n#> Unit: microseconds\n#>                   expr      min       lq      mean    median       uq      max neval cld\n#>   do_something_R(test)  482.107  522.845  725.1376  645.4105  751.585 7582.203   100  a \n#>  do_something_TF(test) 2284.715 2364.157 2620.3784 2554.1020 2805.385 5020.932   100   b\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n#> Unit: milliseconds\n#>                   expr      min       lq      mean    median        uq      max neval cld\n#>   do_something_R(test) 8.348840 8.900812 11.497930 12.247739 12.888065 26.45649   100   b\n#>  do_something_TF(test) 3.795349 4.488069  5.369817  4.812122  5.261759 13.28069   100  a\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n#> Unit: milliseconds\n#>                                     expr      min        lq     mean   median       uq\n#>  tf$matmul(testTF, tf$transpose(testTF)) 8.858726 13.500299 17.66519 15.58615 18.13052\n#>                         test %*% t(test) 4.630520  7.219799 13.35891 10.27140 13.99789\n#>       max neval cld\n#>  214.0673   100   a\n#>  235.8372   100   a\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n#>      [,1] [,2]       [,3]\n#> [1,]    1  0.0 -0.6666667\n#> [2,]   -1  0.5 -0.1666667\n#> [3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n#> [1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    0    2    0\n#> [3,]    0    0    3\n\n# l)\neigen(A)\n#> eigen() decomposition\n#> $values\n#> [1] 3 2 1\n#> \n#> $vectors\n#>           [,1] [,2]       [,3]\n#> [1,] 0.1400280    0  0.4472136\n#> [2,] 0.9801961    1 -0.8944272\n#> [3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n#> [1] 6\nlibrary(tensorflow)\nlibrary(keras)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n#> tf.Tensor(\n#> [[ 1.          0.         -0.66666667]\n#>  [-1.          0.5        -0.16666667]\n#>  [ 0.          0.          0.33333333]], shape=(3, 3), dtype=float64)\n\n# j)    diag(A)\ntf$linalg$diag_part(A)\n#> tf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n#> tf.Tensor(\n#> [[1. 0. 0.]\n#>  [0. 2. 0.]\n#>  [0. 0. 3.]], shape=(3, 3), dtype=float64)\n\n# l)    eigen(A)\ntf$linalg$eigh(A)\n#> [[1]]\n#> tf.Tensor([-0.56155281  3.          3.56155281], shape=(3), dtype=float64)\n#> \n#> [[2]]\n#> tf.Tensor(\n#> [[-0.78820544  0.         -0.61541221]\n#>  [ 0.61541221  0.         -0.78820544]\n#>  [ 0.          1.         -0.        ]], shape=(3, 3), dtype=float64)\n\n# m)    det(A)\ntf$linalg$det(A)\n#> tf.Tensor(6.0, shape=(), dtype=float64)\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\nx = tf$constant(2.0)\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n(tape$gradient(y, x))\n#> tf.Tensor(20.0, shape=(), dtype=float32)\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n#> tf.Tensor(10.0, shape=(), dtype=float32)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\nsummary(lm(y~x))\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.67893 -0.16399  0.00968  0.15058  0.51099 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.004865   0.027447   0.177     0.86    \n#> x1          2.191511   0.023243  94.287   <2e-16 ***\n#> x2          2.741690   0.025328 108.249   <2e-16 ***\n#> x3          1.179181   0.023644  49.872   <2e-16 ***\n#> x4          0.591873   0.025154  23.530   <2e-16 ***\n#> x5          2.302417   0.022575 101.991   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2645 on 94 degrees of freedom\n#> Multiple R-squared:  0.9974, Adjusted R-squared:  0.9972 \n#> F-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n#> <tf.Variable 'Variable:0' shape=(5, 1) dtype=float64, numpy=\n#> array([[2.19290567],\n#>        [2.74534135],\n#>        [1.1714656 ],\n#>        [0.58811305],\n#>        [2.30174942]])>\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  2.217 2.719 1.165 0.593 2.303\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n  grads = tape$gradient(loss, interceptTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(interceptTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n#> <tf.Variable 'Variable:0' shape=(3, 1) dtype=float64, numpy=\n#> array([[2.46391571],\n#>        [2.45852885],\n#>        [1.00566707]])>\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  2.47 2.465 1.003\nk_print_tensor(interceptTF, message = \"Resulting intercept:\\n\")\n#> <tf.Variable 'Variable:0' shape=(1, 1) dtype=float64, numpy=array([[4.22135068]])>\ncat(\"Original intercept: \", intercept, \"\\n\")\n#> Original intercept:  4.09"},{"path":"tensorflowintro.html","id":"introduction-to-pytorch","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.2 Introduction to PyTorch","text":"PyTorch another famous library deep learning. Like TensorFlow, Torch written C++ API Python. 2020, RStudio team released R-Torch, R-TensorFlow calls Python API background, R-Torch API built directly C++ Torch library!Useful links:PyTorch documentation (Python API, bust just replace “.” “$.”)R-Torch websiteTo get started Torch, load library check installation worked.","code":"\nlibrary(torch)"},{"path":"tensorflowintro.html","id":"data-containers-1","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.2.1 Data Containers","text":"Unlike TensorFlow, Torch doesn’t two data containers mutable immutable variables. variables initialized via torch_tensor function:mark variables mutable (track operations automatic differentiation) set argument ‘requires_grad’ true torch_tensor function:","code":"\na = torch_tensor(1.)\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)"},{"path":"tensorflowintro.html","id":"basic-operations-1","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.2.2 Basic Operations","text":"now can define variables math :R-Torch package provides common R methods (advantage TensorFlow).operators also automatically transform R numbers tensors attempting add tensor R number:TensorFlow, explicitly transform tensors back R:","code":"\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n#> torch_tensor\n#>  5\n#> [ CPUFloatType{1} ]\nprint(b)\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ]\nc = a$add(b)\nprint(c)\n#> torch_tensor\n#>  15\n#> [ CPUFloatType{1} ]\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n#> torch_tensor\n#>  15\n#> [ CPUFloatType{1} ]\nprint(a/b)\n#> torch_tensor\n#>  0.5000\n#> [ CPUFloatType{1} ]\nprint(a*b)\n#> torch_tensor\n#>  50\n#> [ CPUFloatType{1} ]\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ]\nclass(d)\n#> [1] \"torch_tensor\" \"R7\"\nclass(as.numeric(d))\n#> [1] \"numeric\""},{"path":"tensorflowintro.html","id":"data-types-1","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.2.3 Data Types","text":"Similar TensorFlow:’s difference! TensorFlow get error, R-Torch, m automatically casted double (float64). However, still bad practice!course try provide corresponding PyTorch code snippets Keras/TensorFlow examples.","code":"\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b "},{"path":"tensorflowintro.html","id":"exercises-1","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.2.4 Exercises","text":"Rewrite following expressions (g) torch:\nexercise compares speed R torch\nfirst exercise rewrite following function torch:, provide skeleton TensorFlow function:can compare speed using Microbenchmark package:Try different matrix sizes test matrix compare speed.Tip: look torch_mean documentation “dim” argument.Compare following different matrix sizes:test = matrix(0.0, 1000L, 500L)testTorch = torch_tensor(test)Also try following:\nR faster (first time)?R functions used (apply, mean, “-”) also implemented C.\nR functions used (apply, mean, “-”) also implemented C.problem large enough torch overhead.\nproblem large enough torch overhead.Google find write following tasks torch:\nTorch supports automatic differentiation (analytical numerical!).\nLet’s look function \\(f(x) = 5 x^2 + 3\\) derivative \\(f'(x) = 10x\\).\n\\(f'(5)\\) get \\(10\\).Let’s torch Define function:want calculate derivative \\(x = 2.0\\):automatic differentiation, forward \\(x\\) function call $backward() method result:print gradient:can also calculate second order derivative \\(f''(x) = 10\\):happening ? Think discuss .advanced example: Linear regressionIn case first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows normal distribution == error).R following fit linear regression model:Let’s build model TensorFlow.\n, use now variable data container type (remember mutable need type weights (\\(\\boldsymbol{w}\\)) regression model). want model learn weights.input (predictors, independent variables features, \\(\\boldsymbol{X}\\)) observed (response, \\(\\boldsymbol{y}\\)) constant learned/optimized.Discuss code, go code line line try understand .Additional exercise:Play around simulation, increase/decrease number weights, add intercept (also need additional variable model).\n","code":"\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n#> [1] 1\n\n# b)\nmean(x)\n#> [1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n#> [1] 1\n\n# d) \nwhich.min(x)\n#> [1] 100\n\n# e) Tip: Use Google! \norder(x)\n#>   [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83  82  81  80\n#>  [22]  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65  64  63  62  61  60  59\n#>  [43]  58  57  56  55  54  53  52  51  50  49  48  47  46  45  44  43  42  41  40  39  38\n#>  [64]  37  36  35  34  33  32  31  30  29  28  27  26  25  24  23  22  21  20  19  18  17\n#>  [85]  16  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n#>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]\n#>  [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305 10.43705\n#>  [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910 10.42317\n#>  [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496 10.40910\n#>  [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061 10.39482\n#>  [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605 10.38034\n#>  [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127 10.36565\n#>  [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628 10.35073\n#>  [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105 10.33559\n#>  [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559 10.32022\n#> [10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989 10.30461\n#>          [,10]\n#>  [1,] 10.42079\n#>  [2,] 10.40699\n#>  [3,] 10.39299\n#>  [4,] 10.37879\n#>  [5,] 10.36439\n#>  [6,] 10.34977\n#>  [7,] 10.33495\n#>  [8,] 10.31989\n#>  [9,] 10.30461\n#> [10,] 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n#> [1] TRUE\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\ntorch_min(y) # Float!\n#> torch_tensor\n#> 1\n#> [ CPUFloatType{} ]\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n#> [1] 50.5\nmean(y)\n#> [1] 50.5\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n#> torch_tensor\n#> 50.5\n#> [ CPUFloatType{} ]\ntorch_mean(y)  # Float!\n#> torch_tensor\n#> 50.5\n#> [ CPUFloatType{} ]\n\n# c)    which.max(x)\ntorch_argmax(x)\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\ntorch_argmax(y)\n#> torch_tensor\n#> 1\n#> [ CPULongType{} ]\n\n# d)    which.min(x)\ntorch_argmin(x)\n#> torch_tensor\n#> 100\n#> [ CPULongType{} ]\n\n# e)    order(x)\ntorch_argsort(x)\n#> torch_tensor\n#>  100\n#>   99\n#>   98\n#>   97\n#>   96\n#>   95\n#>   94\n#>   93\n#>   92\n#>   91\n#>   90\n#>   89\n#>   88\n#>   87\n#>   86\n#>   85\n#>   84\n#>   83\n#>   82\n#>   81\n#>   80\n#>   79\n#>   78\n#>   77\n#>   76\n#>   75\n#>   74\n#>   73\n#>   72\n#>   71\n#> ... [the output was truncated (use n=-1 to disable)]\n#> [ CPULongType{100} ]\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n#> torch_tensor\n#>  4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n#>  4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n#>  4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n#>  4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n#>  4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n#>  3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n#>  3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n#>  3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n#>  2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n#>  2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n#> [ CPUFloatType{10,10} ]\n\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n#> torch_tensor\n#>  1\n#> [ CPUBoolType{1} ]\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n#> Unit: microseconds\n#>                      expr     min      lq     mean  median       uq      max neval cld\n#>      do_something_R(test) 559.919 601.561 665.1723 635.357 670.9635 2900.346   100   b\n#>  do_something_torch(test) 280.732 311.779 395.8254 360.512 393.6015 3119.541   100  a\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n#> Unit: milliseconds\n#>                      expr      min       lq      mean   median        uq       max neval\n#>      do_something_R(test) 8.360527 8.727116 10.659170 9.053977 12.688436 16.171785   100\n#>  do_something_torch(test) 2.060355 2.347657  2.748587 2.530404  2.712845  8.117902   100\n#>  cld\n#>    b\n#>   a\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n#> Unit: milliseconds\n#>                                    expr      min       lq      mean   median        uq\n#>  torch_matmul(testTorch, testTorch$t()) 1.955332 2.034681  3.339554 2.172294  2.387997\n#>                        test %*% t(test) 5.950190 6.378973 11.445755 6.740855 13.849891\n#>        max neval cld\n#>   14.48899   100  a \n#>  179.41118   100   b\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n#>      [,1] [,2]       [,3]\n#> [1,]    1  0.0 -0.6666667\n#> [2,]   -1  0.5 -0.1666667\n#> [3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n#> [1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    0    2    0\n#> [3,]    0    0    3\n\n# l)\neigen(A)\n#> eigen() decomposition\n#> $values\n#> [1] 3 2 1\n#> \n#> $vectors\n#>           [,1] [,2]       [,3]\n#> [1,] 0.1400280    0  0.4472136\n#> [2,] 0.9801961    1 -0.8944272\n#> [3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n#> [1] 6\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n#> torch_tensor\n#>  1.0000  0.0000 -0.6667\n#> -1.0000  0.5000 -0.1667\n#>  0.0000  0.0000  0.3333\n#> [ CPUFloatType{3,3} ]\n\n# j)    diag(A)\ntorch_diag(A)\n#> torch_tensor\n#>  1\n#>  2\n#>  3\n#> [ CPUFloatType{3} ]\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n#> tf.Tensor(\n#> [[1. 0. 0.]\n#>  [0. 2. 0.]\n#>  [0. 0. 3.]], shape=(3, 3), dtype=float64)\ntorch_diag(A)$diag()\n#> torch_tensor\n#>  1  0  0\n#>  0  2  0\n#>  0  0  3\n#> [ CPUFloatType{3,3} ]\n\n# l)    eigen(A)\nlinalg_eigh(A)\n#> [[1]]\n#> torch_tensor\n#> -0.5616\n#>  3.0000\n#>  3.5616\n#> [ CPUFloatType{3} ]\n#> \n#> [[2]]\n#> torch_tensor\n#> -0.7882  0.0000  0.6154\n#>  0.6154  0.0000  0.7882\n#>  0.0000  1.0000  0.0000\n#> [ CPUFloatType{3,3} ]\n\n# m)    det(A)\nlinalg_det(A)\n#> torch_tensor\n#> 6\n#> [ CPUFloatType{} ]\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ny$backward(retain_graph=TRUE )\nx$grad\n#> torch_tensor\n#>  20\n#> [ CPUFloatType{1} ]\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n#> torch_tensor\n#>  10\n#> [ CPUFloatType{1} ][ grad_fn = <MulBackward0> ]\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\nsummary(lm(y~x))\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.67893 -0.16399  0.00968  0.15058  0.51099 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.004865   0.027447   0.177     0.86    \n#> x1          2.191511   0.023243  94.287   <2e-16 ***\n#> x2          2.741690   0.025328 108.249   <2e-16 ***\n#> x3          1.179181   0.023644  49.872   <2e-16 ***\n#> x4          0.591873   0.025154  23.530   <2e-16 ***\n#> x5          2.302417   0.022575 101.991   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2645 on 94 degrees of freedom\n#> Multiple R-squared:  0.9974, Adjusted R-squared:  0.9972 \n#> F-statistic:  7171 on 5 and 94 DF,  p-value: < 2.2e-16\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n#> [1] \"Loss: 4.4065318107605\"\n#> [1] \"Loss: 2.37925982475281\"\n#> [1] \"Loss: 0.901207089424133\"\n#> [1] \"Loss: 0.403193950653076\"\n#> [1] \"Loss: 0.296265482902527\"\n#> [1] \"Loss: 0.268377900123596\"\n#> [1] \"Loss: 0.232994794845581\"\n#> [1] \"Loss: 0.219554618000984\"\n#> [1] \"Loss: 0.215328559279442\"\n#> [1] \"Loss: 0.213282063603401\"\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n#> Inferred weights:  0.701 3.089 1.801 1.123 3.452\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  0.67 3.085 1.787 1.121 3.455\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n#> [1] \"Loss: 3.51533484458923\"\n#> [1] \"Loss: 1.74870145320892\"\n#> [1] \"Loss: 0.414169400930405\"\n#> [1] \"Loss: 0.518697261810303\"\n#> [1] \"Loss: 0.293963462114334\"\n#> [1] \"Loss: 0.263338685035706\"\n#> [1] \"Loss: 0.258341372013092\"\n#> [1] \"Loss: 0.254723280668259\"\n#> [1] \"Loss: 0.252453774213791\"\n#> [1] \"Loss: 0.25116890668869\"\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n#> Inferred weights:  3.118 -0.349 2.107\ncat(\"Original weights: \", w, \"\\n\")\n#> Original weights:  3.131 -0.353 2.11\n\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n#> Inferred intercept:  2.836\ncat(\"Original intercept: \", intercept, \"\\n\")\n#> Original intercept:  2.832"},{"path":"tensorflowintro.html","id":"kerastorch-framework","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.3 Keras/Torch Framework","text":"seen can use TensorFlow directly R, use knowledge implement neural network TensorFlow directly R. However, can quite cumbersome. simple problems, usually faster use higher-level API helps us implementing machine learning models TensorFlow. common Keras.Keras powerful framework building training neural networks lines codes. Since end 2018, Keras TensorFlow completely interoperable, allowing us utilize best .objective lesson familiarize Keras. installed TensorFlow, Keras can found within TensorFlow: tf.keras. However, RStudio team built R package top tf.keras, convenient use . load Keras package, type","code":"\nlibrary(keras)\n# or library(torch)"},{"path":"tensorflowintro.html","id":"example-workflow-in-keras-torch","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.3.1 Example workflow in Keras / Torch","text":"show Keras works, now build small classifier predict three species iris data set. Load necessary packages data sets:neural networks, beneficial scale predictors (scaling = centering standardization, see ?scale).\nalso split data predictors (X) response (Y = three species).Additionally, Keras/TensorFlow handle factors create contrasts (one-hot encoding).\n, specify number categories. can tricky beginner, programming languages like Python C++, arrays start zero. Thus, specify 3 number classes three species, classes 0,1,2,3. Keep mind.prepared data, now see typical workflow specify model Keras/Torch.1. Initialize sequential model Keras:KerasTorchTorch users can skip step.sequential Keras model higher order type model within Keras consists one input one output model.2. Add hidden layers model (learn hidden layers next days).specifying hidden layers, also specify shape called activation function.\ncan think activation function decision forwarded next neuron (learn later).\nwant know topic even depth, consider watching videos presented section 3.4.shape input number predictors (4) shape output number classes (3).KerasTorchThe Torch syntax similar, give list layers “nn_sequential” function. , specify softmax activation function extra layer:softmax scales potential multidimensional vector interval \\((0, 1]\\) component. sum components equals 1. might useful example handling probabilities. Ensure ther labels start 0! Otherwise softmax function work well!3. Compile model loss function (: cross entropy) optimizer (: Adamax).learn options later, now, worry “learning_rate” (“lr” Torch earlier TensorFlow) argument, cross entropy optimizer.KerasTorchSpecify optimizer parameters trained (case parameters network):4. Fit model 30 iterations (epochs)KerasTorchIn Torch, jump directly training loop, however, write training loop:Get batch data.Predict batch.Ccalculate loss predictions true labels.Backpropagate error.Update weights.Go step 1 repeat.5. Plot training history:KerasTorch6. Create predictions:KerasGet probabilities:plant, want know species got highest probability:Torch7. Calculate Accuracy (often correct):KerasTorch8. Plot predictions, see done good job:see, building neural network easy Keras Torch can already .","code":"\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(torch)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata(iris)\nhead(iris)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\nX = scale(iris[,1:4])\nY = iris[,5]\nY = to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    1    0    0\n#> [3,]    1    0    0\n#> [4,]    1    0    0\n#> [5,]    1    0    0\n#> [6,]    1    0    0\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 3L, activation = \"softmax\") \nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n#> Model: \"sequential\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_3 (Dense)                        (None, 20)                          100           \n#>  dense_2 (Dense)                        (None, 20)                          420           \n#>  dense_1 (Dense)                        (None, 20)                          420           \n#>  dense (Dense)                          (None, 3)                           63            \n#> ==========================================================================================\n#> Total params: 1,003\n#> Trainable params: 1,003\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel_history =\n  model %>%\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\nplot(model_history)\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\npredictions = predict(model, X) # Probabilities for each class.\nhead(predictions) # Quasi-probabilities for each species.\n#>           [,1]       [,2]        [,3]\n#> [1,] 0.9819264 0.01476339 0.003310232\n#> [2,] 0.9563531 0.03986335 0.003783490\n#> [3,] 0.9830711 0.01501246 0.001916326\n#> [4,] 0.9789233 0.01915258 0.001923956\n#> [5,] 0.9871404 0.01057778 0.002281806\n#> [6,] 0.9808626 0.01525488 0.003882431\npreds = apply(predictions, 1, which.max) \nprint(preds)\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 3 3 3 2 3 2 3 2 2 2 2 3 2 3 2 3 3 2 2 2 3 2 2 2 2 3 3 3 3 2 2 2 2 3\n#>  [85] 2 3 3 2 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3\n#> [127] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n#>   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n#>  [43] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3\n#>  [85] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#> [127] 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\nmean(preds == as.integer(iris$Species))\n#> [1] 0.8666667\nmean(preds_torch == as.integer(iris$Species))\n#> [1] 0.98\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\npar(oldpar)   # Reset par."},{"path":"tensorflowintro.html","id":"exercises-2","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.3.2 Exercises","text":"now build regression airquality data set Keras/Torch. want predict variable “Ozone.”Tasks:\n1. Complete steps code chunk model successfully trained!\n2. Try different learning rates neural network sizes (increase/decrease number hidden layers neurons (units) layer). happes?KerasLoad prepare data set:Explore data summary() plot():NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Build sequential Keras model.Build sequential Keras model.Add hidden layers (input output layer already specified, add hidden layers ):Add hidden layers (input output layer already specified, add hidden layers ):use 5L input shape?one output node “linear” activation layer?Compile model.“mean_squared_error” loss?Fit model:Tip: matrices accepted \\(\\boldsymbol{X}\\) \\(\\boldsymbol{y}\\) Keras. R often drops one column matrix vector (change back matrix!)Plot training history.Plot training history.Create predictions.Create predictions.Compare Keras model linear model:Compare Keras model linear model:TorchLoad prepare data set:Explore data summary() plot():NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).NAs data, remove Keras handle NAs.\ndon’t know remove NAs data.frame, use Google (e.g. query: “remove-rows-----nas-missing-values--data-frame”).Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Split data predictors (\\(\\boldsymbol{X}\\)) response (\\(\\boldsymbol{y}\\), Ozone) scale \\(\\boldsymbol{X}\\) matrix.Pass list layer objects sequential network class torch (input output layer already specified, add hidden layers ):Pass list layer objects sequential network class torch (input output layer already specified, add hidden layers ):use 5L input shape?one output node activation layer?Create optimizerWe pass network’s parameters optimizer (different keras?)Fit modelIn torch write trainings loop . Complete trainings loop:Tips:Number training $ steps = Number rows / batchsize * Epochs $Search torch::nnf_… correct loss function (mse…)Make sure X_torch Y_torch data type! (can set dtype via torch_tensor(…, dtype = …))\n_ Check dimension Y_torch, need matrix!Plot training history.Plot training history.Create predictions.Create predictions.Compare Torch model linear model:Compare Torch model linear model:\n1. NAs data, remove Keras Torch handle NAs!2. Split data predictors response scale matrix.Keras3. Build sequential Keras model.4. Add hidden layers (input output layer already specified, add hidden layers ).use 5L input shape, 5 predictors. Analogously, use 1L 1d response.\nwant compression, dilation nonlinear effects, use simple linear layer (equal activation function ). activation functions, look example . wait next days.\nmay also seen previously shown link activation functions detail.5. Compile model.mean_squared_error ordinary least squares approach regression analysis.6. Fit model.7. Plot training history.8. Create predictions.9. Compare Keras model linear model.Torch3. Pass list layer objects sequential network class torch (input output layer already specified, add hidden layers ):use 5L input shape, 5 predictors. Analogously, use 1L 1d response.\nwant compression, dilation nonlinear effects, use simple linear layer (equal activation function ). activation functions, look example . wait next days.\nmay also seen previously shown link activation functions detail.4. Create optimizerWe pass network’s parameters optimizer (different keras?)keras use compile function pass optimizer loss function model whereas torch pass network’s parameters optimizer.5. Fit modelIn torch write trainings loop . Complete trainings loop:6. Plot training history.7. Create predictions.8. Compare Torch model linear model:Look slightly complex model compare loss plot accuracy contrast former.KerasTorchYou see, complex model works better, can learn functional form features response better (necessary).\nkeep overfitting problem mind!Look little change learning rates next 2 models compare loss plot accuracy contrast former.KerasTorchYou can see, higher learning rate yields little bit worse results. optimum jumped .KerasTorchYou can see, lower learning rate, optimum (compared run learning rate 0.05) yet reached (epochs gone ).\nalso , mind overfitting problem. many epochs, things might get worse!next task differs Torch Keras users. Keras users learn inner working training Torch users learn simplify generalize training loop.KerasSimilar Torch, write training loop following. training loop consists several steps:Sample batches X Y dataOpen gradientTape create computational graph (autodiff)Make predictions calculate lossUpdate parameters based gradients loss (go back 1. repeat)TorchKeras Torch use dataloaders generate data batches. Dataloaders objects return batches data infinetly. Keras create dataloader object automatically fit function, Torch write :Define dataset object. object informs dataloader function inputs, outputs, length (nrow), sample .Create instance dataset object calling passing actual data itPass initiated dataset dataloader functionOur dataloader object initiated. initiated object returns list two elements, batch x batch y. initated object stops returning batches dataset completly transversed (worries, don’t ).training also changed now:previous sampling wasn’t ideal, ?Now change code iris data set.\nTip: tf$keras$losses$… can find various loss functions.\nKerasTorchRemarks:Mind different input output layer numbers.loss function increases randomly, different subsets data drawn.\ndownside stochastic gradient descent.positive thing stochastic gradient descent , local valleys hills may left global ones can found instead.","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\nsummary(data)\n#>      Ozone           Solar.R           Wind             Temp           Month      \n#>  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   Min.   :5.000  \n#>  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   1st Qu.:6.000  \n#>  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   Median :7.000  \n#>  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   Mean   :6.993  \n#>  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   3rd Qu.:8.000  \n#>  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   Max.   :9.000  \n#>  NA's   :37       NA's   :7                                                       \n#>       Day      \n#>  Min.   : 1.0  \n#>  1st Qu.: 8.0  \n#>  Median :16.0  \n#>  Mean   :15.8  \n#>  3rd Qu.:23.0  \n#>  Max.   :31.0  \n#> \nplot(data)\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\nprint(rmse_keras)\nlibrary(torch)\n\ndata = airquality\nsummary(data)\n#>      Ozone           Solar.R           Wind             Temp           Month      \n#>  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   Min.   :5.000  \n#>  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   1st Qu.:6.000  \n#>  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   Median :7.000  \n#>  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   Mean   :6.993  \n#>  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   3rd Qu.:8.000  \n#>  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   Max.   :9.000  \n#>  NA's   :37       NA's   :7                                                       \n#>       Day      \n#>  Min.   : 1.0  \n#>  1st Qu.: 8.0  \n#>  Median :16.0  \n#>  Mean   :15.8  \n#>  3rd Qu.:23.0  \n#>  Max.   :31.0  \n#> \nplot(data)\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    ...\n    nn_linear(20L, 1L),\n  )\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n# Calculate number of training steps.\nepochs = ...\nbatch_size = 32\nsteps = ...\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, ...) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = ...\n  Y_batch = ...\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = ...\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\nprint(rmse_torch)\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n#>      Ozone          Solar.R           Wind            Temp           Month      \n#>  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00   Min.   :5.000  \n#>  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00   1st Qu.:6.000  \n#>  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00   Median :7.000  \n#>  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79   Mean   :7.216  \n#>  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50   3rd Qu.:9.000  \n#>  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00   Max.   :9.000  \n#>       Day       \n#>  Min.   : 1.00  \n#>  1st Qu.: 9.00  \n#>  Median :16.00  \n#>  Mean   :15.95  \n#>  3rd Qu.:22.50  \n#>  Max.   :31.00\nx = scale(data[,2:6])\ny = data[,1]\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 20L) %>%\n  layer_dense(units = 1L, activation = \"linear\")\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 173.4729\npred_keras = predict(model, x)\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 9.621961\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n# Calculate number of training steps.\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\nprint(rmse_torch)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 71.67677\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 6.08194\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_torch)\n#> [1] 5.120534\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 22.55181\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 3.476982\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.1)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_torch)\n#> [1] 6.458905\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001))\n\nmodel_history =\n  model %>%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nmodel %>%\n  evaluate(x, y)\n#>     loss \n#> 317.8695\n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_keras)\n#> [1] 12.31473\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n#> [1] 14.78897\nprint(rmse_torch)\n#> [1] 12.48754\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n#> $bX\n#>         Solar.R        Wind        Temp      Month        Day\n#> 87  -1.13877323 -0.37654514  0.44147123 -0.1467431  1.1546835\n#> 117  0.58361881 -1.83815816  0.33653910  0.5319436  1.0398360\n#> 129 -1.01809608  1.56290291  0.65133550  1.2106304 -1.1422676\n#> 121  0.44100036 -2.14734553  1.70065685  0.5319436  1.4992262\n#> 91   0.74817856 -0.71384046  0.54640337 -0.1467431  1.6140738\n#> 137 -1.76410028  0.26993754 -0.71278225  1.2106304 -0.2234871\n#> 21  -1.93963068 -0.06735777 -1.97196786 -1.5041165  0.5804458\n#> 141 -1.73118833  0.10128988 -0.18812157  1.2106304  0.2359031\n#> 78   0.97856221  0.10128988  0.44147123 -0.1467431  0.1210555\n#> 15  -1.31430363  0.91642022 -2.07689999 -1.5041165 -0.1086396\n#> 38  -0.63412333 -0.06735777  0.44147123 -0.8254298 -1.0274200\n#> 49  -1.62148183 -0.20789749 -1.34237505 -0.8254298  0.2359031\n#> 123  0.03508631 -1.02302783  1.70065685  0.5319436  1.7289213\n#> 136  0.58361881 -1.02302783 -0.08318944  1.2106304 -0.3383347\n#> 120  0.19964606 -0.06735777  2.01545325  0.5319436  1.3843787\n#> 114 -1.63245248  1.22560759 -0.60785011  0.5319436  0.6952933\n#> 145 -1.87380678 -0.20789749 -0.71278225  1.2106304  0.6952933\n#> 140  0.43002971  1.08506788 -1.13251078  1.2106304  0.1210555\n#> 64   0.56167751 -0.20789749  0.33653910 -0.1467431 -1.4868103\n#> 118  0.33129386 -0.54519280  0.86119977  0.5319436  1.1546835\n#> 128 -0.98518413 -0.71384046  0.96613190  1.2106304 -1.2571152\n#> 62   0.92370896 -1.64140257  0.65133550 -0.1467431 -1.7165054\n#> 125  0.13382216 -1.36032314  1.49079258  1.2106304 -1.6016578\n#> 4    1.40641756  0.43858520 -1.65717146 -1.5041165 -1.3719627\n#> 79   1.09923936 -1.02302783  0.65133550 -0.1467431  0.2359031\n#> 82  -1.95060133 -0.85438017 -0.39798584 -0.1467431  0.5804458\n#> 149  0.08993956 -0.85438017 -0.81771438  1.2106304  1.1546835\n#> 17   1.34059366  0.57912491 -1.23744292 -1.5041165  0.1210555\n#> 48   1.08826871  3.02451593 -0.60785011 -0.8254298  0.1210555\n#> 130  0.73720791  0.26993754  0.23160696  1.2106304 -1.0274200\n#> 132  0.49585361  0.26993754 -0.29305371  1.2106304 -0.7977249\n#> 30   0.41905906 -1.19167548  0.12667483 -1.5041165  1.6140738\n#> \n#> $bY\n#>  [1]  20 168  32 118  64   9   1  13  35  18  29  20  85  28  76   9  23  18  32  73  47\n#> [22] 135  78  18  61  16  30  34  37  20  21 115\n\n\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n#> Loss:  645.1247 \n#> Loss:  257.9622 \n#> Loss:  257.9248 \n#> Loss:  424.3474 \n#> Loss:  132.1914 \n#> Loss:  201.8619 \n#> Loss:  225.3891 \n#> Loss:  111.7508 \n#> Loss:  343.3166 \n#> Loss:  255.3797 \n#> Loss:  245.1779 \n#> Loss:  227.4517 \n#> Loss:  222.4553 \n#> Loss:  348.0878 \n#> Loss:  365.9766 \n#> Loss:  178.8896 \n#> Loss:  220.2557 \n#> Loss:  344.3786 \n#> Loss:  238.2619 \n#> Loss:  324.3969\nlibrary(torch)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = matrix(data[,1], ncol = 1L)\n\n\ntorch_dataset = torch::dataset(\n    name = \"airquality\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\nmodel_torch = nn_sequential(\n  nn_linear(5L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 1L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 387.950054\n#> Loss at epoch 20: 282.698284\n#> Loss at epoch 30: 257.855061\n#> Loss at epoch 40: 244.420776\n#> Loss at epoch 50: 217.362137\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n#> Loss:  0.002592299 \n#> Loss:  0.0004166169 \n#> Loss:  0.0005878718 \n#> Loss:  0.0001814893 \n#> Loss:  0.0003771982 \n#> Loss:  0.0006105618 \n#> Loss:  0.0003895268 \n#> Loss:  0.0001912034 \n#> Loss:  0.0002297373 \n#> Loss:  0.0001141062 \n#> Loss:  0.0002618438 \n#> Loss:  0.0001288175 \n#> Loss:  5.752431e-05 \n#> Loss:  0.000256366 \n#> Loss:  0.0002148773 \n#> Loss:  0.0001434388 \n#> Loss:  0.0001920019 \n#> Loss:  0.0001954518 \n#> Loss:  7.47276e-05 \n#> Loss:  2.274193e-05 \n#> Loss:  0.000115741 \n#> Loss:  2.059802e-05 \n#> Loss:  7.065996e-05 \n#> Loss:  1.295879e-05 \n#> Loss:  6.738321e-05 \n#> Loss:  2.543455e-05\nlibrary(torch)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = as.integer(iris$Species)\n\n\ntorch_dataset = torch::dataset(\n    name = \"iris\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(4L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 3L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 16.298816\n#> Loss at epoch 20: 8.492697\n#> Loss at epoch 30: 5.744957\n#> Loss at epoch 40: 4.344102\n#> Loss at epoch 50: 3.493564\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)"},{"path":"tensorflowintro.html","id":"basicMath","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.4 Underlying mathematical concepts - optional","text":"yet familiar underlying concepts neural networks want know , suggested read / view following videos / sites. Consider Links videos descriptions parentheses optional bonus.might useful understand concepts depth.(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially animated graphic interesting).)(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially animated graphic interesting).)https://en.wikipedia.org/wiki/Gradient_descent#Descriptionhttps://en.wikipedia.org/wiki/Gradient_descent#DescriptionNeural networks (Backpropagation, etc.).Neural networks (Backpropagation, etc.).Activation functions detail (requires prerequisite).Activation functions detail (requires prerequisite).Videos topic:Gradient descent explained(Stochastic gradient descent explained)(Entropy explained)Short explanation entropy, cross entropy Kullback–Leibler divergenceDeep Learning (chapter 1)neural networks learn - Deep Learning (chapter 2)Backpropagation - Deep Learning (chapter 3)Another video backpropagation (extends previous one) - Deep Learning (chapter 4)","code":""},{"path":"tensorflowintro.html","id":"caveats-of-neural-network-optimization","chapter":"3 Introduction to TensorFlow, Keras, and Torch","heading":"3.4.1 Caveats of neural network optimization","text":"Depending activation functions, might occur network won’t get updated, even high learning rates (called vanishing gradient, especially “sigmoid” functions).\nFurthermore, updates might overshoot (called exploding gradients) activation functions result many zeros (especially “relu,” dying relu).general, first layers network tend learn (much) slowly subsequent ones.","code":""},{"path":"fundamental.html","id":"fundamental","chapter":"4 Common Machine Learning algorithms","heading":"4 Common Machine Learning algorithms","text":"","code":""},{"path":"fundamental.html","id":"machine-learning-principles","chapter":"4 Common Machine Learning algorithms","heading":"4.1 Machine Learning Principles","text":"","code":""},{"path":"fundamental.html","id":"optimization","chapter":"4 Common Machine Learning algorithms","heading":"4.1.1 Optimization","text":"Wikipedia: “optimization problem problem finding best solution feasible solutions.”need “optimization?”Somehow, need tell algorithm learn. called loss function, expresses goal . also need find configurations loss function attains minimum. job optimizer. Thus, optimization consists :loss function (e.g. tell algorithm training step many observations misclassified) guides training machine learning algorithms.loss function (e.g. tell algorithm training step many observations misclassified) guides training machine learning algorithms.optimizer, tries update weights machine learning algorithms way loss function minimized.optimizer, tries update weights machine learning algorithms way loss function minimized.Calculating global optimum analytically non-trivial problem thus bunch diverse optimization algorithms evolved.optimization algorithms inspired biological systems e.g. ants, bees even slimes. optimizers explained following video, look:","code":""},{"path":"fundamental.html","id":"questions-2","chapter":"4 Common Machine Learning algorithms","heading":"4.1.2 Questions","text":"\n      \n        makeMultipleChoiceForm(\n                'lecture, said , training, machine learning parameters optimised get good fit (loss function) training data. following statements loss functions correct?',\n                'checkbox',\n                [\n                    {\n                        'answer':'loss function measures difference (current) machine learning model prediction data.',\n                        'correct':true,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n                    },\n                    {\n                        'answer':'specify simple line machine learning model, loss functions lead line.',\n                        'correct':false,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n                    },\n                    {\n                        'answer':'Cross-Entropy Kullback–Leibler divergence common loss functions.',\n                        'correct':true,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n                    },\n                    {\n                        'answer':'regression, one sensible loss function, mean squared error.',\n                        'correct':false,\n                        'explanationIfSelected':'',\n                        'explanationIfNotSelected':'',\n                        'explanationGeneral':''\n            }\n          ],\n          ''\n        );\n      ","code":""},{"path":"fundamental.html","id":"small-optimization-example","chapter":"4 Common Machine Learning algorithms","heading":"4.1.2.1 Small Optimization Example","text":"easy example optimization can think quadratic function:function easy, can randomly probe identify optimum plotting.minimal value \\(x = 0\\) (honest, can calculate analytically simple case).can also use optimizer optim-function (first argument starting value):opt$par return best values found optimizer, really close zero :)","code":"\nfunc = function(x){ return(x^2) }\nset.seed(123)\n\na = rnorm(100)\nplot(a, func(a))\nset.seed(123)\n\nopt = optim(1.0, func, method = \"Brent\", lower = -100, upper = 100)\nprint(opt$par)\n#> [1] -3.552714e-15"},{"path":"fundamental.html","id":"advanced-optimization-example","chapter":"4 Common Machine Learning algorithms","heading":"4.1.3 Advanced Optimization Example","text":"Optimization also done fitting linear regression model. Thereby, optimize weights (intercept slope). Just using lm(y~x) simple. want hand also better understand optimization works.example take airquality data set. First, sure NAs . split response (Ozone) predictors (Month, Day, Solar.R, Wind, Temp). Additionally beneficial optimizer, different predictors support, thus scale .model want optimize: \\(Ozone = Solar.R \\cdot X1 + Wind \\cdot X2 + Temp \\cdot X3 + Month \\cdot X4 + Day \\cdot X5 + X6\\)assume residuals normally distributed, loss function mean squared error: \\(\\mathrm{mean}(predicted~ozone - true~ozone)^{2}\\)task now find parameters \\(X1,\\dots,X6\\) loss function minimal. Therefore, implement function, takes parameters returns loss.example can sample weights see loss changes weights.can try find optimum bruteforce (means use random set weights see loss function minimal):cases, bruteforce isn’t good approach. might work well parameters, increasing complexity parameters take long time. Furthermore guaranteed finds stable solution continuous data. R optim function helps computing optimum faster.see, found better loss value, far model evaluations. can see number model evaluations viaBy comparing weights optimizer estimated weights lm() function, see self-written code obtains weights lm.look numbers, keep mind optimization algorithms use random numbers thus results might differ run (without setting seed).Let’s optimize linear regression model using tensorflow/torch, autodiff stochastic gradient descent. Gradient descent updates iteraviely weights/variables interest following rule:\\[ w_{new} = w_{old} - \\eta \\nabla f(w_{old})   \\]= learning rate (step size) \\(f(x)\\) loss function.Also look Torch example, use (similar R optim example) LFBGS optimizer.TensorflowTorch","code":"\ndata = airquality[complete.cases(airquality$Ozone) & complete.cases(airquality$Solar.R),]\nX = scale(data[,-1])\nY = data$Ozone\nlinear_regression = function(w){\n  pred = w[1] +  # intercept\n         w[2]*X[,1] + # Solar.R\n         w[3]*X[,2] + # Wind\n         w[4]*X[,3] + # Temp\n         w[5]*X[,4] + # Month\n         w[6]*X[,5] \n  # or X * w[2:6]^T + w[1]\n  # loss  = MSE, we want to find the optimal weights \n  # to minimize the sum of squared residuals.\n  loss = mean((pred - Y)^2)\n  return(loss)\n}\nset.seed(123)\n\nlinear_regression(runif(6))\n#> [1] 2866.355\nset.seed(123)\n\nrandom_search = matrix(runif(6*5000, -10, 10), 5000, 6)\nlosses = apply(random_search, 1, linear_regression)\nplot(losses, type = \"l\")\nmin(losses)\n#> [1] 1602.35\nrandom_search[which.min(losses),]\n#> [1]  9.3257134  5.1024129 -9.6783025  7.6759366 -0.9435499 -1.4808634\nopt = optim(runif(6, -1, 1), linear_regression, method = \"BFGS\")\nopt$par\n#> [1]  42.099071   4.582640 -11.806113  18.066734  -4.479160   2.384711\nopt$value\n#> [1] 411.5579\nopt$counts\n#> function gradient \n#>       22       11\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n#> Loaded Tensorflow version 2.9.1\n\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(ncol(X), 0, 0.01), ncol(X), 1), dtype=\"float32\")\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .05)), 1, 1, dtype=\"float32\") \n\nget_batch = function() {\n  indices = sample.int(nrow(X), 20)\n  return(list(tf$constant(X[indices,], dtype=\"float32\"), \n              tf$constant(as.matrix(Y)[indices,,drop=FALSE], dtype=\"float32\")))\n}\nlearning_rate = tf$constant( 0.5, dtype = \"float32\")\nfor(i in 1:2000){\n  batch = get_batch()\n  xTF = batch[[1]]\n  yTF = batch[[2]]\n  \n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n  grads = tape$gradient(loss, wTF)\n  .n = wTF$assign( wTF - tf$multiply(learning_rate, grads ))\n  grads_inter = tape$gradient(loss, interceptTF)\n  .n = interceptTF$assign( interceptTF - tf$multiply(learning_rate, grads_inter ))\n\n  if(!i%%100){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n}\nlibrary(torch)\n\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(ncol(X), 0, 0.01), ncol(X), 1), \n                      dtype=torch_float32(), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .05), 1, 1), dtype=torch_float32(), \n                              requires_grad = TRUE)\n\nget_batch = function() {\n  indices = sample.int(nrow(X), 40)\n  return(list(torch_tensor(X[indices,], dtype=torch_float32()), \n              torch_tensor(as.matrix(Y)[indices,,drop=FALSE], dtype=torch_float32())))\n}\n\noptim = torch::optim_lbfgs(params = list(wTorch, interceptTorch), lr = 0.1, line_search_fn =  \"strong_wolfe\")\n\ntrain_step = function() {\n  optim$zero_grad()\n  pred = torch_add(interceptTorch, torch_matmul(xTorch, wTorch))\n  loss = torch_sqrt(torch_mean(torch_pow(yTorch - pred, 2)))\n  loss$backward()\n  return(loss)\n}\n\nfor(i in 1:30){\n  batch = get_batch()\n  xTorch = batch[[1]]\n  yTorch = batch[[2]]\n  \n  loss = optim$step(train_step)\n\n  if(!i%%10){ cat(\"Loss: \", as.numeric(loss), \"\\n\")}  # Every 10 times.\n}\n#> Loss:  23.50033 \n#> Loss:  25.81845 \n#> Loss:  21.38388#> LM weights:\n#>  42.0991 4.58262 -11.80607 18.06679 -4.479175 2.384705\n#> Optim weights:\n#> TF weights:\n#>  41.74698 4.607593 -10.34761 18.43785 -4.653901 2.504827\n#> Torch LBFGS weights:\n#>  45.92921 4.999026 -10.84635 24.09501 -8.427817 -4.406448"},{"path":"fundamental.html","id":"regularization","chapter":"4 Common Machine Learning algorithms","heading":"4.1.4 Regularization","text":"Regularization means adding information structure system order solve ill-posed optimization problem prevent overfitting. many ways regularizing machine learning model. important distinction shrinkage estimators estimators based model averaging.Shrikage estimators based idea adding penalty loss function penalizes deviations model parameters particular value (typically 0). way, estimates “shrunk” specified default value. practice, important penalties least absolute shrinkage selection operator; also Lasso LASSO, penalty proportional sum absolute deviations (\\(L1\\) penalty), Tikhonov regularization aka Ridge regression, penalty proportional sum squared distances reference (\\(L2\\) penalty). Thus, loss function optimize given \\[\nloss = fit - \\lambda \\cdot d\n\\]fit refers standard loss function, \\(\\lambda\\) strength regularization, \\(d\\) chosen metric, e.g. \\(L1\\) \\(L2\\):\\[\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n\\]\n\\[\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n\\]\\(\\lambda\\) possibly d typically optimized cross-validation. \\(L1\\) \\(L2\\) can also combined called elastic net (see Zou Hastie (2005)).Model averaging refers entire set techniques, including boosting, bagging averaging techniques. general principle predictions made combining (= averaging) several models. based insight often efficient many simpler models average , one “super model.” reasons complicated, explained detail Dormann et al. (2018).particular important application averaging boosting, idea many weak learners combined model average, resulting strong learner. Another related method bootstrap aggregating, also called bagging. Idea boostrap (use random sampling replacement ) data, average bootstrapped predictions.see techniques work practice, let’s first focus LASSO Ridge regularization weights neural networks. can imagine LASSO Ridge act similar rubber band weights pulls zero data strongly push away zero. leads important weights, supported data, estimated different zero, whereas unimportant model structures reduced (shrunken) zero.LASSO \\(\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)\\) Ridge \\(\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)\\) slightly different properties. best understood express effective prior preference create parameters:can see, LASSO creates strong preference towards exactly zero, falls less strongly towards tails. means parameters tend estimated either exactly zero, , , free Ridge. reason, LASSO often interpreted model selection method.Ridge, hand, certain area around zero relatively indifferent deviations zero, thus rarely leading exactly zero values. However, create stronger shrinkage values deviate significantly zero.can implement linear regression also Keras, specify hidden layers:KerasTorchTensorFlow thus Keras also allow use using LASSO Ridge weights.\nLets see happens put \\(L1\\) (LASSO) regularization weights:KerasOne can clearly see parameters pulled towards zero regularization.TorchIn Torch, specify regularization calculating loss.","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n#> Loaded Tensorflow version 2.9.1\n\ndata = airquality[complete.cases(airquality),]\nX = scale(data[,-1])\nY = data$Ozone\n# L1/L2 on linear model.\n\nmodel = keras_model_sequential()\nmodel %>%\n layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(X)[2]))\nsummary(model)\n#> Model: \"sequential\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense (Dense)                          (None, 1)                           6             \n#> ==========================================================================================\n#> Total params: 6\n#> Trainable params: 6\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel %>%\n compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5),\n         metrics = c(metric_mean_squared_error))\n\nmodel_history =\n model %>%\n fit(x = X, y = Y, epochs = 100L, batch_size = 20L, shuffle = TRUE)\n\nunconstrained = model$get_weights()\nsummary(lm(Y~X))\n#> \n#> Call:\n#> lm(formula = Y ~ X)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> XSolar.R       4.583      2.135   2.147   0.0341 *  \n#> XWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> XTemp         18.067      2.610   6.922 3.66e-10 ***\n#> XMonth        -4.479      2.230  -2.009   0.0471 *  \n#> XDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 1L)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())\nfor(i in 1:500){\n  indices = sample.int(nrow(X), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  loss$sum()$backward()\n  opt$step()\n}\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\nmodel_torch$parameters\n#> $`0.weight`\n#> torch_tensor\n#>   4.1083 -10.1831  18.2815  -4.3478   1.2937\n#> [ CPUFloatType{1,5} ][ requires_grad = TRUE ]\n#> \n#> $`0.bias`\n#> torch_tensor\n#>  42.0958\n#> [ CPUFloatType{1} ][ requires_grad = TRUE ]\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>% # Remind the penalty lambda that is set to 10 here.\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(X)[2]), \n              kernel_regularizer = regularizer_l1(10),\n              bias_regularizer = regularizer_l1(10))\nsummary(model)\n#> Model: \"sequential_1\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_1 (Dense)                        (None, 1)                           6             \n#> ==========================================================================================\n#> Total params: 6\n#> Trainable params: 6\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5),\n          metrics = c(metric_mean_squared_error))\n\nmodel_history =\n  model %>%\n  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)\n\nl1 = model$get_weights()\nsummary(lm(Y~X))\n#> \n#> Call:\n#> lm(formula = Y ~ X)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> XSolar.R       4.583      2.135   2.147   0.0341 *  \n#> XWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> XTemp         18.067      2.610   6.922 3.66e-10 ***\n#> XMonth        -4.479      2.230  -2.009   0.0471 *  \n#> XDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\ncbind(unlist(l1), unlist(unconstrained))\n#>             [,1]       [,2]\n#> [1,]  1.69559026   4.576884\n#> [2,] -8.43734646 -11.771938\n#> [3,] 12.91758442  18.096060\n#> [4,]  0.01775982  -4.407187\n#> [5,]  0.01594419   2.432310\n#> [6,] 33.05084229  42.205029\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 1L)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(matrix(Y, ncol = 1L), dtype = torch_float32())\nfor(i in 1:500){\n  indices = sample.int(nrow(X), 20L)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_mse_loss(pred, Y_torch[indices,,drop = FALSE])\n  \n  # Add L1:\n  for(i in 1:length(model_torch$parameters)){\n    # Remind the penalty lambda that is set to 10 here.\n    loss = loss + model_torch$parameters[[i]]$abs()$sum()*10.0\n  }\n  \n  loss$sum()$backward()\n  opt$step()\n}\ncoef(lm(Y~X))\n#> (Intercept)    XSolar.R       XWind       XTemp      XMonth        XDay \n#>   42.099099    4.582620  -11.806072   18.066786   -4.479175    2.384705\nmodel_torch$parameters\n#> $`0.weight`\n#> torch_tensor\n#>   0.7787  -6.5937  14.6648  -0.0670   0.0475\n#> [ CPUFloatType{1,5} ][ requires_grad = TRUE ]\n#> \n#> $`0.bias`\n#> torch_tensor\n#>  37.2661\n#> [ CPUFloatType{1} ][ requires_grad = TRUE ]"},{"path":"fundamental.html","id":"exerexer","chapter":"4 Common Machine Learning algorithms","heading":"4.1.5 Exercise","text":"high learning rates optima might jumped .low learning rates might land local optima might take long.KerasTorch\nKerasTorchPlay around parameters !","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\niris = datasets::iris\nX = scale(iris[,1:4])\nY = iris[,5]\nY = keras::k_one_hot(as.integer(Y)-1L, 3)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %>%\n  layer_dense(units = 20L, activation = \"relu\", ) %>%\n  layer_dense(units = 20L, activation = \"relu\", ) %>%\n  layer_dense(units = 3L, activation = \"softmax\")\n# Softmax scales to (0, 1]; 3 output nodes for 3 response classes/labels.\n# The labels MUST start at 0!\n\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer_adamax(learning_rate = 0.5),\n          metrics = c(metric_categorical_accuracy)\n  )\n\nmodel_history =\n  model %>%\n  fit(x = X, y = Y, epochs = 30L, batch_size = 20L, shuffle = TRUE)\n\nmodel %>%\n  evaluate(X, Y)\n#>                 loss categorical_accuracy \n#>           0.04419739           0.98666668\n\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\niris = datasets::iris\nX = scale(iris[,1:4])\nYt = iris[,5]\nYt = as.integer(Yt)\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 20L),\n  nn_relu(),\n  nn_linear(in_features = 20L, out_features = 20L, bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 20L, out_features = 20L, bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 20L, out_features = 3L, bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nbatch_size = 20L\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(Yt, dtype = torch_long())\nlosses = rep(NA, 500)\nfor(i in 1:500){\n  indices = sample.int(nrow(X), batch_size)\n  opt$zero_grad()\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  losses[[i]] = as.numeric(loss)\n  loss$sum()$backward()\n  opt$step()\n}\n\nplot(losses, main = \"Torch training history\", xlab = \"Epoch\", ylab = \"Loss\")"},{"path":"fundamental.html","id":"tree-based-machine-learning-algorithms","chapter":"4 Common Machine Learning algorithms","heading":"4.2 Tree-based Machine Learning Algorithms","text":"Famous machine learning algorithms Random Forest Gradient Boosted trees based classification regression trees.\nHint: Tree-based algorithms distance based thus need scaling.want know little bit concepts described following - like decision, classification regression trees well random forests - might watch following videos:Decision trees\n\n\n* Regression trees\n\n\n\n* Random forests\n\n\n\n* Pruning trees\n\n\n\nwatching videos, know different hyperparameters prevent trees / forests something don’t want.","code":""},{"path":"fundamental.html","id":"classification-and-regression-trees","chapter":"4 Common Machine Learning algorithms","heading":"4.2.1 Classification and Regression Trees","text":"Tree-based models general use series -rules generate predictions one decision trees.\nlecture, explore regression classification trees example airquality data set. one important hyperparameter regression trees: “minsplit.”controls depth tree (see help rpart description).controls complexity tree can thus also seen regularization parameter.first prepare visualize data afterwards fit decision tree.Fit visualize one(!) regression tree:Visualize predictions:angular form prediction line typical regression trees weakness .","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\n\ndata = airquality[complete.cases(airquality),]\nrt = rpart(Ozone~., data = data, control = rpart.control(minsplit = 10))\nrpart.plot(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")"},{"path":"fundamental.html","id":"random-forest","chapter":"4 Common Machine Learning algorithms","heading":"4.2.2 Random Forest","text":"overcome weakness, random forest uses ensemble regression/classification trees. Thus, random forest principle nothing else normal regression/classification tree, uses idea “wisdom crowd” : asking many people (regression/classification trees) one can make informed decision (prediction/classification). want buy new phone example also wouldn’t go directly shop, search internet ask friends family.two randomization steps random forest responsible success:Bootstrap samples tree (sample observations replacement data set. phone like everyone experience phone).split, sample subset predictors considered potential splitting criterion (phone like everyone decision criteria).\nAnnotation: building decision tree (random forests consist many decision trees), one splits data point according features. example females males, big small people crowd, con split crowd gender size size gender build decision tree.Applying random forest follows principle methods : visualize data (already done often airquality data set, thus skip ), fit algorithm plot outcomes.Fit random forest visualize predictions:One advantage random forests get importance variables. split tree, improvement split-criterion importance measure attributed splitting variable, accumulated trees forest separately variable. Thus variable importance shows us important variable averaged trees.several important hyperparameters random forest can tune get better results:Similar minsplit parameter regression classification trees, hyperparameter “nodesize” controls complexity \\(\\rightarrow\\) Minimum size terminal nodes tree. Setting number larger causes smaller trees grown (thus take less time). Note default values different classification (1) regression (5).mtry: Number features randomly sampled candidates split.","code":"\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\npred = predict(rf, data)\nplot(Ozone~Temp, data = data)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nrf$importance\n#>         IncNodePurity\n#> Solar.R      17969.59\n#> Wind         31978.36\n#> Temp         34176.71\n#> Month        10753.73\n#> Day          15436.47"},{"path":"fundamental.html","id":"boosted-regression-trees","chapter":"4 Common Machine Learning algorithms","heading":"4.2.3 Boosted Regression Trees","text":"Random forests fit hundreds trees independent . , idea boosted regression tree comes . Maybe learn errors previous weak learners made thus enhance performance algorithm.boosted regression tree (BRT) starts simple regression tree (weak learner) sequentially fits additional trees improve results.\ntwo different strategies :AdaBoost: Wrong classified observations (previous tree) get higher weight therefore next trees focus difficult/missclassified observations.Gradient boosting (state art): sequential model fit residual errors previous model (strongly simplified, actual algorithm complex).can fit boosted regression tree using xgboost, transform data xgb.Dmatrix.parameter “nrounds” controls many sequential trees fit, example 16. predict new data, can limit number trees used prevent overfitting (remember: new tree tries improve predictions previous trees).Let us visualize predictions different numbers trees:also ways control complexity boosted regression tree algorithm:max_depth: Maximum depth tree.shrinkage (tree get weight weight decrease number trees).specified final model, can obtain importance variables like random forests:One important strength xgboost can directly cross-validation (independent boosted regression tree !) specify properties parameter “n-fold”:Annotation: original data set randomly partitioned \\(n\\) equal sized subsamples. time, model trained \\(n - 1\\) subsets (training set) tested left set (test set) judge performance.three-folded cross-validation, actually fit three different boosted regression tree models (xgboost models) \\(\\approx 67\\%\\) data points. Afterwards, judge performance respective holdout. now tells us well model performed.","code":"\nlibrary(xgboost)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\nbrt = xgboost(data_xg, nrounds = 16L)\n#> [1]  train-rmse:39.724624 \n#> [2]  train-rmse:30.225761 \n#> [3]  train-rmse:23.134840 \n#> [4]  train-rmse:17.899179 \n#> [5]  train-rmse:14.097785 \n#> [6]  train-rmse:11.375457 \n#> [7]  train-rmse:9.391276 \n#> [8]  train-rmse:7.889690 \n#> [9]  train-rmse:6.646586 \n#> [10] train-rmse:5.804859 \n#> [11] train-rmse:5.128437 \n#> [12] train-rmse:4.456416 \n#> [13] train-rmse:4.069464 \n#> [14] train-rmse:3.674615 \n#> [15] train-rmse:3.424578 \n#> [16] train-rmse:3.191301\noldpar = par(mfrow = c(2, 2))\nfor(i in 1:4){\n  pred = predict(brt, newdata = data_xg, ntreelimit = i)\n  plot(data$Temp, data$Ozone, main = i)\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n#> [14:25:06] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n#> [14:25:06] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n#> [14:25:06] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n#> [14:25:06] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\npar(oldpar)\nxgboost::xgb.importance(model = brt)\n#>    Feature        Gain     Cover  Frequency\n#> 1:    Temp 0.570071903 0.2958229 0.24836601\n#> 2:    Wind 0.348230710 0.3419576 0.24183007\n#> 3: Solar.R 0.058795542 0.1571072 0.30718954\n#> 4:     Day 0.019529993 0.1779925 0.16993464\n#> 5:   Month 0.003371853 0.0271197 0.03267974\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n#> [1] 17.89918\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\nset.seed(123)\n\nbrt = xgboost(data_xg, nrounds = 5L)\n#> [1]  train-rmse:39.724624 \n#> [2]  train-rmse:30.225761 \n#> [3]  train-rmse:23.134840 \n#> [4]  train-rmse:17.899179 \n#> [5]  train-rmse:14.097785\nbrt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L,\n                         nrounds = 3L, nthreads = 4L)\n#> [1]  train-rmse:39.895106+2.127355   test-rmse:40.685477+5.745327 \n#> [2]  train-rmse:30.367660+1.728788   test-rmse:32.255812+5.572963 \n#> [3]  train-rmse:23.446237+1.366757   test-rmse:27.282435+5.746244\nprint(brt_cv)\n#> ##### xgb.cv 3-folds\n#>  iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n#>     1        39.89511       2.127355       40.68548      5.745327\n#>     2        30.36766       1.728788       32.25581      5.572963\n#>     3        23.44624       1.366757       27.28244      5.746244"},{"path":"fundamental.html","id":"exercises-3","chapter":"4 Common Machine Learning algorithms","heading":"4.2.4 Exercises","text":"use following code snippet see influence mincut trees.Try different mincut parameters see happens.\n(Compare root mean squared error different mincut parameters explain see.\nCompare predictions different mincut parameters explain happens.)\nwrong snippet ?\n\n\n    use following code snippet explore random forest:Try different values nodesize mtry describe predictions depend parameters.\n\n\n    Run code play different parameters xgboost (especially parameters control complexity) describe see!Tip: look boosting.gif.\n\n\n\n    implemented simplified boosted regression tree using R just fun.\nGo code line line try understand . Ask, questions solve.\n","code":"\nlibrary(tree)\nset.seed(123)\n\ndata = airquality\nrt = tree(Ozone~., data = data,\n          control = tree.control(mincut = 1L, nobs = nrow(data)))\n\nplot(rt)\ntext(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n#>         IncNodePurity\n#> Solar.R      17969.59\n#> Wind         31978.36\n#> Temp         34176.71\n#> Month        10753.73\n#> Day          15436.47\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n#> RMSE:  9.507848\n\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\n\nmodel = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                         nrounds = 500L, verbose = 0L)\npred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n               ntreelimit = 10L)\n\nsaveGIF(\n  {\n    for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n      pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                     ntreelimit = i)\n      image(matrix(pred, 100, 100), main = paste0(\"Trees: \", i),\n            axes = FALSE, las = 2)\n      axis(1, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1))\n      axis(2, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n    }\n  },\n  movie.name = \"boosting.gif\", autobrowse = FALSE\n)\nlibrary(tree)\nset.seed(123)\n\ndepth = 1L\n\n#### Simulate Data\nx = runif(1000, -5, 5)\ny = x * sin(x) * 2 + rnorm(1000, 0, cos(x) + 1.8)\ndata = data.frame(x = x, y = y)\nplot(y~x)\n\n#### Helper function for single tree fit.\nget_model = function(x, y){\n  control = tree.control(nobs = length(x), mincut = 20L)\n  model = tree(y~x, data.frame(x = x, y = y), control = control)\n  pred = predict(model, data.frame(x = x, y = y))\n  return(list(model = model, pred = pred))\n}\n\n#### Boost function.\nget_boosting_model = function(depth){\n  pred = NULL\n  m_list = list()\n  for(i in 1:depth){\n    if(i == 1){\n      m = get_model(x, y)\n      pred = m$pred\n    }else{\n      y_res = y - pred\n      m = get_model(x, y_res)\n      pred = pred + m$pred\n    }\n    m_list[[i]] = m$model\n  }\n  model_list <<- m_list  # This writes outside function scope!\n  return(pred)\n}\n\n### Main.\npred = get_boosting_model(10L)[order(data$x)]\n\nlength(model_list)\n#> [1] 10\nplot(model_list[[1]])\n\nplot(y~x)\nlines(x = data$x[order(data$x)], get_boosting_model(1L)[order(data$x)],\n      col = 'red', lwd = 2)\nlines(x = data$x[order(data$x)], get_boosting_model(100L)[order(data$x)],\n      col = 'green', lwd = 2)"},{"path":"fundamental.html","id":"distance-based-algorithms","chapter":"4 Common Machine Learning algorithms","heading":"4.3 Distance-based Algorithms","text":"chapter, introduce support-vector machines (SVMs) distance-based methods\nHint: Distance-based models need scaling!","code":""},{"path":"fundamental.html","id":"k-nearest-neighbor","chapter":"4 Common Machine Learning algorithms","heading":"4.3.1 K-Nearest-Neighbor","text":"K-nearest-neighbor (kNN) simple algorithm stores available cases classifies new data based similarity measure. mostly used classify data point based \\(k\\) nearest neighbors classified.Let us first see example:class decide blue point? classes nearest points? Well, procedure used k-nearest-neighbors classifier thus actually “real” learning k-nearest-neighbors classification.applying k-nearest-neighbors classification, first scale data set, deal distances want influence predictors. Imagine one variable values -10.000 10.000 another -1 1. influence first variable distance points much stronger influence second variable.\niris data set, split data training test set . follow usual pipeline.Fit model create predictions:","code":"\nx = scale(iris[,1:4])\ny = iris[,5]\nplot(x[-100,1], x[-100, 3], col = y)\npoints(x[100,1], x[100, 3], col = \"blue\", pch = 18, cex = 1.3)\ndata = iris\ndata[,1:4] = apply(data[,1:4],2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\nlibrary(kknn)\nset.seed(123)\n\nknn = kknn(Species~., train = train, test = test)\nsummary(knn)\n#> \n#> Call:\n#> kknn(formula = Species ~ ., train = train, test = test)\n#> \n#> Response: \"nominal\"\n#>           fit prob.setosa prob.versicolor prob.virginica\n#> 1      setosa           1      0.00000000      0.0000000\n#> 2      setosa           1      0.00000000      0.0000000\n#> 3      setosa           1      0.00000000      0.0000000\n#> 4      setosa           1      0.00000000      0.0000000\n#> 5      setosa           1      0.00000000      0.0000000\n#> 6      setosa           1      0.00000000      0.0000000\n#> 7      setosa           1      0.00000000      0.0000000\n#> 8      setosa           1      0.00000000      0.0000000\n#> 9      setosa           1      0.00000000      0.0000000\n#> 10     setosa           1      0.00000000      0.0000000\n#> 11     setosa           1      0.00000000      0.0000000\n#> 12     setosa           1      0.00000000      0.0000000\n#> 13     setosa           1      0.00000000      0.0000000\n#> 14 versicolor           0      0.86605852      0.1339415\n#> 15 versicolor           0      0.74027417      0.2597258\n#> 16 versicolor           0      0.91487300      0.0851270\n#> 17 versicolor           0      0.98430840      0.0156916\n#> 18 versicolor           0      0.91487300      0.0851270\n#> 19 versicolor           0      1.00000000      0.0000000\n#> 20 versicolor           0      1.00000000      0.0000000\n#> 21 versicolor           0      1.00000000      0.0000000\n#> 22 versicolor           0      1.00000000      0.0000000\n#> 23 versicolor           0      1.00000000      0.0000000\n#> 24 versicolor           0      1.00000000      0.0000000\n#> 25 versicolor           0      1.00000000      0.0000000\n#> 26 versicolor           0      1.00000000      0.0000000\n#> 27 versicolor           0      0.86605852      0.1339415\n#> 28 versicolor           0      1.00000000      0.0000000\n#> 29  virginica           0      0.00000000      1.0000000\n#> 30  virginica           0      0.00000000      1.0000000\n#> 31  virginica           0      0.00000000      1.0000000\n#> 32  virginica           0      0.00000000      1.0000000\n#> 33  virginica           0      0.08512700      0.9148730\n#> 34  virginica           0      0.22169561      0.7783044\n#> 35  virginica           0      0.00000000      1.0000000\n#> 36  virginica           0      0.23111986      0.7688801\n#> 37 versicolor           0      1.00000000      0.0000000\n#> 38  virginica           0      0.04881448      0.9511855\n#> 39 versicolor           0      0.64309579      0.3569042\n#> 40 versicolor           0      0.67748579      0.3225142\n#> 41  virginica           0      0.17288113      0.8271189\n#> 42  virginica           0      0.00000000      1.0000000\n#> 43  virginica           0      0.00000000      1.0000000\n#> 44  virginica           0      0.00000000      1.0000000\n#> 45  virginica           0      0.35690421      0.6430958\ntable(test$Species, fitted(knn))\n#>             \n#>              setosa versicolor virginica\n#>   setosa         13          0         0\n#>   versicolor      0         15         0\n#>   virginica       0          3        14"},{"path":"fundamental.html","id":"support-vector-machines-svms","chapter":"4 Common Machine Learning algorithms","heading":"4.3.2 Support Vector Machines (SVMs)","text":"Support vectors machines different approach. try divide predictor space sectors class. , support-vector machine fits parameters hyperplane (\\(n-1\\) dimensional subspace \\(n\\)-dimensional space) predictor space optimizing distance hyperplane nearest point class.Fitting support-vector machine:Support-vector machines can work linearly separable problems. (problem called linearly separable exists least one line plane points one class one side hyperplane points others classes side).possible, however, can use called kernel trick, maps predictor space (higher dimensional) space problem linear separable. identified boundaries higher-dimensional space, can project back original dimensions.seen, work every kernel. Hence, problem find actual correct kernel, optimization procedure can thus approximated.","code":"\nlibrary(e1071)\n\ndata = iris\ndata[,1:4] = apply(data[,1:4], 2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\nsm = svm(Species~., data = train, kernel = \"linear\")\npred = predict(sm, newdata = test)\noldpar = par(mfrow = c(1, 2))\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  pred, main = \"predicted\")\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  test$Species, main = \"observed\")\npar(oldpar)\n\nmean(pred == test$Species) # Accuracy.\n#> [1] 0.9777778\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(X, 1, function(t) exp(-t[1]^2 - t[2]^2))\ny = ifelse(1/(1+exp(-y)) < 0.62, 0, 1)\n\nimage(matrix(y, 100, 100))\nanimation::saveGIF(\n  {\n    for(i in c(\"truth\", \"linear\", \"radial\", \"sigmoid\")){\n      if(i == \"truth\"){\n        image(matrix(y, 100,100),\n        main = \"Ground truth\", axes = FALSE, las = 2)\n      }else{\n        sv = e1071::svm(x = x, y = factor(y), kernel = i)\n        image(matrix(as.numeric(as.character(predict(sv, x))), 100, 100),\n        main = paste0(\"Kernel: \", i), axes = FALSE, las = 2)\n        axis(1, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    }\n  },\n  movie.name = \"svm.gif\", autobrowse = FALSE\n)"},{"path":"fundamental.html","id":"exercises-4","chapter":"4 Common Machine Learning algorithms","heading":"4.3.3 Exercises","text":"use Sonar data set explore support-vector machines k-neartest-neighbor classifier.Split Sonar data set mlbench library training- testset 50% group. useful split?\nresponse variable “class.” trying classify class.\n\n\n    Fit standard k-nearest-neighbor classifier support vector machine linear kernel (check help), report fitted better.\n\n    Calculate accuracies algorithms.\n    Fit different kernels compare accuracies.\n    Try fit different seed training test set generation.\n    ","code":"\nlibrary(mlbench)\nset.seed(123)\n\ndata(Sonar)\ndata = Sonar\nindices = sample.int(nrow(Sonar), 0.5 * nrow(Sonar))"},{"path":"fundamental.html","id":"artificial-neural-networks","chapter":"4 Common Machine Learning algorithms","heading":"4.4 Artificial Neural Networks","text":"KerasNow, come artificial neural networks (ANNs), topic regularization important. can specify regularization layer via kernel_regularization (/bias_regularization) argument.TorchAgain, regularization Torch:Let’s visualize first (input) layer:Additionally usual \\(L1\\) \\(L2\\) regularization another regularization: called dropout-layer (learn detail later).","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\nsummary(data)\n#>      Ozone           Solar.R           Wind             Temp           Month      \n#>  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   Min.   :5.000  \n#>  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   1st Qu.:6.000  \n#>  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   Median :7.000  \n#>  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   Mean   :6.993  \n#>  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   3rd Qu.:8.000  \n#>  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   Max.   :9.000  \n#>  NA's   :37       NA's   :7                                                       \n#>       Day      \n#>  Min.   : 1.0  \n#>  1st Qu.: 8.0  \n#>  Median :16.0  \n#>  Mean   :15.8  \n#>  3rd Qu.:23.0  \n#>  Max.   :31.0  \n#> \ndata = data[complete.cases(data),] # Remove NAs.\nsummary(data)\n#>      Ozone          Solar.R           Wind            Temp           Month      \n#>  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00   Min.   :5.000  \n#>  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00   1st Qu.:6.000  \n#>  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00   Median :7.000  \n#>  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79   Mean   :7.216  \n#>  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50   3rd Qu.:9.000  \n#>  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00   Max.   :9.000  \n#>       Day       \n#>  Min.   : 1.00  \n#>  1st Qu.: 9.00  \n#>  Median :16.00  \n#>  Mean   :15.95  \n#>  3rd Qu.:22.50  \n#>  Max.   :31.00\n\nX = scale(data[,2:6])\nY = data[,1]\n\nmodel = keras_model_sequential()\npenalty = 0.1\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             input_shape = list(5L),\n             kernel_regularizer = regularizer_l1(penalty)) %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             kernel_regularizer = regularizer_l1(penalty) ) %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             kernel_regularizer = regularizer_l1(penalty)) %>%\n # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\",\n             kernel_regularizer = regularizer_l1(penalty))\n\nmodel %>%\n compile(\n   loss = loss_mean_squared_error,\n   keras::optimizer_adamax(learning_rate = 0.1)\n  )\n\nmodel_history =\n model %>%\n fit(x = X, y = matrix(Y, ncol = 1L), epochs = 100L,\n     batch_size = 20L, shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nweights = lapply(model$weights, function(w) w$numpy() )\nfields::image.plot(weights[[1]])\nlibrary(torch)\ntorch_dataset = torch::dataset(\n    name = \"data\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\n\ndata = airquality\nsummary(data)\n#>      Ozone           Solar.R           Wind             Temp           Month      \n#>  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   Min.   :5.000  \n#>  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   1st Qu.:6.000  \n#>  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   Median :7.000  \n#>  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   Mean   :6.993  \n#>  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   3rd Qu.:8.000  \n#>  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00   Max.   :9.000  \n#>  NA's   :37       NA's   :7                                                       \n#>       Day      \n#>  Min.   : 1.0  \n#>  1st Qu.: 8.0  \n#>  Median :16.0  \n#>  Mean   :15.8  \n#>  3rd Qu.:23.0  \n#>  Max.   :31.0  \n#> \ndata = data[complete.cases(data),] # Remove NAs.\nX = scale(data[,2:6])\nY = data[,1,drop=FALSE]\n\ndataset = torch_dataset(X,Y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(X)[2], out_features = 100L),\n  nn_relu(),\n  nn_linear(100L, 100L),\n  nn_relu(),\n  nn_linear(100L, 100L),\n  nn_relu(),\n  nn_linear(100L, 1L),\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.1)\n\n\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n        # Add L1 (only on the 'kernel weights'):\n      \n      for(p in model_torch$parameters) {\n        if(length(dim(p)) > 1) loss = loss + p$abs()$sum()*0.1\n      }\n\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 398.715637\n#> Loss at epoch 20: 362.533501\n#> Loss at epoch 30: 378.814766\n#> Loss at epoch 40: 288.005226\n#> Loss at epoch 50: 265.120636\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nfields::image.plot(as.matrix(model_torch$parameters$`0.weight`))"},{"path":"fundamental.html","id":"exercise-1","chapter":"4 Common Machine Learning algorithms","heading":"4.4.1 Exercise","text":"following code working example next exercises:KerasTorchWhat happens change regularization \\(L1\\) \\(L2\\)?\nKerasTorchTry different regularization strengths, try push weights zero. strategy push parameters zero?\nNow less regularization:KerasTorchKerasTorchPlay around ! Ask questions .Use combination \\(L1\\) \\(L2\\) regularization (Keras function ). kind regularization called advantage approach?\nKerasTorchUse holdout/validation split!KerasIn Keras can tell model keep specific percentage data holdout (validation_split argument fit function):TorchIn Torch create two dataloaders, one training data one validation data:CitoAs might noticed, fit function torch write fit/training function . However new package called ‘cito’ (based torch) allows everything need one line code.Run code view loss train validation (test) set viewer/plot panel. Increase network size (e.g. number hidden layers number hidden neurons layer). happens validation loss? ?\n\n    Add \\(L1\\) / \\(L2\\) regularization neural network try keep test loss close training loss. Try higher epoch numbers!Explain strategy helps achieve .\nAdding regularization (\\(L1\\) \\(L2\\)):KerasTorchCitoAdding higher regularization (\\(L1\\) \\(L2\\)):KerasTorchCitoKeep mind: higher regularization, training validation loss keep relatively high!Adding normal regularization (\\(L1\\) \\(L2\\)) use larger learning rate:KerasTorchCitoAdding normal regularization (\\(L1\\) \\(L2\\)) use low learning rate:KerasTorchCitoAdding low regularization (\\(L1\\) \\(L2\\)) use low learning rate:KerasTorchCitoPlay around regularization kind (\\(L1\\), \\(L2\\), \\(L1,L2\\)) strength learning rate also !","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = data$Ozone\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 1L, activation = \"linear\", input_shape = list(dim(x)[2]), \n             kernel_regularizer = regularizer_l1(10),\n             bias_regularizer = regularizer_l1(10))\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.5))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = y, epochs = 60L, batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\nl1 = model$get_weights()\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\n# The last parameter is the intercept (first parameter in lm).\ncat(\"L1:         \", round(c(unlist(l1)[length(unlist(l1))],\n                            unlist(l1)[1:(length(unlist(l1)) - 1 )]), 3))\n#> L1:          36.766 1.193 -8.446 13.394 -0.108 0.034\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = matrix(data$Ozone, ncol = 1)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\nlambda = torch_tensor(10.)\n\nepochs = 50L\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      for(p in model_torch$parameters) {\n        # if(length(dim(p)) > 1) loss = loss + torch_norm(p, p = 1L)\n        loss = loss + lambda*torch_norm(p, p = 1L)\n      }\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 1531.039734\n#> Loss at epoch 20: 1140.322968\n#> Loss at epoch 30: 1111.737778\n#> Loss at epoch 40: 1114.639465\n#> Loss at epoch 50: 1100.512360\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n\n\nl1_torch = model_torch$parameters\n\nlinear = lm(y~x)\nsummary(linear)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -37.014 -12.284  -3.302   8.454  95.348 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   42.099      1.980  21.264  < 2e-16 ***\n#> xSolar.R       4.583      2.135   2.147   0.0341 *  \n#> xWind        -11.806      2.293  -5.149 1.23e-06 ***\n#> xTemp         18.067      2.610   6.922 3.66e-10 ***\n#> xMonth        -4.479      2.230  -2.009   0.0471 *  \n#> xDay           2.385      2.000   1.192   0.2358    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 20.86 on 105 degrees of freedom\n#> Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n#> F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n\ncat(\"Linear:     \", round(coef(linear), 3))\n#> Linear:      42.099 4.583 -11.806 18.067 -4.479 2.385\ncat(\"L1:         \", round(c(as.numeric( l1_torch$`0.bias` ),as.numeric( l1_torch$`0.weight` )), 3))\n#> L1:          36.997 1.446 -8.213 13.754 -0.101 -0.033\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]\nx = scale(data[,2:6])\ny = data[,1]\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\", input_shape = list(5L)) %>%\n  layer_dense(units = 100L) %>%\n  layer_dense(units = 100L) %>%\n  # One output dimension with a linear activation function.\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %>%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %>%\n    fit(x = x, y = matrix(y, ncol = 1L), epochs = 50L, batch_size = 20L,\n        shuffle = TRUE, validation_split = 0.2)\n\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\nx = scale(data[,-1])\ny = matrix(data$Ozone, ncol = 1L)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(x)[2], out_features = 50L,  bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 50L, out_features = 50L,  bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 50L, out_features = 1L,  bias = TRUE)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.5)\n\nindices = sample.int(nrow(x), 0.8*nrow(x))\ndataset = torch_dataset(x[indices, ],y[indices, ,drop=FALSE])\ndataset_val = torch_dataset(x[-indices, ],y[-indices, ,drop=FALSE])\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\ndataloader_val = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\nepochs = 50L\ntrain_losses = c()\nval_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  val_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  ## Calculate validation loss ##\n  coro::loop(\n    for(batch in dataloader_val) { \n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      val_loss = c(val_loss, loss$item())\n    }\n  )\n  \n  \n  train_losses = c(train_losses, mean(train_loss))\n  val_losses = c(val_losses, mean(val_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f  Val loss: %3f\\n\", epoch, mean(train_loss), mean(val_loss)))\n}\n#> Loss at epoch 10: 667.048848  Val loss: 541.639791\n#> Loss at epoch 20: 312.970846  Val loss: 305.456772\n#> Loss at epoch 30: 215.118795  Val loss: 210.573797\n#> Loss at epoch 40: 145.177879  Val loss: 146.719173\n#> Loss at epoch 50: 114.253807  Val loss: 104.073695\n\nmatplot(cbind(train_losses, val_losses), type = \"o\", pch = c(15, 16),\n        col = c(\"darkblue\", \"darkred\"), lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nlegend(\"topright\", bty = \"n\", \n       legend = c(\"Train loss\", \"Val loss\"), \n       pch = c(15, 16), col = c(\"darkblue\", \"darkred\"))\n# devtools::install_github(\"citoverse/cito\")\nlibrary(cito)\n\ndata = airquality\ndata = data[complete.cases(data), ]\ndata[,2:6] = scale(data[,2:6])\n\nmodel = dnn(Ozone~., \n            data = data, \n            loss = \"mse\", \n            hidden = rep(100L, 3L), \n            activation = rep(\"relu\", 3),\n            validation = 0.2)\n#> Loss at epoch 1: training: 2727.625, validation: 2943.772, lr: 0.01000#> Loss at epoch 2: training: 2219.949, validation: 1751.822, lr: 0.01000\n#> Loss at epoch 3: training: 1161.460, validation: 749.765, lr: 0.01000\n#> Loss at epoch 4: training: 839.385, validation: 670.991, lr: 0.01000\n#> Loss at epoch 5: training: 503.821, validation: 247.064, lr: 0.01000\n#> Loss at epoch 6: training: 397.692, validation: 237.813, lr: 0.01000\n#> Loss at epoch 7: training: 404.759, validation: 279.735, lr: 0.01000\n#> Loss at epoch 8: training: 385.765, validation: 499.828, lr: 0.01000\n#> Loss at epoch 9: training: 403.803, validation: 410.069, lr: 0.01000\n#> Loss at epoch 10: training: 364.150, validation: 302.399, lr: 0.01000\n#> Loss at epoch 11: training: 363.741, validation: 311.543, lr: 0.01000\n#> Loss at epoch 12: training: 335.237, validation: 395.540, lr: 0.01000\n#> Loss at epoch 13: training: 324.224, validation: 366.540, lr: 0.01000\n#> Loss at epoch 14: training: 305.847, validation: 297.235, lr: 0.01000\n#> Loss at epoch 15: training: 306.413, validation: 302.437, lr: 0.01000\n#> Loss at epoch 16: training: 300.281, validation: 346.101, lr: 0.01000\n#> Loss at epoch 17: training: 298.922, validation: 336.537, lr: 0.01000\n#> Loss at epoch 18: training: 294.127, validation: 308.364, lr: 0.01000\n#> Loss at epoch 19: training: 291.176, validation: 316.795, lr: 0.01000\n#> Loss at epoch 20: training: 284.996, validation: 336.547, lr: 0.01000\n#> Loss at epoch 21: training: 280.721, validation: 319.395, lr: 0.01000\n#> Loss at epoch 22: training: 276.327, validation: 304.193, lr: 0.01000\n#> Loss at epoch 23: training: 272.264, validation: 314.108, lr: 0.01000\n#> Loss at epoch 24: training: 267.946, validation: 317.537, lr: 0.01000\n#> Loss at epoch 25: training: 263.750, validation: 305.008, lr: 0.01000\n#> Loss at epoch 26: training: 259.456, validation: 306.201, lr: 0.01000\n#> Loss at epoch 27: training: 254.371, validation: 314.152, lr: 0.01000\n#> Loss at epoch 28: training: 249.940, validation: 309.370, lr: 0.01000\n#> Loss at epoch 29: training: 244.954, validation: 310.888, lr: 0.01000\n#> Loss at epoch 30: training: 239.748, validation: 317.827, lr: 0.01000\n#> Loss at epoch 31: training: 234.382, validation: 316.355, lr: 0.01000\n#> Loss at epoch 32: training: 228.147, validation: 317.936, lr: 0.01000"},{"path":"workflow.html","id":"workflow","chapter":"5 Machine Learning workflow","heading":"5 Machine Learning workflow","text":"","code":""},{"path":"workflow.html","id":"the-standard-machine-learning-pipeline-at-the-eexample-of-the-titanic-data-set","chapter":"5 Machine Learning workflow","heading":"5.1 The Standard Machine Learning Pipeline at the Eexample of the Titanic Data set","text":"specialize tuning, important understand machine learning always consists pipeline actions.typical machine learning workflow consist :Data cleaning exploration (EDA = explorative data analysis) example tidyverse.Preprocessing feature selection.Splitting data set training test set evaluation.Model fitting.Model evaluation.New predictions.(optional) video explains entire pipeline slightly different perspective:following example, use tidyverse, collection R packages data science / data manipulation mainly developed Hadley Wickham. video explains basics can found :Another good reference “R data science” Hadley Wickham: .lecture need Titanic data set provided us. can find GRIPS (datasets.RData data set submission section) http://rhsbio6.uni-regensburg.de:8500.split data set already training test/prediction data sets (test/prediction split one column less train split, result known priori).","code":""},{"path":"workflow.html","id":"data-cleaning","chapter":"5 Machine Learning workflow","heading":"5.1.1 Data Cleaning","text":"Load necessary libraries:Load data set:Standard summaries:name variable consists 1309 unique factors (1309 observations…):However, title name. Let’s extract titles:extract names split name comma “,”split second split name point “.” extract titles.get 18 unique titles:titles low occurrence rate:combine titles low occurrences one title, can easily forcats package.can count titles see new number titles:Add new title variable data set:second example, explore clean numeric “age” variable.Explore variable:20% NAs!\nEither remove observations NAs, impute (fill) missing values, e.g. median age. However, age might depend variables sex, class title. want fill NAs median age groups.\ntidyverse can easily “group” data, .e. nest observations (: group_by sex, pclass title).\ngrouping, operations (median(age….)) done within specified groups.","code":"\nlibrary(tidyverse)\nlibrary(EcoData)\ndata(titanic_ml)\ndata = titanic_ml\nstr(data)\n#> 'data.frame':    1309 obs. of  14 variables:\n#>  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n#>  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n#>  $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n#>  $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n#>  $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n#>  $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\nsummary(data)\n#>      pclass         survived          name               sex           age         \n#>  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667  \n#>  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000  \n#>  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000  \n#>  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811  \n#>  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000  \n#>  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000  \n#>                  NA's   :655                                      NA's   :263      \n#>      sibsp            parch            ticket          fare        \n#>  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896  \n#>  Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454  \n#>  Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295  \n#>  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275  \n#>  Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329  \n#>                                   (Other) :1261   NA's   :1        \n#>              cabin      embarked      boat          body                      home.dest  \n#>                 :1014    :  2           :823   Min.   :  1.0                       :564  \n#>  C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0   New York, NY        : 64  \n#>  B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0   London              : 14  \n#>  G6             :   5   S:914    15     : 37   Mean   :160.8   Montreal, PQ        : 10  \n#>  B96 B98        :   4            14     : 33   3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n#>  C22 C26        :   4            4      : 31   Max.   :328.0   Paris, France       :  9  \n#>  (Other)        : 271            (Other):308   NA's   :1188    (Other)             :639\nhead(data)\n#>      pclass survived                         name    sex  age sibsp parch\n#> 561       2        1        Sinkkonen, Miss. Anna female 30.0     0     0\n#> 321       1        1            Woolner, Mr. Hugh   male   NA     0     0\n#> 1177      3        0     Sage, Mr. Douglas Bullen   male   NA     8     2\n#> 1098      3        0  Palsson, Master. Paul Folke   male  6.0     3     1\n#> 1252      3        0   Tomlin, Mr. Ernest Portage   male 30.5     0     0\n#> 1170      3        0 Saether, Mr. Simon Sivertsen   male 38.5     0     0\n#>                  ticket   fare cabin embarked boat body                home.dest\n#> 561              250648 13.000              S   10   NA Finland / Washington, DC\n#> 321               19947 35.500   C52        S    D   NA          London, England\n#> 1177           CA. 2343 69.550              S        NA                         \n#> 1098             349909 21.075              S        NA                         \n#> 1252             364499  8.050              S        50                         \n#> 1170 SOTON/O.Q. 3101262  7.250              S        32\nlength(unique(data$name))\n#> [1] 1307\nfirst_split = sapply(data$name,\n                     function(x) stringr::str_split(x, pattern = \",\")[[1]][2])\ntitles = sapply(first_split,\n                function(x) strsplit(x, \".\",fixed = TRUE)[[1]][1])\ntable(titles)\n#> titles\n#>          Capt           Col           Don          Dona            Dr      Jonkheer \n#>             1             4             1             1             8             1 \n#>          Lady         Major        Master          Miss          Mlle           Mme \n#>             1             2            61           260             2             1 \n#>            Mr           Mrs            Ms           Rev           Sir  the Countess \n#>           757           197             2             8             1             1\ntitles = stringr::str_trim((titles))\ntitles %>%\n fct_count()\n#> # A tibble: 18 × 2\n#>    f                n\n#>    <fct>        <int>\n#>  1 Capt             1\n#>  2 Col              4\n#>  3 Don              1\n#>  4 Dona             1\n#>  5 Dr               8\n#>  6 Jonkheer         1\n#>  7 Lady             1\n#>  8 Major            2\n#>  9 Master          61\n#> 10 Miss           260\n#> 11 Mlle             2\n#> 12 Mme              1\n#> 13 Mr             757\n#> 14 Mrs            197\n#> 15 Ms               2\n#> 16 Rev              8\n#> 17 Sir              1\n#> 18 the Countess     1\ntitles2 =\n  forcats::fct_collapse(titles,\n                        officer = c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\"),\n                        royal = c(\"Jonkheer\", \"Don\", \"Sir\",\n                                  \"the Countess\", \"Dona\", \"Lady\"),\n                        miss = c(\"Miss\", \"Mlle\"),\n                        mrs = c(\"Mrs\", \"Mme\", \"Ms\")\n                        )\ntitles2 %>%  \n   fct_count()\n#> # A tibble: 6 × 2\n#>   f           n\n#>   <fct>   <int>\n#> 1 officer    23\n#> 2 royal       6\n#> 3 Master     61\n#> 4 miss      262\n#> 5 mrs       200\n#> 6 Mr        757\ndata =\n  data %>%\n    mutate(title = titles2)\nsummary(data)\n#>      pclass         survived          name               sex           age         \n#>  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667  \n#>  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000  \n#>  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000  \n#>  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811  \n#>  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000  \n#>  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000  \n#>                  NA's   :655                                      NA's   :263      \n#>      sibsp            parch            ticket          fare        \n#>  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896  \n#>  Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454  \n#>  Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295  \n#>  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275  \n#>  Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329  \n#>                                   (Other) :1261   NA's   :1        \n#>              cabin      embarked      boat          body                      home.dest  \n#>                 :1014    :  2           :823   Min.   :  1.0                       :564  \n#>  C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0   New York, NY        : 64  \n#>  B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0   London              : 14  \n#>  G6             :   5   S:914    15     : 37   Mean   :160.8   Montreal, PQ        : 10  \n#>  B96 B98        :   4            14     : 33   3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n#>  C22 C26        :   4            4      : 31   Max.   :328.0   Paris, France       :  9  \n#>  (Other)        : 271            (Other):308   NA's   :1188    (Other)             :639  \n#>      title    \n#>  officer: 23  \n#>  royal  :  6  \n#>  Master : 61  \n#>  miss   :262  \n#>  mrs    :200  \n#>  Mr     :757  \n#> \nsum(is.na(data$age)) / nrow(data)\n#> [1] 0.2009167\ndata =\n  data %>%\n    group_by(sex, pclass, title) %>%\n    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %>%\n    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %>%\n    ungroup()"},{"path":"workflow.html","id":"preprocessing-and-feature-selection","chapter":"5 Machine Learning workflow","heading":"5.1.2 Preprocessing and Feature Selection","text":"want use Keras example, handle factors requires data scaled.Normally, one predictors, show pipeline , sub-selected bunch predictors .\nfirst scale numeric predictors change factors two groups/levels integers (can handled Keras).Factors two levels one hot encoded (Make columns every different factor level write 1 respective column every taken feature value 0 else. example: \\(\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}\\)):add dummy encoded variables data set:","code":"\ndata_sub =\n  data %>%\n    select(survived, sex, age2, fare2, title, pclass) %>%\n    mutate(age2 = scales::rescale(age2, c(0, 1)),\n           fare2 = scales::rescale(fare2, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           title = as.integer(title) - 1L, pclass = as.integer(pclass - 1L))\none_title = model.matrix(~0+as.factor(title), data = data_sub)\ncolnames(one_title) = levels(data$title)\n\none_sex = model.matrix(~0+as.factor(sex), data = data_sub)\ncolnames(one_sex) = levels(data$sex)\n\none_pclass = model.matrix(~0+as.factor(pclass), data = data_sub)\ncolnames(one_pclass) = paste0(1:length(unique(data$pclass)), \"pclass\")\ndata_sub = cbind(data.frame(survived= data_sub$survived),\n                 one_title, one_sex, age = data_sub$age2,\n                 fare = data_sub$fare2, one_pclass)\nhead(data_sub)\n#>   survived officer royal Master miss mrs Mr female male        age       fare 1pclass\n#> 1        1       0     0      0    1   0  0      1    0 0.37369494 0.02537431       0\n#> 2        1       0     0      0    0   0  1      0    1 0.51774510 0.06929139       1\n#> 3        0       0     0      0    0   0  1      0    1 0.32359053 0.13575256       0\n#> 4        0       0     0      1    0   0  0      0    1 0.07306851 0.04113566       0\n#> 5        0       0     0      0    0   0  1      0    1 0.37995799 0.01571255       0\n#> 6        0       0     0      0    0   0  1      0    1 0.48016680 0.01415106       0\n#>   2pclass 3pclass\n#> 1       1       0\n#> 2       0       0\n#> 3       0       1\n#> 4       0       1\n#> 5       0       1\n#> 6       0       1"},{"path":"workflow.html","id":"split-data","chapter":"5 Machine Learning workflow","heading":"5.1.3 Split Data","text":"splitting consists two splits:outer split (original split, remember got training test split without response “survived”).inner split (split training data set another training test split known response).\ninner split important assess model’s performance potential overfitting.Outer split:Inner split:difference two splits? (Tip: look variable survived.)","code":"\ntrain = data_sub[!is.na(data_sub$survived),]\ntest = data_sub[is.na(data_sub$survived),]\nindices = sample.int(nrow(train), 0.7 * nrow(train))\nsub_train = train[indices,]\nsub_test = train[-indices,]"},{"path":"workflow.html","id":"training","chapter":"5 Machine Learning workflow","heading":"5.1.4 Training","text":"next step fit Keras model training data inner split:KerasTorchNote: “nnf_cross_entropy” expects predictions scale linear predictors (loss function apply softmax!).CitoCito can handle factors already prepared data torch tensorflow use also processed data:","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 20L, input_shape = ncol(sub_train) - 1L,\n              activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  layer_dense(units = 20L, activation = \"relu\") %>%\n  # Output layer consists of the 1-hot encoded variable \"survived\" -> 2 units.\n  layer_dense(units = 2L, activation = \"softmax\")\n\nsummary(model)\n#> Model: \"sequential\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_3 (Dense)                        (None, 20)                          280           \n#>  dense_2 (Dense)                        (None, 20)                          420           \n#>  dense_1 (Dense)                        (None, 20)                          420           \n#>  dense (Dense)                          (None, 2)                           42            \n#> ==========================================================================================\n#> Total params: 1,162\n#> Trainable params: 1,162\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel_history =\nmodel %>%\n  compile(loss = loss_categorical_crossentropy,\n          optimizer = keras::optimizer_adamax(learning_rate = 0.01))\n\nmodel_history =\n  model %>%\n    fit(x = as.matrix(sub_train[,-1]),\n        y = to_categorical(sub_train[,1], num_classes = 2L),\n        epochs = 100L, batch_size = 32L,\n        validation_split = 0.2,   #Again a test set used by the algorithm.\n        shuffle = TRUE)\n\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntorch_dataset = torch::dataset(\n    name = \"data\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\n\n\nmodel_torch = nn_sequential(\n  nn_linear(in_features = dim(sub_train[,-1])[2], out_features = 30L),\n  nn_relu(),\n  nn_linear(30L, 30L),\n  nn_relu(),\n  nn_linear(30L, 2L)\n)\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\ndataset = torch_dataset(as.matrix(sub_train[,-1]), sub_train[,1]+1)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 0.479470\n#> Loss at epoch 20: 0.476513\n#> Loss at epoch 30: 0.464416\n#> Loss at epoch 40: 0.458047\n#> Loss at epoch 50: 0.443187\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nlibrary(cito)\n\nmodel_cito = dnn(survived~., data = sub_train, loss = \"binomial\", hidden = rep(30, 3), activation = rep(\"relu\", 3))\n#> Loss at epoch 1: 0.628682, lr: 0.01000#> Loss at epoch 2: 0.555334, lr: 0.01000\n#> Loss at epoch 3: 0.514781, lr: 0.01000\n#> Loss at epoch 4: 0.493197, lr: 0.01000\n#> Loss at epoch 5: 0.486173, lr: 0.01000\n#> Loss at epoch 6: 0.485266, lr: 0.01000\n#> Loss at epoch 7: 0.477691, lr: 0.01000\n#> Loss at epoch 8: 0.478282, lr: 0.01000\n#> Loss at epoch 9: 0.470990, lr: 0.01000\n#> Loss at epoch 10: 0.474267, lr: 0.01000\n#> Loss at epoch 11: 0.466413, lr: 0.01000\n#> Loss at epoch 12: 0.471069, lr: 0.01000\n#> Loss at epoch 13: 0.467679, lr: 0.01000\n#> Loss at epoch 14: 0.462538, lr: 0.01000\n#> Loss at epoch 15: 0.461307, lr: 0.01000\n#> Loss at epoch 16: 0.464263, lr: 0.01000\n#> Loss at epoch 17: 0.458622, lr: 0.01000\n#> Loss at epoch 18: 0.458754, lr: 0.01000\n#> Loss at epoch 19: 0.459127, lr: 0.01000\n#> Loss at epoch 20: 0.458725, lr: 0.01000\n#> Loss at epoch 21: 0.456209, lr: 0.01000\n#> Loss at epoch 22: 0.461059, lr: 0.01000\n#> Loss at epoch 23: 0.453386, lr: 0.01000\n#> Loss at epoch 24: 0.455715, lr: 0.01000\n#> Loss at epoch 25: 0.456063, lr: 0.01000\n#> Loss at epoch 26: 0.453903, lr: 0.01000\n#> Loss at epoch 27: 0.456146, lr: 0.01000\n#> Loss at epoch 28: 0.454714, lr: 0.01000\n#> Loss at epoch 29: 0.452475, lr: 0.01000\n#> Loss at epoch 30: 0.456571, lr: 0.01000\n#> Loss at epoch 31: 0.449390, lr: 0.01000\n#> Loss at epoch 32: 0.453715, lr: 0.01000"},{"path":"workflow.html","id":"evaluation","chapter":"5 Machine Learning workflow","heading":"5.1.5 Evaluation","text":"predict variable “survived” test set inner split calculate accuracy:KerasTorchDon’t forget use link function (softmax).Cito","code":"\npred =\n  model %>%\n    predict(x = as.matrix(sub_test[,-1]))\n\npredicted = ifelse(pred[,2] < 0.5, 0, 1) # Ternary operator.\nobserved = sub_test[,1]\n(accuracy = mean(predicted == observed))  # (...): Show output.\n#> [1] 0.8121827\nmodel_torch$eval()\npreds_torch = nnf_softmax(model_torch(torch_tensor(as.matrix(sub_test[,-1]))),\n                          dim = 2L)\npreds_torch = as.matrix(preds_torch)\npreds_torch = apply(preds_torch, 1, which.max)\n(accuracy = mean(preds_torch - 1 == observed))\n#> [1] 0.7817259\n\npreds_cito = predict(model_cito, newdata = sub_test)\npreds_cito = ifelse(preds_cito > 0.5, 1, 0)\n(accuracy = mean(preds_cito == observed))\n#> [1] 0.8071066"},{"path":"workflow.html","id":"predictions-and-submission","chapter":"5 Machine Learning workflow","heading":"5.1.6 Predictions and Submission","text":"satisfied performance model inner split, create predictions test data outer split.\n, take observations belong outer test split (use filter function) remove survived (NAs) columns:assess performance test split true survival ratio unknown, however, can now submit predictions submission server http://rhsbio7.uni-regensburg.de:8500.\n, transform survived probabilities actual 0/1 predictions (probabilities allowed) create .csv file:values > 0.5 set 1 values < 0.5 zero.\nsubmission critical change predictions data.frame, select second column (probability survive), save write.csv function:file name used ID submission server, change whatever want long can identify .Annotation: AUC (always) higher probabilities 0/1 data (depending implementation definition AUC). expect upload 0/1 data (usage scenarios, theoretical ones)! Hint cheaters (just forget conversion): upload converted 0/1 data according ifelse(… < 0.5, 0, 1).","code":"\nsubmit = \n  test %>% \n      select(-survived)\npred = model %>% \n  predict(as.matrix(submit))\nwrite.csv(data.frame(y = ifelse(pred[,2] < 0.5, 0, 1)), file = \"Max_1.csv\")"},{"path":"workflow.html","id":"exercises-5","chapter":"5 Machine Learning workflow","heading":"5.1.7 Exercises","text":"tasks follow section machine learning pipelines. use titanic_ml data set EcoData package see pipeline. goal predict passenger survives .First, look data feature engineering / selection. Give try! (Ideas can found .)\nBuild neural network make predictions check performance hold-data.\nKerasTorchCitoPlay around model parameters, optimizer(learning_rate = …), epochs = …, number hidden nodes layers: units = …, regularization: kernel_regularizer = …, bias_regularizer = … - Try maximize model’s accuracy hold-data.Hint: lot different activation functions like “linear,” “softmax,” “relu,” “leaky_relu,” “gelu,” “selu,” “elu,” “exponential,” “sigmoid,” “tanh,” “softplus,” “softsign.”\nEvery activation function properties, requirements (!), advantages disadvantages. Choose wisely!get accuracy least 90%. Try better solution.\nKerasTorchCitoNow try solution (exactly one!) another seed check overfitting (seed!) procedure.\nKerasTorchCitoMake predictions submit via submission server.\n","code":"\nlibrary(tidyverse)\nlibrary(randomForest)\nlibrary(rpart)\nlibrary(EcoData)\nset_random_seed(54321L, disable_gpu = FALSE)    # Already sets R's random seed.\n\ndata(titanic_ml)\ndata = titanic_ml\nstr(data)\n#> 'data.frame':    1309 obs. of  14 variables:\n#>  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n#>  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n#>  $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n#>  $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n#>  $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n#>  $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\nindicesPredict = which(is.na(titanic_ml$survived))\nindicesTrain = which(!is.na(titanic_ml$survived))\nsubset = sample(length(indicesTrain), length(indicesTrain) * 0.7, replace = F)\nindicesTest = sort(indicesTrain[-subset]) # Mind order of instructions!\nindicesTrain = sort(indicesTrain[subset])\n\n# Determine labels. As they are fixed from now on, samples must not be left!\n# Impute when necessary. \nlabelsTrain = data$survived[indicesTrain]\nlabelsTest = data$survived[indicesTest]\n\n# Parameter \"body\" has nearly no information).\n# Parameter \"name\" was already processed above.\n# Parameter \"ticket\" may include (other) information, but leave it out for now.\n# Parameter \"survived\" is the target variable.\ndata = data %>% select(-body, -name, -ticket, -survived)\n\ndata$pclass = as.factor(data$pclass)  # \"pclass\" makes only sense as a factor.\n\nfor(i in 1:3){\n  print(sum(data$cabin[data$pclass == i] == \"\") / sum(data$pclass == i))\n  }\n#> [1] 0.2074303\n#> [1] 0.9169675\n#> [1] 0.977433\n\n# Most people have no cabin (21% in 1st, 92% in 2nd and 98% in 3rd class).\n# Dummy code the availability of a cabin.\n# This is NOT one-hot encoding! Dummy coded variables use n-1 variables!\ndata$cabin = (data$cabin != \"\") * 1\n\n# Impute values for parameter \"embarked\":\n  # Leave parameters with too many levels or causal inverse ones (assumed).\n  tmp = data %>% select(-boat, -home.dest)\n  tmp = data.frame(tmp[complete.cases(tmp),]) # Leave samples with missing values.\n  \n  missingIndices = which(tmp$embarked == \"\")\n  toPredict = tmp[missingIndices, -8]\n  tmp = tmp[-missingIndices,] # Leave samples that should be predicted.\n  tmp$embarked = droplevels(tmp$embarked) # Remove unused levels (\"\").\n  \n  # Random forests and simple regression trees don't need scaling.\n  # Use a simple regression tree instead of a random forest here,\n  # we have only 2 missing values.\n  # And a simple regression tree is easy to visualize.\n  regressionTree = rpart(embarked ~ ., data = tmp,\n                         control = rpart.control(minsplit = 10))\n  rpart.plot::rpart.plot(regressionTree)\n  prediction = predict(regressionTree, toPredict)\n  \n  for(i in 1:nrow(prediction)){\n    index = which(rownames(data) == as.integer(rownames(prediction))[i])\n    data$embarked[index] = colnames(prediction)[which.max(prediction[i,])]\n  }\n  \n  data$embarked = droplevels(data$embarked) # Remove unused levels (\"\").\n\ntable(data$pclass)\n#> \n#>   1   2   3 \n#> 323 277 709\nsum(table(data$pclass))\n#> [1] 1309\nsum(table(data$home.dest))\n#> [1] 1309\nsum(is.na(data$home.dest))\n#> [1] 0\n# \"pclass\", \"sex\", \"sibsp\", \"parch\", \"boat\" and \"home.dest\" have no missing values.\n\nsummary(data$fare)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   0.000   7.896  14.454  33.295  31.275 512.329       1\n# Parameter \"fare\" has 1 NA entry. Impute the mean.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\n\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Impute values for \"age\":\n  tmp = data %>% select(-home.dest) # Leave parameters with too many levels.\n  \n  missingIndices = which(is.na(tmp$age))\n  toPredict = tmp[missingIndices, -3]\n  tmp = tmp[-missingIndices,] # Leave samples that should be predicted.\n  \n  forest = randomForest(x = tmp[,-3], y = tmp$age)\n  prediction = predict(forest, toPredict)\n  \n  for(i in 1:length(prediction)){\n    index = which(rownames(data) == as.integer(names(prediction))[i])\n    data$age[index] = prediction[i]\n  }\n\nstr(data)\n#> 'data.frame':    1309 obs. of  10 variables:\n#>  $ pclass   : Factor w/ 3 levels \"1\",\"2\",\"3\": 2 1 3 3 3 3 3 1 3 1 ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 36.7 13.9 6 30.5 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : num  0 1 0 0 0 0 0 0 0 0 ...\n#>  $ embarked : Factor w/ 3 levels \"C\",\"Q\",\"S\": 3 3 3 3 3 3 3 1 3 1 ...\n#>  $ boat     : Factor w/ 28 levels \"none\",\"1\",\"10\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ home.dest: Factor w/ 370 levels \"unknown\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\ndata = as.data.frame(data)\n\n# One-hot encoding of \"pclass\", \"sex\", \"cabin\", \"embarked\", \"boat\" and \"home.dest\":\nfor(element in c(\"pclass\", \"sex\", \"cabin\", \"embarked\", \"boat\", \"home.dest\")){\n  # Build integer representation:\n  # This MUST start at 0, otherwise, the encoding is wrong!!\n    integers = as.integer(data[[element]])\n    integers = integers - min(integers)\n  \n  # Determine number of classes:\n  num_classes = length(levels(as.factor(data[[element]])))\n  \n  # Encode:\n  encoded = model.matrix(~0+as.factor(integers))\n  \n  # Copy factor names.\n  colnames(encoded) = paste0(element, \"_\", levels(as.factor(data[[element]])))\n  \n  data = data %>% select(-all_of(element))  # Remove original column.\n  data = cbind.data.frame(data, encoded)  # Plug in new (encoded) columns.\n}\n\n# Scale parameters (including one-hot encoded):\ndata = scale(as.matrix(data))\n\n# Split into training, test and prediction set:\n# Be careful! You need matrices, no data.frames!\ntrain = as.matrix(data[indicesTrain,])\ntest = as.matrix(data[indicesTest,])\npredict = as.matrix(data[indicesPredict,])\nprediction =\n  model %>%\n    predict(x = predict)\n\n# Take label with highest probability:\nprediction = (prediction[,1] < prediction[,2]) * 1\n\nwrite.csv(data.frame(y = prediction), file = \"submission_NN.csv\")"},{"path":"workflow.html","id":"mlr","chapter":"5 Machine Learning workflow","heading":"5.2 Bonus - Machine Learning Pipelines with mlr3","text":"seen today, many machine learning algorithms distributed several packages general machine learning pipeline similar models: feature engineering, feature selection, hyperparameter tuning cross-validation.idea mlr3 framework now provide general machine learning interface can use build reproducible automatic machine learning pipelines. key features mlr3 :common machine learning packages integrated mlr3, can easily switch different machine learning algorithms.common ‘language’/workflow specify machine learning pipelines.Support different cross-validation strategies.Hyperparameter tuning supported machine learning algorithms.Ensemble models.Useful links:mlr3-book (still work)mlr3 websitemlr3 cheatsheet","code":""},{"path":"workflow.html","id":"mlr3---the-basic-workflow","chapter":"5 Machine Learning workflow","heading":"5.2.1 mlr3 - The Basic Workflow","text":"mlr3 package actually consists several packages different tasks (e.g. mlr3tuning hyperparameter tuning, mlr3pipelines data preparation pipes). let’s start basic workflow:Let’s drop time, name ID variable create classification task:Create generic pipeline data transformation (imputation \\(\\rightarrow\\) scaling \\(\\rightarrow\\) encoding categorical variables):can even visualize preprocessing graph:Now, test model (random forest) 10-fold cross-validated, :Specify missing target rows validation ignored.Specify cross-validation, learner (machine learning model want use), measurement (AUC).Run (benchmark) model.cool! Preprocessing + 10-fold cross-validation model evaluation lines code!Let’s create final predictions:now submit predictions .still happy results, let’s hyperparameter tuning!","code":"\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n#> 'data.frame':    4687 obs. of  40 variables:\n#>  $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n#>  $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n#>  $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n#>  $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n#>  $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n#>  $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n#>  $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n#>  $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n#>  $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n#>  $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n#>  $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n#>  $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n#>  $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n#>  $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n#>  $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n#>  $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n#>  $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n#>  $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n#>  $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n#>  $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n#>  $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n#>  $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n#>  $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n#>  $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n#>  $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n#>  $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n#>  $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n#>  $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n#>  $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n#>  $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n#>  $ Orbital.Period              : num  419 1040 682 427 503 ...\n#>  $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n#>  $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n#>  $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n#>  $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n#>  $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n#>  $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n#>  $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n#>  $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\ndata = nasa %>% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n#>                    Hazardous           Absolute.Magnitude                Aphelion.Dist \n#>                         4187                            0                            0 \n#>           Asc.Node.Longitude                 Eccentricity    Epoch.Date.Close.Approach \n#>                            0                            0                            0 \n#>             Epoch.Osculation         Est.Dia.in.Feet.max.         Est.Dia.in.Feet.min. \n#>                            0                            0                            0 \n#>           Est.Dia.in.KM.max.           Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n#>                            0                            0                            0 \n#>            Est.Dia.in.M.min.        Est.Dia.in.Miles.max.        Est.Dia.in.Miles.min. \n#>                            0                            0                            0 \n#>                  Inclination  Jupiter.Tisserand.Invariant                 Mean.Anomaly \n#>                            0                            0                            0 \n#>                  Mean.Motion               Miles.per.hour   Minimum.Orbit.Intersection \n#>                            0                            0                            0 \n#>     Miss.Dist..Astronomical.       Miss.Dist..kilometers.            Miss.Dist..lunar. \n#>                            0                            0                            0 \n#>            Miss.Dist..miles.                     Orbit.ID           Orbit.Uncertainity \n#>                            0                            0                            0 \n#>               Orbital.Period               Perihelion.Arg          Perihelion.Distance \n#>                            0                            0                            0 \n#>              Perihelion.Time  Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec \n#>                            0                            0                            0 \n#>              Semi.Major.Axis                Equinox.J2000             Equinox..MISSING \n#>                            0                            0                            0 \n#>          Orbiting.Body.Earth       Orbiting.Body..MISSING \n#>                            0                            0\npreprocessing$plot()\nset.seed(123)\n\ntransformed_task$data()\n#>       Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity\n#>    1:         0        -0.81322649   -0.38042005       -1.140837452 -0.315605975\n#>    2:         0         0.02110348    0.94306517       -1.380254611  0.744287645\n#>    3:         0         0.68365964    0.10199889        0.044905370 -0.068280074\n#>    4:         1        -0.10159210   -0.38415066        1.606769281 -0.392030729\n#>    5:         1        -0.15067034   -0.29632490        0.151458877 -0.516897963\n#>   ---                                                                           \n#> 4683:      <NA>        -0.32244415    0.69173184       -0.171022906  1.043608082\n#> 4684:      <NA>         0.46280759   -0.24203066       -0.009803808 -0.006429588\n#> 4685:      <NA>         1.51798962   -0.56422744        1.514551982 -1.045386877\n#> 4686:      <NA>         0.16833819    0.14193044       -1.080452287  0.017146757\n#> 4687:      <NA>        -0.05251387   -0.08643345       -0.013006704 -0.579210554\n#>       Epoch.Date.Close.Approach Epoch.Osculation Est.Dia.in.Feet.max.\n#>    1:                -4.7929881       0.14026773          0.271417899\n#>    2:                 1.1058704      -0.26325244          0.032130074\n#>    3:                 0.1591740      -7.76281014         -0.012841645\n#>    4:                -0.7630231       0.24229559          0.048493723\n#>    5:                -0.6305034       0.24229559          0.056169717\n#>   ---                                                                \n#> 4683:                 1.3635097       0.24229559          0.089353662\n#> 4684:                 1.3635097       0.05711503         -0.003481174\n#> 4685:                 1.3635097       0.24229559         -0.027260163\n#> 4686:                 1.3635097       0.24229559          0.016872584\n#> 4687:                 1.3635097       0.24229559          0.041493133\n#>       Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max.\n#>    1:          0.313407647        0.300713440        0.256568684       0.271095311\n#>    2:         -0.029173486       -0.020055639        0.057560696       0.031844946\n#>    3:         -0.093558135       -0.080340934        0.020159164      -0.013119734\n#>    4:         -0.005746146        0.001880088        0.071169817       0.048206033\n#>    5:          0.005243343        0.012169879        0.077553695       0.055880826\n#>   ---                                                                             \n#> 4683:          0.052751793        0.056653478        0.105151714       0.089059576\n#> 4684:         -0.080157032       -0.067793075        0.027943967      -0.003760728\n#> 4685:         -0.114200690       -0.099669182        0.008167747      -0.027535994\n#> 4686:         -0.051017172       -0.040508543        0.044871533       0.016589844\n#> 4687:         -0.015768679       -0.007504312        0.065347651       0.041206539\n#>       Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n#>    1:       0.291624502          2.620443e-01           0.258651038   0.5442288\n#>    2:     -12.143577263          4.153888e-02           0.030928225  -0.5925952\n#>    3:      -0.060269734          9.711407e-05         -10.258220292  -0.5164818\n#>    4:       0.015659335          5.661810e-02           0.046501003   0.8225188\n#>    5:       0.025161701          6.369158e-02           0.053806009  -0.6568722\n#>   ---                                                                          \n#> 4683:       0.066241198          9.427082e-02           0.085386142   0.8222493\n#> 4684:      -0.048682099          8.722856e-03          -0.002961897   1.9818623\n#> 4685:      -0.078118891         -1.318965e-02          -0.025591624  -0.5220442\n#> 4686:      -0.023485512          2.747899e-02           0.016408144  -0.5912988\n#> 4687:       0.006993074          5.016700e-02           0.039838758   0.6181969\n#>       Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n#>    1:                   0.3840868  -1.02876096  0.31939530   -0.254130552\n#>    2:                  -0.7801632  -4.55056211 -0.71151122    0.009333354\n#>    3:                  -0.2872777  -4.55056211 -0.34600512   -0.866997591\n#>    4:                   0.3403535   1.02239674  0.28551117    0.039031045\n#>    5:                  -6.2415005   1.13265516  0.03164827   -0.995720084\n#>   ---                                                                    \n#> 4683:                  -0.6412806   0.01560046 -0.51852041    1.403775544\n#> 4684:                   0.1346891   1.08051799  0.17477591    0.970963141\n#> 4685:                   0.4810091   0.89998250  0.36895738   -1.150527134\n#> 4686:                  -0.3061894   0.22720275 -0.35895074   -0.705980518\n#> 4687:                  -0.2665930   0.22740438 -0.31462613   -0.239696213\n#>       Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers.\n#>    1:                -5.45911858               -7.0769260             0.25122963\n#>    2:                 0.07077092               -0.6830928            -1.08492125\n#>    3:                -0.11099960               -0.9035573            -1.40898698\n#>    4:                -5.45911858               -0.7188386            -4.48402327\n#>    5:                -0.02962490               -0.8013948            -1.25881601\n#>   ---                                                                           \n#> 4683:                 0.30711241               -0.2728622            -0.48191427\n#> 4684:                -0.05962478               -0.7879458            -1.23904708\n#> 4685:                -0.10766868               -0.9303542            -1.44837625\n#> 4686:                 0.08529226               -0.7077555            -1.12117355\n#> 4687:                 0.50904764                0.1075071             0.07719897\n#>       Miss.Dist..lunar. Miss.Dist..miles.   Orbit.ID Orbit.Uncertainity Orbital.Period\n#>    1:         0.2398625        0.23810770 -9.6514722         -1.0070872     -0.3013135\n#>    2:        -1.1742128       -1.18860632 -0.2412680          1.3770116      0.7811097\n#>    3:        -4.7878719       -1.53463694 -0.1803606          0.7809869      0.1566040\n#>    4:        -1.2298206       -1.24471124 -0.1803606         -1.0070872     -0.2866969\n#>    5:        -1.3582490       -1.37428752  1.0225620         -1.0070872     -0.1552813\n#>   ---                                                                                 \n#> 4683:        -0.5360384       -0.54472804 -0.1194531         -0.7090748      0.3873214\n#> 4684:        -1.3373272       -1.35317867 -0.3021755          1.3770116     -0.2345610\n#> 4685:        -1.5588644       -1.57669598 -0.3326292          0.7809869     -0.3216884\n#> 4686:        -1.2125793       -1.22731578 -0.1042262          0.7809869      0.1712806\n#> 4687:         0.0556823        0.05228143 -0.2717218          0.4829746      0.1224733\n#>       Perihelion.Arg Perihelion.Distance Perihelion.Time Relative.Velocity.km.per.hr\n#>    1:   -1.170536399         -0.01831583      0.10526107                 -0.28167821\n#>    2:    1.549452700          0.20604472     -0.28203779                 -0.00604459\n#>    3:    1.470307933          0.61816146      0.20313227                 -0.92285430\n#>    4:    0.769006449          0.09005898     -7.86832915                  0.02502487\n#>    5:    0.006829799          0.52730977      0.26755741                 -1.05752264\n#>   ---                                                                               \n#> 4683:   -0.580282684         -0.65810123      0.03734532                  1.45280854\n#> 4684:    0.839430173         -0.18350549      0.09156633                  1.00000402\n#> 4685:   -1.168210857          0.62646993      0.27629790                 -1.21948041\n#> 4686:    0.824836889          0.52899080      0.37994517                 -0.75439966\n#> 4687:    0.016358127          1.22720096      0.37399573                 -0.26657713\n#>       Relative.Velocity.km.per.sec Semi.Major.Axis Equinox.J2000 Equinox..MISSING\n#>    1:                 -0.284140684      -0.2791037             1                0\n#>    2:                 -0.008343348      -7.3370940             1                0\n#>    3:                 -0.925697621       0.2204883             0                1\n#>    4:                  0.022744569      -0.2617714             1                0\n#>    5:                 -1.060445948      -0.1106954             1                0\n#>   ---                                                                            \n#> 4683:                  1.451376301       0.4468886             1                0\n#> 4684:                  0.998302826      -0.2008499             1                0\n#> 4685:                 -1.222499918      -0.3034586             1                0\n#> 4686:                 -0.757142920       0.2353030             1                0\n#> 4687:                 -0.269030636       0.1857979             1                0\n#>       Orbiting.Body.Earth Orbiting.Body..MISSING\n#>    1:                   1                      0\n#>    2:                   1                      0\n#>    3:                   1                      0\n#>    4:                   1                      0\n#>    5:                   1                      0\n#>   ---                                           \n#> 4683:                   1                      0\n#> 4684:                   1                      0\n#> 4685:                   1                      0\n#> 4686:                   1                      0\n#> 4687:                   1                      0\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\nresult = mlr3::resample(transformed_task,\n                        rf, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)"},{"path":"workflow.html","id":"mlr3---hyperparameter-tuning","chapter":"5 Machine Learning workflow","heading":"5.2.2 mlr3 - Hyperparameter Tuning","text":"Machine learning algorithms varying number hyperparameters can (!) high impact predictive performance. list hyperparameters:Random ForestmtryMinimal node sizeK-nearest-neighbors classificationKernelNumber neighborsDistance metricBoosted Regression TreenroundsMaximum depthalphaboosteretagammalambdaWith mlr3, can easily extend example hyperparameter tuning within nested cross-validation (tuning inner cross-validation).Print hyperparameter space random forest learner:Define hyperparameter space random forest:set tuning pipeline need:Inner cross-validation resampling object.Tuning criterion (e.g. AUC).Tuning method (e.g. random block search).Tuning terminator (stop tuning? E.g. \\(n\\) iterations).Now can wrap normally 10-fold cross-validated setup done previously:Yeah, able improve performance!Let’s create final predictions:","code":"\nrf$param_set\n#> <ParamSet>\n#>                               id    class lower upper nlevels        default    parents\n#>  1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5           \n#>  2:       always.split.variables ParamUty    NA    NA     Inf <NoDefault[3]>           \n#>  3:                class.weights ParamUty    NA    NA     Inf                          \n#>  4:                      holdout ParamLgl    NA    NA       2          FALSE           \n#>  5:                   importance ParamFct    NA    NA       4 <NoDefault[3]>           \n#>  6:                   keep.inbag ParamLgl    NA    NA       2          FALSE           \n#>  7:                    max.depth ParamInt     0   Inf     Inf                          \n#>  8:                min.node.size ParamInt     1   Inf     Inf                          \n#>  9:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1           \n#> 10:                      minprop ParamDbl  -Inf   Inf     Inf            0.1           \n#> 11:                         mtry ParamInt     1   Inf     Inf <NoDefault[3]>           \n#> 12:                   mtry.ratio ParamDbl     0     1     Inf <NoDefault[3]>           \n#> 13:            num.random.splits ParamInt     1   Inf     Inf              1  splitrule\n#> 14:                  num.threads ParamInt     1   Inf     Inf              1           \n#> 15:                    num.trees ParamInt     1   Inf     Inf            500           \n#> 16:                    oob.error ParamLgl    NA    NA       2           TRUE           \n#> 17:        regularization.factor ParamUty    NA    NA     Inf              1           \n#> 18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE           \n#> 19:                      replace ParamLgl    NA    NA       2           TRUE           \n#> 20:    respect.unordered.factors ParamFct    NA    NA       3         ignore           \n#> 21:              sample.fraction ParamDbl     0     1     Inf <NoDefault[3]>           \n#> 22:                  save.memory ParamLgl    NA    NA       2          FALSE           \n#> 23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE importance\n#> 24:                    se.method ParamFct    NA    NA       2        infjack           \n#> 25:                         seed ParamInt  -Inf   Inf     Inf                          \n#> 26:         split.select.weights ParamUty    NA    NA     Inf                          \n#> 27:                    splitrule ParamFct    NA    NA       3           gini           \n#> 28:                      verbose ParamLgl    NA    NA       2           TRUE           \n#> 29:                 write.forest ParamLgl    NA    NA       2           TRUE           \n#>                               id    class lower upper nlevels        default    parents\n#>     value\n#>  1:      \n#>  2:      \n#>  3:      \n#>  4:      \n#>  5:      \n#>  6:      \n#>  7:      \n#>  8:      \n#>  9:      \n#> 10:      \n#> 11:      \n#> 12:      \n#> 13:      \n#> 14:     1\n#> 15:      \n#> 16:      \n#> 17:      \n#> 18:      \n#> 19:      \n#> 20:      \n#> 21:      \n#> 22:      \n#> 23:      \n#> 24:      \n#> 25:      \n#> 26:      \n#> 27:      \n#> 28:      \n#> 29:      \n#>     value\nlibrary(paradox)\n\nrf_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamInt$new(\"min.node.size\", lower = 1, upper = 30L),\n           paradox::ParamInt$new(\"mtry\", lower = 1, upper = 30L),\n           paradox::ParamLgl$new(\"regularization.usedepth\", default = TRUE)))\nprint(rf_pars)\n#> <ParamSet>\n#>                         id    class lower upper nlevels        default value\n#> 1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n#> 2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n#> 3: regularization.usedepth ParamLgl    NA    NA       2           TRUE\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = rf, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = rf_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n#> <AutoTuner:classif.ranger.tuned>\n#> * Model: -\n#> * Search Space:\n#> <ParamSet>\n#>                         id    class lower upper nlevels        default value\n#> 1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n#> 2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n#> 3: regularization.usedepth ParamLgl    NA    NA       2           TRUE      \n#> * Packages: mlr3, mlr3tuning, mlr3learners, ranger\n#> * Predict Type: prob\n#> * Feature Types: logical, integer, numeric, character, factor, ordered\n#> * Properties: hotstart_backward, importance, multiclass, oob_error, twoclass,\n#>   weights\nset.seed(123)\n\nouter3 = mlr3::rsmp(\"cv\", folds = 3L)\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = outer3, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)"},{"path":"workflow.html","id":"mlr3---hyperparameter-tuning-with-oversampling","chapter":"5 Machine Learning workflow","heading":"5.2.3 mlr3 - Hyperparameter Tuning with Oversampling","text":"Let’s go one step back, maybe noticed classes unbalanced:Many machine learning algorithms problems unbalanced data imbalance strong cheaper algorithm focus one class (e.g. predicting 0s 1s). need keep mind machine learning algorithms greedy main focus minimize loss function.techniques correct imbalance:Oversampling (oversample undersampled class).Undersampling (undersample oversampled class).SMOTE Synthetic Minority -sampling Technique (briefly, use k-nearest-neighbors classification create new samples around undersampled class)., use oversampling can extending random forest learner:learner now new feature space:can also tune oversampling rate!5 iterations hyperspace much…Let’s create final predictions:reading chapter mlr package, try transfer titanic data set (use titanic_ml data set, already NAs values predict).\nAlternatively, can also use data sets challenge (e.g. plant-pollinator data set, see data set chapter 9).\n","code":"\ntable(data$Hazardous)\n#> \n#>   0   1 \n#> 412  88\nset.seed(123)\n\nrf_over = po(\"classbalancing\", id = \"over\", adjust = \"minor\") %>>% rf\n\n# However rf_over is now a \"graph\",\n# but we can easily transform it back into a learner:\nrf_over_learner = GraphLearner$new(rf_over)\nprint(rf_over_learner)\n#> <GraphLearner:over.classif.ranger>\n#> * Model: -\n#> * Parameters: over.ratio=1, over.reference=all, over.adjust=minor,\n#>   over.shuffle=TRUE, classif.ranger.num.threads=1\n#> * Packages: mlr3, mlr3pipelines, mlr3learners, ranger\n#> * Predict Type: prob\n#> * Feature types: logical, integer, numeric, character, factor, ordered, POSIXct\n#> * Properties: featureless, hotstart_backward, hotstart_forward, importance,\n#>   loglik, missings, multiclass, oob_error, selected_features, twoclass, weights\nrf_over_learner$param_set\n#> <ParamSetCollection>\n#>                                              id    class lower upper nlevels\n#>  1:                        classif.ranger.alpha ParamDbl  -Inf   Inf     Inf\n#>  2:       classif.ranger.always.split.variables ParamUty    NA    NA     Inf\n#>  3:                classif.ranger.class.weights ParamUty    NA    NA     Inf\n#>  4:                      classif.ranger.holdout ParamLgl    NA    NA       2\n#>  5:                   classif.ranger.importance ParamFct    NA    NA       4\n#>  6:                   classif.ranger.keep.inbag ParamLgl    NA    NA       2\n#>  7:                    classif.ranger.max.depth ParamInt     0   Inf     Inf\n#>  8:                classif.ranger.min.node.size ParamInt     1   Inf     Inf\n#>  9:                     classif.ranger.min.prop ParamDbl  -Inf   Inf     Inf\n#> 10:                      classif.ranger.minprop ParamDbl  -Inf   Inf     Inf\n#> 11:                         classif.ranger.mtry ParamInt     1   Inf     Inf\n#> 12:                   classif.ranger.mtry.ratio ParamDbl     0     1     Inf\n#> 13:            classif.ranger.num.random.splits ParamInt     1   Inf     Inf\n#> 14:                  classif.ranger.num.threads ParamInt     1   Inf     Inf\n#> 15:                    classif.ranger.num.trees ParamInt     1   Inf     Inf\n#> 16:                    classif.ranger.oob.error ParamLgl    NA    NA       2\n#> 17:        classif.ranger.regularization.factor ParamUty    NA    NA     Inf\n#> 18:      classif.ranger.regularization.usedepth ParamLgl    NA    NA       2\n#> 19:                      classif.ranger.replace ParamLgl    NA    NA       2\n#> 20:    classif.ranger.respect.unordered.factors ParamFct    NA    NA       3\n#> 21:              classif.ranger.sample.fraction ParamDbl     0     1     Inf\n#> 22:                  classif.ranger.save.memory ParamLgl    NA    NA       2\n#> 23: classif.ranger.scale.permutation.importance ParamLgl    NA    NA       2\n#> 24:                    classif.ranger.se.method ParamFct    NA    NA       2\n#> 25:                         classif.ranger.seed ParamInt  -Inf   Inf     Inf\n#> 26:         classif.ranger.split.select.weights ParamUty    NA    NA     Inf\n#> 27:                    classif.ranger.splitrule ParamFct    NA    NA       3\n#> 28:                      classif.ranger.verbose ParamLgl    NA    NA       2\n#> 29:                 classif.ranger.write.forest ParamLgl    NA    NA       2\n#> 30:                                 over.adjust ParamFct    NA    NA       7\n#> 31:                                  over.ratio ParamDbl     0   Inf     Inf\n#> 32:                              over.reference ParamFct    NA    NA       6\n#> 33:                                over.shuffle ParamLgl    NA    NA       2\n#>                                              id    class lower upper nlevels\n#>            default                   parents value\n#>  1:            0.5                                \n#>  2: <NoDefault[3]>                                \n#>  3:                                               \n#>  4:          FALSE                                \n#>  5: <NoDefault[3]>                                \n#>  6:          FALSE                                \n#>  7:                                               \n#>  8:                                               \n#>  9:            0.1                                \n#> 10:            0.1                                \n#> 11: <NoDefault[3]>                                \n#> 12: <NoDefault[3]>                                \n#> 13:              1  classif.ranger.splitrule      \n#> 14:              1                               1\n#> 15:            500                                \n#> 16:           TRUE                                \n#> 17:              1                                \n#> 18:          FALSE                                \n#> 19:           TRUE                                \n#> 20:         ignore                                \n#> 21: <NoDefault[3]>                                \n#> 22:          FALSE                                \n#> 23:          FALSE classif.ranger.importance      \n#> 24:        infjack                                \n#> 25:                                               \n#> 26:                                               \n#> 27:           gini                                \n#> 28:           TRUE                                \n#> 29:           TRUE                                \n#> 30: <NoDefault[3]>                           minor\n#> 31: <NoDefault[3]>                               1\n#> 32: <NoDefault[3]>                             all\n#> 33: <NoDefault[3]>                            TRUE\n#>            default                   parents value\nset.seed(123)\n\nrf_pars_over = \n    paradox::ParamSet$new(\n      list(paradox::ParamInt$new(\"over.ratio\", lower = 1, upper = 7L),\n           paradox::ParamInt$new(\"classif.ranger.min.node.size\",\n                                 lower = 1, upper = 30L),\n           paradox::ParamInt$new(\"classif.ranger.mtry\", lower = 1,\n                                 upper = 30L),\n           paradox::ParamLgl$new(\"classif.ranger.regularization.usedepth\",\n                                 default = TRUE)))\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\n\nlearner_tuner_over = AutoTuner$new(learner = rf_over_learner, \n                                   measure = measurement, \n                                   tuner = tuner, \n                                   terminator = terminator,\n                                   search_space = rf_pars_over,\n                                   resampling = inner3)\nprint(learner_tuner)\n#> <AutoTuner:classif.ranger.tuned>\n#> * Model: -\n#> * Search Space:\n#> <ParamSet>\n#>                         id    class lower upper nlevels        default value\n#> 1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n#> 2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n#> 3: regularization.usedepth ParamLgl    NA    NA       2           TRUE      \n#> * Packages: mlr3, mlr3tuning, mlr3learners, ranger\n#> * Predict Type: prob\n#> * Feature Types: logical, integer, numeric, character, factor, ordered\n#> * Properties: hotstart_backward, importance, multiclass, oob_error, twoclass,\n#>   weights\nset.seed(123)\n\nouter3 = mlr3::rsmp(\"cv\", folds = 3L)\nresult = mlr3::resample(transformed_task, learner_tuner_over,\n                        resampling = outer3, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\nset.seed(123)\n\ndata(titanic_ml)\nstr(titanic_ml)\n#> 'data.frame':    1309 obs. of  14 variables:\n#>  $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n#>  $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n#>  $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n#>  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n#>  $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n#>  $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n#>  $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n#>  $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n#>  $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n#>  $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n#>  $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n#>  $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n#>  $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n#>  $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\ndata = titanic_ml %>% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n#>  survived       age      boat     cabin  embarked      fare home.dest     parch    pclass \n#>       655       263         0         0         0         0         0         0         0 \n#>       sex     sibsp \n#>         0         0\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n\n# result = mlr3::resample(transformed_task, rf,\n#                         resampling = cv10, store_models = TRUE)\n# \n# # Calculate the average AUC of the holdouts.\n# result$aggregate(measurement)\n# \n# pred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\n# row_ids = (1:nrow(data))[is.na(data$survived)])$data$prob[, \"1\", drop = FALSE])\n# \n# dim(pred)\n# predictions = round(apply(pred, 1, mean))\n# \n# write.csv(data.frame(y = predictions), file = \"submission_RF.csv\")"},{"path":"deep.html","id":"deep","chapter":"6 Deep Learning","heading":"6 Deep Learning","text":"section, discuss , different (deep) network architectures different means regularize improve deep architectures.","code":""},{"path":"deep.html","id":"network-architectures","chapter":"6 Deep Learning","heading":"6.1 Network Architectures","text":"","code":""},{"path":"deep.html","id":"deep-neural-networks-dnns","chapter":"6 Deep Learning","heading":"6.1.1 Deep Neural Networks (DNNs)","text":"Deep neural networks basically simple artificial neural networks, hidden layers.","code":""},{"path":"deep.html","id":"convolutional-neural-networks-cnns","chapter":"6 Deep Learning","heading":"6.1.2 Convolutional Neural Networks (CNNs)","text":"main purpose convolutional neural networks image recognition. (Sound can understood image well!)\nconvolutional neural network, least one convolution layer, additional normal, fully connected deep neural network layers.Neurons convolution layer connected small spatially contiguous area input layer (receptive field). use structure (feature map) scan entire features / neurons (e.g. picture). Think feature map kernel filter (imagine sliding window weighted pixels) used scan image. name already indicating, operation convolution mathematics.\nkernel weights optimized, use weights across entire input neurons (shared weights).resulting (hidden) convolutional layer training called feature map. can think feature map map shows “shapes” expressed kernel appear input. One kernel / feature map enough, typically many shapes want recognize. Thus, input layer typically connected several feature maps, can aggregated followed second layer feature maps, .get one convolution map/layer kernel one convolutional layer.","code":""},{"path":"deep.html","id":"recurrent-neural-networks-rnns","chapter":"6 Deep Learning","heading":"6.1.3 Recurrent Neural Networks (RNNs)","text":"Recurrent neural networks used model sequential data, .e. temporal sequence exhibits temporal dynamic behavior. good introduction topic:","code":""},{"path":"deep.html","id":"example-predicting-drought","chapter":"6 Deep Learning","heading":"6.1.3.1 Example: Predicting drought","text":"following code snippet shows many (technical) things need building complex network structures, even LSTM cells (following example doesn’t functionality, just example process two different inputs different ways within one network):KerasTorch","code":"\nutils::download.file(\"https://www.dropbox.com/s/bqooarazbvg1g7u/weather_soil.RDS?raw=1\", destfile = \"weather_soil.RDS\")\ndata = readRDS(\"weather_soil.RDS\")\nX = data$train # Features of the last 180 days\ndim(X)\n#> [1] 999 180  21\n# 999 batches of 180 days with 21 features each\nY = data$target\ndim(Y)\n#> [1] 999   6\n# 999 batches of 6 week drought predictions\n\n# let's visualize drought over 24 months:\n# -> We have to take 16 batches (16*6 = 96 weaks ( = 24 months) )\nplot(as.vector(Y[1:16,]), type = \"l\", xlab = \"week\", ylab = \"Drought\")\nlibrary(keras)\n\nholdout = 700:999\n\nmodel = keras_model_sequential()\nmodel %>% \n  layer_rnn(cell = layer_lstm_cell(units = 60L),input_shape = dim(X)[2:3]) %>% \n  layer_dense(units = 6L)\n\nmodel %>% compile(loss = loss_mean_squared_error, optimizer = optimizer_adamax(learning_rate = 0.01))\n  \nmodel %>% fit(x = X[-holdout,,], y = Y[-holdout,], epochs = 30L)\n\npreds = \n  model %>% predict(x = X[-holdout,,], y = Y[-holdout,])\n\n\nmatplot(cbind(as.vector(preds[1:48,]),  \n              as.vector(Y[701:748,])), \n        col = c(\"darkblue\", \"darkred\"),\n        type = \"o\", \n        pch = c(15, 16),\n        xlab = \"week\", ylab = \"Drought\")\nlegend(\"topright\", bty = \"n\", \n       col = c(\"darkblue\", \"darkred\"),\n      pch = c(15, 16), \n      legend = c(\"Prediction\", \"True Values\"))\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ntf$keras$backend$clear_session()  # Resets especially layer counter.\n\ninputDimension1 = 50L\ninputDimension2 = 10L\n\ninput1 = layer_input(shape = inputDimension1)\ninput2 = layer_input(shape = inputDimension2)\n\nmodelInput2 = input2 %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = inputDimension2, activation = \"gelu\")\n\nmodelMemory = input1 %>%\n  layer_embedding(input_dim = inputDimension1, output_dim = 64L) %>%\n  layer_lstm(units = 64L) %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 2L, activation = \"sigmoid\")\n\nmodelDeep = input1 %>%\n  layer_dropout(rate = 0.5) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dense(units = 5L, activation = \"sigmoid\")\n\nmodelMain = layer_concatenate(c(modelMemory, modelDeep, modelInput2)) %>%\n  layer_dropout(rate = 0.25) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 64L, activation = \"relu\") %>%\n  layer_dense(units = 2L, activation = \"sigmoid\")\n\nmodel = keras_model(\n  inputs = c(input1, input2),\n  outputs = c(modelMain)  # Use the whole modelMain (resp. its output) as output.\n)\n\nsummary(model)\n#> Model: \"model\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                Output Shape        Param #    Connected to                  \n#> ==========================================================================================\n#>  input_1 (InputLayer)        [(None, 50)]        0          []                            \n#>  dropout_3 (Dropout)         (None, 50)          0          ['input_1[0][0]']             \n#>  dense_5 (Dense)             (None, 64)          3264       ['dropout_3[0][0]']           \n#>  embedding (Embedding)       (None, 50, 64)      3200       ['input_1[0][0]']             \n#>  dropout_2 (Dropout)         (None, 64)          0          ['dense_5[0][0]']             \n#>  lstm (LSTM)                 (None, 64)          33024      ['embedding[0][0]']           \n#>  dense_4 (Dense)             (None, 64)          4160       ['dropout_2[0][0]']           \n#>  input_2 (InputLayer)        [(None, 10)]        0          []                            \n#>  dropout_1 (Dropout)         (None, 64)          0          ['lstm[0][0]']                \n#>  dense_3 (Dense)             (None, 64)          4160       ['dense_4[0][0]']             \n#>  dropout (Dropout)           (None, 10)          0          ['input_2[0][0]']             \n#>  dense_1 (Dense)             (None, 2)           130        ['dropout_1[0][0]']           \n#>  dense_2 (Dense)             (None, 5)           325        ['dense_3[0][0]']             \n#>  dense (Dense)               (None, 10)          110        ['dropout[0][0]']             \n#>  concatenate (Concatenate)   (None, 17)          0          ['dense_1[0][0]',             \n#>                                                              'dense_2[0][0]',             \n#>                                                              'dense[0][0]']               \n#>  dropout_5 (Dropout)         (None, 17)          0          ['concatenate[0][0]']         \n#>  dense_8 (Dense)             (None, 64)          1152       ['dropout_5[0][0]']           \n#>  dropout_4 (Dropout)         (None, 64)          0          ['dense_8[0][0]']             \n#>  dense_7 (Dense)             (None, 64)          4160       ['dropout_4[0][0]']           \n#>  dense_6 (Dense)             (None, 2)           130        ['dense_7[0][0]']             \n#> ==========================================================================================\n#> Total params: 53,815\n#> Trainable params: 53,815\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n# model %>% plot_model()\nlibrary(torch)\n\nmodel_torch = nn_module(\n  initialize = function(type, inputDimension1 = 50L, inputDimension2 = 10L) {\n    self$dim1 = inputDimension1\n    self$dim2 = inputDimension2\n    self$modelInput2 = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(in_features = self$dim2, out_features = self$dim2),\n      nn_selu()\n    )\n    self$modelMemory = nn_sequential(\n      nn_embedding(self$dim1, 64),\n      nn_lstm(64, 64)\n    )\n    self$modelMemoryOutput = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(64L, 2L),\n      nn_sigmoid()\n    )\n    \n    self$modelDeep = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(self$dim1, 64L),\n      nn_relu(),\n      nn_dropout(0.3),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_linear(64, 5),\n      nn_sigmoid()\n    )\n    \n    self$modelMain = nn_sequential(\n      nn_linear(7+self$dim2, 64),\n      nn_relu(),\n      nn_dropout(0.5),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_dropout(),\n      nn_linear(64, 2),\n      nn_sigmoid()\n    )\n  },\n  \n  forward = function(x) {\n    input1 = x[[1]]\n    input2 = x[[2]]\n    out2 = self$modelInput2(input2)\n    out1 = self$modelMemoryOutput( self$modelMemory(input1)$view(list(dim(input1)[1], -1)) )\n    out3 = self$modelDeep(input1)\n    out = self$modelMain(torch_cat(list(out1, out2, out3), 2))\n    return(out)\n  }\n  \n)\n\n(model_torch())\n#> An `nn_module` containing 54,071 parameters.\n#> \n#> ── Modules ───────────────────────────────────────────────────────────────────────────────\n#> • modelInput2: <nn_sequential> #110 parameters\n#> • modelMemory: <nn_sequential> #36,480 parameters\n#> • modelMemoryOutput: <nn_sequential> #130 parameters\n#> • modelDeep: <nn_sequential> #11,909 parameters\n#> • modelMain: <nn_sequential> #5,442 parameters"},{"path":"deep.html","id":"natural-language-processing-nlp","chapter":"6 Deep Learning","heading":"6.1.4 Natural Language Processing (NLP)","text":"Natural language processing actually task network structure, area deep learning natural language processing, particular network structures used. following video give idea NLP .See also blog post linked Youtube video accompanying code. Moreover, article shows natural language processing works Keras, however, written Python. challenge, can take code implement R.","code":""},{"path":"deep.html","id":"case-study-dropout-and-early-stopping-in-dnns","chapter":"6 Deep Learning","heading":"6.2 Case Study: Dropout and Early Stopping in DNNs","text":"Regularization deep neural networks important problem overfitting. Standard regularization statistics like \\(L1\\) \\(L2\\) regularization often fuzzy require lot tuning. stable robust methods:Early stopping: Early stopping allows us stop training instance test loss decrease anymore validation loss starts increasing.Dropout: Dropout layer randomly sets input units 0 frequency given rate step training time, helps prevent overfitting. Dropout robust \\(L1\\) \\(L2\\), tuning dropout rate can beneficial rate \\(0.2-0.5\\) often works quite well.Data preparationSee 5.2 explanation preprocessing pipeline.Early stoppingKerasTorchCitoThe validation loss first decreases starts increasing , can explain behavior?\n\\(\\rightarrow\\) Overfitting!Let’s try \\(L1+L2\\) regularization:KerasTorchCitoBetter, validation loss still starts increasing 40 epochs. can use early stopping end training validation loss starts increasing !KerasTorchCitoPatience number epochs wait aborting training.Dropout - another type regularizationSrivastava et al. (2014) suggests dropout rate 50% internal hidden layers 20% input layer. One advantage dropout training independent number epochs .e. validation loss usually doesn’t start increase several epochs.KerasOf course, can still combine early stopping dropout, normally good idea since improves training efficiency (e.g. start 1000 epochs know training aborted doesn’t improve anymore).TorchCito","code":"\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3pipelines)\ndata(nasa)\nstr(nasa)\n#> 'data.frame':    4687 obs. of  40 variables:\n#>  $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n#>  $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n#>  $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n#>  $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n#>  $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n#>  $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n#>  $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n#>  $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n#>  $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n#>  $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n#>  $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n#>  $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n#>  $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n#>  $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n#>  $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n#>  $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n#>  $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n#>  $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n#>  $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n#>  $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n#>  $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n#>  $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n#>  $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n#>  $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n#>  $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n#>  $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n#>  $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n#>  $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n#>  $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n#>  $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n#>  $ Orbital.Period              : num  419 1040 682 427 503 ...\n#>  $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n#>  $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n#>  $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n#>  $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n#>  $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n#>  $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n#>  $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n#>  $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n\ndata = nasa %>% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \ndata = preprocessing$train(task)[[1]]$data()\n\ntrain = data[!is.na(data$Hazardous),]\nsubmit = data[is.na(data$Hazardous),]\n\nX = scale(train %>% select(-Hazardous))\nY = train %>% select(Hazardous)\nY = model.matrix(~0+as.factor(Hazardous), data = Y)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 50L, activation = \"relu\", input_shape = ncol(X)) %>%\n  layer_dense(units = 50L, activation = \"relu\") %>%\n  layer_dense(units = 50L, activation = \"relu\") %>%\n  layer_dense(units = ncol(Y), activation = \"softmax\") \n\nmodel %>%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n#> Model: \"sequential\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_12 (Dense)                       (None, 50)                          1900          \n#>  dense_11 (Dense)                       (None, 50)                          2550          \n#>  dense_10 (Dense)                       (None, 50)                          2550          \n#>  dense_9 (Dense)                        (None, 2)                           102           \n#> ==========================================================================================\n#> Total params: 7,102\n#> Trainable params: 7,102\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel_history =\n  model %>%\n    fit(x = X, y = Y, \n        epochs = 50L, batch_size = 20L, \n        shuffle = TRUE, validation_split = 0.4)\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(ncol(X), 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 2L)\n)\n\nYT = apply(Y, 1, which.max)\n\ndataset_nasa = dataset(\n  name = \"nasa\",\n  initialize = function(nasa){\n    self$X = nasa$X\n    self$Y = nasa$Y\n  },\n  .getitem = function(i){\n    X = self$X[i,,drop = FALSE] %>% torch_tensor()\n    Y = self$Y[i] %>% torch_tensor()\n    list(X, Y)\n  },\n  .length = function(){\n    nrow(self$X)\n  })\n\ntrain_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), \n                      batch_size = 32, shuffle = TRUE)\ntest_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), \n                      batch_size = 32)\n\nmodel_torch$train()\n\nopt = optim_adam(model_torch$parameters, 0.01)\n\ntrain_losses = c()\ntest_losses = c()\nfor(epoch in 1:50){\n  train_loss = c()\n  test_loss = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  \n  coro::loop(\n    for(batch in test_dl){\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      test_loss = c(test_loss, loss$item())\n    }\n  )\n\n  train_losses = c(train_losses, mean(train_loss))\n  test_losses = c(test_losses, mean(test_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 0.148818\n#> Loss at epoch 20: 0.000151\n#> Loss at epoch 30: 0.000016\n#> Loss at epoch 40: 0.000006\n#> Loss at epoch 50: 0.000003\n\nmatplot(cbind(train_losses, test_losses), type = \"o\", pch = c(15, 16),\n        col = c(\"darkblue\", \"darkred\"), lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nlegend(\"topright\", bty = \"n\", col = c(\"darkblue\", \"darkred\"),\n       lty = 1, pch = c(15, 16), legend = c(\"Train loss\", \"Val loss\") )\nlibrary(cito)\ndata_train = data.frame(target = apply(Y, 1, which.max)-1, X)\nmodel_cito = dnn(\n  target~., \n  data = data_train,\n  loss = \"binomial\",\n  validation = 0.4,\n  hidden = rep(50L, 3L),\n  activation = rep(\"relu\", 3),\n)\n#> Loss at epoch 1: training: 0.546, validation: 0.466, lr: 0.01000#> Loss at epoch 2: training: 0.375, validation: 0.477, lr: 0.01000\n#> Loss at epoch 3: training: 0.332, validation: 0.530, lr: 0.01000\n#> Loss at epoch 4: training: 0.267, validation: 0.637, lr: 0.01000\n#> Loss at epoch 5: training: 0.215, validation: 0.778, lr: 0.01000\n#> Loss at epoch 6: training: 0.154, validation: 1.074, lr: 0.01000\n#> Loss at epoch 7: training: 0.115, validation: 1.300, lr: 0.01000\n#> Loss at epoch 8: training: 0.074, validation: 1.353, lr: 0.01000\n#> Loss at epoch 9: training: 0.071, validation: 1.844, lr: 0.01000\n#> Loss at epoch 10: training: 0.162, validation: 1.740, lr: 0.01000\n#> Loss at epoch 11: training: 0.392, validation: 1.154, lr: 0.01000\n#> Loss at epoch 12: training: 0.137, validation: 1.002, lr: 0.01000\n#> Loss at epoch 13: training: 0.073, validation: 1.219, lr: 0.01000\n#> Loss at epoch 14: training: 0.035, validation: 1.313, lr: 0.01000\n#> Loss at epoch 15: training: 0.017, validation: 1.391, lr: 0.01000\n#> Loss at epoch 16: training: 0.008, validation: 1.512, lr: 0.01000\n#> Loss at epoch 17: training: 0.003, validation: 1.645, lr: 0.01000\n#> Loss at epoch 18: training: 0.001, validation: 1.730, lr: 0.01000\n#> Loss at epoch 19: training: 0.001, validation: 1.782, lr: 0.01000\n#> Loss at epoch 20: training: 0.001, validation: 1.832, lr: 0.01000\n#> Loss at epoch 21: training: 0.000, validation: 1.931, lr: 0.01000\n#> Loss at epoch 22: training: 0.000, validation: 2.006, lr: 0.01000\n#> Loss at epoch 23: training: 0.000, validation: 2.061, lr: 0.01000\n#> Loss at epoch 24: training: 0.000, validation: 2.091, lr: 0.01000\n#> Loss at epoch 25: training: 0.000, validation: 2.109, lr: 0.01000\n#> Loss at epoch 26: training: 0.000, validation: 2.120, lr: 0.01000\n#> Loss at epoch 27: training: 0.000, validation: 2.130, lr: 0.01000\n#> Loss at epoch 28: training: 0.000, validation: 2.138, lr: 0.01000\n#> Loss at epoch 29: training: 0.000, validation: 2.147, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 2.154, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 2.162, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 2.168, lr: 0.01000\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 50L, activation = \"relu\", input_shape = ncol(X),\n              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%\n  layer_dense(units = 50L, activation = \"relu\",\n              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%\n  layer_dense(units = 50L, activation = \"relu\",\n              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%\n  layer_dense(units = ncol(Y), activation = \"softmax\",\n              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) \n\nmodel %>%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n#> Model: \"sequential_1\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_16 (Dense)                       (None, 50)                          1900          \n#>  dense_15 (Dense)                       (None, 50)                          2550          \n#>  dense_14 (Dense)                       (None, 50)                          2550          \n#>  dense_13 (Dense)                       (None, 2)                           102           \n#> ==========================================================================================\n#> Total params: 7,102\n#> Trainable params: 7,102\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nmodel_history =\n  model %>%\n    fit(x = X, y = Y, \n        epochs = 100L, batch_size = 20L, \n        shuffle = TRUE, validation_split = 0.4)\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(ncol(X), 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 2L)\n)\n\nYT = apply(Y, 1, which.max)\n\ndataset_nasa = dataset(\n  name = \"nasa\",\n  initialize = function(nasa){\n    self$X = nasa$X\n    self$Y = nasa$Y\n  },\n  .getitem = function(i){\n    X = self$X[i,,drop = FALSE] %>% torch_tensor()\n    Y = self$Y[i] %>% torch_tensor()\n    list(X, Y)\n  },\n  .length = function(){\n    nrow(self$X)\n  })\n\ntrain_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), \n                      batch_size = 32, shuffle = TRUE)\ntest_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), \n                      batch_size = 32)\n\nmodel_torch$train()\n\nopt = optim_adam(model_torch$parameters, 0.01)\n\ntrain_losses = c()\ntest_losses = c()\nfor(epoch in 1:50){\n  train_loss = c()\n  test_loss = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      l1 = 0\n      for(p in model_torch$parameters) l1 = l1 + torch_norm(p, 1)\n      l2 = 0\n      for(p in model_torch$parameters) l2 = l2 + torch_norm(p, 2)\n      loss = loss + 0.001*l1 + 0.001*l2\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  \n  coro::loop(\n    for(batch in test_dl){\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      test_loss = c(test_loss, loss$item())\n    }\n  )\n\n  train_losses = c(train_losses, mean(train_loss))\n  test_losses = c(test_losses, mean(test_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 0.334997\n#> Loss at epoch 20: 0.249275\n#> Loss at epoch 30: 0.165154\n#> Loss at epoch 40: 0.230803\n#> Loss at epoch 50: 0.152237\n\nmatplot(cbind(train_losses, test_losses), type = \"o\", pch = c(15, 16),\n        col = c(\"darkblue\", \"darkred\"), lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nlegend(\"topright\", bty = \"n\", col = c(\"darkblue\", \"darkred\"),\n       lty = 1, pch = c(15, 16), legend = c(\"Train loss\", \"Val loss\") )\nlibrary(cito)\ndata_train = data.frame(target = apply(Y, 1, which.max)-1, X)\nmodel_cito = dnn(\n  target~., \n  data = data_train,\n  loss = \"binomial\",\n  validation = 0.4,\n  hidden = rep(50L, 3L),\n  activation = rep(\"relu\", 3),\n  lambda = 0.001,\n  alpha = 0.5\n)\n#> Loss at epoch 1: training: 0.760, validation: 0.652, lr: 0.01000#> Loss at epoch 2: training: 0.533, validation: 0.598, lr: 0.01000\n#> Loss at epoch 3: training: 0.464, validation: 0.642, lr: 0.01000\n#> Loss at epoch 4: training: 0.408, validation: 0.619, lr: 0.01000\n#> Loss at epoch 5: training: 0.358, validation: 0.690, lr: 0.01000\n#> Loss at epoch 6: training: 0.312, validation: 0.769, lr: 0.01000\n#> Loss at epoch 7: training: 0.271, validation: 0.876, lr: 0.01000\n#> Loss at epoch 8: training: 0.254, validation: 0.972, lr: 0.01000\n#> Loss at epoch 9: training: 0.244, validation: 0.887, lr: 0.01000\n#> Loss at epoch 10: training: 0.232, validation: 1.228, lr: 0.01000\n#> Loss at epoch 11: training: 0.194, validation: 1.210, lr: 0.01000\n#> Loss at epoch 12: training: 0.174, validation: 1.185, lr: 0.01000\n#> Loss at epoch 13: training: 0.150, validation: 1.582, lr: 0.01000\n#> Loss at epoch 14: training: 0.150, validation: 1.349, lr: 0.01000\n#> Loss at epoch 15: training: 0.135, validation: 1.483, lr: 0.01000\n#> Loss at epoch 16: training: 0.131, validation: 1.487, lr: 0.01000\n#> Loss at epoch 17: training: 0.131, validation: 1.407, lr: 0.01000\n#> Loss at epoch 18: training: 0.123, validation: 1.426, lr: 0.01000\n#> Loss at epoch 19: training: 0.123, validation: 1.366, lr: 0.01000\n#> Loss at epoch 20: training: 0.113, validation: 1.458, lr: 0.01000\n#> Loss at epoch 21: training: 0.106, validation: 1.415, lr: 0.01000\n#> Loss at epoch 22: training: 0.101, validation: 1.348, lr: 0.01000\n#> Loss at epoch 23: training: 0.097, validation: 1.324, lr: 0.01000\n#> Loss at epoch 24: training: 0.093, validation: 1.273, lr: 0.01000\n#> Loss at epoch 25: training: 0.090, validation: 1.242, lr: 0.01000\n#> Loss at epoch 26: training: 0.088, validation: 1.221, lr: 0.01000\n#> Loss at epoch 27: training: 0.085, validation: 1.204, lr: 0.01000\n#> Loss at epoch 28: training: 0.083, validation: 1.189, lr: 0.01000\n#> Loss at epoch 29: training: 0.081, validation: 1.182, lr: 0.01000\n#> Loss at epoch 30: training: 0.079, validation: 1.174, lr: 0.01000\n#> Loss at epoch 31: training: 0.077, validation: 1.163, lr: 0.01000\n#> Loss at epoch 32: training: 0.076, validation: 1.160, lr: 0.01000\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 50L, activation = \"relu\", input_shape = ncol(X),\n              kernel_regularizer = regularizer_l1_l2( 0.001, 0.001)) %>%\n  layer_dense(units = 50L, activation = \"relu\",\n              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%\n  layer_dense(units = 50L, activation = \"relu\",\n              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) %>%\n  layer_dense(units = ncol(Y), activation = \"softmax\",\n              kernel_regularizer = regularizer_l1_l2(0.001, 0.001)) \n\nmodel %>%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n#> Model: \"sequential_2\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_20 (Dense)                       (None, 50)                          1900          \n#>  dense_19 (Dense)                       (None, 50)                          2550          \n#>  dense_18 (Dense)                       (None, 50)                          2550          \n#>  dense_17 (Dense)                       (None, 2)                           102           \n#> ==========================================================================================\n#> Total params: 7,102\n#> Trainable params: 7,102\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\n\nearly = keras::callback_early_stopping(patience = 5L)\n\nmodel_history =\n  model %>%\n    fit(x = X, y = Y, \n        epochs = 100L, batch_size = 20L, \n        shuffle = TRUE, validation_split = 0.4, callbacks = c(early))\n\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_linear(ncol(X), 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 2L)\n)\n\nYT = apply(Y, 1, which.max)\n\ndataset_nasa = dataset(\n  name = \"nasa\",\n  initialize = function(nasa){\n    self$X = nasa$X\n    self$Y = nasa$Y\n  },\n  .getitem = function(i){\n    X = self$X[i,,drop = FALSE] %>% torch_tensor()\n    Y = self$Y[i] %>% torch_tensor()\n    list(X, Y)\n  },\n  .length = function(){\n    nrow(self$X)\n  })\n\ntrain_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), \n                      batch_size = 32, shuffle = TRUE)\ntest_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), \n                      batch_size = 32)\n\nmodel_torch$train()\n\nopt = optim_adam(model_torch$parameters, 0.01)\n\ntrain_losses = c()\ntest_losses = c()\nearly_epoch = 0\nmin_loss = Inf\npatience = 5\nfor(epoch in 1:50){\n  if(early_epoch >= patience){ break }\n  train_loss = c()\n  test_loss = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      l1 = 0\n      for(p in model_torch$parameters) l1 = l1 + torch_norm(p, 1)\n      l2 = 0\n      for(p in model_torch$parameters) l2 = l2 + torch_norm(p, 2)\n      loss = loss + 0.001*l1 + 0.001*l2\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  \n  coro::loop(\n    for(batch in test_dl){\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      test_loss = c(test_loss, loss$item())\n    }\n  )\n  \n    ### Early stopping ###\n  if(mean(test_loss) < min_loss){\n    min_loss = mean(test_loss)\n    early_epoch = 0\n  } else {\n    early_epoch = early_epoch + 1\n  }\n  ###\n\n  train_losses = c(train_losses, mean(train_loss))\n  test_losses = c(test_losses, mean(test_loss))\n  if(!epoch%%5) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 5: 0.446946\n#> Loss at epoch 10: 0.334997\n#> Loss at epoch 15: 0.267128\n\nmatplot(cbind(train_losses, test_losses), type = \"o\", pch = c(15, 16),\n        col = c(\"darkblue\", \"darkred\"), lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nlegend(\"topright\", bty = \"n\", col = c(\"darkblue\", \"darkred\"),\n       lty = 1, pch = c(15, 16), legend = c(\"Train loss\", \"Val loss\") )\nlibrary(cito)\ndata_train = data.frame(target = apply(Y, 1, which.max)-1, X)\nmodel_cito = dnn(\n  target~., \n  data = data_train,\n  loss = \"binomial\",\n  validation = 0.4,\n  hidden = rep(50L, 3L),\n  activation = rep(\"relu\", 3),\n  lambda = 0.001,\n  alpha = 0.5,\n  early_stopping = 5.\n)\n#> Loss at epoch 1: training: 0.720, validation: 0.619, lr: 0.01000#> Loss at epoch 2: training: 0.521, validation: 0.605, lr: 0.01000\n#> Loss at epoch 3: training: 0.449, validation: 0.622, lr: 0.01000\n#> Loss at epoch 4: training: 0.387, validation: 0.680, lr: 0.01000\n#> Loss at epoch 5: training: 0.330, validation: 0.827, lr: 0.01000\n#> Loss at epoch 6: training: 0.290, validation: 0.924, lr: 0.01000\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dropout(0.2) %>%\n  layer_dense(units = 50L, activation = \"relu\", input_shape = ncol(X)) %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = 50L, activation = \"relu\") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = 50L, activation = \"relu\") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = ncol(Y), activation = \"softmax\") \n\nmodel %>%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 keras::optimizer_adamax(learning_rate = 0.001))\n\nmodel_history =\n  model %>%\n    fit(x = X, y = Y, \n        epochs = 100L, batch_size = 20L, \n        shuffle = TRUE, validation_split = 0.4)\n\nplot(model_history)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = nn_sequential(\n  nn_dropout(0.2),\n  nn_linear(ncol(X), 50L),\n  nn_relu(),\n  nn_dropout(0.5),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_dropout(0.5),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_dropout(0.5),\n  nn_linear(50L, 2L)\n)\n\nYT = apply(Y, 1, which.max)\n\ndataset_nasa = dataset(\n  name = \"nasa\",\n  initialize = function(nasa){\n    self$X = nasa$X\n    self$Y = nasa$Y\n  },\n  .getitem = function(i){\n    X = self$X[i,,drop = FALSE] %>% torch_tensor()\n    Y = self$Y[i] %>% torch_tensor()\n    list(X, Y)\n  },\n  .length = function(){\n    nrow(self$X)\n  })\n\ntrain_dl = dataloader(dataset_nasa(list(X = X[1:400,], Y = YT[1:400])), \n                      batch_size = 32, shuffle = TRUE)\ntest_dl = dataloader(dataset_nasa(list(X = X[101:500,], Y = YT[101:500])), \n                      batch_size = 32)\n\nmodel_torch$train()\n\nopt = optim_adam(model_torch$parameters, 0.01)\n\ntrain_losses = c()\ntest_losses = c()\nearly_epoch = 0\nmin_loss = Inf\npatience = 5\nfor(epoch in 1:50){\n  if(early_epoch >= patience){ break }\n  \n  train_loss = c()\n  test_loss = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  \n  coro::loop(\n    for(batch in test_dl){\n      pred = model_torch(batch[[1]]$squeeze())\n      loss = nnf_cross_entropy(pred, batch[[2]]$squeeze(), reduction = \"mean\")\n      test_loss = c(test_loss, loss$item())\n    }\n  )\n  \n  ### Early stopping ###\n  if(mean(test_loss) < min_loss){\n    min_loss = mean(test_loss)\n    early_epoch = 0\n  } else {\n    early_epoch = early_epoch + 1\n  }\n  ###\n  \n  train_losses = c(train_losses, mean(train_loss))\n  test_losses = c(test_losses, mean(test_loss))\n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 1: 0.621523\n#> Loss at epoch 2: 0.499234\n#> Loss at epoch 3: 0.461054\n#> Loss at epoch 4: 0.459289\n#> Loss at epoch 5: 0.424963\n#> Loss at epoch 6: 0.395236\n#> Loss at epoch 7: 0.439434\n#> Loss at epoch 8: 0.403502\n#> Loss at epoch 9: 0.418872\n#> Loss at epoch 10: 0.395561\n#> Loss at epoch 11: 0.389564\n#> Loss at epoch 12: 0.400194\n#> Loss at epoch 13: 0.429872\n#> Loss at epoch 14: 0.434117\n#> Loss at epoch 15: 0.368484\n#> Loss at epoch 16: 0.380166\n\nmodel_torch$eval() # to turn off dropout\n\nmatplot(cbind(train_losses, test_losses), type = \"o\", pch = c(15, 16),\n        col = c(\"darkblue\", \"darkred\"), lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\nlegend(\"topright\", bty = \"n\", col = c(\"darkblue\", \"darkred\"),\n       lty = 1, pch = c(15, 16), legend = c(\"Train loss\", \"Val loss\") )\nlibrary(cito)\ndata_train = data.frame(target = apply(Y, 1, which.max)-1, X)\nmodel_cito = dnn(\n  target~., \n  data = data_train,\n  loss = \"binomial\",\n  validation = 0.4,\n  hidden = rep(50L, 3L),\n  activation = rep(\"relu\", 3),\n  dropout = 0.5,\n  early_stopping = 5.\n)\n#> Loss at epoch 1: training: 0.534, validation: 0.591, lr: 0.01000#> Loss at epoch 2: training: 0.485, validation: 0.485, lr: 0.01000\n#> Loss at epoch 3: training: 0.433, validation: 0.472, lr: 0.01000\n#> Loss at epoch 4: training: 0.410, validation: 0.506, lr: 0.01000\n#> Loss at epoch 5: training: 0.364, validation: 0.520, lr: 0.01000\n#> Loss at epoch 6: training: 0.380, validation: 0.538, lr: 0.01000\n#> Loss at epoch 7: training: 0.342, validation: 0.609, lr: 0.01000"},{"path":"deep.html","id":"exercises-6","chapter":"6 Deep Learning","heading":"6.2.1 Exercises","text":"section, go tuning “deep” neural network, using NASA data set kaggle (available via EcoData, see data description via help). can start immediately, get inspiration reading section (“Case study: dropout early stopping deep neural network”).basic network problem follows:Data preparation:KerasTorchCitoGo trough code line line try understand . Especially focus general machine learning workflow (remember general steps), new type imputation hand coded 10-fold cross-validation (loop).Use early stopping dropout (.e. use options, explained book now) within algorithm compare .\nTry tune network (play around number layers, width layers, dropout layers, early stopping regularization ) make better predicitons (monitor training validation loss).end, submit predictions submission server (time can also transfer new knowledge titanic data set)!\nBetter predictions:","code":"\nlibrary(tidyverse)\nlibrary(missRanger)\nlibrary(Metrics)\nlibrary(EcoData)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata(\"nasa\")\ndata = nasa\n\ndata$subset = ifelse(is.na(data$Hazardous), \"test\", \"train\")\n\n## Explore and clean data.\nstr(data)\n#> 'data.frame':    4687 obs. of  41 variables:\n#>  $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n#>  $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n#>  $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n#>  $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n#>  $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n#>  $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n#>  $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n#>  $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n#>  $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n#>  $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n#>  $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n#>  $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n#>  $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n#>  $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n#>  $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n#>  $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n#>  $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n#>  $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n#>  $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n#>  $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n#>  $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n#>  $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n#>  $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n#>  $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n#>  $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n#>  $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n#>  $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n#>  $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n#>  $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n#>  $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n#>  $ Orbital.Period              : num  419 1040 682 427 503 ...\n#>  $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n#>  $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n#>  $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n#>  $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n#>  $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n#>  $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n#>  $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n#>  $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n#>  $ subset                      : chr  \"train\" \"train\" \"train\" \"train\" ...\nsummary(data)\n#>  Neo.Reference.ID       Name         Absolute.Magnitude Est.Dia.in.KM.min.\n#>  Min.   :2000433   Min.   :2000433   Min.   :11.16      Min.   : 0.00101  \n#>  1st Qu.:3102682   1st Qu.:3102683   1st Qu.:20.10      1st Qu.: 0.03346  \n#>  Median :3514800   Median :3514800   Median :21.90      Median : 0.11080  \n#>  Mean   :3272675   Mean   :3273113   Mean   :22.27      Mean   : 0.20523  \n#>  3rd Qu.:3690987   3rd Qu.:3690385   3rd Qu.:24.50      3rd Qu.: 0.25384  \n#>  Max.   :3781897   Max.   :3781897   Max.   :32.10      Max.   :15.57955  \n#>  NA's   :53        NA's   :57        NA's   :36         NA's   :60        \n#>  Est.Dia.in.KM.max. Est.Dia.in.M.min.   Est.Dia.in.M.max.  Est.Dia.in.Miles.min.\n#>  Min.   : 0.00226   Min.   :    1.011   Min.   :    2.26   Min.   :0.00063      \n#>  1st Qu.: 0.07482   1st Qu.:   33.462   1st Qu.:   74.82   1st Qu.:0.02079      \n#>  Median : 0.24777   Median :  110.804   Median :  247.77   Median :0.06885      \n#>  Mean   : 0.45754   Mean   :  204.649   Mean   :  458.45   Mean   :0.12734      \n#>  3rd Qu.: 0.56760   3rd Qu.:  253.837   3rd Qu.:  567.60   3rd Qu.:0.15773      \n#>  Max.   :34.83694   Max.   :15579.552   Max.   :34836.94   Max.   :9.68068      \n#>  NA's   :23         NA's   :29          NA's   :46         NA's   :42           \n#>  Est.Dia.in.Miles.max. Est.Dia.in.Feet.min. Est.Dia.in.Feet.max. Close.Approach.Date\n#>  Min.   : 0.00140      Min.   :    3.32     Min.   :     7.41    2016-07-22:  18    \n#>  1st Qu.: 0.04649      1st Qu.:  109.78     1st Qu.:   245.49    2015-01-15:  17    \n#>  Median : 0.15395      Median :  363.53     Median :   812.88    2015-02-15:  16    \n#>  Mean   : 0.28486      Mean   :  670.44     Mean   :  1500.77    2007-11-08:  15    \n#>  3rd Qu.: 0.35269      3rd Qu.:  832.80     3rd Qu.:  1862.19    2012-01-15:  15    \n#>  Max.   :21.64666      Max.   :51114.02     Max.   :114294.42    (Other)   :4577    \n#>  NA's   :50            NA's   :21           NA's   :46           NA's      :  29    \n#>  Epoch.Date.Close.Approach Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr\n#>  Min.   :7.889e+11         Min.   : 0.3355              Min.   :  1208             \n#>  1st Qu.:1.016e+12         1st Qu.: 8.4497              1st Qu.: 30399             \n#>  Median :1.203e+12         Median :12.9370              Median : 46532             \n#>  Mean   :1.180e+12         Mean   :13.9848              Mean   : 50298             \n#>  3rd Qu.:1.356e+12         3rd Qu.:18.0774              3rd Qu.: 65068             \n#>  Max.   :1.473e+12         Max.   :44.6337              Max.   :160681             \n#>  NA's   :43                NA's   :27                   NA's   :28                 \n#>  Miles.per.hour    Miss.Dist..Astronomical. Miss.Dist..lunar.   Miss.Dist..kilometers.\n#>  Min.   :  750.5   Min.   :0.00018          Min.   :  0.06919   Min.   :   26610      \n#>  1st Qu.:18846.7   1st Qu.:0.13341          1st Qu.: 51.89874   1st Qu.:19964907      \n#>  Median :28893.7   Median :0.26497          Median :103.19415   Median :39685408      \n#>  Mean   :31228.0   Mean   :0.25690          Mean   : 99.91366   Mean   :38436154      \n#>  3rd Qu.:40436.9   3rd Qu.:0.38506          3rd Qu.:149.59244   3rd Qu.:57540318      \n#>  Max.   :99841.2   Max.   :0.49988          Max.   :194.45491   Max.   :74781600      \n#>  NA's   :38        NA's   :60               NA's   :30          NA's   :56            \n#>  Miss.Dist..miles.  Orbiting.Body    Orbit.ID             Orbit.Determination.Date\n#>  Min.   :   16535   Earth:4665    Min.   :  1.00   2017-06-21 06:17:20:   9       \n#>  1st Qu.:12454813   NA's :  22    1st Qu.:  9.00   2017-04-06 08:57:13:   8       \n#>  Median :24662435                 Median : 16.00   2017-04-06 09:24:24:   8       \n#>  Mean   :23885560                 Mean   : 28.34   2017-04-06 08:24:13:   7       \n#>  3rd Qu.:35714721                 3rd Qu.: 31.00   2017-04-06 08:26:19:   7       \n#>  Max.   :46467132                 Max.   :611.00   (Other)            :4622       \n#>  NA's   :27                       NA's   :33       NA's               :  26       \n#>  Orbit.Uncertainity Minimum.Orbit.Intersection Jupiter.Tisserand.Invariant\n#>  Min.   :0.000      Min.   :0.00000            Min.   :2.196              \n#>  1st Qu.:0.000      1st Qu.:0.01435            1st Qu.:4.047              \n#>  Median :3.000      Median :0.04653            Median :5.071              \n#>  Mean   :3.521      Mean   :0.08191            Mean   :5.056              \n#>  3rd Qu.:6.000      3rd Qu.:0.12150            3rd Qu.:6.017              \n#>  Max.   :9.000      Max.   :0.47789            Max.   :9.025              \n#>  NA's   :49         NA's   :137                NA's   :56                 \n#>  Epoch.Osculation   Eccentricity     Semi.Major.Axis   Inclination      \n#>  Min.   :2450164   Min.   :0.00752   Min.   :0.6159   Min.   : 0.01451  \n#>  1st Qu.:2458000   1st Qu.:0.24086   1st Qu.:1.0012   1st Qu.: 4.93290  \n#>  Median :2458000   Median :0.37251   Median :1.2422   Median :10.27694  \n#>  Mean   :2457723   Mean   :0.38267   Mean   :1.4009   Mean   :13.36159  \n#>  3rd Qu.:2458000   3rd Qu.:0.51256   3rd Qu.:1.6782   3rd Qu.:19.47848  \n#>  Max.   :2458020   Max.   :0.96026   Max.   :5.0720   Max.   :75.40667  \n#>  NA's   :60        NA's   :39        NA's   :53       NA's   :42        \n#>  Asc.Node.Longitude Orbital.Period   Perihelion.Distance Perihelion.Arg    \n#>  Min.   :  0.0019   Min.   : 176.6   Min.   :0.08074     Min.   :  0.0069  \n#>  1st Qu.: 83.1849   1st Qu.: 365.9   1st Qu.:0.63038     1st Qu.: 95.6430  \n#>  Median :172.6347   Median : 504.9   Median :0.83288     Median :189.7729  \n#>  Mean   :172.1717   Mean   : 635.5   Mean   :0.81316     Mean   :184.0185  \n#>  3rd Qu.:254.8804   3rd Qu.: 793.1   3rd Qu.:0.99718     3rd Qu.:271.9535  \n#>  Max.   :359.9059   Max.   :4172.2   Max.   :1.29983     Max.   :359.9931  \n#>  NA's   :60         NA's   :46       NA's   :22          NA's   :48        \n#>  Aphelion.Dist    Perihelion.Time    Mean.Anomaly       Mean.Motion       Equinox    \n#>  Min.   :0.8038   Min.   :2450100   Min.   :  0.0032   Min.   :0.08628   J2000:4663  \n#>  1st Qu.:1.2661   1st Qu.:2457815   1st Qu.: 87.0069   1st Qu.:0.45147   NA's :  24  \n#>  Median :1.6182   Median :2457972   Median :186.0219   Median :0.71137               \n#>  Mean   :1.9864   Mean   :2457726   Mean   :181.2882   Mean   :0.73732               \n#>  3rd Qu.:2.4497   3rd Qu.:2458108   3rd Qu.:276.6418   3rd Qu.:0.98379               \n#>  Max.   :8.9839   Max.   :2458839   Max.   :359.9180   Max.   :2.03900               \n#>  NA's   :38       NA's   :59        NA's   :40         NA's   :48                    \n#>    Hazardous        subset         \n#>  Min.   :0.000   Length:4687       \n#>  1st Qu.:0.000   Class :character  \n#>  Median :0.000   Mode  :character  \n#>  Mean   :0.176                     \n#>  3rd Qu.:0.000                     \n#>  Max.   :1.000                     \n#>  NA's   :4187\n\n# Remove Equinox and other features.\ndata = data %>% select(-Equinox, -Orbiting.Body,\n                       -Orbit.Determination.Date, -Close.Approach.Date)\n\n# Impute missing values using a random forest.\nimputed = data %>%\n  select(-Hazardous) %>%\n    missRanger::missRanger(maxiter = 5L, num.trees = 20L)\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion\n#>   Variables used to impute:  Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, subset\n#> iter 1:  ...................................\n#> iter 2:  ...................................\n#> iter 3:  ...................................\n#> iter 4:  ...................................\n\n# See the usual function call:\n# data_impute = data %>% select(-Hazardous)\n# imputed = missRanger::missRanger(data_impute, maxiter = 5L, num.trees = 20L)\n\n# Scale data.\ndata = cbind(\n  data %>% select(Hazardous),\n  scale(imputed %>% select(-subset)),\n  data.frame(\"subset\" = data$subset)\n)\n\n## Outer split.\ntrain = data[data$subset == \"train\", ]\ntest = data[data$subset == \"test\", ]\n\ntrain = train %>% select(-subset)\ntest = test %>% select(-subset)\n\n## 10-fold cross-validation:\nlen = nrow(train)\nord = sample.int(len)\nk = 10\ncv_indices = lapply(0:9, function(i) sort(ord[(i*len/k + 1):((i+1)*len/k)]))\n\nresult = matrix(NA, 10L, 2L)\ncolnames(result) = c(\"train_auc\", \"test_auc\")\n\nfor(i in 1:10) {\n  indices = cv_indices[[i]]\n  sub_train = train[-indices,] # Leave one \"bucket\" out.\n  sub_test = train[indices,]\n  \n  # \"Deep\" neural networks and regularization:\n  model = keras_model_sequential()\n  model %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             input_shape = ncol(sub_train) - 1L) %>%\n  layer_dense(units = 100L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"sigmoid\")\n  \n  model %>%\n  keras::compile(loss = loss_binary_crossentropy, \n                 optimizer = optimizer_adamax(0.01))\n  \n  model_history = \n   model %>%\n     fit(\n       x = as.matrix(sub_train %>% select(-Hazardous)),\n       y = as.matrix(sub_train %>% select(Hazardous)),\n       validation_split = 0.2,\n       epochs = 35L, batch = 50L, shuffle = TRUE\n      )\n  \n  plot(model_history)\n  \n  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))\n  pred_test = predict(model, as.matrix(sub_test %>% select(-Hazardous)))\n  \n  # AUC: Area under the (ROC) curve, this is a performance measure [0, 1].\n  # 0.5 is worst for a binary classifier.\n  # 1 perfectly classifies all samples, 0 perfectly misclassifies all samples.\n  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)\n  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)\n}\nprint(result)\n#>       train_auc  test_auc\n#>  [1,] 0.9981234 0.9268293\n#>  [2,] 0.9984570 0.9866667\n#>  [3,] 0.9960277 0.9822222\n#>  [4,] 0.9965881 0.9650000\n#>  [5,] 0.9987838 0.9017857\n#>  [6,] 0.9976351 0.8809524\n#>  [7,] 0.9932803 0.9824561\n#>  [8,] 0.9933126 0.9972900\n#>  [9,] 0.9944652 0.9484127\n#> [10,] 0.9968919 0.9642857\n(colMeans(result))\n#> train_auc  test_auc \n#> 0.9963565 0.9535901\n\n# The model setup seems to be fine.\n\n## Train and predict for outer validation split (on the complete training data):\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\",\n              input_shape = ncol(sub_train) - 1L) %>%\n  layer_dense(units = 100L, activation = \"relu\") %>%\n  layer_dense(units = 1L, activation = \"sigmoid\")\n\nmodel %>%\n  keras::compile(loss = loss_binary_crossentropy, \n                 optimizer = optimizer_adamax(0.01))\n\nmodel_history = \n  model %>%\n    fit(\n      x = as.matrix(train %>% select(-Hazardous)),\n      y = as.matrix(train %>% select(Hazardous)),\n      validation_split = 0.2,\n      epochs = 35L, batch = 50L, shuffle = TRUE\n    )\n\npred = round(predict(model, as.matrix(test[,-1])))\n\nwrite.csv(data.frame(y = pred), file = \"submission_DNN.csv\")\n\nlibrary(torch)\ntorch_dataset = torch::dataset(\n    name = \"data\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\n\n\n\nfor(i in 1:10) {\n  indices = cv_indices[[i]]\n  sub_train = train[-indices,] # Leave one \"bucket\" out.\n  sub_test = train[indices,]\n  \n  # \"Deep\" neural networks and regularization:\n  \n  model_torch = nn_sequential(\n    nn_linear(in_features = ncol(sub_train)-1, out_features = 50L,  bias = TRUE),\n    nn_relu(),\n    nn_linear(in_features = 50L, out_features = 50L,  bias = TRUE),\n    nn_relu(),\n    nn_linear(in_features = 50L, out_features = 1L,  bias = TRUE),\n    nn_sigmoid()\n  )\n  \n  opt = optim_adam(params = model_torch$parameters, lr = 0.1)\n  \n  indices = sample.int(nrow(sub_train), 0.8*nrow(sub_train))\n  dataset = torch_dataset((as.matrix(sub_train %>% select(-Hazardous)))[indices, ],\n                          (as.matrix(sub_train %>% select(Hazardous)))[indices, ,drop=FALSE])\n  dataset_val = torch_dataset((as.matrix(sub_train %>% select(-Hazardous)))[-indices, ],\n                              (as.matrix(sub_train %>% select(Hazardous)))[-indices, ,drop=FALSE])\n  dataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n  dataloader_val = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n  \n  epochs = 50L\n  train_losses = c()\n  val_losses = c()\n  lambda = torch_tensor(0.01)\n  alpha = torch_tensor(0.2)\n  for(epoch in 1:epochs){\n    train_loss = c()\n    val_loss = c()\n    coro::loop(\n      for(batch in dataloader) { \n        opt$zero_grad()\n        pred = model_torch(batch[[1]])\n        loss = nnf_binary_cross_entropy( pred , batch[[2]])\n        for(p in model_torch$parameters) {\n          if(length(dim(p)) > 1) loss = loss + lambda*((1-alpha)*torch_norm(p, 1L) + alpha*torch_norm(p, 2L))\n        }\n        loss$backward()\n        opt$step()\n        train_loss = c(train_loss, loss$item())\n      }\n    )\n    ## Calculate validation loss ##\n    coro::loop(\n      for(batch in dataloader_val) { \n        pred = model_torch(batch[[1]])\n        loss = nnf_binary_cross_entropy(pred, batch[[2]])\n        val_loss = c(val_loss, loss$item())\n      }\n    )\n    \n    train_losses = c(train_losses, mean(train_loss))\n    val_losses = c(val_losses, mean(val_loss))\n    if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f  Val loss: %3f\\n\", epoch, mean(train_loss), mean(val_loss)))\n  }\n  \n  matplot(cbind(train_losses, val_losses), type = \"o\", pch = c(15, 16),\n          col = c(\"darkblue\", \"darkred\"), lty = 1, xlab = \"Epoch\",\n          ylab = \"Loss\", las = 1)\n  legend(\"topright\", bty = \"n\", \n         legend = c(\"Train loss\", \"Val loss\"), \n         pch = c(15, 16), col = c(\"darkblue\", \"darkred\"))\n  \n  \n  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))\n  pred_train = model_torch(torch_tensor(as.matrix(sub_train %>% select(-Hazardous)))) %>% as.numeric\n  pred_test = model_torch(torch_tensor(as.matrix(sub_test %>% select(-Hazardous)))) %>% as.numeric\n  \n  # AUC: Area under the (ROC) curve, this is a performance measure [0, 1].\n  # 0.5 is worst for a binary classifier.\n  # 1 perfectly classifies all samples, 0 perfectly misclassifies all samples.\n  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)\n  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)\n}\n#> Loss at epoch 10: 0.829017  Val loss: 0.182509\n#> Loss at epoch 20: 0.701753  Val loss: 0.172302\n#> Loss at epoch 30: 0.698969  Val loss: 0.157188\n#> Loss at epoch 40: 0.700181  Val loss: 0.146019\n#> Loss at epoch 50: 0.705486  Val loss: 0.176424#> Loss at epoch 10: 0.763818  Val loss: 0.165957\n#> Loss at epoch 20: 0.742365  Val loss: 0.169747\n#> Loss at epoch 30: 0.686639  Val loss: 0.125889\n#> Loss at epoch 40: 0.688209  Val loss: 0.113968\n#> Loss at epoch 50: 0.700927  Val loss: 0.144689#> Loss at epoch 10: 0.803255  Val loss: 0.179034\n#> Loss at epoch 20: 0.728545  Val loss: 0.175123\n#> Loss at epoch 30: 0.698292  Val loss: 0.110397\n#> Loss at epoch 40: 0.690141  Val loss: 0.101464\n#> Loss at epoch 50: 0.710400  Val loss: 0.165217#> Loss at epoch 10: 0.774479  Val loss: 0.124245\n#> Loss at epoch 20: 0.781940  Val loss: 0.100304\n#> Loss at epoch 30: 0.736063  Val loss: 0.113301\n#> Loss at epoch 40: 0.691388  Val loss: 0.081147\n#> Loss at epoch 50: 0.730457  Val loss: 0.119942#> Loss at epoch 10: 0.731244  Val loss: 0.160419\n#> Loss at epoch 20: 0.714435  Val loss: 0.166416\n#> Loss at epoch 30: 0.694688  Val loss: 0.132884\n#> Loss at epoch 40: 0.689699  Val loss: 0.124507\n#> Loss at epoch 50: 0.731581  Val loss: 0.145156#> Loss at epoch 10: 0.809190  Val loss: 0.179954\n#> Loss at epoch 20: 0.725614  Val loss: 0.119201\n#> Loss at epoch 30: 0.714013  Val loss: 0.113931\n#> Loss at epoch 40: 0.690072  Val loss: 0.169328\n#> Loss at epoch 50: 0.677388  Val loss: 0.102784#> Loss at epoch 10: 0.811071  Val loss: 0.219090\n#> Loss at epoch 20: 0.764542  Val loss: 0.147753\n#> Loss at epoch 30: 0.694446  Val loss: 0.168150\n#> Loss at epoch 40: 0.710823  Val loss: 0.141908\n#> Loss at epoch 50: 0.677895  Val loss: 0.128026#> Loss at epoch 10: 0.835809  Val loss: 0.181617\n#> Loss at epoch 20: 0.768913  Val loss: 0.161498\n#> Loss at epoch 30: 0.698743  Val loss: 0.167851\n#> Loss at epoch 40: 0.734401  Val loss: 0.189557\n#> Loss at epoch 50: 0.720234  Val loss: 0.122505#> Loss at epoch 10: 0.833461  Val loss: 0.142397\n#> Loss at epoch 20: 0.703127  Val loss: 0.137720\n#> Loss at epoch 30: 0.760898  Val loss: 0.162455\n#> Loss at epoch 40: 0.728296  Val loss: 0.127886\n#> Loss at epoch 50: 0.690591  Val loss: 0.135698#> Loss at epoch 10: 0.747076  Val loss: 0.217834\n#> Loss at epoch 20: 0.716065  Val loss: 0.165585\n#> Loss at epoch 30: 0.723334  Val loss: 0.185342\n#> Loss at epoch 40: 0.717497  Val loss: 0.166969\n#> Loss at epoch 50: 0.750742  Val loss: 0.220579\nprint(result)\n#>       train_auc  test_auc\n#>  [1,] 0.9734894 0.9945799\n#>  [2,] 0.9789239 1.0000000\n#>  [3,] 0.9741144 0.9866667\n#>  [4,] 0.9821133 0.9475000\n#>  [5,] 0.9843581 0.9523810\n#>  [6,] 0.9879730 0.8690476\n#>  [7,] 0.9793660 0.9978070\n#>  [8,] 0.9854993 1.0000000\n#>  [9,] 0.9762076 0.9861111\n#> [10,] 0.9428716 0.9732143\n(colMeans(result))\n#> train_auc  test_auc \n#> 0.9764917 0.9707308\n\n# The model setup seems to be fine.\n\n## Train and predict for outer validation split (on the complete training data):\nmodel_torch = nn_sequential(\n  nn_linear(in_features = ncol(sub_train)-1, out_features = 50L,  bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 50L, out_features = 50L,  bias = TRUE),\n  nn_relu(),\n  nn_linear(in_features = 50L, out_features = 1L,  bias = TRUE),\n  nn_sigmoid()\n)\n  \nopt = optim_adam(params = model_torch$parameters, lr = 0.1)\ndataset = torch_dataset((as.matrix(train %>% select(-Hazardous)))[indices, ],\n                        (as.matrix(train %>% select(Hazardous)))[indices, ,drop=FALSE])\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\nepochs = 50L\ntrain_losses = c()\nlambda = torch_tensor(0.01)\nalpha = torch_tensor(0.2)\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_binary_cross_entropy( pred , batch[[2]])\n      for(p in model_torch$parameters) {\n        if(length(dim(p)) > 1) loss = loss + lambda*((1-alpha)*torch_norm(p, 1L) + alpha*torch_norm(p, 2L))\n      }\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  \n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f \\n\", epoch, mean(train_loss)))\n}\n#> Loss at epoch 10: 0.821883 \n#> Loss at epoch 20: 0.777563 \n#> Loss at epoch 30: 0.720799 \n#> Loss at epoch 40: 0.713502 \n#> Loss at epoch 50: 0.682197\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = c(\"darkblue\"), lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n  \n\n\npred = round( model_torch(torch_tensor(as.matrix(test[,-1]))) %>% as.numeric )\n\nwrite.csv(data.frame(y = pred), file = \"submission_DNN.csv\")\nlibrary(cito)\n\nfor(i in 1:10) {\n  indices = cv_indices[[i]]\n  sub_train = train[-indices,] # Leave one \"bucket\" out.\n  sub_test = train[indices,]\n  \n  model_cito = dnn(Hazardous~., data = sub_train,validation = 0.2, hidden = rep(100L, 2L), activation = rep(\"relu\", 2), loss = \"binomial\")\n\n  pred_train = predict(model_cito, sub_train)\n  pred_test = predict(model_cito, sub_test)\n  \n  # AUC: Area under the (ROC) curve, this is a performance measure [0, 1].\n  # 0.5 is worst for a binary classifier.\n  # 1 perfectly classifies all samples, 0 perfectly misclassifies all samples.\n  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)\n  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)\n}\n#> Loss at epoch 1: training: 0.388, validation: 0.435, lr: 0.01000#> Loss at epoch 2: training: 0.254, validation: 0.373, lr: 0.01000\n#> Loss at epoch 3: training: 0.165, validation: 0.273, lr: 0.01000\n#> Loss at epoch 4: training: 0.143, validation: 0.428, lr: 0.01000\n#> Loss at epoch 5: training: 0.095, validation: 0.402, lr: 0.01000\n#> Loss at epoch 6: training: 0.166, validation: 0.322, lr: 0.01000\n#> Loss at epoch 7: training: 0.124, validation: 0.311, lr: 0.01000\n#> Loss at epoch 8: training: 0.105, validation: 0.335, lr: 0.01000\n#> Loss at epoch 9: training: 0.072, validation: 0.378, lr: 0.01000\n#> Loss at epoch 10: training: 0.057, validation: 0.249, lr: 0.01000\n#> Loss at epoch 11: training: 0.056, validation: 0.596, lr: 0.01000\n#> Loss at epoch 12: training: 0.027, validation: 0.498, lr: 0.01000\n#> Loss at epoch 13: training: 0.025, validation: 0.640, lr: 0.01000\n#> Loss at epoch 14: training: 0.015, validation: 0.661, lr: 0.01000\n#> Loss at epoch 15: training: 0.009, validation: 0.500, lr: 0.01000\n#> Loss at epoch 16: training: 0.009, validation: 0.588, lr: 0.01000\n#> Loss at epoch 17: training: 0.005, validation: 0.626, lr: 0.01000\n#> Loss at epoch 18: training: 0.004, validation: 0.694, lr: 0.01000\n#> Loss at epoch 19: training: 0.002, validation: 0.706, lr: 0.01000\n#> Loss at epoch 20: training: 0.001, validation: 0.714, lr: 0.01000\n#> Loss at epoch 21: training: 0.001, validation: 0.719, lr: 0.01000\n#> Loss at epoch 22: training: 0.001, validation: 0.737, lr: 0.01000\n#> Loss at epoch 23: training: 0.001, validation: 0.745, lr: 0.01000\n#> Loss at epoch 24: training: 0.001, validation: 0.754, lr: 0.01000\n#> Loss at epoch 25: training: 0.000, validation: 0.760, lr: 0.01000\n#> Loss at epoch 26: training: 0.000, validation: 0.768, lr: 0.01000\n#> Loss at epoch 27: training: 0.000, validation: 0.776, lr: 0.01000\n#> Loss at epoch 28: training: 0.000, validation: 0.782, lr: 0.01000\n#> Loss at epoch 29: training: 0.000, validation: 0.786, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 0.792, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.799, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.802, lr: 0.01000\n#> Loss at epoch 1: training: 0.435, validation: 0.363, lr: 0.01000#> Loss at epoch 2: training: 0.270, validation: 0.355, lr: 0.01000\n#> Loss at epoch 3: training: 0.218, validation: 0.328, lr: 0.01000\n#> Loss at epoch 4: training: 0.196, validation: 0.317, lr: 0.01000\n#> Loss at epoch 5: training: 0.153, validation: 0.320, lr: 0.01000\n#> Loss at epoch 6: training: 0.103, validation: 0.310, lr: 0.01000\n#> Loss at epoch 7: training: 0.090, validation: 0.347, lr: 0.01000\n#> Loss at epoch 8: training: 0.095, validation: 0.393, lr: 0.01000\n#> Loss at epoch 9: training: 0.081, validation: 0.400, lr: 0.01000\n#> Loss at epoch 10: training: 0.060, validation: 0.383, lr: 0.01000\n#> Loss at epoch 11: training: 0.061, validation: 0.415, lr: 0.01000\n#> Loss at epoch 12: training: 0.030, validation: 0.467, lr: 0.01000\n#> Loss at epoch 13: training: 0.029, validation: 0.470, lr: 0.01000\n#> Loss at epoch 14: training: 0.035, validation: 0.549, lr: 0.01000\n#> Loss at epoch 15: training: 0.031, validation: 0.496, lr: 0.01000\n#> Loss at epoch 16: training: 0.027, validation: 0.524, lr: 0.01000\n#> Loss at epoch 17: training: 0.039, validation: 0.555, lr: 0.01000\n#> Loss at epoch 18: training: 0.050, validation: 0.634, lr: 0.01000\n#> Loss at epoch 19: training: 0.069, validation: 0.547, lr: 0.01000\n#> Loss at epoch 20: training: 0.041, validation: 0.407, lr: 0.01000\n#> Loss at epoch 21: training: 0.023, validation: 0.458, lr: 0.01000\n#> Loss at epoch 22: training: 0.015, validation: 0.442, lr: 0.01000\n#> Loss at epoch 23: training: 0.009, validation: 0.473, lr: 0.01000\n#> Loss at epoch 24: training: 0.004, validation: 0.468, lr: 0.01000\n#> Loss at epoch 25: training: 0.002, validation: 0.480, lr: 0.01000\n#> Loss at epoch 26: training: 0.002, validation: 0.487, lr: 0.01000\n#> Loss at epoch 27: training: 0.001, validation: 0.497, lr: 0.01000\n#> Loss at epoch 28: training: 0.001, validation: 0.506, lr: 0.01000\n#> Loss at epoch 29: training: 0.001, validation: 0.513, lr: 0.01000\n#> Loss at epoch 30: training: 0.001, validation: 0.519, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.525, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.531, lr: 0.01000\n#> Loss at epoch 1: training: 0.472, validation: 0.358, lr: 0.01000#> Loss at epoch 2: training: 0.292, validation: 0.327, lr: 0.01000\n#> Loss at epoch 3: training: 0.218, validation: 0.238, lr: 0.01000\n#> Loss at epoch 4: training: 0.161, validation: 0.192, lr: 0.01000\n#> Loss at epoch 5: training: 0.108, validation: 0.237, lr: 0.01000\n#> Loss at epoch 6: training: 0.122, validation: 0.214, lr: 0.01000\n#> Loss at epoch 7: training: 0.086, validation: 0.258, lr: 0.01000\n#> Loss at epoch 8: training: 0.074, validation: 0.242, lr: 0.01000\n#> Loss at epoch 9: training: 0.086, validation: 0.342, lr: 0.01000\n#> Loss at epoch 10: training: 0.077, validation: 0.284, lr: 0.01000\n#> Loss at epoch 11: training: 0.060, validation: 0.246, lr: 0.01000\n#> Loss at epoch 12: training: 0.041, validation: 0.287, lr: 0.01000\n#> Loss at epoch 13: training: 0.038, validation: 0.373, lr: 0.01000\n#> Loss at epoch 14: training: 0.074, validation: 0.452, lr: 0.01000\n#> Loss at epoch 15: training: 0.059, validation: 0.278, lr: 0.01000\n#> Loss at epoch 16: training: 0.029, validation: 0.274, lr: 0.01000\n#> Loss at epoch 17: training: 0.018, validation: 0.315, lr: 0.01000\n#> Loss at epoch 18: training: 0.011, validation: 0.341, lr: 0.01000\n#> Loss at epoch 19: training: 0.009, validation: 0.345, lr: 0.01000\n#> Loss at epoch 20: training: 0.007, validation: 0.352, lr: 0.01000\n#> Loss at epoch 21: training: 0.005, validation: 0.360, lr: 0.01000\n#> Loss at epoch 22: training: 0.004, validation: 0.373, lr: 0.01000\n#> Loss at epoch 23: training: 0.003, validation: 0.377, lr: 0.01000\n#> Loss at epoch 24: training: 0.002, validation: 0.387, lr: 0.01000\n#> Loss at epoch 25: training: 0.001, validation: 0.393, lr: 0.01000\n#> Loss at epoch 26: training: 0.001, validation: 0.407, lr: 0.01000\n#> Loss at epoch 27: training: 0.001, validation: 0.417, lr: 0.01000\n#> Loss at epoch 28: training: 0.001, validation: 0.428, lr: 0.01000\n#> Loss at epoch 29: training: 0.001, validation: 0.437, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 0.445, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.455, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.464, lr: 0.01000\n#> Loss at epoch 1: training: 0.455, validation: 0.400, lr: 0.01000#> Loss at epoch 2: training: 0.257, validation: 0.339, lr: 0.01000\n#> Loss at epoch 3: training: 0.196, validation: 0.305, lr: 0.01000\n#> Loss at epoch 4: training: 0.135, validation: 0.262, lr: 0.01000\n#> Loss at epoch 5: training: 0.089, validation: 0.270, lr: 0.01000\n#> Loss at epoch 6: training: 0.052, validation: 0.406, lr: 0.01000\n#> Loss at epoch 7: training: 0.042, validation: 0.604, lr: 0.01000\n#> Loss at epoch 8: training: 0.075, validation: 0.520, lr: 0.01000\n#> Loss at epoch 9: training: 0.119, validation: 0.606, lr: 0.01000\n#> Loss at epoch 10: training: 0.067, validation: 0.552, lr: 0.01000\n#> Loss at epoch 11: training: 0.036, validation: 0.429, lr: 0.01000\n#> Loss at epoch 12: training: 0.024, validation: 0.553, lr: 0.01000\n#> Loss at epoch 13: training: 0.013, validation: 0.544, lr: 0.01000\n#> Loss at epoch 14: training: 0.014, validation: 0.633, lr: 0.01000\n#> Loss at epoch 15: training: 0.021, validation: 0.620, lr: 0.01000\n#> Loss at epoch 16: training: 0.006, validation: 0.672, lr: 0.01000\n#> Loss at epoch 17: training: 0.003, validation: 0.725, lr: 0.01000\n#> Loss at epoch 18: training: 0.001, validation: 0.760, lr: 0.01000\n#> Loss at epoch 19: training: 0.001, validation: 0.771, lr: 0.01000\n#> Loss at epoch 20: training: 0.001, validation: 0.777, lr: 0.01000\n#> Loss at epoch 21: training: 0.001, validation: 0.783, lr: 0.01000\n#> Loss at epoch 22: training: 0.000, validation: 0.791, lr: 0.01000\n#> Loss at epoch 23: training: 0.000, validation: 0.801, lr: 0.01000\n#> Loss at epoch 24: training: 0.000, validation: 0.814, lr: 0.01000\n#> Loss at epoch 25: training: 0.000, validation: 0.824, lr: 0.01000\n#> Loss at epoch 26: training: 0.000, validation: 0.833, lr: 0.01000\n#> Loss at epoch 27: training: 0.000, validation: 0.841, lr: 0.01000\n#> Loss at epoch 28: training: 0.000, validation: 0.848, lr: 0.01000\n#> Loss at epoch 29: training: 0.000, validation: 0.854, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 0.859, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.863, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.867, lr: 0.01000\n#> Loss at epoch 1: training: 0.495, validation: 0.282, lr: 0.01000#> Loss at epoch 2: training: 0.299, validation: 0.243, lr: 0.01000\n#> Loss at epoch 3: training: 0.250, validation: 0.197, lr: 0.01000\n#> Loss at epoch 4: training: 0.204, validation: 0.171, lr: 0.01000\n#> Loss at epoch 5: training: 0.177, validation: 0.186, lr: 0.01000\n#> Loss at epoch 6: training: 0.154, validation: 0.166, lr: 0.01000\n#> Loss at epoch 7: training: 0.147, validation: 0.137, lr: 0.01000\n#> Loss at epoch 8: training: 0.146, validation: 0.174, lr: 0.01000\n#> Loss at epoch 9: training: 0.156, validation: 0.217, lr: 0.01000\n#> Loss at epoch 10: training: 0.152, validation: 0.176, lr: 0.01000\n#> Loss at epoch 11: training: 0.123, validation: 0.152, lr: 0.01000\n#> Loss at epoch 12: training: 0.093, validation: 0.118, lr: 0.01000\n#> Loss at epoch 13: training: 0.064, validation: 0.203, lr: 0.01000\n#> Loss at epoch 14: training: 0.031, validation: 0.171, lr: 0.01000\n#> Loss at epoch 15: training: 0.050, validation: 0.173, lr: 0.01000\n#> Loss at epoch 16: training: 0.064, validation: 0.191, lr: 0.01000\n#> Loss at epoch 17: training: 0.127, validation: 0.139, lr: 0.01000\n#> Loss at epoch 18: training: 0.092, validation: 0.167, lr: 0.01000\n#> Loss at epoch 19: training: 0.074, validation: 0.129, lr: 0.01000\n#> Loss at epoch 20: training: 0.067, validation: 0.354, lr: 0.01000\n#> Loss at epoch 21: training: 0.073, validation: 0.155, lr: 0.01000\n#> Loss at epoch 22: training: 0.142, validation: 0.143, lr: 0.01000\n#> Loss at epoch 23: training: 0.131, validation: 0.171, lr: 0.01000\n#> Loss at epoch 24: training: 0.044, validation: 0.241, lr: 0.01000\n#> Loss at epoch 25: training: 0.028, validation: 0.191, lr: 0.01000\n#> Loss at epoch 26: training: 0.019, validation: 0.186, lr: 0.01000\n#> Loss at epoch 27: training: 0.014, validation: 0.186, lr: 0.01000\n#> Loss at epoch 28: training: 0.011, validation: 0.198, lr: 0.01000\n#> Loss at epoch 29: training: 0.009, validation: 0.199, lr: 0.01000\n#> Loss at epoch 30: training: 0.007, validation: 0.208, lr: 0.01000\n#> Loss at epoch 31: training: 0.006, validation: 0.202, lr: 0.01000\n#> Loss at epoch 32: training: 0.005, validation: 0.209, lr: 0.01000\n#> Loss at epoch 1: training: 0.453, validation: 0.263, lr: 0.01000#> Loss at epoch 2: training: 0.291, validation: 0.232, lr: 0.01000\n#> Loss at epoch 3: training: 0.217, validation: 0.174, lr: 0.01000\n#> Loss at epoch 4: training: 0.134, validation: 0.155, lr: 0.01000\n#> Loss at epoch 5: training: 0.100, validation: 0.254, lr: 0.01000\n#> Loss at epoch 6: training: 0.213, validation: 0.385, lr: 0.01000\n#> Loss at epoch 7: training: 0.228, validation: 0.137, lr: 0.01000\n#> Loss at epoch 8: training: 0.109, validation: 0.195, lr: 0.01000\n#> Loss at epoch 9: training: 0.054, validation: 0.208, lr: 0.01000\n#> Loss at epoch 10: training: 0.027, validation: 0.299, lr: 0.01000\n#> Loss at epoch 11: training: 0.014, validation: 0.368, lr: 0.01000\n#> Loss at epoch 12: training: 0.007, validation: 0.394, lr: 0.01000\n#> Loss at epoch 13: training: 0.004, validation: 0.403, lr: 0.01000\n#> Loss at epoch 14: training: 0.003, validation: 0.420, lr: 0.01000\n#> Loss at epoch 15: training: 0.002, validation: 0.438, lr: 0.01000\n#> Loss at epoch 16: training: 0.001, validation: 0.454, lr: 0.01000\n#> Loss at epoch 17: training: 0.001, validation: 0.466, lr: 0.01000\n#> Loss at epoch 18: training: 0.001, validation: 0.480, lr: 0.01000\n#> Loss at epoch 19: training: 0.001, validation: 0.491, lr: 0.01000\n#> Loss at epoch 20: training: 0.001, validation: 0.500, lr: 0.01000\n#> Loss at epoch 21: training: 0.000, validation: 0.511, lr: 0.01000\n#> Loss at epoch 22: training: 0.000, validation: 0.521, lr: 0.01000\n#> Loss at epoch 23: training: 0.000, validation: 0.528, lr: 0.01000\n#> Loss at epoch 24: training: 0.000, validation: 0.541, lr: 0.01000\n#> Loss at epoch 25: training: 0.000, validation: 0.550, lr: 0.01000\n#> Loss at epoch 26: training: 0.000, validation: 0.554, lr: 0.01000\n#> Loss at epoch 27: training: 0.000, validation: 0.569, lr: 0.01000\n#> Loss at epoch 28: training: 0.000, validation: 0.569, lr: 0.01000\n#> Loss at epoch 29: training: 0.000, validation: 0.577, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 0.578, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.585, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.591, lr: 0.01000\n#> Loss at epoch 1: training: 0.480, validation: 0.300, lr: 0.01000#> Loss at epoch 2: training: 0.310, validation: 0.221, lr: 0.01000\n#> Loss at epoch 3: training: 0.257, validation: 0.185, lr: 0.01000\n#> Loss at epoch 4: training: 0.201, validation: 0.176, lr: 0.01000\n#> Loss at epoch 5: training: 0.162, validation: 0.213, lr: 0.01000\n#> Loss at epoch 6: training: 0.140, validation: 0.132, lr: 0.01000\n#> Loss at epoch 7: training: 0.087, validation: 0.173, lr: 0.01000\n#> Loss at epoch 8: training: 0.101, validation: 0.152, lr: 0.01000\n#> Loss at epoch 9: training: 0.085, validation: 0.180, lr: 0.01000\n#> Loss at epoch 10: training: 0.061, validation: 0.178, lr: 0.01000\n#> Loss at epoch 11: training: 0.044, validation: 0.158, lr: 0.01000\n#> Loss at epoch 12: training: 0.021, validation: 0.180, lr: 0.01000\n#> Loss at epoch 13: training: 0.011, validation: 0.158, lr: 0.01000\n#> Loss at epoch 14: training: 0.007, validation: 0.203, lr: 0.01000\n#> Loss at epoch 15: training: 0.004, validation: 0.216, lr: 0.01000\n#> Loss at epoch 16: training: 0.002, validation: 0.198, lr: 0.01000\n#> Loss at epoch 17: training: 0.002, validation: 0.206, lr: 0.01000\n#> Loss at epoch 18: training: 0.001, validation: 0.215, lr: 0.01000\n#> Loss at epoch 19: training: 0.001, validation: 0.221, lr: 0.01000\n#> Loss at epoch 20: training: 0.001, validation: 0.221, lr: 0.01000\n#> Loss at epoch 21: training: 0.001, validation: 0.224, lr: 0.01000\n#> Loss at epoch 22: training: 0.001, validation: 0.226, lr: 0.01000\n#> Loss at epoch 23: training: 0.001, validation: 0.229, lr: 0.01000\n#> Loss at epoch 24: training: 0.001, validation: 0.234, lr: 0.01000\n#> Loss at epoch 25: training: 0.000, validation: 0.234, lr: 0.01000\n#> Loss at epoch 26: training: 0.000, validation: 0.231, lr: 0.01000\n#> Loss at epoch 27: training: 0.000, validation: 0.231, lr: 0.01000\n#> Loss at epoch 28: training: 0.000, validation: 0.233, lr: 0.01000\n#> Loss at epoch 29: training: 0.000, validation: 0.233, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 0.232, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.233, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.235, lr: 0.01000\n#> Loss at epoch 1: training: 0.450, validation: 0.334, lr: 0.01000#> Loss at epoch 2: training: 0.293, validation: 0.277, lr: 0.01000\n#> Loss at epoch 3: training: 0.225, validation: 0.245, lr: 0.01000\n#> Loss at epoch 4: training: 0.167, validation: 0.249, lr: 0.01000\n#> Loss at epoch 5: training: 0.108, validation: 0.308, lr: 0.01000\n#> Loss at epoch 6: training: 0.089, validation: 0.344, lr: 0.01000\n#> Loss at epoch 7: training: 0.073, validation: 0.424, lr: 0.01000\n#> Loss at epoch 8: training: 0.067, validation: 0.459, lr: 0.01000\n#> Loss at epoch 9: training: 0.073, validation: 0.444, lr: 0.01000\n#> Loss at epoch 10: training: 0.094, validation: 0.433, lr: 0.01000\n#> Loss at epoch 11: training: 0.138, validation: 0.378, lr: 0.01000\n#> Loss at epoch 12: training: 0.069, validation: 0.357, lr: 0.01000\n#> Loss at epoch 13: training: 0.043, validation: 0.332, lr: 0.01000\n#> Loss at epoch 14: training: 0.021, validation: 0.420, lr: 0.01000\n#> Loss at epoch 15: training: 0.010, validation: 0.475, lr: 0.01000\n#> Loss at epoch 16: training: 0.006, validation: 0.511, lr: 0.01000\n#> Loss at epoch 17: training: 0.004, validation: 0.547, lr: 0.01000\n#> Loss at epoch 18: training: 0.003, validation: 0.561, lr: 0.01000\n#> Loss at epoch 19: training: 0.003, validation: 0.571, lr: 0.01000\n#> Loss at epoch 20: training: 0.003, validation: 0.590, lr: 0.01000\n#> Loss at epoch 21: training: 0.002, validation: 0.606, lr: 0.01000\n#> Loss at epoch 22: training: 0.002, validation: 0.614, lr: 0.01000\n#> Loss at epoch 23: training: 0.002, validation: 0.639, lr: 0.01000\n#> Loss at epoch 24: training: 0.002, validation: 0.655, lr: 0.01000\n#> Loss at epoch 25: training: 0.002, validation: 0.657, lr: 0.01000\n#> Loss at epoch 26: training: 0.002, validation: 0.664, lr: 0.01000\n#> Loss at epoch 27: training: 0.002, validation: 0.669, lr: 0.01000\n#> Loss at epoch 28: training: 0.002, validation: 0.673, lr: 0.01000\n#> Loss at epoch 29: training: 0.002, validation: 0.677, lr: 0.01000\n#> Loss at epoch 30: training: 0.002, validation: 0.681, lr: 0.01000\n#> Loss at epoch 31: training: 0.002, validation: 0.685, lr: 0.01000\n#> Loss at epoch 32: training: 0.002, validation: 0.688, lr: 0.01000\n#> Loss at epoch 1: training: 0.422, validation: 0.342, lr: 0.01000#> Loss at epoch 2: training: 0.245, validation: 0.315, lr: 0.01000\n#> Loss at epoch 3: training: 0.172, validation: 0.229, lr: 0.01000\n#> Loss at epoch 4: training: 0.126, validation: 0.215, lr: 0.01000\n#> Loss at epoch 5: training: 0.098, validation: 0.239, lr: 0.01000\n#> Loss at epoch 6: training: 0.090, validation: 0.262, lr: 0.01000\n#> Loss at epoch 7: training: 0.069, validation: 0.249, lr: 0.01000\n#> Loss at epoch 8: training: 0.063, validation: 0.212, lr: 0.01000\n#> Loss at epoch 9: training: 0.039, validation: 0.257, lr: 0.01000\n#> Loss at epoch 10: training: 0.036, validation: 0.334, lr: 0.01000\n#> Loss at epoch 11: training: 0.027, validation: 0.310, lr: 0.01000\n#> Loss at epoch 12: training: 0.027, validation: 0.445, lr: 0.01000\n#> Loss at epoch 13: training: 0.095, validation: 0.563, lr: 0.01000\n#> Loss at epoch 14: training: 0.106, validation: 0.375, lr: 0.01000\n#> Loss at epoch 15: training: 0.039, validation: 0.233, lr: 0.01000\n#> Loss at epoch 16: training: 0.026, validation: 0.267, lr: 0.01000\n#> Loss at epoch 17: training: 0.008, validation: 0.230, lr: 0.01000\n#> Loss at epoch 18: training: 0.005, validation: 0.235, lr: 0.01000\n#> Loss at epoch 19: training: 0.003, validation: 0.240, lr: 0.01000\n#> Loss at epoch 20: training: 0.002, validation: 0.252, lr: 0.01000\n#> Loss at epoch 21: training: 0.002, validation: 0.263, lr: 0.01000\n#> Loss at epoch 22: training: 0.001, validation: 0.273, lr: 0.01000\n#> Loss at epoch 23: training: 0.001, validation: 0.281, lr: 0.01000\n#> Loss at epoch 24: training: 0.001, validation: 0.288, lr: 0.01000\n#> Loss at epoch 25: training: 0.001, validation: 0.293, lr: 0.01000\n#> Loss at epoch 26: training: 0.001, validation: 0.298, lr: 0.01000\n#> Loss at epoch 27: training: 0.001, validation: 0.303, lr: 0.01000\n#> Loss at epoch 28: training: 0.001, validation: 0.307, lr: 0.01000\n#> Loss at epoch 29: training: 0.000, validation: 0.311, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 0.315, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.319, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.323, lr: 0.01000\n#> Loss at epoch 1: training: 0.443, validation: 0.338, lr: 0.01000#> Loss at epoch 2: training: 0.269, validation: 0.259, lr: 0.01000\n#> Loss at epoch 3: training: 0.213, validation: 0.226, lr: 0.01000\n#> Loss at epoch 4: training: 0.172, validation: 0.207, lr: 0.01000\n#> Loss at epoch 5: training: 0.126, validation: 0.180, lr: 0.01000\n#> Loss at epoch 6: training: 0.102, validation: 0.350, lr: 0.01000\n#> Loss at epoch 7: training: 0.085, validation: 0.327, lr: 0.01000\n#> Loss at epoch 8: training: 0.109, validation: 0.303, lr: 0.01000\n#> Loss at epoch 9: training: 0.080, validation: 0.245, lr: 0.01000\n#> Loss at epoch 10: training: 0.045, validation: 0.428, lr: 0.01000\n#> Loss at epoch 11: training: 0.028, validation: 0.370, lr: 0.01000\n#> Loss at epoch 12: training: 0.023, validation: 0.417, lr: 0.01000\n#> Loss at epoch 13: training: 0.016, validation: 0.443, lr: 0.01000\n#> Loss at epoch 14: training: 0.008, validation: 0.373, lr: 0.01000\n#> Loss at epoch 15: training: 0.004, validation: 0.460, lr: 0.01000\n#> Loss at epoch 16: training: 0.003, validation: 0.416, lr: 0.01000\n#> Loss at epoch 17: training: 0.002, validation: 0.482, lr: 0.01000\n#> Loss at epoch 18: training: 0.001, validation: 0.446, lr: 0.01000\n#> Loss at epoch 19: training: 0.001, validation: 0.445, lr: 0.01000\n#> Loss at epoch 20: training: 0.001, validation: 0.466, lr: 0.01000\n#> Loss at epoch 21: training: 0.001, validation: 0.470, lr: 0.01000\n#> Loss at epoch 22: training: 0.000, validation: 0.470, lr: 0.01000\n#> Loss at epoch 23: training: 0.000, validation: 0.473, lr: 0.01000\n#> Loss at epoch 24: training: 0.000, validation: 0.475, lr: 0.01000\n#> Loss at epoch 25: training: 0.000, validation: 0.478, lr: 0.01000\n#> Loss at epoch 26: training: 0.000, validation: 0.479, lr: 0.01000\n#> Loss at epoch 27: training: 0.000, validation: 0.482, lr: 0.01000\n#> Loss at epoch 28: training: 0.000, validation: 0.484, lr: 0.01000\n#> Loss at epoch 29: training: 0.000, validation: 0.486, lr: 0.01000\n#> Loss at epoch 30: training: 0.000, validation: 0.488, lr: 0.01000\n#> Loss at epoch 31: training: 0.000, validation: 0.491, lr: 0.01000\n#> Loss at epoch 32: training: 0.000, validation: 0.492, lr: 0.01000\nprint(result)\n#>       train_auc  test_auc\n#>  [1,] 0.9914020 0.9430894\n#>  [2,] 0.9909064 0.9777778\n#>  [3,] 0.9963232 0.9777778\n#>  [4,] 0.9893852 0.9400000\n#>  [5,] 0.9968243 0.9255952\n#>  [6,] 0.9945608 0.8690476\n#>  [7,] 0.9995426 0.9868421\n#>  [8,] 0.9904637 0.9837398\n#>  [9,] 0.9976998 0.9682540\n#> [10,] 0.9942568 0.9642857\n(colMeans(result))\n#> train_auc  test_auc \n#> 0.9941365 0.9536409\n\n# The model setup seems to be fine.\n\n## Train and predict for outer validation split (on the complete training data):\nmodel_cito = dnn(Hazardous~., data = train, hidden = rep(100L, 2L), activation = rep(\"relu\", 2), loss = \"binomial\")\n#> Loss at epoch 1: 0.435567, lr: 0.01000#> Loss at epoch 2: 0.284994, lr: 0.01000\n#> Loss at epoch 3: 0.200466, lr: 0.01000\n#> Loss at epoch 4: 0.152101, lr: 0.01000\n#> Loss at epoch 5: 0.136190, lr: 0.01000\n#> Loss at epoch 6: 0.107190, lr: 0.01000\n#> Loss at epoch 7: 0.080262, lr: 0.01000\n#> Loss at epoch 8: 0.090661, lr: 0.01000\n#> Loss at epoch 9: 0.128607, lr: 0.01000\n#> Loss at epoch 10: 0.104285, lr: 0.01000\n#> Loss at epoch 11: 0.065403, lr: 0.01000\n#> Loss at epoch 12: 0.048835, lr: 0.01000\n#> Loss at epoch 13: 0.043236, lr: 0.01000\n#> Loss at epoch 14: 0.025492, lr: 0.01000\n#> Loss at epoch 15: 0.015401, lr: 0.01000\n#> Loss at epoch 16: 0.006055, lr: 0.01000\n#> Loss at epoch 17: 0.004264, lr: 0.01000\n#> Loss at epoch 18: 0.001732, lr: 0.01000\n#> Loss at epoch 19: 0.001325, lr: 0.01000\n#> Loss at epoch 20: 0.001001, lr: 0.01000\n#> Loss at epoch 21: 0.000834, lr: 0.01000\n#> Loss at epoch 22: 0.000696, lr: 0.01000\n#> Loss at epoch 23: 0.000589, lr: 0.01000\n#> Loss at epoch 24: 0.000506, lr: 0.01000\n#> Loss at epoch 25: 0.000444, lr: 0.01000\n#> Loss at epoch 26: 0.000392, lr: 0.01000\n#> Loss at epoch 27: 0.000348, lr: 0.01000\n#> Loss at epoch 28: 0.000311, lr: 0.01000\n#> Loss at epoch 29: 0.000281, lr: 0.01000\n#> Loss at epoch 30: 0.000255, lr: 0.01000\n#> Loss at epoch 31: 0.000235, lr: 0.01000\n#> Loss at epoch 32: 0.000215, lr: 0.01000\ntest$Hazardous = 1 # rows with NA wills be removed\npred = round(predict(model_cito, newdata= test))\n\nwrite.csv(data.frame(y = pred), file = \"submission_DNN.csv\")\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tidyverse)\nlibrary(missRanger)\nlibrary(Metrics)\nlibrary(EcoData)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata(\"nasa\")\ndata = nasa\n\ndata$subset = ifelse(is.na(data$Hazardous), \"test\", \"train\")\ndata = data %>% select(-Equinox, -Orbiting.Body,\n                       -Orbit.Determination.Date, -Close.Approach.Date)\nimputed = data %>%\n  select(-Hazardous) %>%\n  missRanger::missRanger(maxiter = 5L, num.trees = 20L)\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion\n#>   Variables used to impute:  Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbit.ID, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, subset\n#> iter 1:  ...................................\n#> iter 2:  ...................................\n#> iter 3:  ...................................\n#> iter 4:  ...................................\n\ndata = cbind(\n  data %>% select(Hazardous),\n  scale(imputed %>% select(-subset)),\n  data.frame(\"subset\" = data$subset)\n)\n\ntrain = data[data$subset == \"train\", ]\ntest = data[data$subset == \"test\", ]\n\ntrain = train %>% select(-subset)\ntest = test %>% select(-subset)\n\nlen = nrow(train)\nord = sample.int(len)\nk = 10\ncv_indices = lapply(0:9, function(i) sort(ord[(i*len/k + 1):((i+1)*len/k)]))\n\nresult = matrix(NA, 10L, 2L)\ncolnames(result) = c(\"train_auc\", \"test_auc\")\n\nfor(i in 1:10) {\n  indices = cv_indices[[i]]\n  sub_train = train[-indices,]\n  sub_test = train[indices,]\n  \n  # \"Deep\" neural networks and regularization:\n  model = keras_model_sequential()\n  model %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             input_shape = ncol(sub_train) - 1L) %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 100L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 1L, activation = \"sigmoid\")\n  \n  early_stopping = callback_early_stopping(monitor = \"val_loss\", patience = 5L)\n  # You need a validation split for this!\n  \n  model %>%\n  keras::compile(loss = loss_binary_crossentropy, \n                 optimizer = optimizer_adamax(0.01))\n  \n  model_history = \n   model %>%\n     fit(\n       x = as.matrix(sub_train %>% select(-Hazardous)),\n       y = as.matrix(sub_train %>% select(Hazardous)),\n       callbacks = c(early_stopping),\n       validation_split = 0.2,\n       epochs = 35L, batch = 50L, shuffle = TRUE\n      )\n  \n  plot(model_history)\n  \n  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))\n  pred_test = predict(model, as.matrix(sub_test %>% select(-Hazardous)))\n  \n  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)\n  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)\n}\nprint(result)\n#>       train_auc  test_auc\n#>  [1,] 0.9956669 0.9376694\n#>  [2,] 0.9939923 0.9911111\n#>  [3,] 0.9859492 0.9422222\n#>  [4,] 0.9965536 0.9500000\n#>  [5,] 0.9976351 0.9107143\n#>  [6,] 0.9961486 0.8809524\n#>  [7,] 0.9894807 0.9868421\n#>  [8,] 0.9908902 1.0000000\n#>  [9,] 0.9937105 0.9563492\n#> [10,] 0.9947297 0.9642857\n(colMeans(result))\n#> train_auc  test_auc \n#> 0.9934757 0.9520146\n\n# The model setup seems to be fine.\n\n## Train and predict for outer validation split (on the complete training data):\nmodel = keras_model_sequential()\nmodel %>%\n  layer_dense(units = 100L, activation = \"relu\",\n              input_shape = ncol(sub_train) - 1L) %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 100L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 1L, activation = \"sigmoid\")\n\nearly_stopping = callback_early_stopping(monitor = \"val_loss\", patience = 5L)\n# You need a validation split for this!\n\nmodel %>%\n  keras::compile(loss = loss_binary_crossentropy, \n                 optimizer = optimizer_adamax(0.01))\n\nmodel_history = \n  model %>%\n    fit(\n      x = as.matrix(train %>% select(-Hazardous)),\n      y = as.matrix(train %>% select(Hazardous)),\n      callbacks = c(early_stopping),\n      validation_split = 0.2,\n      epochs = 35L, batch = 50L, shuffle = TRUE\n    )\n\npred = round(predict(model, as.matrix(test[,-1])))\n\nwrite.csv(data.frame(y = pred), file = \"submission_DNN_dropout_early.csv\")\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nresult = matrix(NA, 10L, 2L)\ncolnames(result) = c(\"train_auc\", \"test_auc\")\n\nfor(i in 1:10) {\n  indices = cv_indices[[i]]\n  sub_train = train[-indices,]\n  sub_test = train[indices,]\n  \n  # \"Deep\" neural networks and regularization:\n  model = keras_model_sequential()\n  model %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             input_shape = ncol(sub_train) - 1L) %>%\n  layer_dropout(rate = 0.45) %>%\n  layer_dense(units = 50L, activation = \"gelu\",\n              kernel_regularizer = regularizer_l1(.00125),\n              bias_regularizer = regularizer_l1_l2(.25)\n              ) %>%\n  layer_dropout(rate = 0.35) %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 1L, activation = \"sigmoid\")\n  \n  early_stopping = callback_early_stopping(monitor = \"val_loss\", patience = 8L)\n  # You need a validation split for this!\n  \n  model %>%\n  keras::compile(loss = loss_binary_crossentropy, \n                 optimizer = optimizer_adamax(0.0072))\n  \n  model_history = \n   model %>%\n     fit(\n       x = as.matrix(sub_train %>% select(-Hazardous)),\n       y = as.matrix(sub_train %>% select(Hazardous)), \n       callbacks = c(early_stopping),\n       validation_split = 0.2,\n       epochs = 50L, batch = 50L, shuffle = TRUE\n      )\n  \n  plot(model_history)\n  \n  pred_train = predict(model, as.matrix(sub_train %>% select(-Hazardous)))\n  pred_test = predict(model, as.matrix(sub_test %>% select(-Hazardous)))\n  \n  result[i, 1] = Metrics::auc(sub_train$Hazardous, pred_train)\n  result[i, 2] = Metrics::auc(sub_test$Hazardous, pred_test)\n}\nprint(result)\n#>       train_auc  test_auc\n#>  [1,] 0.9970657 0.9593496\n#>  [2,] 0.9906438 0.9955556\n#>  [3,] 0.9927448 0.9866667\n#>  [4,] 0.9954508 0.9625000\n#>  [5,] 0.9927365 0.9285714\n#>  [6,] 0.9948311 0.8839286\n#>  [7,] 0.9899733 0.9912281\n#>  [8,] 0.9926644 1.0000000\n#>  [9,] 0.9923088 0.9543651\n#> [10,] 0.9901014 0.9791667\n(colMeans(result))\n#> train_auc  test_auc \n#> 0.9928520 0.9641332\n\n\nmodel = keras_model_sequential()\n  model %>%\n  layer_dense(units = 100L, activation = \"relu\",\n             input_shape = ncol(sub_train) - 1L) %>%\n  layer_dropout(rate = 0.45) %>%\n  layer_dense(units = 50L, activation = \"gelu\",\n              kernel_regularizer = regularizer_l1(.00125),\n              bias_regularizer = regularizer_l1_l2(.25)\n              ) %>%\n  layer_dropout(rate = 0.35) %>%\n  layer_dense(units = 30L, activation = \"relu\") %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 1L, activation = \"sigmoid\")\n  \n  early_stopping = callback_early_stopping(monitor = \"val_loss\", patience = 8L)\n  # You need a validation split for this!\n  \n  model %>%\n  keras::compile(loss = loss_binary_crossentropy, \n                 optimizer = optimizer_adamax(0.0072))\n  \n  model_history = \n   model %>%\n     fit(\n        x = as.matrix(train %>% select(-Hazardous)),\n        y = as.matrix(train %>% select(Hazardous)),\n        callbacks = c(early_stopping),\n        validation_split = 0.2,\n        epochs = 50L, batch = 50L, shuffle = TRUE\n      )\n\npred = round(predict(model, as.matrix(test[,-1])))\n\nwrite.csv(data.frame(y = pred), file = \"submission_DNN_optimal.csv\")"},{"path":"deep.html","id":"case-study-fitting-a-cnn-on-mnist","chapter":"6 Deep Learning","heading":"6.3 Case Study: Fitting a CNN on MNIST","text":"show use convolutional neural networks MNIST data set. data set maybe one famous image data sets. consists 60,000 handwritten digits 0-9., define helper functions:MNIST data set famous automatic download function Keras:Let’s visualize digits:Similar normal machine learning workflow, scale pixels (0-255) range \\([0, 1]\\) one hot encode response. scaling pixels, use arrays instead matrices. Arrays called tensors mathematics 2D array/tensor typically called matrix.last dimension denotes number channels image. case one channel images black white.times, least 3 color channels, example RGB (red, green, blue) HSV (hue, saturation, value), sometimes several additional dimensions like transparency.build convolutional model, specify kernel. case, use 16 convolutional kernels (filters) size \\(2\\times2\\). 2D kernels images 2D. movies example, one use 3D kernels (third dimension correspond time color channels).KerasTorchBuild dataloader:Build convolutional neural network:\ncalculate shapes layers :start input shape (batch_size, 1, 28, 28)First convolutional layer shape (input channel = 1, number feature maps = 16, kernel size = 2)Output: batch_size = 32, number feature maps = 16, dimensions feature map = \\((27 , 27)\\)\nWit kernel size two stride = 1 lose one pixel dimension…\nQuestions:happens increase stride?happens increase kernel size?Pooling layer summarizes feature mapkernel_size = 2L stride = 2L halfs pixel dimensions image.Fully connected layerNow flatten final output convolutional neural network model use normal fully connected layer, calculate number inputs fully connected layer:Build network:additionally used pooling layer downsizing resulting feature maps.\nWithout specification, \\(2\\times2\\) pooling layer taken automatically.\nPooling layers take input feature map divide (case) parts \\(2\\times2\\) size. respective pooling operation executed.\nevery input map/layer, get one (downsized) output map/layer.using max pooling layer (sever methods like mean pooling), maximum value 4 parts taken forwarded .\nExample input:use max pooling every field:resulting pooled information :example, \\(4\\times6\\) layer transformed \\(2\\times3\\) layer thus downsized.\nsimilar biological process called lateral inhibition active neurons inhibit activity neighboring neurons.\n’s loss information often useful aggregating information prevent overfitting.another convolutional pooling layer flatten output. means following dense layer treats previous layer full layer (dense layer connected weights last feature maps). can imagine like reshaping matrix (2D) simple 1D vector. full vector used.\nflattened layer, can simply use typical output layer.KerasThe rest usual: First compile model., train model:TorchTrain model:Evaluation:","code":"\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nrotate = function(x){ t(apply(x, 2, rev)) }\n\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE,\n     main = paste0(\"Label: \", as.character(title)))\n}\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\noldpar = par(mfrow = c(1, 3))\n.n = sapply(1:3, function(x) imgPlot(train$x[x,,], train$y[x]))\npar(oldpar)\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\ntest_y = to_categorical(test$y, 10)\nmodel = keras_model_sequential()\nmodel %>%\n layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n layer_max_pooling_2d() %>%\n layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n layer_max_pooling_2d() %>%\n layer_flatten() %>%\n layer_dense(100L, activation = \"relu\") %>%\n layer_dense(10L, activation = \"softmax\")\nsummary(model)\n#> Model: \"sequential_33\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  conv2d_1 (Conv2D)                      (None, 27, 27, 16)                  80            \n#>  max_pooling2d_1 (MaxPooling2D)         (None, 13, 13, 16)                  0             \n#>  conv2d (Conv2D)                        (None, 11, 11, 16)                  2320          \n#>  max_pooling2d (MaxPooling2D)           (None, 5, 5, 16)                    0             \n#>  flatten (Flatten)                      (None, 400)                         0             \n#>  dense_111 (Dense)                      (None, 100)                         40100         \n#>  dense_110 (Dense)                      (None, 10)                          1010          \n#> ==========================================================================================\n#> Total params: 43,510\n#> Trainable params: 43,510\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________Prepare/download data:\nlibrary(torch)\nlibrary(torchvision)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntrain_ds = mnist_dataset(\n  \".\",\n  download = TRUE,\n  train = TRUE,\n  transform = transform_to_tensor\n)\n\ntest_ds = mnist_dataset(\n  \".\",\n  download = TRUE,\n  train = FALSE,\n  transform = transform_to_tensor\n)\ntrain_dl = dataloader(train_ds, batch_size = 32, shuffle = TRUE)\ntest_dl = dataloader(test_ds, batch_size = 32)\nfirst_batch = train_dl$.iter()\ndf = first_batch$.next()\n\ndf$x$size()\n#> [1] 32  1 28 28\nsample = df$x\nsample$size()\n#> [1] 32  1 28 28\nconv1 = nn_conv2d(1, 16L, 2L, stride = 1L)\n(sample %>% conv1)$size()\n#> [1] 32 16 27 27\n(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()\n#> [1] 32 16 13 13\ndims = (sample %>% conv1 %>%\n          nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()\n# Without the batch size of course.\nfinal = prod(dims[-1]) \nprint(final)\n#> [1] 2704\nfc = nn_linear(final, 10L)\n(sample %>% conv1 %>% nnf_max_pool2d(kernel_size = 2L, stride = 2L)\n  %>% torch_flatten(start_dim = 2L) %>% fc)$size()\n#> [1] 32 10\nnet = nn_module(\n  \"mnist\",\n  initialize = function(){\n    self$conv1 = nn_conv2d(1, 16L, 2L)\n    self$conv2 = nn_conv2d(16L, 16L, 3L)\n    self$fc1 = nn_linear(400L, 100L)\n    self$fc2 = nn_linear(100L, 10L)\n  },\n  forward = function(x){\n    x %>%\n      self$conv1() %>%\n      nnf_relu() %>%\n      nnf_max_pool2d(2) %>%\n      self$conv2() %>%\n      nnf_relu() %>%\n      nnf_max_pool2d(2) %>%\n      torch_flatten(start_dim = 2) %>%\n      self$fc1() %>%\n      nnf_relu() %>%\n      self$fc2()\n  }\n)1   2   |   5   8   |   3   6\n6   5   |   2   4   |   8   1\n------------------------------\n9   4   |   3   7   |   2   5\n0   3   |   2   7   |   4   9max(1, 2, 6, 5)   |   max(5, 8, 2, 4)   |   max(3, 6, 8, 1)\n-----------------------------------------------------------\nmax(9, 4, 0, 3)   |   max(3, 7, 2, 7)   |   max(2, 5, 4, 9)6   |   8   |   8\n------------------\n9   |   7   |   9\nmodel %>%\n  keras::compile(\n      optimizer = keras::optimizer_adamax(0.01),\n      loss = loss_categorical_crossentropy\n  )\nsummary(model)\n#> Model: \"sequential_33\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  conv2d_1 (Conv2D)                      (None, 27, 27, 16)                  80            \n#>  max_pooling2d_1 (MaxPooling2D)         (None, 13, 13, 16)                  0             \n#>  conv2d (Conv2D)                        (None, 11, 11, 16)                  2320          \n#>  max_pooling2d (MaxPooling2D)           (None, 5, 5, 16)                    0             \n#>  flatten (Flatten)                      (None, 400)                         0             \n#>  dense_111 (Dense)                      (None, 100)                         40100         \n#>  dense_110 (Dense)                      (None, 10)                          1010          \n#> ==========================================================================================\n#> Total params: 43,510\n#> Trainable params: 43,510\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nepochs = 5L\nbatch_size = 32L\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = net()\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nfor(e in 1:3){\n  losses = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]], reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      losses = c(losses, loss$item())\n    }\n  )\n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", e, mean(losses)))\n}\nmodel_torch$eval()\n\ntest_losses = c()\ntotal = 0\ncorrect = 0\n\ncoro::loop(\n  for(batch in test_dl){\n    output = model_torch(batch[[1]])\n    labels = batch[[2]]\n    loss = nnf_cross_entropy(output, labels)\n    test_losses = c(test_losses, loss$item())\n    predicted = torch_max(output$data(), dim = 2)[[2]]\n    total = total + labels$size(1)\n    correct = correct + (predicted == labels)$sum()$item()\n  }\n)\n\nmean(test_losses)\ntest_accuracy =  correct/total\ntest_accuracy"},{"path":"deep.html","id":"advanced-training-techniques","chapter":"6 Deep Learning","heading":"6.4 Advanced Training Techniques","text":"","code":""},{"path":"deep.html","id":"data-augmentation","chapter":"6 Deep Learning","heading":"6.4.1 Data Augmentation","text":"train convolutional neural network using little data common problem. Data augmentation helps artificially increase number images.idea convolutional neural network learns specific structures edges images. Rotating, adding noise, zooming preserve overall key structure interested , model see new images search key structures.Luckily, easy use data augmentation Keras.show , use flower data set. define generator object (specific object infinitely draws samples data set). generator can turn data augmentation.KerasUsing data augmentation can artificially increase number images.TorchIn Torch, change transform function (train dataloader):","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = EcoData::dataset_flower()\ntrain = data$train/255\nlabels = data$labels\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L),\n                input_shape = c(80L, 80L, 3L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L),\n                activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),\n                strides = c(2L, 2L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dropout(0.5) %>%\n  layer_dense(units = 5L, activation = \"softmax\")\n\n  \n# Data augmentation.\naug = image_data_generator(rotation_range = 90, \n                           zoom_range = c(0.3), \n                           horizontal_flip = TRUE, \n                           vertical_flip = TRUE)\n\n# Data preparation / splitting.\nindices = sample.int(nrow(train), 0.1 * nrow(train))\ngenerator = flow_images_from_data(train[-indices,,,],\n                                  k_one_hot(labels[-indices], num_classes = 5L),\n                                  generator = aug,\n                                  batch_size = 25L,\n                                  shuffle = TRUE)\n\ntest = train[indices,,,]\n\n## Training loop with early stopping:\n\n# As we use an iterator (the generator), validation loss is not applicable.\n# An available metric is the normal loss.\nearly = keras::callback_early_stopping(patience = 2L, monitor = \"loss\")\n\nmodel %>%\n    keras::compile(loss = loss_categorical_crossentropy,\n                   optimizer = keras::optimizer_adamax(learning_rate = 0.01))\n\nmodel %>%\n    fit(generator, epochs = 20L, batch_size = 25L,\n        shuffle = TRUE, callbacks = c(early))\n\n# Predictions on the training set:\npred = predict(model, data$train[-indices,,,]) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, labels[-indices])\ntable(pred)\n\n# Predictions on the holdout / test set:\npred = predict(model, test) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, labels[indices])\ntable(pred)\n\n# If you want to predict on the holdout for submission, use:\npred = predict(model, EcoData::dataset_flower()$test/255) %>%\n  apply(1, which.max) - 1\ntable(pred)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntrain_transforms = function(img){\n  img %>%\n    transform_to_tensor() %>%\n    transform_random_horizontal_flip(p = 0.3) %>%\n    transform_random_resized_crop(size = c(28L, 28L)) %>%\n    transform_random_vertical_flip(0.3)\n}\n\ntrain_ds = mnist_dataset(\".\", download = TRUE, train = TRUE,\n                         transform = train_transforms)\ntest_ds = mnist_dataset(\".\", download = TRUE, train = FALSE,\n                        transform = transform_to_tensor)\n\ntrain_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)\ntest_dl = dataloader(test_ds, batch_size = 100L)\n\nmodel_torch = net()\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nfor(e in 1:1){\n  losses = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]], reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      losses = c(losses, loss$item())\n    }\n  )\n  \n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", e, mean(losses)))\n}\n\nmodel_torch$eval()\n\ntest_losses = c()\ntotal = 0\ncorrect = 0\n\ncoro::loop(\n  for(batch in test_dl){\n    output = model_torch(batch[[1]])\n    labels = batch[[2]]\n    loss = nnf_cross_entropy(output, labels)\n    test_losses = c(test_losses, loss$item())\n    predicted = torch_max(output$data(), dim = 2)[[2]]\n    total = total + labels$size(1)\n    correct = correct + (predicted == labels)$sum()$item()\n  }\n)\n\ntest_accuracy =  correct/total\nprint(test_accuracy)"},{"path":"deep.html","id":"transfer","chapter":"6 Deep Learning","heading":"6.4.2 Transfer Learning","text":"Another approach reduce necessary number images speed convergence models use transfer learning.main idea transfer learning convolutional layers mainly one task - learning identify highly correlated neighboring features. knowledge used new tasks.\nconvolutional layers learn structures edges images top layer, dense layer actual classifier convolutional neural network specific task.\nThus, one think train top layer classifier. , confronted sets different edges/structures decide label based ., sounds complicated quite easy Keras Torch.KerasWe now CIFAR10 data set, prepare data:Keras provides download functions famous architectures/convolutional neural network models already trained imagenet data set (another famous data set). trained networks come already without top layer, set include_top false change input shape.Now, use sequential model just “keras_model” can specify inputs outputs. Thereby, output top layer, inputs densenet inputs, already pre-trained.next step want freeze layers except last layer. Freezing means trained: want train complete model, want train last layer. can check number trainable weights via summary(model).usual training:seen, transfer learning can easily done using Keras.TorchFlower data setLet’s flower data set:","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = keras::dataset_cifar10()\ntrain = data$train\ntest = data$test\n\nrm(data)\n\nimage = train$x[5,,,]\nimage %>%\n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\ntrain_x = array(train$x/255, c(dim(train$x)))\ntest_x = array(test$x/255, c(dim(test$x)))\ntrain_y = to_categorical(train$y, 10)\ntest_y = to_categorical(test$y, 10)\n\nrm(train, test)\ndensenet = application_densenet201(include_top = FALSE,\n                                   input_shape  = c(32L, 32L, 3L))\nmodel = keras::keras_model(\n  inputs = densenet$input,\n  outputs = layer_flatten(\n    layer_dense(densenet$output, units = 10L, activation = \"softmax\")\n  )\n)\n\n# Notice that this snippet just creates one (!) new layer.\n# The densenet's inputs are connected with the model's inputs.\n# The densenet's outputs are connected with our own layer (with 10 nodes).\n# This layer is also the output layer of the model.\nmodel %>% freeze_weights(to = length(model$layers) - 1)\nsummary(model)\n#> Model: \"model\"\n#> __________________________________________________________________________________________\n#>  Layer (type)             Output Shape     Param #  Connected to               Trainable  \n#> ==========================================================================================\n#>  input_1 (InputLayer)     [(None, 32, 32,  0        []                         N          \n#>                            3)]                                                            \n#>  zero_padding2d (ZeroPadd  (None, 38, 38,   0       ['input_1[0][0]']          N          \n#>  ing2D)                   3)                                                              \n#>  conv1/conv (Conv2D)      (None, 16, 16,   9408     ['zero_padding2d[0][0]']   N          \n#>                           64)                                                             \n#>  conv1/bn (BatchNormaliza  (None, 16, 16,   256     ['conv1/conv[0][0]']       N          \n#>  tion)                    64)                                                             \n#>  conv1/relu (Activation)  (None, 16, 16,   0        ['conv1/bn[0][0]']         N          \n#>                           64)                                                             \n#>  zero_padding2d_1 (ZeroPa  (None, 18, 18,   0       ['conv1/relu[0][0]']       N          \n#>  dding2D)                 64)                                                             \n#>  pool1 (MaxPooling2D)     (None, 8, 8, 64  0        ['zero_padding2d_1[0][0]'  N          \n#>                           )                         ]                                     \n#>  conv2_block1_0_bn (Batch  (None, 8, 8, 64  256     ['pool1[0][0]']            N          \n#>  Normalization)           )                                                               \n#>  conv2_block1_0_relu (Act  (None, 8, 8, 64  0       ['conv2_block1_0_bn[0][0]  N          \n#>  ivation)                 )                         ']                                    \n#>  conv2_block1_1_conv (Con  (None, 8, 8, 12  8192    ['conv2_block1_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv2_block1_1_bn (Batch  (None, 8, 8, 12  512     ['conv2_block1_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv2_block1_1_relu (Act  (None, 8, 8, 12  0       ['conv2_block1_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv2_block1_2_conv (Con  (None, 8, 8, 32  36864   ['conv2_block1_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv2_block1_concat (Con  (None, 8, 8, 96  0       ['pool1[0][0]',            N          \n#>  catenate)                )                          'conv2_block1_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv2_block2_0_bn (Batch  (None, 8, 8, 96  384     ['conv2_block1_concat[0][  N          \n#>  Normalization)           )                         0]']                                  \n#>  conv2_block2_0_relu (Act  (None, 8, 8, 96  0       ['conv2_block2_0_bn[0][0]  N          \n#>  ivation)                 )                         ']                                    \n#>  conv2_block2_1_conv (Con  (None, 8, 8, 12  12288   ['conv2_block2_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv2_block2_1_bn (Batch  (None, 8, 8, 12  512     ['conv2_block2_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv2_block2_1_relu (Act  (None, 8, 8, 12  0       ['conv2_block2_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv2_block2_2_conv (Con  (None, 8, 8, 32  36864   ['conv2_block2_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv2_block2_concat (Con  (None, 8, 8, 12  0       ['conv2_block1_concat[0][  N          \n#>  catenate)                8)                        0]',                                  \n#>                                                      'conv2_block2_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv2_block3_0_bn (Batch  (None, 8, 8, 12  512     ['conv2_block2_concat[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv2_block3_0_relu (Act  (None, 8, 8, 12  0       ['conv2_block3_0_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv2_block3_1_conv (Con  (None, 8, 8, 12  16384   ['conv2_block3_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv2_block3_1_bn (Batch  (None, 8, 8, 12  512     ['conv2_block3_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv2_block3_1_relu (Act  (None, 8, 8, 12  0       ['conv2_block3_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv2_block3_2_conv (Con  (None, 8, 8, 32  36864   ['conv2_block3_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv2_block3_concat (Con  (None, 8, 8, 16  0       ['conv2_block2_concat[0][  N          \n#>  catenate)                0)                        0]',                                  \n#>                                                      'conv2_block3_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv2_block4_0_bn (Batch  (None, 8, 8, 16  640     ['conv2_block3_concat[0][  N          \n#>  Normalization)           0)                        0]']                                  \n#>  conv2_block4_0_relu (Act  (None, 8, 8, 16  0       ['conv2_block4_0_bn[0][0]  N          \n#>  ivation)                 0)                        ']                                    \n#>  conv2_block4_1_conv (Con  (None, 8, 8, 12  20480   ['conv2_block4_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv2_block4_1_bn (Batch  (None, 8, 8, 12  512     ['conv2_block4_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv2_block4_1_relu (Act  (None, 8, 8, 12  0       ['conv2_block4_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv2_block4_2_conv (Con  (None, 8, 8, 32  36864   ['conv2_block4_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv2_block4_concat (Con  (None, 8, 8, 19  0       ['conv2_block3_concat[0][  N          \n#>  catenate)                2)                        0]',                                  \n#>                                                      'conv2_block4_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv2_block5_0_bn (Batch  (None, 8, 8, 19  768     ['conv2_block4_concat[0][  N          \n#>  Normalization)           2)                        0]']                                  \n#>  conv2_block5_0_relu (Act  (None, 8, 8, 19  0       ['conv2_block5_0_bn[0][0]  N          \n#>  ivation)                 2)                        ']                                    \n#>  conv2_block5_1_conv (Con  (None, 8, 8, 12  24576   ['conv2_block5_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv2_block5_1_bn (Batch  (None, 8, 8, 12  512     ['conv2_block5_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv2_block5_1_relu (Act  (None, 8, 8, 12  0       ['conv2_block5_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv2_block5_2_conv (Con  (None, 8, 8, 32  36864   ['conv2_block5_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv2_block5_concat (Con  (None, 8, 8, 22  0       ['conv2_block4_concat[0][  N          \n#>  catenate)                4)                        0]',                                  \n#>                                                      'conv2_block5_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv2_block6_0_bn (Batch  (None, 8, 8, 22  896     ['conv2_block5_concat[0][  N          \n#>  Normalization)           4)                        0]']                                  \n#>  conv2_block6_0_relu (Act  (None, 8, 8, 22  0       ['conv2_block6_0_bn[0][0]  N          \n#>  ivation)                 4)                        ']                                    \n#>  conv2_block6_1_conv (Con  (None, 8, 8, 12  28672   ['conv2_block6_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv2_block6_1_bn (Batch  (None, 8, 8, 12  512     ['conv2_block6_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv2_block6_1_relu (Act  (None, 8, 8, 12  0       ['conv2_block6_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv2_block6_2_conv (Con  (None, 8, 8, 32  36864   ['conv2_block6_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv2_block6_concat (Con  (None, 8, 8, 25  0       ['conv2_block5_concat[0][  N          \n#>  catenate)                6)                        0]',                                  \n#>                                                      'conv2_block6_2_conv[0][             \n#>                                                     0]']                                  \n#>  pool2_bn (BatchNormaliza  (None, 8, 8, 25  1024    ['conv2_block6_concat[0][  N          \n#>  tion)                    6)                        0]']                                  \n#>  pool2_relu (Activation)  (None, 8, 8, 25  0        ['pool2_bn[0][0]']         N          \n#>                           6)                                                              \n#>  pool2_conv (Conv2D)      (None, 8, 8, 12  32768    ['pool2_relu[0][0]']       N          \n#>                           8)                                                              \n#>  pool2_pool (AveragePooli  (None, 4, 4, 12  0       ['pool2_conv[0][0]']       N          \n#>  ng2D)                    8)                                                              \n#>  conv3_block1_0_bn (Batch  (None, 4, 4, 12  512     ['pool2_pool[0][0]']       N          \n#>  Normalization)           8)                                                              \n#>  conv3_block1_0_relu (Act  (None, 4, 4, 12  0       ['conv3_block1_0_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block1_1_conv (Con  (None, 4, 4, 12  16384   ['conv3_block1_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block1_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block1_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block1_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block1_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block1_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block1_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block1_concat (Con  (None, 4, 4, 16  0       ['pool2_pool[0][0]',       N          \n#>  catenate)                0)                         'conv3_block1_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block2_0_bn (Batch  (None, 4, 4, 16  640     ['conv3_block1_concat[0][  N          \n#>  Normalization)           0)                        0]']                                  \n#>  conv3_block2_0_relu (Act  (None, 4, 4, 16  0       ['conv3_block2_0_bn[0][0]  N          \n#>  ivation)                 0)                        ']                                    \n#>  conv3_block2_1_conv (Con  (None, 4, 4, 12  20480   ['conv3_block2_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block2_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block2_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block2_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block2_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block2_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block2_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block2_concat (Con  (None, 4, 4, 19  0       ['conv3_block1_concat[0][  N          \n#>  catenate)                2)                        0]',                                  \n#>                                                      'conv3_block2_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block3_0_bn (Batch  (None, 4, 4, 19  768     ['conv3_block2_concat[0][  N          \n#>  Normalization)           2)                        0]']                                  \n#>  conv3_block3_0_relu (Act  (None, 4, 4, 19  0       ['conv3_block3_0_bn[0][0]  N          \n#>  ivation)                 2)                        ']                                    \n#>  conv3_block3_1_conv (Con  (None, 4, 4, 12  24576   ['conv3_block3_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block3_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block3_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block3_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block3_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block3_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block3_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block3_concat (Con  (None, 4, 4, 22  0       ['conv3_block2_concat[0][  N          \n#>  catenate)                4)                        0]',                                  \n#>                                                      'conv3_block3_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block4_0_bn (Batch  (None, 4, 4, 22  896     ['conv3_block3_concat[0][  N          \n#>  Normalization)           4)                        0]']                                  \n#>  conv3_block4_0_relu (Act  (None, 4, 4, 22  0       ['conv3_block4_0_bn[0][0]  N          \n#>  ivation)                 4)                        ']                                    \n#>  conv3_block4_1_conv (Con  (None, 4, 4, 12  28672   ['conv3_block4_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block4_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block4_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block4_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block4_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block4_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block4_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block4_concat (Con  (None, 4, 4, 25  0       ['conv3_block3_concat[0][  N          \n#>  catenate)                6)                        0]',                                  \n#>                                                      'conv3_block4_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block5_0_bn (Batch  (None, 4, 4, 25  1024    ['conv3_block4_concat[0][  N          \n#>  Normalization)           6)                        0]']                                  \n#>  conv3_block5_0_relu (Act  (None, 4, 4, 25  0       ['conv3_block5_0_bn[0][0]  N          \n#>  ivation)                 6)                        ']                                    \n#>  conv3_block5_1_conv (Con  (None, 4, 4, 12  32768   ['conv3_block5_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block5_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block5_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block5_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block5_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block5_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block5_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block5_concat (Con  (None, 4, 4, 28  0       ['conv3_block4_concat[0][  N          \n#>  catenate)                8)                        0]',                                  \n#>                                                      'conv3_block5_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block6_0_bn (Batch  (None, 4, 4, 28  1152    ['conv3_block5_concat[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block6_0_relu (Act  (None, 4, 4, 28  0       ['conv3_block6_0_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block6_1_conv (Con  (None, 4, 4, 12  36864   ['conv3_block6_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block6_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block6_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block6_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block6_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block6_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block6_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block6_concat (Con  (None, 4, 4, 32  0       ['conv3_block5_concat[0][  N          \n#>  catenate)                0)                        0]',                                  \n#>                                                      'conv3_block6_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block7_0_bn (Batch  (None, 4, 4, 32  1280    ['conv3_block6_concat[0][  N          \n#>  Normalization)           0)                        0]']                                  \n#>  conv3_block7_0_relu (Act  (None, 4, 4, 32  0       ['conv3_block7_0_bn[0][0]  N          \n#>  ivation)                 0)                        ']                                    \n#>  conv3_block7_1_conv (Con  (None, 4, 4, 12  40960   ['conv3_block7_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block7_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block7_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block7_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block7_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block7_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block7_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block7_concat (Con  (None, 4, 4, 35  0       ['conv3_block6_concat[0][  N          \n#>  catenate)                2)                        0]',                                  \n#>                                                      'conv3_block7_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block8_0_bn (Batch  (None, 4, 4, 35  1408    ['conv3_block7_concat[0][  N          \n#>  Normalization)           2)                        0]']                                  \n#>  conv3_block8_0_relu (Act  (None, 4, 4, 35  0       ['conv3_block8_0_bn[0][0]  N          \n#>  ivation)                 2)                        ']                                    \n#>  conv3_block8_1_conv (Con  (None, 4, 4, 12  45056   ['conv3_block8_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block8_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block8_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block8_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block8_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block8_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block8_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block8_concat (Con  (None, 4, 4, 38  0       ['conv3_block7_concat[0][  N          \n#>  catenate)                4)                        0]',                                  \n#>                                                      'conv3_block8_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block9_0_bn (Batch  (None, 4, 4, 38  1536    ['conv3_block8_concat[0][  N          \n#>  Normalization)           4)                        0]']                                  \n#>  conv3_block9_0_relu (Act  (None, 4, 4, 38  0       ['conv3_block9_0_bn[0][0]  N          \n#>  ivation)                 4)                        ']                                    \n#>  conv3_block9_1_conv (Con  (None, 4, 4, 12  49152   ['conv3_block9_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv3_block9_1_bn (Batch  (None, 4, 4, 12  512     ['conv3_block9_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv3_block9_1_relu (Act  (None, 4, 4, 12  0       ['conv3_block9_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv3_block9_2_conv (Con  (None, 4, 4, 32  36864   ['conv3_block9_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv3_block9_concat (Con  (None, 4, 4, 41  0       ['conv3_block8_concat[0][  N          \n#>  catenate)                6)                        0]',                                  \n#>                                                      'conv3_block9_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv3_block10_0_bn (Batc  (None, 4, 4, 41  1664    ['conv3_block9_concat[0][  N          \n#>  hNormalization)          6)                        0]']                                  \n#>  conv3_block10_0_relu (Ac  (None, 4, 4, 41  0       ['conv3_block10_0_bn[0][0  N          \n#>  tivation)                6)                        ]']                                   \n#>  conv3_block10_1_conv (Co  (None, 4, 4, 12  53248   ['conv3_block10_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv3_block10_1_bn (Batc  (None, 4, 4, 12  512     ['conv3_block10_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv3_block10_1_relu (Ac  (None, 4, 4, 12  0       ['conv3_block10_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv3_block10_2_conv (Co  (None, 4, 4, 32  36864   ['conv3_block10_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv3_block10_concat (Co  (None, 4, 4, 44  0       ['conv3_block9_concat[0][  N          \n#>  ncatenate)               8)                        0]',                                  \n#>                                                      'conv3_block10_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv3_block11_0_bn (Batc  (None, 4, 4, 44  1792    ['conv3_block10_concat[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv3_block11_0_relu (Ac  (None, 4, 4, 44  0       ['conv3_block11_0_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv3_block11_1_conv (Co  (None, 4, 4, 12  57344   ['conv3_block11_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv3_block11_1_bn (Batc  (None, 4, 4, 12  512     ['conv3_block11_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv3_block11_1_relu (Ac  (None, 4, 4, 12  0       ['conv3_block11_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv3_block11_2_conv (Co  (None, 4, 4, 32  36864   ['conv3_block11_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv3_block11_concat (Co  (None, 4, 4, 48  0       ['conv3_block10_concat[0]  N          \n#>  ncatenate)               0)                        [0]',                                 \n#>                                                      'conv3_block11_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv3_block12_0_bn (Batc  (None, 4, 4, 48  1920    ['conv3_block11_concat[0]  N          \n#>  hNormalization)          0)                        [0]']                                 \n#>  conv3_block12_0_relu (Ac  (None, 4, 4, 48  0       ['conv3_block12_0_bn[0][0  N          \n#>  tivation)                0)                        ]']                                   \n#>  conv3_block12_1_conv (Co  (None, 4, 4, 12  61440   ['conv3_block12_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv3_block12_1_bn (Batc  (None, 4, 4, 12  512     ['conv3_block12_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv3_block12_1_relu (Ac  (None, 4, 4, 12  0       ['conv3_block12_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv3_block12_2_conv (Co  (None, 4, 4, 32  36864   ['conv3_block12_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv3_block12_concat (Co  (None, 4, 4, 51  0       ['conv3_block11_concat[0]  N          \n#>  ncatenate)               2)                        [0]',                                 \n#>                                                      'conv3_block12_2_conv[0]             \n#>                                                     [0]']                                 \n#>  pool3_bn (BatchNormaliza  (None, 4, 4, 51  2048    ['conv3_block12_concat[0]  N          \n#>  tion)                    2)                        [0]']                                 \n#>  pool3_relu (Activation)  (None, 4, 4, 51  0        ['pool3_bn[0][0]']         N          \n#>                           2)                                                              \n#>  pool3_conv (Conv2D)      (None, 4, 4, 25  131072   ['pool3_relu[0][0]']       N          \n#>                           6)                                                              \n#>  pool3_pool (AveragePooli  (None, 2, 2, 25  0       ['pool3_conv[0][0]']       N          \n#>  ng2D)                    6)                                                              \n#>  conv4_block1_0_bn (Batch  (None, 2, 2, 25  1024    ['pool3_pool[0][0]']       N          \n#>  Normalization)           6)                                                              \n#>  conv4_block1_0_relu (Act  (None, 2, 2, 25  0       ['conv4_block1_0_bn[0][0]  N          \n#>  ivation)                 6)                        ']                                    \n#>  conv4_block1_1_conv (Con  (None, 2, 2, 12  32768   ['conv4_block1_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block1_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block1_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block1_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block1_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block1_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block1_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block1_concat (Con  (None, 2, 2, 28  0       ['pool3_pool[0][0]',       N          \n#>  catenate)                8)                         'conv4_block1_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block2_0_bn (Batch  (None, 2, 2, 28  1152    ['conv4_block1_concat[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block2_0_relu (Act  (None, 2, 2, 28  0       ['conv4_block2_0_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block2_1_conv (Con  (None, 2, 2, 12  36864   ['conv4_block2_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block2_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block2_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block2_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block2_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block2_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block2_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block2_concat (Con  (None, 2, 2, 32  0       ['conv4_block1_concat[0][  N          \n#>  catenate)                0)                        0]',                                  \n#>                                                      'conv4_block2_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block3_0_bn (Batch  (None, 2, 2, 32  1280    ['conv4_block2_concat[0][  N          \n#>  Normalization)           0)                        0]']                                  \n#>  conv4_block3_0_relu (Act  (None, 2, 2, 32  0       ['conv4_block3_0_bn[0][0]  N          \n#>  ivation)                 0)                        ']                                    \n#>  conv4_block3_1_conv (Con  (None, 2, 2, 12  40960   ['conv4_block3_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block3_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block3_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block3_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block3_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block3_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block3_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block3_concat (Con  (None, 2, 2, 35  0       ['conv4_block2_concat[0][  N          \n#>  catenate)                2)                        0]',                                  \n#>                                                      'conv4_block3_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block4_0_bn (Batch  (None, 2, 2, 35  1408    ['conv4_block3_concat[0][  N          \n#>  Normalization)           2)                        0]']                                  \n#>  conv4_block4_0_relu (Act  (None, 2, 2, 35  0       ['conv4_block4_0_bn[0][0]  N          \n#>  ivation)                 2)                        ']                                    \n#>  conv4_block4_1_conv (Con  (None, 2, 2, 12  45056   ['conv4_block4_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block4_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block4_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block4_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block4_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block4_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block4_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block4_concat (Con  (None, 2, 2, 38  0       ['conv4_block3_concat[0][  N          \n#>  catenate)                4)                        0]',                                  \n#>                                                      'conv4_block4_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block5_0_bn (Batch  (None, 2, 2, 38  1536    ['conv4_block4_concat[0][  N          \n#>  Normalization)           4)                        0]']                                  \n#>  conv4_block5_0_relu (Act  (None, 2, 2, 38  0       ['conv4_block5_0_bn[0][0]  N          \n#>  ivation)                 4)                        ']                                    \n#>  conv4_block5_1_conv (Con  (None, 2, 2, 12  49152   ['conv4_block5_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block5_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block5_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block5_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block5_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block5_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block5_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block5_concat (Con  (None, 2, 2, 41  0       ['conv4_block4_concat[0][  N          \n#>  catenate)                6)                        0]',                                  \n#>                                                      'conv4_block5_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block6_0_bn (Batch  (None, 2, 2, 41  1664    ['conv4_block5_concat[0][  N          \n#>  Normalization)           6)                        0]']                                  \n#>  conv4_block6_0_relu (Act  (None, 2, 2, 41  0       ['conv4_block6_0_bn[0][0]  N          \n#>  ivation)                 6)                        ']                                    \n#>  conv4_block6_1_conv (Con  (None, 2, 2, 12  53248   ['conv4_block6_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block6_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block6_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block6_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block6_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block6_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block6_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block6_concat (Con  (None, 2, 2, 44  0       ['conv4_block5_concat[0][  N          \n#>  catenate)                8)                        0]',                                  \n#>                                                      'conv4_block6_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block7_0_bn (Batch  (None, 2, 2, 44  1792    ['conv4_block6_concat[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block7_0_relu (Act  (None, 2, 2, 44  0       ['conv4_block7_0_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block7_1_conv (Con  (None, 2, 2, 12  57344   ['conv4_block7_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block7_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block7_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block7_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block7_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block7_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block7_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block7_concat (Con  (None, 2, 2, 48  0       ['conv4_block6_concat[0][  N          \n#>  catenate)                0)                        0]',                                  \n#>                                                      'conv4_block7_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block8_0_bn (Batch  (None, 2, 2, 48  1920    ['conv4_block7_concat[0][  N          \n#>  Normalization)           0)                        0]']                                  \n#>  conv4_block8_0_relu (Act  (None, 2, 2, 48  0       ['conv4_block8_0_bn[0][0]  N          \n#>  ivation)                 0)                        ']                                    \n#>  conv4_block8_1_conv (Con  (None, 2, 2, 12  61440   ['conv4_block8_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block8_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block8_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block8_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block8_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block8_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block8_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block8_concat (Con  (None, 2, 2, 51  0       ['conv4_block7_concat[0][  N          \n#>  catenate)                2)                        0]',                                  \n#>                                                      'conv4_block8_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block9_0_bn (Batch  (None, 2, 2, 51  2048    ['conv4_block8_concat[0][  N          \n#>  Normalization)           2)                        0]']                                  \n#>  conv4_block9_0_relu (Act  (None, 2, 2, 51  0       ['conv4_block9_0_bn[0][0]  N          \n#>  ivation)                 2)                        ']                                    \n#>  conv4_block9_1_conv (Con  (None, 2, 2, 12  65536   ['conv4_block9_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv4_block9_1_bn (Batch  (None, 2, 2, 12  512     ['conv4_block9_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv4_block9_1_relu (Act  (None, 2, 2, 12  0       ['conv4_block9_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv4_block9_2_conv (Con  (None, 2, 2, 32  36864   ['conv4_block9_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv4_block9_concat (Con  (None, 2, 2, 54  0       ['conv4_block8_concat[0][  N          \n#>  catenate)                4)                        0]',                                  \n#>                                                      'conv4_block9_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv4_block10_0_bn (Batc  (None, 2, 2, 54  2176    ['conv4_block9_concat[0][  N          \n#>  hNormalization)          4)                        0]']                                  \n#>  conv4_block10_0_relu (Ac  (None, 2, 2, 54  0       ['conv4_block10_0_bn[0][0  N          \n#>  tivation)                4)                        ]']                                   \n#>  conv4_block10_1_conv (Co  (None, 2, 2, 12  69632   ['conv4_block10_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block10_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block10_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block10_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block10_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block10_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block10_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block10_concat (Co  (None, 2, 2, 57  0       ['conv4_block9_concat[0][  N          \n#>  ncatenate)               6)                        0]',                                  \n#>                                                      'conv4_block10_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block11_0_bn (Batc  (None, 2, 2, 57  2304    ['conv4_block10_concat[0]  N          \n#>  hNormalization)          6)                        [0]']                                 \n#>  conv4_block11_0_relu (Ac  (None, 2, 2, 57  0       ['conv4_block11_0_bn[0][0  N          \n#>  tivation)                6)                        ]']                                   \n#>  conv4_block11_1_conv (Co  (None, 2, 2, 12  73728   ['conv4_block11_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block11_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block11_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block11_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block11_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block11_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block11_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block11_concat (Co  (None, 2, 2, 60  0       ['conv4_block10_concat[0]  N          \n#>  ncatenate)               8)                        [0]',                                 \n#>                                                      'conv4_block11_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block12_0_bn (Batc  (None, 2, 2, 60  2432    ['conv4_block11_concat[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block12_0_relu (Ac  (None, 2, 2, 60  0       ['conv4_block12_0_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block12_1_conv (Co  (None, 2, 2, 12  77824   ['conv4_block12_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block12_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block12_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block12_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block12_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block12_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block12_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block12_concat (Co  (None, 2, 2, 64  0       ['conv4_block11_concat[0]  N          \n#>  ncatenate)               0)                        [0]',                                 \n#>                                                      'conv4_block12_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block13_0_bn (Batc  (None, 2, 2, 64  2560    ['conv4_block12_concat[0]  N          \n#>  hNormalization)          0)                        [0]']                                 \n#>  conv4_block13_0_relu (Ac  (None, 2, 2, 64  0       ['conv4_block13_0_bn[0][0  N          \n#>  tivation)                0)                        ]']                                   \n#>  conv4_block13_1_conv (Co  (None, 2, 2, 12  81920   ['conv4_block13_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block13_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block13_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block13_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block13_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block13_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block13_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block13_concat (Co  (None, 2, 2, 67  0       ['conv4_block12_concat[0]  N          \n#>  ncatenate)               2)                        [0]',                                 \n#>                                                      'conv4_block13_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block14_0_bn (Batc  (None, 2, 2, 67  2688    ['conv4_block13_concat[0]  N          \n#>  hNormalization)          2)                        [0]']                                 \n#>  conv4_block14_0_relu (Ac  (None, 2, 2, 67  0       ['conv4_block14_0_bn[0][0  N          \n#>  tivation)                2)                        ]']                                   \n#>  conv4_block14_1_conv (Co  (None, 2, 2, 12  86016   ['conv4_block14_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block14_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block14_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block14_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block14_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block14_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block14_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block14_concat (Co  (None, 2, 2, 70  0       ['conv4_block13_concat[0]  N          \n#>  ncatenate)               4)                        [0]',                                 \n#>                                                      'conv4_block14_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block15_0_bn (Batc  (None, 2, 2, 70  2816    ['conv4_block14_concat[0]  N          \n#>  hNormalization)          4)                        [0]']                                 \n#>  conv4_block15_0_relu (Ac  (None, 2, 2, 70  0       ['conv4_block15_0_bn[0][0  N          \n#>  tivation)                4)                        ]']                                   \n#>  conv4_block15_1_conv (Co  (None, 2, 2, 12  90112   ['conv4_block15_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block15_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block15_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block15_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block15_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block15_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block15_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block15_concat (Co  (None, 2, 2, 73  0       ['conv4_block14_concat[0]  N          \n#>  ncatenate)               6)                        [0]',                                 \n#>                                                      'conv4_block15_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block16_0_bn (Batc  (None, 2, 2, 73  2944    ['conv4_block15_concat[0]  N          \n#>  hNormalization)          6)                        [0]']                                 \n#>  conv4_block16_0_relu (Ac  (None, 2, 2, 73  0       ['conv4_block16_0_bn[0][0  N          \n#>  tivation)                6)                        ]']                                   \n#>  conv4_block16_1_conv (Co  (None, 2, 2, 12  94208   ['conv4_block16_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block16_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block16_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block16_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block16_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block16_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block16_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block16_concat (Co  (None, 2, 2, 76  0       ['conv4_block15_concat[0]  N          \n#>  ncatenate)               8)                        [0]',                                 \n#>                                                      'conv4_block16_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block17_0_bn (Batc  (None, 2, 2, 76  3072    ['conv4_block16_concat[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block17_0_relu (Ac  (None, 2, 2, 76  0       ['conv4_block17_0_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block17_1_conv (Co  (None, 2, 2, 12  98304   ['conv4_block17_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block17_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block17_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block17_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block17_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block17_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block17_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block17_concat (Co  (None, 2, 2, 80  0       ['conv4_block16_concat[0]  N          \n#>  ncatenate)               0)                        [0]',                                 \n#>                                                      'conv4_block17_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block18_0_bn (Batc  (None, 2, 2, 80  3200    ['conv4_block17_concat[0]  N          \n#>  hNormalization)          0)                        [0]']                                 \n#>  conv4_block18_0_relu (Ac  (None, 2, 2, 80  0       ['conv4_block18_0_bn[0][0  N          \n#>  tivation)                0)                        ]']                                   \n#>  conv4_block18_1_conv (Co  (None, 2, 2, 12  102400  ['conv4_block18_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block18_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block18_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block18_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block18_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block18_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block18_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block18_concat (Co  (None, 2, 2, 83  0       ['conv4_block17_concat[0]  N          \n#>  ncatenate)               2)                        [0]',                                 \n#>                                                      'conv4_block18_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block19_0_bn (Batc  (None, 2, 2, 83  3328    ['conv4_block18_concat[0]  N          \n#>  hNormalization)          2)                        [0]']                                 \n#>  conv4_block19_0_relu (Ac  (None, 2, 2, 83  0       ['conv4_block19_0_bn[0][0  N          \n#>  tivation)                2)                        ]']                                   \n#>  conv4_block19_1_conv (Co  (None, 2, 2, 12  106496  ['conv4_block19_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block19_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block19_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block19_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block19_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block19_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block19_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block19_concat (Co  (None, 2, 2, 86  0       ['conv4_block18_concat[0]  N          \n#>  ncatenate)               4)                        [0]',                                 \n#>                                                      'conv4_block19_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block20_0_bn (Batc  (None, 2, 2, 86  3456    ['conv4_block19_concat[0]  N          \n#>  hNormalization)          4)                        [0]']                                 \n#>  conv4_block20_0_relu (Ac  (None, 2, 2, 86  0       ['conv4_block20_0_bn[0][0  N          \n#>  tivation)                4)                        ]']                                   \n#>  conv4_block20_1_conv (Co  (None, 2, 2, 12  110592  ['conv4_block20_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block20_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block20_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block20_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block20_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block20_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block20_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block20_concat (Co  (None, 2, 2, 89  0       ['conv4_block19_concat[0]  N          \n#>  ncatenate)               6)                        [0]',                                 \n#>                                                      'conv4_block20_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block21_0_bn (Batc  (None, 2, 2, 89  3584    ['conv4_block20_concat[0]  N          \n#>  hNormalization)          6)                        [0]']                                 \n#>  conv4_block21_0_relu (Ac  (None, 2, 2, 89  0       ['conv4_block21_0_bn[0][0  N          \n#>  tivation)                6)                        ]']                                   \n#>  conv4_block21_1_conv (Co  (None, 2, 2, 12  114688  ['conv4_block21_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block21_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block21_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block21_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block21_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block21_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block21_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block21_concat (Co  (None, 2, 2, 92  0       ['conv4_block20_concat[0]  N          \n#>  ncatenate)               8)                        [0]',                                 \n#>                                                      'conv4_block21_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block22_0_bn (Batc  (None, 2, 2, 92  3712    ['conv4_block21_concat[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block22_0_relu (Ac  (None, 2, 2, 92  0       ['conv4_block22_0_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block22_1_conv (Co  (None, 2, 2, 12  118784  ['conv4_block22_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block22_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block22_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block22_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block22_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block22_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block22_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block22_concat (Co  (None, 2, 2, 96  0       ['conv4_block21_concat[0]  N          \n#>  ncatenate)               0)                        [0]',                                 \n#>                                                      'conv4_block22_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block23_0_bn (Batc  (None, 2, 2, 96  3840    ['conv4_block22_concat[0]  N          \n#>  hNormalization)          0)                        [0]']                                 \n#>  conv4_block23_0_relu (Ac  (None, 2, 2, 96  0       ['conv4_block23_0_bn[0][0  N          \n#>  tivation)                0)                        ]']                                   \n#>  conv4_block23_1_conv (Co  (None, 2, 2, 12  122880  ['conv4_block23_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block23_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block23_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block23_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block23_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block23_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block23_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block23_concat (Co  (None, 2, 2, 99  0       ['conv4_block22_concat[0]  N          \n#>  ncatenate)               2)                        [0]',                                 \n#>                                                      'conv4_block23_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block24_0_bn (Batc  (None, 2, 2, 99  3968    ['conv4_block23_concat[0]  N          \n#>  hNormalization)          2)                        [0]']                                 \n#>  conv4_block24_0_relu (Ac  (None, 2, 2, 99  0       ['conv4_block24_0_bn[0][0  N          \n#>  tivation)                2)                        ]']                                   \n#>  conv4_block24_1_conv (Co  (None, 2, 2, 12  126976  ['conv4_block24_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block24_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block24_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block24_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block24_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block24_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block24_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block24_concat (Co  (None, 2, 2, 10  0       ['conv4_block23_concat[0]  N          \n#>  ncatenate)               24)                       [0]',                                 \n#>                                                      'conv4_block24_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block25_0_bn (Batc  (None, 2, 2, 10  4096    ['conv4_block24_concat[0]  N          \n#>  hNormalization)          24)                       [0]']                                 \n#>  conv4_block25_0_relu (Ac  (None, 2, 2, 10  0       ['conv4_block25_0_bn[0][0  N          \n#>  tivation)                24)                       ]']                                   \n#>  conv4_block25_1_conv (Co  (None, 2, 2, 12  131072  ['conv4_block25_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block25_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block25_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block25_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block25_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block25_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block25_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block25_concat (Co  (None, 2, 2, 10  0       ['conv4_block24_concat[0]  N          \n#>  ncatenate)               56)                       [0]',                                 \n#>                                                      'conv4_block25_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block26_0_bn (Batc  (None, 2, 2, 10  4224    ['conv4_block25_concat[0]  N          \n#>  hNormalization)          56)                       [0]']                                 \n#>  conv4_block26_0_relu (Ac  (None, 2, 2, 10  0       ['conv4_block26_0_bn[0][0  N          \n#>  tivation)                56)                       ]']                                   \n#>  conv4_block26_1_conv (Co  (None, 2, 2, 12  135168  ['conv4_block26_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block26_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block26_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block26_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block26_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block26_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block26_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block26_concat (Co  (None, 2, 2, 10  0       ['conv4_block25_concat[0]  N          \n#>  ncatenate)               88)                       [0]',                                 \n#>                                                      'conv4_block26_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block27_0_bn (Batc  (None, 2, 2, 10  4352    ['conv4_block26_concat[0]  N          \n#>  hNormalization)          88)                       [0]']                                 \n#>  conv4_block27_0_relu (Ac  (None, 2, 2, 10  0       ['conv4_block27_0_bn[0][0  N          \n#>  tivation)                88)                       ]']                                   \n#>  conv4_block27_1_conv (Co  (None, 2, 2, 12  139264  ['conv4_block27_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block27_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block27_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block27_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block27_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block27_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block27_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block27_concat (Co  (None, 2, 2, 11  0       ['conv4_block26_concat[0]  N          \n#>  ncatenate)               20)                       [0]',                                 \n#>                                                      'conv4_block27_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block28_0_bn (Batc  (None, 2, 2, 11  4480    ['conv4_block27_concat[0]  N          \n#>  hNormalization)          20)                       [0]']                                 \n#>  conv4_block28_0_relu (Ac  (None, 2, 2, 11  0       ['conv4_block28_0_bn[0][0  N          \n#>  tivation)                20)                       ]']                                   \n#>  conv4_block28_1_conv (Co  (None, 2, 2, 12  143360  ['conv4_block28_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block28_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block28_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block28_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block28_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block28_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block28_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block28_concat (Co  (None, 2, 2, 11  0       ['conv4_block27_concat[0]  N          \n#>  ncatenate)               52)                       [0]',                                 \n#>                                                      'conv4_block28_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block29_0_bn (Batc  (None, 2, 2, 11  4608    ['conv4_block28_concat[0]  N          \n#>  hNormalization)          52)                       [0]']                                 \n#>  conv4_block29_0_relu (Ac  (None, 2, 2, 11  0       ['conv4_block29_0_bn[0][0  N          \n#>  tivation)                52)                       ]']                                   \n#>  conv4_block29_1_conv (Co  (None, 2, 2, 12  147456  ['conv4_block29_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block29_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block29_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block29_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block29_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block29_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block29_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block29_concat (Co  (None, 2, 2, 11  0       ['conv4_block28_concat[0]  N          \n#>  ncatenate)               84)                       [0]',                                 \n#>                                                      'conv4_block29_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block30_0_bn (Batc  (None, 2, 2, 11  4736    ['conv4_block29_concat[0]  N          \n#>  hNormalization)          84)                       [0]']                                 \n#>  conv4_block30_0_relu (Ac  (None, 2, 2, 11  0       ['conv4_block30_0_bn[0][0  N          \n#>  tivation)                84)                       ]']                                   \n#>  conv4_block30_1_conv (Co  (None, 2, 2, 12  151552  ['conv4_block30_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block30_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block30_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block30_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block30_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block30_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block30_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block30_concat (Co  (None, 2, 2, 12  0       ['conv4_block29_concat[0]  N          \n#>  ncatenate)               16)                       [0]',                                 \n#>                                                      'conv4_block30_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block31_0_bn (Batc  (None, 2, 2, 12  4864    ['conv4_block30_concat[0]  N          \n#>  hNormalization)          16)                       [0]']                                 \n#>  conv4_block31_0_relu (Ac  (None, 2, 2, 12  0       ['conv4_block31_0_bn[0][0  N          \n#>  tivation)                16)                       ]']                                   \n#>  conv4_block31_1_conv (Co  (None, 2, 2, 12  155648  ['conv4_block31_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block31_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block31_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block31_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block31_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block31_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block31_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block31_concat (Co  (None, 2, 2, 12  0       ['conv4_block30_concat[0]  N          \n#>  ncatenate)               48)                       [0]',                                 \n#>                                                      'conv4_block31_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block32_0_bn (Batc  (None, 2, 2, 12  4992    ['conv4_block31_concat[0]  N          \n#>  hNormalization)          48)                       [0]']                                 \n#>  conv4_block32_0_relu (Ac  (None, 2, 2, 12  0       ['conv4_block32_0_bn[0][0  N          \n#>  tivation)                48)                       ]']                                   \n#>  conv4_block32_1_conv (Co  (None, 2, 2, 12  159744  ['conv4_block32_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block32_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block32_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block32_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block32_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block32_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block32_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block32_concat (Co  (None, 2, 2, 12  0       ['conv4_block31_concat[0]  N          \n#>  ncatenate)               80)                       [0]',                                 \n#>                                                      'conv4_block32_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block33_0_bn (Batc  (None, 2, 2, 12  5120    ['conv4_block32_concat[0]  N          \n#>  hNormalization)          80)                       [0]']                                 \n#>  conv4_block33_0_relu (Ac  (None, 2, 2, 12  0       ['conv4_block33_0_bn[0][0  N          \n#>  tivation)                80)                       ]']                                   \n#>  conv4_block33_1_conv (Co  (None, 2, 2, 12  163840  ['conv4_block33_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block33_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block33_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block33_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block33_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block33_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block33_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block33_concat (Co  (None, 2, 2, 13  0       ['conv4_block32_concat[0]  N          \n#>  ncatenate)               12)                       [0]',                                 \n#>                                                      'conv4_block33_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block34_0_bn (Batc  (None, 2, 2, 13  5248    ['conv4_block33_concat[0]  N          \n#>  hNormalization)          12)                       [0]']                                 \n#>  conv4_block34_0_relu (Ac  (None, 2, 2, 13  0       ['conv4_block34_0_bn[0][0  N          \n#>  tivation)                12)                       ]']                                   \n#>  conv4_block34_1_conv (Co  (None, 2, 2, 12  167936  ['conv4_block34_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block34_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block34_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block34_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block34_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block34_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block34_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block34_concat (Co  (None, 2, 2, 13  0       ['conv4_block33_concat[0]  N          \n#>  ncatenate)               44)                       [0]',                                 \n#>                                                      'conv4_block34_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block35_0_bn (Batc  (None, 2, 2, 13  5376    ['conv4_block34_concat[0]  N          \n#>  hNormalization)          44)                       [0]']                                 \n#>  conv4_block35_0_relu (Ac  (None, 2, 2, 13  0       ['conv4_block35_0_bn[0][0  N          \n#>  tivation)                44)                       ]']                                   \n#>  conv4_block35_1_conv (Co  (None, 2, 2, 12  172032  ['conv4_block35_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block35_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block35_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block35_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block35_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block35_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block35_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block35_concat (Co  (None, 2, 2, 13  0       ['conv4_block34_concat[0]  N          \n#>  ncatenate)               76)                       [0]',                                 \n#>                                                      'conv4_block35_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block36_0_bn (Batc  (None, 2, 2, 13  5504    ['conv4_block35_concat[0]  N          \n#>  hNormalization)          76)                       [0]']                                 \n#>  conv4_block36_0_relu (Ac  (None, 2, 2, 13  0       ['conv4_block36_0_bn[0][0  N          \n#>  tivation)                76)                       ]']                                   \n#>  conv4_block36_1_conv (Co  (None, 2, 2, 12  176128  ['conv4_block36_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block36_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block36_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block36_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block36_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block36_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block36_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block36_concat (Co  (None, 2, 2, 14  0       ['conv4_block35_concat[0]  N          \n#>  ncatenate)               08)                       [0]',                                 \n#>                                                      'conv4_block36_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block37_0_bn (Batc  (None, 2, 2, 14  5632    ['conv4_block36_concat[0]  N          \n#>  hNormalization)          08)                       [0]']                                 \n#>  conv4_block37_0_relu (Ac  (None, 2, 2, 14  0       ['conv4_block37_0_bn[0][0  N          \n#>  tivation)                08)                       ]']                                   \n#>  conv4_block37_1_conv (Co  (None, 2, 2, 12  180224  ['conv4_block37_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block37_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block37_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block37_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block37_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block37_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block37_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block37_concat (Co  (None, 2, 2, 14  0       ['conv4_block36_concat[0]  N          \n#>  ncatenate)               40)                       [0]',                                 \n#>                                                      'conv4_block37_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block38_0_bn (Batc  (None, 2, 2, 14  5760    ['conv4_block37_concat[0]  N          \n#>  hNormalization)          40)                       [0]']                                 \n#>  conv4_block38_0_relu (Ac  (None, 2, 2, 14  0       ['conv4_block38_0_bn[0][0  N          \n#>  tivation)                40)                       ]']                                   \n#>  conv4_block38_1_conv (Co  (None, 2, 2, 12  184320  ['conv4_block38_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block38_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block38_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block38_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block38_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block38_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block38_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block38_concat (Co  (None, 2, 2, 14  0       ['conv4_block37_concat[0]  N          \n#>  ncatenate)               72)                       [0]',                                 \n#>                                                      'conv4_block38_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block39_0_bn (Batc  (None, 2, 2, 14  5888    ['conv4_block38_concat[0]  N          \n#>  hNormalization)          72)                       [0]']                                 \n#>  conv4_block39_0_relu (Ac  (None, 2, 2, 14  0       ['conv4_block39_0_bn[0][0  N          \n#>  tivation)                72)                       ]']                                   \n#>  conv4_block39_1_conv (Co  (None, 2, 2, 12  188416  ['conv4_block39_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block39_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block39_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block39_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block39_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block39_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block39_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block39_concat (Co  (None, 2, 2, 15  0       ['conv4_block38_concat[0]  N          \n#>  ncatenate)               04)                       [0]',                                 \n#>                                                      'conv4_block39_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block40_0_bn (Batc  (None, 2, 2, 15  6016    ['conv4_block39_concat[0]  N          \n#>  hNormalization)          04)                       [0]']                                 \n#>  conv4_block40_0_relu (Ac  (None, 2, 2, 15  0       ['conv4_block40_0_bn[0][0  N          \n#>  tivation)                04)                       ]']                                   \n#>  conv4_block40_1_conv (Co  (None, 2, 2, 12  192512  ['conv4_block40_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block40_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block40_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block40_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block40_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block40_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block40_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block40_concat (Co  (None, 2, 2, 15  0       ['conv4_block39_concat[0]  N          \n#>  ncatenate)               36)                       [0]',                                 \n#>                                                      'conv4_block40_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block41_0_bn (Batc  (None, 2, 2, 15  6144    ['conv4_block40_concat[0]  N          \n#>  hNormalization)          36)                       [0]']                                 \n#>  conv4_block41_0_relu (Ac  (None, 2, 2, 15  0       ['conv4_block41_0_bn[0][0  N          \n#>  tivation)                36)                       ]']                                   \n#>  conv4_block41_1_conv (Co  (None, 2, 2, 12  196608  ['conv4_block41_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block41_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block41_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block41_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block41_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block41_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block41_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block41_concat (Co  (None, 2, 2, 15  0       ['conv4_block40_concat[0]  N          \n#>  ncatenate)               68)                       [0]',                                 \n#>                                                      'conv4_block41_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block42_0_bn (Batc  (None, 2, 2, 15  6272    ['conv4_block41_concat[0]  N          \n#>  hNormalization)          68)                       [0]']                                 \n#>  conv4_block42_0_relu (Ac  (None, 2, 2, 15  0       ['conv4_block42_0_bn[0][0  N          \n#>  tivation)                68)                       ]']                                   \n#>  conv4_block42_1_conv (Co  (None, 2, 2, 12  200704  ['conv4_block42_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block42_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block42_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block42_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block42_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block42_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block42_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block42_concat (Co  (None, 2, 2, 16  0       ['conv4_block41_concat[0]  N          \n#>  ncatenate)               00)                       [0]',                                 \n#>                                                      'conv4_block42_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block43_0_bn (Batc  (None, 2, 2, 16  6400    ['conv4_block42_concat[0]  N          \n#>  hNormalization)          00)                       [0]']                                 \n#>  conv4_block43_0_relu (Ac  (None, 2, 2, 16  0       ['conv4_block43_0_bn[0][0  N          \n#>  tivation)                00)                       ]']                                   \n#>  conv4_block43_1_conv (Co  (None, 2, 2, 12  204800  ['conv4_block43_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block43_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block43_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block43_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block43_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block43_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block43_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block43_concat (Co  (None, 2, 2, 16  0       ['conv4_block42_concat[0]  N          \n#>  ncatenate)               32)                       [0]',                                 \n#>                                                      'conv4_block43_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block44_0_bn (Batc  (None, 2, 2, 16  6528    ['conv4_block43_concat[0]  N          \n#>  hNormalization)          32)                       [0]']                                 \n#>  conv4_block44_0_relu (Ac  (None, 2, 2, 16  0       ['conv4_block44_0_bn[0][0  N          \n#>  tivation)                32)                       ]']                                   \n#>  conv4_block44_1_conv (Co  (None, 2, 2, 12  208896  ['conv4_block44_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block44_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block44_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block44_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block44_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block44_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block44_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block44_concat (Co  (None, 2, 2, 16  0       ['conv4_block43_concat[0]  N          \n#>  ncatenate)               64)                       [0]',                                 \n#>                                                      'conv4_block44_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block45_0_bn (Batc  (None, 2, 2, 16  6656    ['conv4_block44_concat[0]  N          \n#>  hNormalization)          64)                       [0]']                                 \n#>  conv4_block45_0_relu (Ac  (None, 2, 2, 16  0       ['conv4_block45_0_bn[0][0  N          \n#>  tivation)                64)                       ]']                                   \n#>  conv4_block45_1_conv (Co  (None, 2, 2, 12  212992  ['conv4_block45_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block45_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block45_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block45_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block45_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block45_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block45_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block45_concat (Co  (None, 2, 2, 16  0       ['conv4_block44_concat[0]  N          \n#>  ncatenate)               96)                       [0]',                                 \n#>                                                      'conv4_block45_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block46_0_bn (Batc  (None, 2, 2, 16  6784    ['conv4_block45_concat[0]  N          \n#>  hNormalization)          96)                       [0]']                                 \n#>  conv4_block46_0_relu (Ac  (None, 2, 2, 16  0       ['conv4_block46_0_bn[0][0  N          \n#>  tivation)                96)                       ]']                                   \n#>  conv4_block46_1_conv (Co  (None, 2, 2, 12  217088  ['conv4_block46_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block46_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block46_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block46_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block46_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block46_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block46_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block46_concat (Co  (None, 2, 2, 17  0       ['conv4_block45_concat[0]  N          \n#>  ncatenate)               28)                       [0]',                                 \n#>                                                      'conv4_block46_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block47_0_bn (Batc  (None, 2, 2, 17  6912    ['conv4_block46_concat[0]  N          \n#>  hNormalization)          28)                       [0]']                                 \n#>  conv4_block47_0_relu (Ac  (None, 2, 2, 17  0       ['conv4_block47_0_bn[0][0  N          \n#>  tivation)                28)                       ]']                                   \n#>  conv4_block47_1_conv (Co  (None, 2, 2, 12  221184  ['conv4_block47_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block47_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block47_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block47_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block47_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block47_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block47_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block47_concat (Co  (None, 2, 2, 17  0       ['conv4_block46_concat[0]  N          \n#>  ncatenate)               60)                       [0]',                                 \n#>                                                      'conv4_block47_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv4_block48_0_bn (Batc  (None, 2, 2, 17  7040    ['conv4_block47_concat[0]  N          \n#>  hNormalization)          60)                       [0]']                                 \n#>  conv4_block48_0_relu (Ac  (None, 2, 2, 17  0       ['conv4_block48_0_bn[0][0  N          \n#>  tivation)                60)                       ]']                                   \n#>  conv4_block48_1_conv (Co  (None, 2, 2, 12  225280  ['conv4_block48_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv4_block48_1_bn (Batc  (None, 2, 2, 12  512     ['conv4_block48_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv4_block48_1_relu (Ac  (None, 2, 2, 12  0       ['conv4_block48_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv4_block48_2_conv (Co  (None, 2, 2, 32  36864   ['conv4_block48_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv4_block48_concat (Co  (None, 2, 2, 17  0       ['conv4_block47_concat[0]  N          \n#>  ncatenate)               92)                       [0]',                                 \n#>                                                      'conv4_block48_2_conv[0]             \n#>                                                     [0]']                                 \n#>  pool4_bn (BatchNormaliza  (None, 2, 2, 17  7168    ['conv4_block48_concat[0]  N          \n#>  tion)                    92)                       [0]']                                 \n#>  pool4_relu (Activation)  (None, 2, 2, 17  0        ['pool4_bn[0][0]']         N          \n#>                           92)                                                             \n#>  pool4_conv (Conv2D)      (None, 2, 2, 89  1605632  ['pool4_relu[0][0]']       N          \n#>                           6)                                                              \n#>  pool4_pool (AveragePooli  (None, 1, 1, 89  0       ['pool4_conv[0][0]']       N          \n#>  ng2D)                    6)                                                              \n#>  conv5_block1_0_bn (Batch  (None, 1, 1, 89  3584    ['pool4_pool[0][0]']       N          \n#>  Normalization)           6)                                                              \n#>  conv5_block1_0_relu (Act  (None, 1, 1, 89  0       ['conv5_block1_0_bn[0][0]  N          \n#>  ivation)                 6)                        ']                                    \n#>  conv5_block1_1_conv (Con  (None, 1, 1, 12  114688  ['conv5_block1_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block1_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block1_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block1_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block1_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block1_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block1_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block1_concat (Con  (None, 1, 1, 92  0       ['pool4_pool[0][0]',       N          \n#>  catenate)                8)                         'conv5_block1_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block2_0_bn (Batch  (None, 1, 1, 92  3712    ['conv5_block1_concat[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block2_0_relu (Act  (None, 1, 1, 92  0       ['conv5_block2_0_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block2_1_conv (Con  (None, 1, 1, 12  118784  ['conv5_block2_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block2_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block2_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block2_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block2_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block2_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block2_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block2_concat (Con  (None, 1, 1, 96  0       ['conv5_block1_concat[0][  N          \n#>  catenate)                0)                        0]',                                  \n#>                                                      'conv5_block2_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block3_0_bn (Batch  (None, 1, 1, 96  3840    ['conv5_block2_concat[0][  N          \n#>  Normalization)           0)                        0]']                                  \n#>  conv5_block3_0_relu (Act  (None, 1, 1, 96  0       ['conv5_block3_0_bn[0][0]  N          \n#>  ivation)                 0)                        ']                                    \n#>  conv5_block3_1_conv (Con  (None, 1, 1, 12  122880  ['conv5_block3_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block3_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block3_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block3_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block3_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block3_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block3_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block3_concat (Con  (None, 1, 1, 99  0       ['conv5_block2_concat[0][  N          \n#>  catenate)                2)                        0]',                                  \n#>                                                      'conv5_block3_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block4_0_bn (Batch  (None, 1, 1, 99  3968    ['conv5_block3_concat[0][  N          \n#>  Normalization)           2)                        0]']                                  \n#>  conv5_block4_0_relu (Act  (None, 1, 1, 99  0       ['conv5_block4_0_bn[0][0]  N          \n#>  ivation)                 2)                        ']                                    \n#>  conv5_block4_1_conv (Con  (None, 1, 1, 12  126976  ['conv5_block4_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block4_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block4_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block4_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block4_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block4_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block4_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block4_concat (Con  (None, 1, 1, 10  0       ['conv5_block3_concat[0][  N          \n#>  catenate)                24)                       0]',                                  \n#>                                                      'conv5_block4_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block5_0_bn (Batch  (None, 1, 1, 10  4096    ['conv5_block4_concat[0][  N          \n#>  Normalization)           24)                       0]']                                  \n#>  conv5_block5_0_relu (Act  (None, 1, 1, 10  0       ['conv5_block5_0_bn[0][0]  N          \n#>  ivation)                 24)                       ']                                    \n#>  conv5_block5_1_conv (Con  (None, 1, 1, 12  131072  ['conv5_block5_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block5_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block5_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block5_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block5_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block5_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block5_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block5_concat (Con  (None, 1, 1, 10  0       ['conv5_block4_concat[0][  N          \n#>  catenate)                56)                       0]',                                  \n#>                                                      'conv5_block5_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block6_0_bn (Batch  (None, 1, 1, 10  4224    ['conv5_block5_concat[0][  N          \n#>  Normalization)           56)                       0]']                                  \n#>  conv5_block6_0_relu (Act  (None, 1, 1, 10  0       ['conv5_block6_0_bn[0][0]  N          \n#>  ivation)                 56)                       ']                                    \n#>  conv5_block6_1_conv (Con  (None, 1, 1, 12  135168  ['conv5_block6_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block6_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block6_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block6_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block6_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block6_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block6_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block6_concat (Con  (None, 1, 1, 10  0       ['conv5_block5_concat[0][  N          \n#>  catenate)                88)                       0]',                                  \n#>                                                      'conv5_block6_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block7_0_bn (Batch  (None, 1, 1, 10  4352    ['conv5_block6_concat[0][  N          \n#>  Normalization)           88)                       0]']                                  \n#>  conv5_block7_0_relu (Act  (None, 1, 1, 10  0       ['conv5_block7_0_bn[0][0]  N          \n#>  ivation)                 88)                       ']                                    \n#>  conv5_block7_1_conv (Con  (None, 1, 1, 12  139264  ['conv5_block7_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block7_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block7_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block7_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block7_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block7_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block7_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block7_concat (Con  (None, 1, 1, 11  0       ['conv5_block6_concat[0][  N          \n#>  catenate)                20)                       0]',                                  \n#>                                                      'conv5_block7_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block8_0_bn (Batch  (None, 1, 1, 11  4480    ['conv5_block7_concat[0][  N          \n#>  Normalization)           20)                       0]']                                  \n#>  conv5_block8_0_relu (Act  (None, 1, 1, 11  0       ['conv5_block8_0_bn[0][0]  N          \n#>  ivation)                 20)                       ']                                    \n#>  conv5_block8_1_conv (Con  (None, 1, 1, 12  143360  ['conv5_block8_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block8_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block8_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block8_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block8_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block8_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block8_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block8_concat (Con  (None, 1, 1, 11  0       ['conv5_block7_concat[0][  N          \n#>  catenate)                52)                       0]',                                  \n#>                                                      'conv5_block8_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block9_0_bn (Batch  (None, 1, 1, 11  4608    ['conv5_block8_concat[0][  N          \n#>  Normalization)           52)                       0]']                                  \n#>  conv5_block9_0_relu (Act  (None, 1, 1, 11  0       ['conv5_block9_0_bn[0][0]  N          \n#>  ivation)                 52)                       ']                                    \n#>  conv5_block9_1_conv (Con  (None, 1, 1, 12  147456  ['conv5_block9_0_relu[0][  N          \n#>  v2D)                     8)                        0]']                                  \n#>  conv5_block9_1_bn (Batch  (None, 1, 1, 12  512     ['conv5_block9_1_conv[0][  N          \n#>  Normalization)           8)                        0]']                                  \n#>  conv5_block9_1_relu (Act  (None, 1, 1, 12  0       ['conv5_block9_1_bn[0][0]  N          \n#>  ivation)                 8)                        ']                                    \n#>  conv5_block9_2_conv (Con  (None, 1, 1, 32  36864   ['conv5_block9_1_relu[0][  N          \n#>  v2D)                     )                         0]']                                  \n#>  conv5_block9_concat (Con  (None, 1, 1, 11  0       ['conv5_block8_concat[0][  N          \n#>  catenate)                84)                       0]',                                  \n#>                                                      'conv5_block9_2_conv[0][             \n#>                                                     0]']                                  \n#>  conv5_block10_0_bn (Batc  (None, 1, 1, 11  4736    ['conv5_block9_concat[0][  N          \n#>  hNormalization)          84)                       0]']                                  \n#>  conv5_block10_0_relu (Ac  (None, 1, 1, 11  0       ['conv5_block10_0_bn[0][0  N          \n#>  tivation)                84)                       ]']                                   \n#>  conv5_block10_1_conv (Co  (None, 1, 1, 12  151552  ['conv5_block10_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block10_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block10_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block10_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block10_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block10_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block10_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block10_concat (Co  (None, 1, 1, 12  0       ['conv5_block9_concat[0][  N          \n#>  ncatenate)               16)                       0]',                                  \n#>                                                      'conv5_block10_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block11_0_bn (Batc  (None, 1, 1, 12  4864    ['conv5_block10_concat[0]  N          \n#>  hNormalization)          16)                       [0]']                                 \n#>  conv5_block11_0_relu (Ac  (None, 1, 1, 12  0       ['conv5_block11_0_bn[0][0  N          \n#>  tivation)                16)                       ]']                                   \n#>  conv5_block11_1_conv (Co  (None, 1, 1, 12  155648  ['conv5_block11_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block11_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block11_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block11_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block11_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block11_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block11_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block11_concat (Co  (None, 1, 1, 12  0       ['conv5_block10_concat[0]  N          \n#>  ncatenate)               48)                       [0]',                                 \n#>                                                      'conv5_block11_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block12_0_bn (Batc  (None, 1, 1, 12  4992    ['conv5_block11_concat[0]  N          \n#>  hNormalization)          48)                       [0]']                                 \n#>  conv5_block12_0_relu (Ac  (None, 1, 1, 12  0       ['conv5_block12_0_bn[0][0  N          \n#>  tivation)                48)                       ]']                                   \n#>  conv5_block12_1_conv (Co  (None, 1, 1, 12  159744  ['conv5_block12_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block12_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block12_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block12_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block12_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block12_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block12_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block12_concat (Co  (None, 1, 1, 12  0       ['conv5_block11_concat[0]  N          \n#>  ncatenate)               80)                       [0]',                                 \n#>                                                      'conv5_block12_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block13_0_bn (Batc  (None, 1, 1, 12  5120    ['conv5_block12_concat[0]  N          \n#>  hNormalization)          80)                       [0]']                                 \n#>  conv5_block13_0_relu (Ac  (None, 1, 1, 12  0       ['conv5_block13_0_bn[0][0  N          \n#>  tivation)                80)                       ]']                                   \n#>  conv5_block13_1_conv (Co  (None, 1, 1, 12  163840  ['conv5_block13_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block13_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block13_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block13_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block13_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block13_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block13_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block13_concat (Co  (None, 1, 1, 13  0       ['conv5_block12_concat[0]  N          \n#>  ncatenate)               12)                       [0]',                                 \n#>                                                      'conv5_block13_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block14_0_bn (Batc  (None, 1, 1, 13  5248    ['conv5_block13_concat[0]  N          \n#>  hNormalization)          12)                       [0]']                                 \n#>  conv5_block14_0_relu (Ac  (None, 1, 1, 13  0       ['conv5_block14_0_bn[0][0  N          \n#>  tivation)                12)                       ]']                                   \n#>  conv5_block14_1_conv (Co  (None, 1, 1, 12  167936  ['conv5_block14_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block14_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block14_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block14_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block14_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block14_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block14_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block14_concat (Co  (None, 1, 1, 13  0       ['conv5_block13_concat[0]  N          \n#>  ncatenate)               44)                       [0]',                                 \n#>                                                      'conv5_block14_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block15_0_bn (Batc  (None, 1, 1, 13  5376    ['conv5_block14_concat[0]  N          \n#>  hNormalization)          44)                       [0]']                                 \n#>  conv5_block15_0_relu (Ac  (None, 1, 1, 13  0       ['conv5_block15_0_bn[0][0  N          \n#>  tivation)                44)                       ]']                                   \n#>  conv5_block15_1_conv (Co  (None, 1, 1, 12  172032  ['conv5_block15_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block15_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block15_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block15_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block15_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block15_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block15_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block15_concat (Co  (None, 1, 1, 13  0       ['conv5_block14_concat[0]  N          \n#>  ncatenate)               76)                       [0]',                                 \n#>                                                      'conv5_block15_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block16_0_bn (Batc  (None, 1, 1, 13  5504    ['conv5_block15_concat[0]  N          \n#>  hNormalization)          76)                       [0]']                                 \n#>  conv5_block16_0_relu (Ac  (None, 1, 1, 13  0       ['conv5_block16_0_bn[0][0  N          \n#>  tivation)                76)                       ]']                                   \n#>  conv5_block16_1_conv (Co  (None, 1, 1, 12  176128  ['conv5_block16_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block16_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block16_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block16_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block16_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block16_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block16_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block16_concat (Co  (None, 1, 1, 14  0       ['conv5_block15_concat[0]  N          \n#>  ncatenate)               08)                       [0]',                                 \n#>                                                      'conv5_block16_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block17_0_bn (Batc  (None, 1, 1, 14  5632    ['conv5_block16_concat[0]  N          \n#>  hNormalization)          08)                       [0]']                                 \n#>  conv5_block17_0_relu (Ac  (None, 1, 1, 14  0       ['conv5_block17_0_bn[0][0  N          \n#>  tivation)                08)                       ]']                                   \n#>  conv5_block17_1_conv (Co  (None, 1, 1, 12  180224  ['conv5_block17_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block17_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block17_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block17_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block17_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block17_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block17_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block17_concat (Co  (None, 1, 1, 14  0       ['conv5_block16_concat[0]  N          \n#>  ncatenate)               40)                       [0]',                                 \n#>                                                      'conv5_block17_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block18_0_bn (Batc  (None, 1, 1, 14  5760    ['conv5_block17_concat[0]  N          \n#>  hNormalization)          40)                       [0]']                                 \n#>  conv5_block18_0_relu (Ac  (None, 1, 1, 14  0       ['conv5_block18_0_bn[0][0  N          \n#>  tivation)                40)                       ]']                                   \n#>  conv5_block18_1_conv (Co  (None, 1, 1, 12  184320  ['conv5_block18_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block18_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block18_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block18_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block18_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block18_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block18_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block18_concat (Co  (None, 1, 1, 14  0       ['conv5_block17_concat[0]  N          \n#>  ncatenate)               72)                       [0]',                                 \n#>                                                      'conv5_block18_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block19_0_bn (Batc  (None, 1, 1, 14  5888    ['conv5_block18_concat[0]  N          \n#>  hNormalization)          72)                       [0]']                                 \n#>  conv5_block19_0_relu (Ac  (None, 1, 1, 14  0       ['conv5_block19_0_bn[0][0  N          \n#>  tivation)                72)                       ]']                                   \n#>  conv5_block19_1_conv (Co  (None, 1, 1, 12  188416  ['conv5_block19_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block19_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block19_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block19_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block19_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block19_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block19_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block19_concat (Co  (None, 1, 1, 15  0       ['conv5_block18_concat[0]  N          \n#>  ncatenate)               04)                       [0]',                                 \n#>                                                      'conv5_block19_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block20_0_bn (Batc  (None, 1, 1, 15  6016    ['conv5_block19_concat[0]  N          \n#>  hNormalization)          04)                       [0]']                                 \n#>  conv5_block20_0_relu (Ac  (None, 1, 1, 15  0       ['conv5_block20_0_bn[0][0  N          \n#>  tivation)                04)                       ]']                                   \n#>  conv5_block20_1_conv (Co  (None, 1, 1, 12  192512  ['conv5_block20_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block20_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block20_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block20_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block20_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block20_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block20_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block20_concat (Co  (None, 1, 1, 15  0       ['conv5_block19_concat[0]  N          \n#>  ncatenate)               36)                       [0]',                                 \n#>                                                      'conv5_block20_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block21_0_bn (Batc  (None, 1, 1, 15  6144    ['conv5_block20_concat[0]  N          \n#>  hNormalization)          36)                       [0]']                                 \n#>  conv5_block21_0_relu (Ac  (None, 1, 1, 15  0       ['conv5_block21_0_bn[0][0  N          \n#>  tivation)                36)                       ]']                                   \n#>  conv5_block21_1_conv (Co  (None, 1, 1, 12  196608  ['conv5_block21_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block21_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block21_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block21_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block21_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block21_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block21_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block21_concat (Co  (None, 1, 1, 15  0       ['conv5_block20_concat[0]  N          \n#>  ncatenate)               68)                       [0]',                                 \n#>                                                      'conv5_block21_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block22_0_bn (Batc  (None, 1, 1, 15  6272    ['conv5_block21_concat[0]  N          \n#>  hNormalization)          68)                       [0]']                                 \n#>  conv5_block22_0_relu (Ac  (None, 1, 1, 15  0       ['conv5_block22_0_bn[0][0  N          \n#>  tivation)                68)                       ]']                                   \n#>  conv5_block22_1_conv (Co  (None, 1, 1, 12  200704  ['conv5_block22_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block22_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block22_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block22_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block22_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block22_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block22_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block22_concat (Co  (None, 1, 1, 16  0       ['conv5_block21_concat[0]  N          \n#>  ncatenate)               00)                       [0]',                                 \n#>                                                      'conv5_block22_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block23_0_bn (Batc  (None, 1, 1, 16  6400    ['conv5_block22_concat[0]  N          \n#>  hNormalization)          00)                       [0]']                                 \n#>  conv5_block23_0_relu (Ac  (None, 1, 1, 16  0       ['conv5_block23_0_bn[0][0  N          \n#>  tivation)                00)                       ]']                                   \n#>  conv5_block23_1_conv (Co  (None, 1, 1, 12  204800  ['conv5_block23_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block23_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block23_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block23_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block23_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block23_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block23_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block23_concat (Co  (None, 1, 1, 16  0       ['conv5_block22_concat[0]  N          \n#>  ncatenate)               32)                       [0]',                                 \n#>                                                      'conv5_block23_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block24_0_bn (Batc  (None, 1, 1, 16  6528    ['conv5_block23_concat[0]  N          \n#>  hNormalization)          32)                       [0]']                                 \n#>  conv5_block24_0_relu (Ac  (None, 1, 1, 16  0       ['conv5_block24_0_bn[0][0  N          \n#>  tivation)                32)                       ]']                                   \n#>  conv5_block24_1_conv (Co  (None, 1, 1, 12  208896  ['conv5_block24_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block24_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block24_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block24_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block24_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block24_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block24_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block24_concat (Co  (None, 1, 1, 16  0       ['conv5_block23_concat[0]  N          \n#>  ncatenate)               64)                       [0]',                                 \n#>                                                      'conv5_block24_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block25_0_bn (Batc  (None, 1, 1, 16  6656    ['conv5_block24_concat[0]  N          \n#>  hNormalization)          64)                       [0]']                                 \n#>  conv5_block25_0_relu (Ac  (None, 1, 1, 16  0       ['conv5_block25_0_bn[0][0  N          \n#>  tivation)                64)                       ]']                                   \n#>  conv5_block25_1_conv (Co  (None, 1, 1, 12  212992  ['conv5_block25_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block25_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block25_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block25_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block25_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block25_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block25_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block25_concat (Co  (None, 1, 1, 16  0       ['conv5_block24_concat[0]  N          \n#>  ncatenate)               96)                       [0]',                                 \n#>                                                      'conv5_block25_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block26_0_bn (Batc  (None, 1, 1, 16  6784    ['conv5_block25_concat[0]  N          \n#>  hNormalization)          96)                       [0]']                                 \n#>  conv5_block26_0_relu (Ac  (None, 1, 1, 16  0       ['conv5_block26_0_bn[0][0  N          \n#>  tivation)                96)                       ]']                                   \n#>  conv5_block26_1_conv (Co  (None, 1, 1, 12  217088  ['conv5_block26_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block26_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block26_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block26_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block26_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block26_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block26_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block26_concat (Co  (None, 1, 1, 17  0       ['conv5_block25_concat[0]  N          \n#>  ncatenate)               28)                       [0]',                                 \n#>                                                      'conv5_block26_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block27_0_bn (Batc  (None, 1, 1, 17  6912    ['conv5_block26_concat[0]  N          \n#>  hNormalization)          28)                       [0]']                                 \n#>  conv5_block27_0_relu (Ac  (None, 1, 1, 17  0       ['conv5_block27_0_bn[0][0  N          \n#>  tivation)                28)                       ]']                                   \n#>  conv5_block27_1_conv (Co  (None, 1, 1, 12  221184  ['conv5_block27_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block27_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block27_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block27_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block27_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block27_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block27_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block27_concat (Co  (None, 1, 1, 17  0       ['conv5_block26_concat[0]  N          \n#>  ncatenate)               60)                       [0]',                                 \n#>                                                      'conv5_block27_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block28_0_bn (Batc  (None, 1, 1, 17  7040    ['conv5_block27_concat[0]  N          \n#>  hNormalization)          60)                       [0]']                                 \n#>  conv5_block28_0_relu (Ac  (None, 1, 1, 17  0       ['conv5_block28_0_bn[0][0  N          \n#>  tivation)                60)                       ]']                                   \n#>  conv5_block28_1_conv (Co  (None, 1, 1, 12  225280  ['conv5_block28_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block28_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block28_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block28_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block28_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block28_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block28_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block28_concat (Co  (None, 1, 1, 17  0       ['conv5_block27_concat[0]  N          \n#>  ncatenate)               92)                       [0]',                                 \n#>                                                      'conv5_block28_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block29_0_bn (Batc  (None, 1, 1, 17  7168    ['conv5_block28_concat[0]  N          \n#>  hNormalization)          92)                       [0]']                                 \n#>  conv5_block29_0_relu (Ac  (None, 1, 1, 17  0       ['conv5_block29_0_bn[0][0  N          \n#>  tivation)                92)                       ]']                                   \n#>  conv5_block29_1_conv (Co  (None, 1, 1, 12  229376  ['conv5_block29_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block29_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block29_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block29_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block29_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block29_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block29_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block29_concat (Co  (None, 1, 1, 18  0       ['conv5_block28_concat[0]  N          \n#>  ncatenate)               24)                       [0]',                                 \n#>                                                      'conv5_block29_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block30_0_bn (Batc  (None, 1, 1, 18  7296    ['conv5_block29_concat[0]  N          \n#>  hNormalization)          24)                       [0]']                                 \n#>  conv5_block30_0_relu (Ac  (None, 1, 1, 18  0       ['conv5_block30_0_bn[0][0  N          \n#>  tivation)                24)                       ]']                                   \n#>  conv5_block30_1_conv (Co  (None, 1, 1, 12  233472  ['conv5_block30_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block30_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block30_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block30_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block30_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block30_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block30_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block30_concat (Co  (None, 1, 1, 18  0       ['conv5_block29_concat[0]  N          \n#>  ncatenate)               56)                       [0]',                                 \n#>                                                      'conv5_block30_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block31_0_bn (Batc  (None, 1, 1, 18  7424    ['conv5_block30_concat[0]  N          \n#>  hNormalization)          56)                       [0]']                                 \n#>  conv5_block31_0_relu (Ac  (None, 1, 1, 18  0       ['conv5_block31_0_bn[0][0  N          \n#>  tivation)                56)                       ]']                                   \n#>  conv5_block31_1_conv (Co  (None, 1, 1, 12  237568  ['conv5_block31_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block31_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block31_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block31_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block31_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block31_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block31_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block31_concat (Co  (None, 1, 1, 18  0       ['conv5_block30_concat[0]  N          \n#>  ncatenate)               88)                       [0]',                                 \n#>                                                      'conv5_block31_2_conv[0]             \n#>                                                     [0]']                                 \n#>  conv5_block32_0_bn (Batc  (None, 1, 1, 18  7552    ['conv5_block31_concat[0]  N          \n#>  hNormalization)          88)                       [0]']                                 \n#>  conv5_block32_0_relu (Ac  (None, 1, 1, 18  0       ['conv5_block32_0_bn[0][0  N          \n#>  tivation)                88)                       ]']                                   \n#>  conv5_block32_1_conv (Co  (None, 1, 1, 12  241664  ['conv5_block32_0_relu[0]  N          \n#>  nv2D)                    8)                        [0]']                                 \n#>  conv5_block32_1_bn (Batc  (None, 1, 1, 12  512     ['conv5_block32_1_conv[0]  N          \n#>  hNormalization)          8)                        [0]']                                 \n#>  conv5_block32_1_relu (Ac  (None, 1, 1, 12  0       ['conv5_block32_1_bn[0][0  N          \n#>  tivation)                8)                        ]']                                   \n#>  conv5_block32_2_conv (Co  (None, 1, 1, 32  36864   ['conv5_block32_1_relu[0]  N          \n#>  nv2D)                    )                         [0]']                                 \n#>  conv5_block32_concat (Co  (None, 1, 1, 19  0       ['conv5_block31_concat[0]  N          \n#>  ncatenate)               20)                       [0]',                                 \n#>                                                      'conv5_block32_2_conv[0]             \n#>                                                     [0]']                                 \n#>  bn (BatchNormalization)  (None, 1, 1, 19  7680     ['conv5_block32_concat[0]  N          \n#>                           20)                       [0]']                                 \n#>  relu (Activation)        (None, 1, 1, 19  0        ['bn[0][0]']               N          \n#>                           20)                                                             \n#>  dense_112 (Dense)        (None, 1, 1, 10  19210    ['relu[0][0]']             N          \n#>                           )                                                               \n#>  flatten_1 (Flatten)      (None, 10)       0        ['dense_112[0][0]']        Y          \n#> ==========================================================================================\n#> Total params: 18,341,194\n#> Trainable params: 0\n#> Non-trainable params: 18,341,194\n#> __________________________________________________________________________________________\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel %>%\n  keras::compile(loss = loss_categorical_crossentropy, \n                 optimizer = optimizer_adamax())\n\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = 1L,\n    batch_size = 32L,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\nlibrary(torchvision)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntrain_ds = cifar10_dataset(\".\", download = TRUE, train = TRUE,\n                           transform = transform_to_tensor)\ntest_ds = cifar10_dataset(\".\", download = TRUE, train = FALSE,\n                          transform = transform_to_tensor)\n\ntrain_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)\ntest_dl = dataloader(test_ds, batch_size = 100L)\n\nmodel_torch = model_resnet18(pretrained = TRUE)\n\n# We will set all model parameters to constant values:\nmodel_torch$parameters %>%\n  purrr::walk(function(param) param$requires_grad_(FALSE))\n\n# Let's replace the last layer (last layer is named 'fc') with our own layer:\ninFeat = model_torch$fc$in_features\nmodel_torch$fc = nn_linear(inFeat, out_features = 10L)\n\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nfor(e in 1:1){\n  losses = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]], reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      losses = c(losses, loss$item())\n    }\n  )\n  \n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", e, mean(losses)))\n}\n\nmodel_torch$eval()\n\ntest_losses = c()\ntotal = 0\ncorrect = 0\n\ncoro::loop(\n  for(batch in test_dl){\n    output = model_torch(batch[[1]])\n    labels = batch[[2]]\n    loss = nnf_cross_entropy(output, labels)\n    test_losses = c(test_losses, loss$item())\n    predicted = torch_max(output$data(), dim = 2)[[2]]\n    total = total + labels$size(1)\n    correct = correct + (predicted == labels)$sum()$item()\n  }\n)\n\ntest_accuracy =  correct/total\nprint(test_accuracy)\nlibrary(keras)\nlibrary(tensorflow)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = EcoData::dataset_flower()\ntrain = data$train\ntest = data$test\nlabels = data$labels\n\ndensenet = keras::application_densenet201(include_top = FALSE,\n                                          input_shape = list(80L, 80L, 3L))\n\nkeras::freeze_weights(densenet)\n\nmodel = keras_model(inputs = densenet$input, \n                    outputs = densenet$output %>%\n                      layer_flatten() %>%\n                      layer_dropout(0.2) %>%\n                      layer_dense(units = 200L) %>%\n                      layer_dropout(0.2) %>%\n                      layer_dense(units = 5L, activation = \"softmax\"))\n\n# Data augmentation.\naug = image_data_generator(rotation_range = 180, zoom_range = 0.4,\n                           width_shift_range = 0.2, height_shift_range = 0.2,\n                           vertical_flip = TRUE, horizontal_flip = TRUE,\n                           preprocessing_function = imagenet_preprocess_input)\n\n# Data preparation / splitting.\nindices = sample.int(nrow(train), 0.1 * nrow(train))\ngenerator = flow_images_from_data(train[-indices,,,]/255,\n                                  k_one_hot(labels[-indices], num_classes = 5L), \n                                  batch_size = 25L, shuffle = TRUE,\n                                  generator = aug)\n\ntest = imagenet_preprocess_input(train[indices,,,])\ntest_labels = k_one_hot(labels[indices], num_classes = 5L)\n\n## Training loop with early stopping:\n\n# As we use an iterator (the generator), validation loss is not applicable.\n# An available metric is the normal loss.\nearly = keras::callback_early_stopping(patience = 2L, monitor = \"loss\")\n\nmodel %>%\n    keras::compile(loss = loss_categorical_crossentropy,\n                   optimizer = keras::optimizer_rmsprop(learning_rate = 0.0005))\n\nmodel %>%\n    fit(generator, epochs = 8L, batch_size = 45L,\n        shuffle = TRUE, callbacks = c(early))\n\npred = predict(model, imagenet_preprocess_input(data$test))\npred = apply(pred, 1, which.max) - 1"},{"path":"deep.html","id":"batch-size-and-learning-rate","chapter":"6 Deep Learning","heading":"6.4.3 Batch Size and Learning Rate","text":"chapter, influence batch size learning rate explored using MNIST data set.\ninterested topic (), read \narticle.","code":""},{"path":"deep.html","id":"batch-size","chapter":"6 Deep Learning","heading":"6.4.3.1 Batch Size","text":"Different batch sizes may massively influence outcome training step. Finding suitable batch size task .general rule thumb:lower batch size, longer calculations take, less (!) memory needed accurate training (including less overfitting).higher batch size, wider training steps (like higher learning rate).Changing batch sizes learning rates always possible might “heal” previous mistakes. Maybe “push” system local neighborhood.also depends respective problem.Higher batch size:Lower batch size:Lowest (1) batch size:Highest (complete) batch size:might run, much memory needed.","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dense(100L, activation = \"relu\") %>%\n  layer_dense(10L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(\n      optimizer = keras::optimizer_adamax(learning_rate = 0.01),\n      loss = loss_categorical_crossentropy\n  )\n\nepochs = 5L\nbatch_size = 32L\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\npred = model %>% predict(test_x) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, test$y) # 0.9884\n#> [1] 0.989\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dense(100L, activation = \"relu\") %>%\n  layer_dense(10L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(\n    optimizer = keras::optimizer_adamax(learning_rate = 0.01),\n    loss = loss_categorical_crossentropy\n  )\n\nepochs = 5L\nbatch_size = 100L\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\npred = model %>% predict(test_x) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, test$y) # 0.9864\n#> [1] 0.987\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dense(100L, activation = \"relu\") %>%\n  layer_dense(10L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(\n    optimizer = keras::optimizer_adamax(learning_rate = 0.01),\n    loss = loss_categorical_crossentropy\n  )\n\nepochs = 5L\nbatch_size = 10L\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\npred = model %>% predict(test_x) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, test$y) # 0.9869\n#> [1] 0.9877\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dense(100L, activation = \"relu\") %>%\n  layer_dense(10L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(\n    optimizer = keras::optimizer_adamax(learning_rate = 0.01),\n    loss = loss_categorical_crossentropy\n  )\n\nepochs = 5L\nbatch_size = 1L\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\npred = model %>% predict(test_x) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, test$y) # 0.982\n#> [1] 0.9822\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dense(100L, activation = \"relu\") %>%\n  layer_dense(10L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(\n    optimizer = keras::optimizer_adamax(learning_rate = 0.01),\n    loss = loss_categorical_crossentropy\n  )\n\nepochs = 5L\nbatch_size = nrow(test_x)\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\npred = model %>% predict(test_x) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, test$y) # ???"},{"path":"deep.html","id":"learning-rate","chapter":"6 Deep Learning","heading":"6.4.3.2 Learning Rate","text":"Choosing high learning rate beginning may yield acceptable results relatively fast. take much time get small learning rate. keeping learning rate high may result jumping desired optimal values. decreasing learning rate time might help.TensorFlow / Keras can manage changing learning rate. may also periodic function increase one. limited decreasing learning rate. Defining learning rates bit complicated just using inbuilt Keras functions. need self-made learning rate, can find recipe .example inbuilt functions managing learning rates Keras:function declaration adamax optimizer follows:can easily specify decay way. Mind interval boundaries suitable parameter values!Except decay learning rate, example identical first one concerning batch size (0.9884).","code":"\noptimizer_adamax(\n  learning_rate = 0.002,\n  beta_1 = 0.9,\n  beta_2 = 0.999,\n  epsilon = NULL,\n  decay = 0,\n  clipnorm = NULL,\n  clipvalue = NULL,\n  ...\n)\n# learning_rate:  float >= 0. Learning rate.\n# beta_1:         The exponential decay rate for the 1st moment estimates.\n                  # float, 0 < beta < 1. Generally close to 1.\n# beta_2:         The exponential decay rate for the 2nd moment estimates.\n                  # float, 0 < beta < 1. Generally close to 1.\n# epsilon:        float >= 0. Fuzz factor. If NULL, defaults to k_epsilon().\n# decay:            float >= 0. Learning rate decay over each update.\n# clipnorm:       Gradients will be clipped when their L2 norm exceeds this value.\n# clipvalue:      Gradients will be clipped when their absolute value exceeds this value.\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %>%\n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dense(100L, activation = \"relu\") %>%\n  layer_dense(10L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(\n    optimizer = keras::optimizer_adamax(learning_rate = 0.02, decay = 0.002),\n    loss = loss_categorical_crossentropy\n  )\n\nepochs = 5L\nbatch_size = 32L\nmodel %>%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\npred = model %>% predict(test_x) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, test$y) # 0.9882\n#> [1] 0.9884"},{"path":"deep.html","id":"conclusion","chapter":"6 Deep Learning","heading":"6.4.3.3 Conclusion","text":"() complex interplay batch size, learning rate optimization algorithm. topic deep course.times, longer training increasing batch size rather decreasing learning rate recommended.\nmatter research also personal attitude. course, also depends respective problem.","code":""},{"path":"deep.html","id":"caveat-about-learning-rates-and-activation-functions-already-mentioned-in-the-intro","chapter":"6 Deep Learning","heading":"6.4.3.4 Caveat About Learning Rates and Activation Functions (already mentioned in the intro)","text":"Depending activation functions, might occur network won’t get updated, even high learning rates (called vanishing gradient, especially “sigmoid” functions).\nFurthermore, updates might overshoot (called exploding gradients) activation functions result many zeros (especially “relu,” dying relu).general, first layers network tend learn (much) slowly subsequent ones.","code":""},{"path":"deep.html","id":"exercise-2","chapter":"6 Deep Learning","heading":"6.4.4 Exercise","text":"next exercise flower data set Ecodata package.Follow steps, build convolutional neural network.end, submit predictions submission server. extra time, look kaggle find flower data set challenge specific architectures tailored data set.\nfollowing code shows different behavior context data augmentation model complexity.topic overfitting can seen cleary: Compare simple model performance training test data. compare complex even regularized models performance training test data.see simple models tend overfit.Even complex model:following snippet offers solution data generation oversampling undersampling, distribution classes equal flower data set.Read example following way:avoid crashes system, might want memory management, like:model training, unload training set load test set.Generally remove data used longer.Lazy load new images (shown ) generator (later shown section 8, manually written training loop).can see, network works principle (76% accuracy training data). Mind, binary classification problem expecting roughly 20% accuracy chance.Just little hint: complex networks always better. won’t shown explicitly (computing-intensive). can try example 64 filter kernels per layer copy one convolutional layers (including pooling layer) see happens.Now, training without holdouts get power.Maybe can find (much?) better networks, fit already better 84% training data.\nMind, 5 classes. model predicts 3 4 , surprising, accuracy low.","code":"\nlibrary(tensorflow)\nlibrary(keras)\n\nflowerCNN = function(\n  networkSize = c(\"small\", \"medium\", \"big\")[3],\n  useGenerator = c(1, 2, NA)[3],\n  batch_size = 25L,\n  epochs = 10L,\n  learning_rate = 0.01,\n  percentageTrain = 0.9\n){\n  gc(FALSE) # Clean up system (use garbage collection).\n  set_random_seed(321L, disable_gpu = FALSE)    # Already sets R's random seed.\n  \n  ###############\n  # Prepare training and test sets:\n  \n  train = EcoData::dataset_flower()$train/255\n  indicesTrain = sample.int(nrow(train), percentageTrain * nrow(train))\n  test = train[-indicesTrain,,,]\n  train = train[indicesTrain,,,]\n  \n  labelsTrain = EcoData::dataset_flower()$labels\n  labelsTest = labelsTrain[-indicesTrain]\n  labelsTrain = labelsTrain[indicesTrain]\n  \n  \n  ###############\n  # Models:\n  \n  model = keras_model_sequential()\n  \n  if(networkSize == \"small\"){\n    modelString = \"small model\"\n    \n    model %>% \n      layer_conv_2d(filters = 4L, kernel_size = 2L,\n                    input_shape = list(80L, 80L, 3L)) %>% \n      layer_max_pooling_2d() %>% \n      layer_flatten() %>% \n      layer_dense(units = 5L, activation = \"softmax\")\n    \n  }else if(networkSize == \"medium\"){\n    modelString = \"medium model\"\n    \n    model %>%\n      layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L),\n                    input_shape = c(80L, 80L, 3L), activation = \"relu\") %>%\n      layer_max_pooling_2d() %>%\n      layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L),\n                    activation = \"relu\") %>%\n      layer_max_pooling_2d() %>%\n      layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),\n                    strides = c(2L, 2L), activation = \"relu\") %>%\n      layer_max_pooling_2d() %>%\n      layer_flatten() %>%\n      layer_dropout(0.5) %>%\n      layer_dense(units = 5L, activation = \"softmax\")\n    \n  } else if(networkSize == \"big\"){\n    modelString = \"big model\"\n    \n    model %>%\n      layer_conv_2d(filter = 48L, kernel_size = c(5L, 5L),\n                    input_shape = c(80L, 80L, 3L), activation = \"leaky_relu\") %>%\n      layer_max_pooling_2d() %>%\n      layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L),\n                    activation = \"leaky_relu\") %>%\n      layer_max_pooling_2d() %>%\n      layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),\n                    strides = c(2L, 2L), activation = \"leaky_relu\") %>%\n      layer_max_pooling_2d() %>%\n      layer_flatten() %>%\n      layer_dropout(0.35) %>%\n      layer_dense(units = 256L, activation = \"relu\",\n                  bias_regularizer = regularizer_l2(.25)\n      ) %>%\n      layer_dropout(0.4) %>%\n      layer_dense(units = 128L, activation = \"leaky_relu\") %>%\n      layer_dropout(0.4) %>%\n      layer_dense(units = 64L, activation = \"leaky_relu\") %>%\n      layer_dropout(0.4) %>%\n      layer_dense(units = 5L, activation = \"softmax\")\n  }\n\n\n  ###############\n  # Generators for augmentation:\n  \n  if(!is.na(useGenerator)){\n    if(useGenerator == 1){\n      generatorString = \"generator 1\"\n      \n      generator = keras::flow_images_from_data(\n        x = train,\n        y = k_one_hot(labelsTrain, num_classes = 5L),\n        generator = keras::image_data_generator(\n          rotation_range = 180,\n          zoom_range = c(0.3),\n          horizontal_flip = TRUE,\n          vertical_flip = TRUE,\n          samplewise_center = TRUE,\n          samplewise_std_normalization = TRUE),\n        batch_size = batch_size,\n        shuffle = TRUE\n      )\n      \n    }else if(useGenerator == 2){\n      generatorString = \"generator 2\"\n      \n      generator = keras::flow_images_from_data(\n        x = train,\n        y = keras::k_one_hot(labelsTrain, 5L),\n        batch_size = batch_size\n      )\n    }\n  }else{ generatorString = \"no generator\" }\n  \n  \n  ###############\n  # Model selection and compilation:\n\n  model %>%\n    keras::compile(loss = loss_categorical_crossentropy,\n                   optimizer = keras::optimizer_adamax(learning_rate = learning_rate))\n  \n  if(is.na(useGenerator)){ # Use no generator.\n    model %>%\n      fit(x = train, y = to_categorical(matrix(labelsTrain, ncol = 1L), 5L),\n          epochs = epochs, batch_size = batch_size, shuffle = TRUE)\n  }else{\n    model %>%\n      fit(generator, epochs = epochs, batch_size = batch_size, shuffle = TRUE)\n  }\n  \n  \n  ###############\n  # Predictions:\n  \n  cat(paste0(\"\\nModalities: \", modelString, \", \", generatorString, \"\\n\"))\n  \n  # Predictions on the training set:\n  predTrain = predict(model, train) %>% apply(1, which.max) - 1\n  cat(paste0(\"\\nAccuracy on training set: \",\n               round(Metrics::accuracy(predTrain, labelsTrain), 2), \"\\n\"))\n  print(round(table(predTrain) / nrow(train), 2))\n  \n  # Predictions on the test set:\n  predTest = predict(model, test) %>% apply(1, which.max) - 1\n  cat(paste0(\"\\nAccuracy on test set: \",\n               round(Metrics::accuracy(predTest, labelsTest), 2), \"\\n\"))\n  print(round(table(predTest) / nrow(test), 2))\n  \n  # Predictions on the holdout for submission:\n  predHoldout = predict(model, EcoData::dataset_flower()$test/255) %>%\n    apply(1, which.max) - 1\n  \n  return(predHoldout)\n}\n\n\nfor(networkSize in c(\"small\", \"medium\", \"big\")){\n  for(useGenerator in c(1, 2, NA)){\n    pred = flowerCNN(networkSize = networkSize, useGenerator = useGenerator)\n  }\n}\n#> \n#> Modalities: small model, generator 1\n#> \n#> Accuracy on training set: 0.33\n#> predTrain\n#>    0    1    2    3    4 \n#> 0.36 0.48 0.12 0.00 0.03 \n#> \n#> Accuracy on test set: 0.37\n#> predTest\n#>    0    1    2    4 \n#> 0.37 0.45 0.13 0.05 \n#> \n#> Modalities: small model, generator 2\n#> \n#> Accuracy on training set: 0.89\n#> predTrain\n#>    0    1    2    3    4 \n#> 0.16 0.26 0.16 0.18 0.24 \n#> \n#> Accuracy on test set: 0.45\n#> predTest\n#>    0    1    2    3    4 \n#> 0.11 0.31 0.13 0.24 0.22 \n#> \n#> Modalities: small model, no generator\n#> \n#> Accuracy on training set: 0.91\n#> predTrain\n#>    0    1    2    3    4 \n#> 0.16 0.27 0.17 0.17 0.23 \n#> \n#> Accuracy on test set: 0.46\n#> predTest\n#>    0    1    2    3    4 \n#> 0.13 0.31 0.17 0.22 0.17 \n#> \n#> Modalities: medium model, generator 1\n#> \n#> Accuracy on training set: 0.28\n#> predTrain\n#>    0    1    2    4 \n#> 0.00 0.93 0.05 0.01 \n#> \n#> Accuracy on test set: 0.3\n#> predTest\n#>    1    2    4 \n#> 0.92 0.06 0.01 \n#> \n#> Modalities: medium model, generator 2\n#> \n#> Accuracy on training set: 0.69\n#> predTrain\n#>    0    1    2    3    4 \n#> 0.18 0.30 0.14 0.20 0.17 \n#> \n#> Accuracy on test set: 0.65\n#> predTest\n#>    0    1    2    3    4 \n#> 0.18 0.29 0.16 0.21 0.17 \n#> \n#> Modalities: medium model, no generator\n#> \n#> Accuracy on training set: 0.74\n#> predTrain\n#>    0    1    2    3    4 \n#> 0.15 0.31 0.24 0.16 0.14 \n#> \n#> Accuracy on test set: 0.66\n#> predTest\n#>    0    1    2    3    4 \n#> 0.14 0.31 0.24 0.16 0.14 \n#> \n#> Modalities: big model, generator 1\n#> \n#> Accuracy on training set: 0.27\n#> predTrain\n#>    1    2    4 \n#> 0.95 0.02 0.02 \n#> \n#> Accuracy on test set: 0.29\n#> predTest\n#>    1    2    4 \n#> 0.95 0.03 0.02 \n#> \n#> Modalities: big model, generator 2\n#> \n#> Accuracy on training set: 0.68\n#> predTrain\n#>    0    1    2    3    4 \n#> 0.19 0.33 0.05 0.19 0.24 \n#> \n#> Accuracy on test set: 0.61\n#> predTest\n#>    0    1    2    3    4 \n#> 0.17 0.36 0.05 0.19 0.23 \n#> \n#> Modalities: big model, no generator\n#> \n#> Accuracy on training set: 0.71\n#> predTrain\n#>    0    1    2    3    4 \n#> 0.15 0.28 0.12 0.18 0.27 \n#> \n#> Accuracy on test set: 0.64\n#> predTest\n#>    0    1    2    3    4 \n#> 0.14 0.31 0.12 0.16 0.28\ngetData = function(oversample = TRUE, undersample = FALSE){\n  # \"undersample\" has priority over \"oversample\".\n  \n  # As the whole task is very compute-intensive and needs much memory,\n  # pack data acquisition in a function.\n  # The used local memory is cleaned automatically at the end of the scope.\n  \n  data = EcoData::dataset_flower()\n  # <<-: Global scope.\n  trainLocal = data$train/255\n  labelsLocal = data$labels\n  \n  \n  print(table(labelsLocal)) # The classes are not equally distributed.\n  # Many models tend to predict class 1 overproportionally often.\n  \n  if(undersample){\n    n = min(table(labelsLocal))\n    \n    # Minimal size of classes times number of classes.\n    total = n * length(levels(as.factor(labelsLocal)))\n    newIndices = rep(FALSE, total)\n    \n    for(i in 1:length(levels(as.factor(labelsLocal)))){\n      newIndices[sample(which(labelsLocal == i - 1), n, replace = FALSE)] = TRUE\n    }\n    \n    newIndices = which(newIndices)\n    \n    trainingSet <- trainLocal[newIndices,,,]\n    flowerLabels <- labelsLocal[newIndices]\n    \n    print(table(flowerLabels))\n    return(list(trainingSet, flowerLabels, trainLocal))\n  }\n  \n  if(!oversample){\n    trainingSet <- trainLocal\n    flowerLabels <- labelsLocal\n    return(list(trainingSet, flowerLabels, trainLocal))\n  }\n  \n  n = round(max(table(labelsLocal)) + 14) # Number of samples to extend each class to.\n  \n  ## Sample new data (with replacement):\n  for(i in 1:length(levels(as.factor(labelsLocal)))){\n    missing = n - table(labelsLocal)[i]  # Number of elements missing compared to n.\n    indices = which(labelsLocal == i - 1)  # Indices of all elements of class i.\n    newIndices = sample(indices, missing, replace = TRUE)\n    \n    trainLocal <- abind::abind(trainLocal, trainLocal[newIndices,,,], along = 1)\n    # As only new indices are added, there is no confusion with using the old ones.\n    \n    labelsLocal = c(labelsLocal, rep(as.integer(i - 1), missing))\n  }\n  \n  trainingSet <- trainLocal\n  flowerLabels <- labelsLocal\n  \n  print(table(flowerLabels))\n  \n  return(list(trainingSet, flowerLabels, trainLocal))\n}\nif(!exists(\"done\")){  # Do not calculate this more often than 1 time.\n  trainingSet = c()\n  flowerLabels = c()\n  data = getData(oversample = FALSE, undersample = FALSE)\n  trainingSet = data[[1]]\n  flowerLabels = data[[2]]\n  trainLocal = data[[3]]\n  done = 1\n}\nlibrary(keras)\nlibrary(tensorflow)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nif(!exists(\"doneWithTest\")){  # Do not calculate this more often than 1 time.\n  trainingSet = c()\n  flowerLabels = c()\n  data = getData(oversample = FALSE, undersample = FALSE)\n  trainingSet = data[[1]]\n  flowerLabels = data[[2]]\n  trainLocal = data[[3]]\n  doneWithTest = 1\n}\n#> labelsLocal\n#>   0   1   2   3   4 \n#> 538 736 548 513 688\n\nmodel = keras_model_sequential()\nmodel %>%\n  layer_conv_2d(filter = 48L, kernel_size = c(4L, 4L),\n                input_shape = c(80L, 80L, 3L), activation = \"relu\") %>% \n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L), activation = \"elu\") %>% \n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filter = 64L, kernel_size = c(2L, 2L), activation = \"gelu\") %>% \n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dropout(0.33) %>%\n  layer_dense(units = 750L, activation = \"gelu\",\n              bias_regularizer = regularizer_l1(.75),\n              kernel_regularizer = regularizer_l2(.0055)\n  ) %>%\n  layer_dropout(0.4) %>%\n  layer_dense(units = 175L, activation = \"gelu\") %>%\n  layer_dropout(0.35) %>%\n  layer_dense(units = 75L, activation = \"relu\") %>%\n  layer_dense(units = 50L, activation = \"gelu\",\n              bias_regularizer = regularizer_l2(.75),\n              kernel_regularizer = regularizer_l1(.0055)\n  ) %>%\n  layer_dropout(0.3) %>% \n  layer_dense(units = 5L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 optimizer = optimizer_adamax(learning_rate = 0.011))\n\nmodel %>%\n  fit(x = trainingSet, y = to_categorical(matrix(flowerLabels, ncol = 1L), 5L),\n      epochs = 50L, batch_size = 100L, shuffle = TRUE, validation_split = 0.2)\n\npred = model %>% predict(trainingSet)\npred_classes = apply(pred, 1, which.max)\nprint(table(pred_classes))\n#> pred_classes\n#>   1   2   3   4 \n#> 566 933 660 864\n\nMetrics::accuracy(pred_classes - 1L, flowerLabels)\n#> [1] 0.5630169\n\n#pred_classes = model %>% predict(EcoData::dataset_flower()$test/255) %>%\n#  apply(1, which.max) - 1L  # Do not forget \"/255\" and  \"- 1L\"!!\n\n#write.csv(pred_classes, file = \"flower_CNN.csv\")\nlibrary(keras)\nlibrary(tensorflow)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nif(!exists(\"doneWithoutTest\")){  # Do not calculate this more often than 1 time.\n  trainingSet = c()\n  flowerLabels = c()\n  data = getData(oversample = FALSE, undersample = FALSE)\n  trainingSet = data[[1]]\n  flowerLabels = data[[2]]\n  trainLocal = data[[3]]\n  doneWithoutTest = 1\n}\n#> labelsLocal\n#>   0   1   2   3   4 \n#> 538 736 548 513 688\n\nmodel = keras_model_sequential()\nmodel %>% \n  layer_conv_2d(filter = 48L, kernel_size = c(4L, 4L),\n                input_shape = c(80L, 80L, 3L), activation = \"relu\") %>% \n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filter = 48L, kernel_size = c(3L, 3L), activation = \"elu\") %>% \n  layer_max_pooling_2d() %>%\n  layer_conv_2d(filter = 64L, kernel_size = c(2L, 2L), activation = \"gelu\") %>% \n  layer_max_pooling_2d() %>%\n  layer_flatten() %>%\n  layer_dropout(0.33) %>%\n  layer_dense(units = 750L, activation = \"gelu\",\n              bias_regularizer = regularizer_l1(.75),\n              kernel_regularizer = regularizer_l2(.0055)\n  ) %>%\n  layer_dropout(0.4) %>%\n  layer_dense(units = 175L, activation = \"gelu\") %>%\n  layer_dropout(0.35) %>%\n  layer_dense(units = 75L, activation = \"relu\") %>%\n  layer_dense(units = 50L, activation = \"gelu\",\n              bias_regularizer = regularizer_l2(.75),\n              kernel_regularizer = regularizer_l1(.0055)\n  ) %>%\n  layer_dropout(0.3) %>% \n  layer_dense(units = 5L, activation = \"softmax\")\n\nmodel %>%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 optimizer = optimizer_adamax(learning_rate = 0.011))\n\nmodel %>%\n  fit(x = trainingSet, y = to_categorical(matrix(flowerLabels, ncol = 1L), 5L),\n      epochs = 50L, batch_size = 100L, shuffle = TRUE)\n\npred_classes = apply(model %>% predict(trainingSet), 1, which.max)\nprint(table(pred_classes))\n#> pred_classes\n#>    1    2    3    4    5 \n#>  639 1232  205   46  901\n\nMetrics::accuracy(pred_classes - 1L, flowerLabels)\n#> [1] 0.5732716\n\npred_classes = model %>% predict(EcoData::dataset_flower()$test/255) %>%\n  apply(1, which.max) - 1L  # Do not forget \"/255\" and  \"- 1L\"!!\n\nwrite.csv(pred_classes, file = \"flower_CNN.csv\")"},{"path":"interpretation.html","id":"interpretation","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7 Interpretation and Causality With Machine Learning","text":"","code":""},{"path":"interpretation.html","id":"explainable-ai","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1 Explainable AI","text":"goal explainable AI (xAI, aka interpretable machine learning) explain fitted machine learning model makes certain predictions. typical example understand important different variables predictions. incentives range better technical understanding models understanding data important improving predictions questions fairness discrimination (e.g. understand algorithm uses skin color make decision).","code":""},{"path":"interpretation.html","id":"a-practical-example","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.1 A Practical Example","text":"lecture work another famous data set, Boston housing data set:fit random forest use iml package xAI, see https://christophm.github.io/interpretable-ml-book/.xAI packages written generic, .e. can handle almost machine learning models.\nwant use , first create predictor object, holds model data. iml package uses R6 classes, means new objects can created calling Predictor$new(). (worry know R6 classes , just use command.)","code":"\nlibrary(iml)\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nset.seed(123)\n\ndata(\"Boston\", package = \"MASS\")\nrf = randomForest(medv ~ ., data = Boston, ntree = 50)\nX = Boston[which(names(Boston) != \"medv\")]\npredictor = Predictor$new(rf, data = X, y = Boston$medv)\n# \"Predictor\" is an object generator."},{"path":"interpretation.html","id":"feature-importance","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.2 Feature Importance","text":"Feature importance mistaken random forest variable importance though related. tells us important individual variables predictions, can calculated machine learning models based permutation approach (look book):","code":"\nimp = FeatureImp$new(predictor, loss = \"mae\")\nplot(imp)"},{"path":"interpretation.html","id":"partial-dependencies","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.3 Partial Dependencies","text":"Partial dependencies similar allEffects plots normal regressions. idea visualize “marginal effects” predictors (“feature” argument specify variable want visualize):Partial dependencies can also plotted single observations:One disadvantage partial dependencies sensitive correlated predictors. Accumulated local effects can used accounting correlation predictors.","code":"\neff = FeatureEffect$new(predictor, feature = \"rm\", method = \"pdp\",\n                        grid.size = 30)\nplot(eff)\neff = FeatureEffect$new(predictor, feature = \"rm\", method = \"pdp+ice\",\n                        grid.size = 30)\nplot(eff)"},{"path":"interpretation.html","id":"accumulated-local-effects","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.4 Accumulated Local Effects","text":"Accumulated local effects (ALE) basically partial dependencies plots try correct correlations predictors.collinearity, shouldn’t see much difference partial dependencies ALE plots.","code":"\nale = FeatureEffect$new(predictor, feature = \"rm\", method = \"ale\")\nale$plot()"},{"path":"interpretation.html","id":"friedmans-h-statistic","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.5 Friedman’s H-statistic","text":"H-statistic can used find interactions predictors. However, , keep mind H-statistic sensible correlation predictors:","code":"\ninteract = Interaction$new(predictor, \"lstat\")\nplot(interact)"},{"path":"interpretation.html","id":"global-explainer---simplifying-the-machine-learning-model","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.6 Global Explainer - Simplifying the Machine Learning Model","text":"Another idea simplifying machine learning model another simpler model decision tree. create predictions machine learning model lot different input values fit decision tree predictions. can interpret easier model.","code":"\nlibrary(partykit)\n\ntree = TreeSurrogate$new(predictor, maxdepth = 2)\nplot(tree$tree)"},{"path":"interpretation.html","id":"local-explainer---lime-explaining-single-instances-observations","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.7 Local Explainer - LIME Explaining Single Instances (observations)","text":"global approach simplify entire machine learning-black-box model via simpler model, interpretable.However, sometimes interested understanding single predictions generated. LIME (Local interpretable model-agnostic explanations) approach explores feature space around one observation based locally fits simpler model (e.g. linear model):","code":"\nlime.explain = LocalModel$new(predictor, x.interest = X[1,])\nlime.explain$results\n#>               beta x.recoded    effect x.original feature feature.value\n#> rm       4.1893817     6.575 27.545185      6.575      rm      rm=6.575\n#> ptratio -0.5307031    15.300 -8.119758       15.3 ptratio  ptratio=15.3\n#> lstat   -0.4398104     4.980 -2.190256       4.98   lstat    lstat=4.98\nplot(lime.explain)"},{"path":"interpretation.html","id":"local-explainer---shapley","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.1.8 Local Explainer - Shapley","text":"Shapley method computes called Shapley value, feature contributions single predictions, based approach cooperative game theory. idea feature value instance “player” game, prediction reward. Shapley value tells us fairly distribute reward among features.","code":"\nshapley = Shapley$new(predictor, x.interest = X[1,])\nshapley$plot()"},{"path":"interpretation.html","id":"causal-inference-and-machine-learning","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.2 Causal Inference and Machine Learning","text":"xAI aims explaining predictions made. general, xAI != causality. xAI methods measure variables used predictions algorithm, far variables improve predictions. important point note : variable causes something, also expect helps predicting thing. opposite, however, generally true - often possible variable doesn’t cause anything can predict something.statistics courses (particular course: Advanced Biostatistics), discuss issue causality full length. , don’t want go details, , general resist interpret indicators importance xAI causal effects. tell something ’s going algorithm, ’s going reality.","code":""},{"path":"interpretation.html","id":"causalInference","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.2.1 Causal Inference on Static Data","text":"Methods causal inference depend whether dynamic static data. latter common case. static data, problem confounding. several correlated predictors, can get spurious correlations given predictor response, although causal effect general.Multiple regression methods able correct predictors thus isolate causal effect. necessarily true machine learning algorithms xAI methods. bug, feature - making good predictions, often problem, rather advantage also use non-causal predictors.example indicators variable importance random forest algorithm. purpose script show random forest variable importance split importance values collinear variables evenly, even collinearity low enough variables separable correctly separated lm / ANOVA.first simulate data set 2 predictors strongly correlated, one effect response.lm / anova correctly identify \\(x1\\) causal variable.Fit random forest show variable importance:Variable importance now split nearly evenly.Task: understand - remember:random forest works - variables randomly hidden regression tree trees forest built.Remember \\(x1 \\propto x2\\), can use \\(x2\\) replacement \\(x1\\).Remember variable importance measures average contributions different variables trees forest.","code":"\nset.seed(123)\n\n# Simulation parameters.\nn = 1000\ncol = 0.7\n\n# Create collinear predictors.\nx1 = runif(n)\nx2 = col * x1 + (1-col) * runif(n)\n\n# Response is only influenced by x1.\ny = x1 + rnorm(n)\nsummary(lm(y ~ x1 + x2))\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x2)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.0709 -0.6939  0.0102  0.6976  3.3373 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.02837    0.08705   0.326 0.744536    \n#> x1           1.07383    0.27819   3.860 0.000121 ***\n#> x2          -0.04547    0.37370  -0.122 0.903186    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.011 on 997 degrees of freedom\n#> Multiple R-squared:  0.08104,    Adjusted R-squared:  0.0792 \n#> F-statistic: 43.96 on 2 and 997 DF,  p-value: < 2.2e-16\nset.seed(123)\n\nfit = randomForest(y ~ x1 + x2, importance = TRUE)\nvarImpPlot(fit)"},{"path":"interpretation.html","id":"structural-equation-models","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.2.2 Structural Equation Models","text":"causal relationships get complicated, possible adjust correctly simple lm. case, statistics, usually use structural equation models (SEMs). Structural equation models designed estimate entire causal diagrams. two main SEM packages R: anything non-normal, currently estimate directed acyclic graph (depicts causal relations) piece-wise CRAN package piecewiseSEM. Example vegetation data set:linear structural equation models, can estimate entire directed acyclic graph . also allows unobserved variables directed acyclic graph. One popular packages lavaan.default plot options nice .Another plotting option using semPlot.","code":"\nlibrary(piecewiseSEM)\n\nmod = psem(\n lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover,\n    data = keeley),\n lm(firesev ~ elev + age + cover, data = keeley),\n lm(cover ~ age + elev + hetero + abiotic, data = keeley)\n)\nsummary(mod)\nplot(mod)\nlibrary(lavaan)\n\nmod = \"\n rich ~ distance + elev + abiotic + age + hetero + firesev + cover\n firesev ~ elev + age + cover\n cover ~ age + elev + abiotic\n\"\nfit = sem(mod, data = keeley)\nsummary(fit)\n#> lavaan 0.6-12 ended normally after 1 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                        16\n#> \n#>   Number of observations                            90\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                10.437\n#>   Degrees of freedom                                 5\n#>   P-value (Chi-square)                           0.064\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Regressions:\n#>                    Estimate   Std.Err  z-value  P(>|z|)\n#>   rich ~                                               \n#>     distance           0.616    0.177    3.485    0.000\n#>     elev              -0.009    0.006   -1.644    0.100\n#>     abiotic            0.488    0.156    3.134    0.002\n#>     age                0.024    0.105    0.229    0.819\n#>     hetero            44.414    9.831    4.517    0.000\n#>     firesev           -1.018    0.759   -1.341    0.180\n#>     cover             12.400    3.841    3.228    0.001\n#>   firesev ~                                            \n#>     elev              -0.001    0.001   -0.951    0.342\n#>     age                0.047    0.013    3.757    0.000\n#>     cover             -1.521    0.509   -2.991    0.003\n#>   cover ~                                              \n#>     age               -0.009    0.002   -3.875    0.000\n#>     elev               0.000    0.000    2.520    0.012\n#>     abiotic           -0.000    0.004   -0.115    0.909\n#> \n#> Variances:\n#>                    Estimate   Std.Err  z-value  P(>|z|)\n#>    .rich              97.844   14.586    6.708    0.000\n#>    .firesev            1.887    0.281    6.708    0.000\n#>    .cover              0.081    0.012    6.708    0.000\nlibrary(lavaanPlot)\n\nlavaanPlot(model = fit)\nlibrary(semPlot)\n\nsemPaths(fit)"},{"path":"interpretation.html","id":"automatic-causal-discovery","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.2.3 Automatic Causal Discovery","text":"get causal graph? statistics, common “guess” afterwards residual checks, way guess structure regression. complicated problems, however, unsatisfying. groups therefore work -called causal discovery algorithms, .e. algorithms automatically generate causal graphs data. One classic algorithms sort PC algorithm. example using pcalg package:Loading data:First, skeleton algorithm creates basic graph without connections (skeleton graph).missing direction errors. PC algorithm now makes tests conditional independence, allows fixing part (typically ) directions causal arrows.","code":"\nlibrary(pcalg)\ndata(\"gmG\", package = \"pcalg\") # Loads data sets gmG and gmG8.\nsuffStat = list(C = cor(gmG8$x), n = nrow(gmG8$x))\nvarNames = gmG8$g@nodes\nskel.gmG8 = skeleton(suffStat, indepTest = gaussCItest,\nlabels = varNames, alpha = 0.01)\nRgraphviz::plot(skel.gmG8)\n#> Warning in !is.null(main) && nchar(main) > 0: 'length(x) = 2 > 1' in coercion to\n#> 'logical(1)'\npc.gmG8 = pc(suffStat, indepTest = gaussCItest,\nlabels = varNames, alpha = 0.01)\nRgraphviz::plot(pc.gmG8 )\n#> Warning in !is.null(main) && nchar(main) > 0: 'length(x) = 2 > 1' in coercion to\n#> 'logical(1)'"},{"path":"interpretation.html","id":"causal-inference-on-dynamic-data","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.2.4 Causal Inference on Dynamic Data","text":"working dynamic data, can use additional piece information - cause usually precedes effect, means can test time-lag cause effect determine direction causality. way testing causality known Granger causality, Granger methods. example:","code":"\nlibrary(lmtest)\n\n## What came first: the chicken or the egg?\ndata(ChickEgg)\ngrangertest(egg ~ chicken, order = 3, data = ChickEgg)\n#> Granger causality test\n#> \n#> Model 1: egg ~ Lags(egg, 1:3) + Lags(chicken, 1:3)\n#> Model 2: egg ~ Lags(egg, 1:3)\n#>   Res.Df Df      F Pr(>F)\n#> 1     44                 \n#> 2     47 -3 0.5916 0.6238\ngrangertest(chicken ~ egg, order = 3, data = ChickEgg)\n#> Granger causality test\n#> \n#> Model 1: chicken ~ Lags(chicken, 1:3) + Lags(egg, 1:3)\n#> Model 2: chicken ~ Lags(chicken, 1:3)\n#>   Res.Df Df     F   Pr(>F)   \n#> 1     44                     \n#> 2     47 -3 5.405 0.002966 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"interpretation.html","id":"outlook-for-machine-learning","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.2.5 Outlook for Machine Learning","text":"seen, already methods / algorithms discovering causality large data sets, systematic transfer concepts machine learning, particular deep learning, still infancy. moment, field actively researched changes extremely fast, recommend using Google see currently going . Particular business industry, large interest learning causal effect large data sets. opinion, great topic young scientists specialize .","code":""},{"path":"interpretation.html","id":"exercises-7","chapter":"7 Interpretation and Causality With Machine Learning","heading":"7.2.6 Exercises","text":"Use one non-image based data sets (preferably Wine, also described data sets section 9 wasn’t used yet, can also use Nasa Titanic) fit random forest. Explore / interpret fitted model using iml (see also book: https://christophm.github.io/interpretable-ml-book/).\nMind feature importance, random forest’s variable importance related equal!\nVariable importance measure determining importance creating forest (.e. fitting). Feature importance measure important variable prediction.Maybe want see explanation methods well. Surely can use techniques section .show section 7.2.1 chapter, random forest split variable importance across collinear predictors, linear regression model (lm()) can identify predictor causally affecting response (least theory, confounders controlled). boosted regression tree artificial neural network? Take random forest example add boosted regression tree (easier, can use example https://rdrr.io/cran/xgboost/man/xgb.importance.html) artificial neural network, look better random forest identifying causal predictors.\nEvery method yields slightly different results, main ingredient alcohol (sulphates).’re done previous tasks still time appetite, improve submissions competition, particular Wine data set. Possible ideas:Use MLR framework (section 5.2).Use MLR framework (section 5.2).Try Transfer learning (section 6.4.2). winner last years challenge.Try Transfer learning (section 6.4.2). winner last years challenge.Search kaggle ideas / try copy ideas. winner two years ago.Search kaggle ideas / try copy ideas. winner two years ago.little example (unbalanced!) Wine data set\nRecognize overfitting model selection strategy changing seed times (keeping model constant) increase percentage test data.\nFurthermore, consider fitting random forest good quality well.final predictions, use whole data set without holdouts:","code":"\nlibrary(randomForest)\nlibrary(\"iml\")\nset.seed(1234)\n\ndata = as.data.frame(EcoData::wine)\nsubmission = data[which(is.na(data$quality)), -which(colnames(data) == \"quality\")]\ndata = data[complete.cases(data), ] # Removes sumbmission data as well.\n\n# Remark: Features don't need to be scaled for regression trees.\n\nrf = randomForest(quality ~ ., data = data)\npred = round(predict(rf, data))\ntable(pred, data$quality)\n#>     \n#> pred   3   4   5   6   7   8\n#>    4   2   4   0   0   0   0\n#>    5   0   6 133   0   0   0\n#>    6   0   0   3 113  10   0\n#>    7   0   0   0   0  25   3\n(accuracy = mean(pred == data$quality)) # Fits pretty well (on the training data...)\n#> [1] 0.9197324\n\n# For submission:\n#write.csv(round(predict(rf, submission)), file = \"wine_RF.csv\")\n\n# Standard depiction of importance:\nrf$importance\n#>                      IncNodePurity\n#> fixed.acidity            12.279077\n#> volatile.acidity         21.997368\n#> citric.acid              13.751476\n#> residual.sugar           10.787817\n#> chlorides                11.587301\n#> free.sulfur.dioxide       9.966824\n#> total.sulfur.dioxide     13.977669\n#> density                  17.068801\n#> pH                       10.302109\n#> sulphates                20.273156\n#> alcohol                  37.316264\n\n# IML:\npredictor = Predictor$new(\n    rf, data = data[,which(names(data) != \"quality\")], y = data$quality)\n\n# Mind: This is stochastical!\nimportance = FeatureImp$new(predictor, loss = \"mae\")\n\nplot(importance)\n\n# Comparison between standard importance and IML importance:\nimportanceRf = rownames(rf$importance)[order(rf$importance, decreasing = TRUE)]\nimportanceIML = importance$results[1]\ncomparison = cbind(importanceIML, importanceRf)\ncolnames(comparison) = c(\"IML\", \"RF\")\nas.matrix(comparison)\n#>       IML                    RF                    \n#>  [1,] \"alcohol\"              \"alcohol\"             \n#>  [2,] \"sulphates\"            \"volatile.acidity\"    \n#>  [3,] \"volatile.acidity\"     \"sulphates\"           \n#>  [4,] \"density\"              \"density\"             \n#>  [5,] \"citric.acid\"          \"total.sulfur.dioxide\"\n#>  [6,] \"total.sulfur.dioxide\" \"citric.acid\"         \n#>  [7,] \"fixed.acidity\"        \"fixed.acidity\"       \n#>  [8,] \"chlorides\"            \"chlorides\"           \n#>  [9,] \"pH\"                   \"residual.sugar\"      \n#> [10,] \"free.sulfur.dioxide\"  \"pH\"                  \n#> [11,] \"residual.sugar\"       \"free.sulfur.dioxide\"\nlibrary(xgboost)\nset.seed(1234)\n\ndata = as.data.frame(EcoData::wine)\nsubmission = data[which(is.na(data$quality)), -which(colnames(data) == \"quality\")]\ndata = data[complete.cases(data), ] # Removes sumbmission data as well.\n\ndata_xg = xgb.DMatrix(\n  data = as.matrix(data[,which(names(data) != \"quality\")]),\n  label = data$quality\n)\nbrt = xgboost(data_xg, nrounds = 24)\n#> [1]  train-rmse:3.656523 \n#> [2]  train-rmse:2.609494 \n#> [3]  train-rmse:1.884807 \n#> [4]  train-rmse:1.384918 \n#> [5]  train-rmse:1.037362 \n#> [6]  train-rmse:0.800110 \n#> [7]  train-rmse:0.629324 \n#> [8]  train-rmse:0.508917 \n#> [9]  train-rmse:0.426155 \n#> [10] train-rmse:0.369580 \n#> [11] train-rmse:0.313017 \n#> [12] train-rmse:0.274227 \n#> [13] train-rmse:0.236959 \n#> [14] train-rmse:0.207364 \n#> [15] train-rmse:0.195811 \n#> [16] train-rmse:0.182500 \n#> [17] train-rmse:0.173310 \n#> [18] train-rmse:0.154747 \n#> [19] train-rmse:0.144045 \n#> [20] train-rmse:0.139083 \n#> [21] train-rmse:0.129605 \n#> [22] train-rmse:0.118541 \n#> [23] train-rmse:0.110689 \n#> [24] train-rmse:0.097798\n\npred = round(predict(brt, newdata = data_xg)) # Only integers are allowed.\ntable(pred, data$quality)\n#>     \n#> pred   3   4   5   6   7   8\n#>    3   2   0   0   0   0   0\n#>    4   0  10   0   0   0   0\n#>    5   0   0 136   0   0   0\n#>    6   0   0   0 113   1   0\n#>    7   0   0   0   0  34   0\n#>    8   0   0   0   0   0   3\n(accuracy = mean(pred == data$quality)) # Fits very well (on the training data...)\n#> [1] 0.9966555\n\n# For submission:\n#write.csv(round(predict(rf, submission)), file = \"wine_RF.csv\")\n\n# Look at variable importance:\nxgboost::xgb.importance(model = brt)\n#>                  Feature       Gain      Cover  Frequency\n#>  1:              alcohol 0.28363726 0.13667721 0.07073509\n#>  2:            sulphates 0.11331763 0.07015405 0.06657420\n#>  3:        fixed.acidity 0.09844523 0.11359510 0.19278779\n#>  4:     volatile.acidity 0.09582794 0.07098397 0.12760055\n#>  5: total.sulfur.dioxide 0.09207986 0.09147259 0.07212205\n#>  6:              density 0.07374576 0.14910006 0.08321775\n#>  7:            chlorides 0.06025521 0.07972405 0.08876560\n#>  8:       residual.sugar 0.05307944 0.07202137 0.08044383\n#>  9:  free.sulfur.dioxide 0.04602742 0.04743503 0.06518724\n#> 10:                   pH 0.04477577 0.12562892 0.07489598\n#> 11:          citric.acid 0.03880847 0.04320764 0.07766990\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(123L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nreadin = function(percentageTest = 0.2, aggregate = 0){\n    # Parameter \"aggregate\" packs the classes with very low abundances into one.\n    # If \"aggregate\" equals to NA, NaN, Null, 0 or FALSE, no aggregation is performed.\n    # Else, the given number is the boundary.\n    # Every class with less elements than the boundary is aggregated into one.\n    \n    # WARNING: These classes cannot be distinguished from then on!\n    # Using the predictions for submission needs further processing!\n    \n    # Just for random selection of features, independent of the amount of function calls.\n    set.seed(12345)\n    \n    train = as.data.frame(EcoData::wine)\n    indicesTrain = which(!is.na(train$quality))\n    labelsTrain = train$quality[indicesTrain]\n    labelsTrain = labelsTrain - min(labelsTrain)  # Start at 0 (for softmax).\n    train = train[, -which(colnames(train) == \"quality\")]\n    \n    if(!is.na(aggregate) & aggregate){\n        indices = names(table(labelsTrain)[\n            table(labelsTrain) < aggregate & table(labelsTrain) > 0\n        ])\n        if(length(indices)){\n            labelsTrain[labelsTrain %in% indices] = -1\n            labelsTrain = as.factor(labelsTrain)\n            levels(labelsTrain) = 1:length(levels(labelsTrain)) - 1\n            labelsTrain = as.integer(labelsTrain)\n        }\n    }\n    \n    # Impute missing values (before any splitting, to get the highest power):\n    train = missRanger::missRanger(\n        data = train,\n        maxiter = 10L,\n        seed = 123,\n        num.trees = 200L\n    )\n    \n    # Separate submission data (mind scaling!):\n    submission = scale(train[-indicesTrain,])\n    train = scale(train[indicesTrain,])\n    \n    # Very asymmetric training data:\n    cat(paste0(\"Size of training set: \", length(labelsTrain), \"\\n\"))\n    print(table(labelsTrain))\n    \n    if(percentageTest == 0){\n      return(list(\n        \"labelsTrain\" = labelsTrain,\n        \"labelsTest\" = list(),\n        \"train\" = train,\n        \"test\" = list(),\n        \"submission\" = submission\n      ))\n    }\n    \n    # Split into training and test set:\n    len = nrow(train)\n    indicesTest = sample(x = 1:len, size = percentageTest * len, replace = FALSE)\n    test = as.data.frame(train[indicesTest,])\n    labelsTest = labelsTrain[indicesTest]\n    train = as.data.frame(train[-indicesTest,])\n    labelsTrain = labelsTrain[-indicesTest]\n    \n    return(list(\n        \"labelsTrain\" = labelsTrain,\n        \"labelsTest\" = labelsTest,\n        \"train\" = train,\n        \"test\" = test,\n        \"submission\" = submission\n    ))\n}\n\nretVal = readin(aggregate = 0)\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates\n#>   Variables used to impute:  fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol\n#> iter 1:  ..........\n#> iter 2:  ..........\n#> iter 3:  ..........\n#> Size of training set: 694\n#> labelsTrain\n#>   0   1   2   3   4   5 \n#>   7  25 309 261  84   8\nlabelsTrain = retVal[[\"labelsTrain\"]]\nlabelsTest = retVal[[\"labelsTest\"]]\ntrain = retVal[[\"train\"]]\ntest = retVal[[\"test\"]]\nsubmission = retVal[[\"submission\"]]\nrm(retVal)\n\nclassNumber = length(table(labelsTrain))\n\nmodel = keras_model_sequential()\nmodel %>%\n    layer_dense(units = 200L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    input_shape = ncol(train)) %>%\n    layer_dropout(0.45) %>%\n    layer_dense(units = 100L, activation = \"relu\",\n    bias_regularizer = regularizer_l1_l2(0.5)) %>%\n    layer_dropout(0.2) %>%\n    layer_dense(units = 100L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    bias_regularizer = regularizer_l1_l2(0.1)) %>%\n    layer_dropout(0.25) %>%\n    layer_dense(units = 50L, activation = \"gelu\") %>%\n    layer_dense(units = 25L, activation = \"elu\") %>%\n    layer_dropout(0.35) %>%\n    # We need probabilities. So we use the softmax function.\n    # Remember, the labels MUST start at 0!\n    layer_dense(units = classNumber, activation = \"softmax\")\n\nmodel %>%\n    keras::compile(loss = loss_binary_crossentropy,\n                   optimizer = optimizer_adamax(learning_rate = 0.015))\n\nmodel_history = \n    model %>% # Mind the matrix property (no data.frame)!\n        fit(x = as.matrix(train), y = k_one_hot(labelsTrain, classNumber),\n            epochs = 80L, batch = 12L, shuffle = TRUE)\n\nplot(model_history)\n\n# Accuracy on training set (!)\npred = predict(model, as.matrix(train)) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, labelsTrain)\n#> [1] 0.7733813\ntable(pred, labelsTrain)\n#>     labelsTrain\n#> pred   0   1   2   3   4   5\n#>    0   1   0   0   0   0   0\n#>    1   0   1   0   0   0   0\n#>    2   6  14 212  33   2   0\n#>    3   0   8  25 179  25   3\n#>    4   0   0   1   5  37   4\n\n# Accuracy on test set\npred = predict(model, as.matrix(test)) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, labelsTest)\n#> [1] 0.6594203\ntable(pred, labelsTest)\n#>     labelsTest\n#> pred  1  2  3  4  5\n#>    1  0  1  0  0  0\n#>    2  2 53 13  2  0\n#>    3  0 16 31 11  0\n#>    4  0  1  0  7  1\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nretVal = readin(percentageTest = 0, aggregate = 0)\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates\n#>   Variables used to impute:  fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol\n#> iter 1:  ..........\n#> iter 2:  ..........\n#> iter 3:  ..........\n#> Size of training set: 694\n#> labelsTrain\n#>   0   1   2   3   4   5 \n#>   7  25 309 261  84   8\nlabelsTrain = retVal[[\"labelsTrain\"]]\nlabelsTest = retVal[[\"labelsTest\"]]\ntrain = retVal[[\"train\"]]\ntest = retVal[[\"test\"]]\nsubmission = retVal[[\"submission\"]]\nrm(retVal)\n\nclassNumber = length(table(labelsTrain))\n\nmodel = keras_model_sequential()\nmodel %>%\n    layer_dense(units = 200L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    input_shape = ncol(train)) %>%\n    layer_dropout(0.45) %>%\n    layer_dense(units = 100L, activation = \"relu\",\n    bias_regularizer = regularizer_l1_l2(0.5)) %>%\n    layer_dropout(0.2) %>%\n    layer_dense(units = 100L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    bias_regularizer = regularizer_l1_l2(0.1)) %>%\n    layer_dropout(0.25) %>%\n    layer_dense(units = 50L, activation = \"gelu\") %>%\n    layer_dense(units = 25L, activation = \"elu\") %>%\n    layer_dropout(0.35) %>%\n    # We need probabilities. So we use the softmax function.\n    # Remember, the labels MUST start at 0!\n    layer_dense(units = classNumber, activation = \"softmax\")\n\nmodel %>%\n    keras::compile(loss = loss_binary_crossentropy,\n                   optimizer = optimizer_adamax(learning_rate = 0.015))\n\nmodel_history = \n    model %>% # Mind the matrix property (no data.frame)!\n        fit(x = as.matrix(train), y = k_one_hot(labelsTrain, classNumber),\n            epochs = 80L, batch = 12L, shuffle = TRUE)\n\nplot(model_history)\n\n# Accuracy on training set (!)\npred = predict(model, as.matrix(train)) %>% apply(1, which.max) - 1\nMetrics::accuracy(pred, labelsTrain)\n#> [1] 0.7881844\ntable(pred, labelsTrain)\n#>     labelsTrain\n#> pred   0   1   2   3   4   5\n#>    1   3   7   1   0   0   0\n#>    2   4  15 266  31   4   0\n#>    3   0   3  36 209  15   2\n#>    4   0   0   6  21  65   6\n\n# Reverse subtraction (for start at 0) and create submission file.\nwrite.csv(pred + min(as.data.frame(EcoData::wine)$quality, na.rm = TRUE),\n          file = \"wine_NN.csv\")"},{"path":"gan.html","id":"gan","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8 Generative Modeling and Reinforcement Learning","text":"explore machine learning ideas today.","code":""},{"path":"gan.html","id":"autoencoder","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.1 Autoencoder","text":"autoencoder (AE) type artificial neural network unsupervised learning. idea similar data compression: first part network compresses (encodes) data low dimensional space (e.g. 2-4 dimensions) second part network decompresses (reverses encoding) learns reconstruct data (think hourglass).useful? method similar dimension reduction technique (e.g. PCA) advantage don’t make distributional assumptions (see PCA). instance, first train autoencoder genomic expression data thousands features, compress 2-4 dimensions, use clustering.","code":""},{"path":"gan.html","id":"autoencoder---deep-neural-network-mnist","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.1.1 Autoencoder - Deep Neural Network MNIST","text":"now write autoencoder MNIST data set.Let’s start (usual) MNIST example:don’t need labels , images inputs time outputs final autoencoder.encoder: image (784 dimensions) \\(\\rightarrow\\) 2 dimensionsOur decoder: 2 dimensions \\(\\rightarrow\\) 784 dimensions (image)can use non-sequential model type connect two models. (transfer learning chapter.)now show example image unfitted autoencoder, see train autoencoder.Fit autoencoder (inputs == outputs!):Visualization latent variables:picture shows 2-dimensional encoded values numbers MNIST data set number depicting via respective color.","code":"\nlibrary(keras)\nlibrary(tensorflow)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n#> Loaded Tensorflow version 2.9.1\n\ndata = keras::dataset_mnist()\nrotate = function(x){ t(apply(x, 2, rev)) }\n\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  if(title != \"\"){ main = paste0(\"Label: \", as.character(title)) }\n  else{ main = \"\" }\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE, main = main)\n}\n\ntrain = data[[1]]\ntest = data[[2]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L))\ntest_x = array(test[[1]]/255, c(dim(test[[1]])[1], 784L))\ndown_size_model = keras_model_sequential()\ndown_size_model %>% \n  layer_dense(units = 100L, input_shape = c(784L), activation = \"relu\") %>% \n  layer_dense(units = 20L, activation = \"relu\") %>% \n  layer_dense(units = 2L, activation = \"linear\")\nup_size_model = keras_model_sequential()\nup_size_model %>% \n  layer_dense(units = 20L, input_shape = c(2L), activation = \"relu\") %>% \n  layer_dense(units = 100L, activation = \"relu\") %>% \n  layer_dense(units = 784L, activation = \"sigmoid\")\nautoencoder = keras_model(inputs = down_size_model$input, \n                          outputs = up_size_model(down_size_model$output))\nautoencoder$compile(loss = loss_binary_crossentropy,\n                    optimizer = optimizer_adamax(0.01))\nsummary(autoencoder)\n#> Model: \"model\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  dense_2_input (InputLayer)             [(None, 784)]                       0             \n#>  dense_2 (Dense)                        (None, 100)                         78500         \n#>  dense_1 (Dense)                        (None, 20)                          2020          \n#>  dense (Dense)                          (None, 2)                           42            \n#>  sequential_1 (Sequential)              (None, 784)                         81344         \n#> ==========================================================================================\n#> Total params: 161,906\n#> Trainable params: 161,906\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\nimage = autoencoder(train_x[1,,drop = FALSE])\noldpar = par(mfrow = c(1, 2))\nimgPlot(array(train_x[1,,drop = FALSE], c(28, 28)), title = \"Before\")\nimgPlot(array(image$numpy(), c(28, 28)), title = \"After\")\npar(oldpar)\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(123L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nautoencoder %>% \n  fit(x = train_x, y = train_x, epochs = 5L, batch_size = 128L)\npred_dim = down_size_model(test_x)\nreconstr_pred = up_size_model(pred_dim)\nimgPlot(array(reconstr_pred[10,]$numpy(), dim = c(28L, 28L)), title = \"\")\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\noldpar = par(mfrow = c(1, 1))\nplot(pred_dim$numpy()[,1], pred_dim$numpy()[,2], col = ownColors[test[[2]]+1L])\npar(oldpar)"},{"path":"gan.html","id":"autoencoder---mnist-convolutional-neural-networks","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.1.2 Autoencoder - MNIST Convolutional Neural Networks","text":"can also use convolutional neural networks instead side deep neural networks. Moreover, inverse convolutional layer (layer_conv_2d_transpose; “deconvolution”):Prepare data:define downsize model:Define upsize model:Combine two models fit :Test :","code":"\ndata = tf$keras$datasets$mnist$load_data()\ntrain = data[[1]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]]), 1L))\ntest_x = array(data[[2]][[1]]/255, c(dim(data[[2]][[1]]/255), 1L))\ndown_size_model = keras_model_sequential()\ndown_size_model %>% \n layer_conv_2d(filters = 32L, activation = \"relu\", kernel_size = c(2L, 2L), \n                          input_shape = c(28L, 28L, 1L),\n               strides = c(4L, 4L)) %>% \n layer_conv_2d(filters = 16L, activation = \"relu\", \n                           kernel_size = c(7L, 7L), strides = c(1L, 1L)) %>% \n layer_flatten() %>% \n layer_dense(units = 2L, activation = \"linear\")\nup_size_model = keras_model_sequential()\nup_size_model %>% \n layer_dense(units = 8L, activation = \"relu\", input_shape = c(2L)) %>% \n layer_reshape(target_shape = c(1L, 1L, 8L)) %>% \n layer_conv_2d_transpose(filters = 16L, kernel_size = c(7, 7),\n                         activation = \"relu\", strides = c(1L, 1L)) %>% \n layer_conv_2d_transpose(filters = 32L, activation = \"relu\",\n                         kernel_size = c(2, 2), strides = c(4L, 4L)) %>% \n layer_conv_2d(filters = 1, kernel_size = c(1L, 1L),\n               strides = c(1L, 1L), activation = \"sigmoid\")\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nautoencoder = tf$keras$models$Model(inputs = down_size_model$input,\n                                    outputs = up_size_model(down_size_model$output))\n\nautoencoder$compile(loss = loss_binary_crossentropy,\n                    optimizer = optimizer_rmsprop(0.001))\n\nautoencoder$fit(x = tf$constant(train_x), y = tf$constant(train_x),\n                epochs = 10L, batch_size = 128L)\n#> <keras.callbacks.History object at 0x7fdecc353750>\npred_dim = down_size_model(tf$constant(test_x, \"float32\"))\nreconstr_pred = autoencoder(tf$constant(test_x, \"float32\"))\nimgPlot(reconstr_pred[10,,,]$numpy()[,,1])\n\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\nplot(pred_dim[,1]$numpy(), pred_dim[,2]$numpy(), col = ownColors[test[[2]]+1L])\n\n## Generate new images!\nnew = matrix(c(10, 10), 1, 2)\nimgPlot(array(up_size_model(new)$numpy(), c(28L, 28L)))\n\nnew = matrix(c(5, 5), 1, 2)\nimgPlot(array(up_size_model(new)$numpy(), c(28L, 28L)))"},{"path":"gan.html","id":"VAE","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.1.3 Variational Autoencoder (VAE)","text":"difference variational normal autoencoder variational autoencoder assumes distribution latent variables (latent variables observed composed variables) parameters distribution learned. Thus new objects can generated inserting valid (!) (regard assumed distribution) “seeds” decoder.\nachieve property less randomly chosen points low dimensional latent space meaningful yield suitable results decoding, latent space/training process must regularized.\nprocess, input VAE encoded distribution latent space rather single point.building variational autoencoders, use TensorFlow probability, first, need split data .use TensorFlow probability define priors latent variables.Build two networks:Compile fit model:show works:","code":"\nlibrary(tfprobability)\n\ndata = tf$keras$datasets$mnist$load_data()\n#> Loaded Tensorflow version 2.9.1\ntrain = data[[1]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]]), 1L))\nlibrary(tfprobability)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ntfp = reticulate::import(\"tensorflow_probability\")\nencoded = 2L\nprior = tfd_independent(tfd_normal(c(0.0, 0.0), 1.0), 1L)\n\ndown_size_model = keras_model_sequential()\ndown_size_model %>% \n  layer_conv_2d(filters = 32L, activation = \"relu\", kernel_size = c(2L, 2L),\n               input_shape = c(28L, 28L, 1L), strides = c(4L, 4L)) %>% \n  layer_conv_2d(filters = 16L, activation = \"relu\",\n               kernel_size = c(7L, 7L), strides = c(1L, 1L)) %>% \n  layer_flatten() %>% \n  layer_dense(units = 4L, activation = \"linear\") %>% \n  layer_independent_normal(2L,\n                           activity_regularizer =\n                             tfp$layers$KLDivergenceRegularizer(distribution_b = prior))\n\nup_size_model = keras_model_sequential()\nup_size_model %>% \n  layer_dense(units = 8L, activation = \"relu\", input_shape = c(2L)) %>% \n  layer_reshape(target_shape = c(1L, 1L, 8L)) %>% \n  layer_conv_2d_transpose(filters = 16L, kernel_size = c(7, 7),\n                         activation = \"relu\", strides = c(1L, 1L),\n                         use_bias = FALSE) %>% \n  layer_conv_2d_transpose(filters = 32L, activation = \"relu\",\n                         kernel_size = c(2L, 2L), strides = c(4L, 4L),\n                         use_bias = FALSE) %>% \n  layer_conv_2d(filters = 1, kernel_size = c(1L, 1L),\n               strides = c(1L, 1L), activation = \"sigmoid\",\n               use_bias = FALSE)\n\nVAE = keras_model(inputs = down_size_model$inputs,\n                  outputs = up_size_model(down_size_model$outputs))\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nloss_binary = function(true, pred){\n  return(loss_binary_crossentropy(true, pred) * 28.0 * 28.0)\n}\nVAE$compile(loss = loss_binary, optimizer = optimizer_adamax())\n\nVAE$fit(train_x, train_x, epochs = 5L)\n#> <keras.callbacks.History object at 0x7fe5e0554790>\ndist = down_size_model(train_x[1:2000,,,,drop = FALSE])\nimages = up_size_model(dist$sample()[1:5,])\n\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\noldpar = par(mfrow = c(1, 1))\nimgPlot(images[1,,,1]$numpy())\nplot(dist$mean()$numpy()[,1], dist$mean()$numpy()[,2], col = ownColors[train[[2]]+1L])\npar(oldpar)"},{"path":"gan.html","id":"GANS","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.2 Generative Adversarial Networks (GANs)","text":"idea generative adversarial network (GAN) two neural networks contest “game.” One network creating data trying “trick” network deciding generated data real. generator (similar decoder autoencoders) creates new images noise. discriminator getting mix true (data set) artificially generated images generator. Thereby, loss generator rises fakes identified fakes discriminator (simple binary cross entropy loss, 0/1…). loss discriminator rises fakes identified real images (class 0) real images fakes (class 1), binary cross entropy.Binary cross entropy:\nEntropy Shannon entropy (named Claude Shannon) \\(\\mathbf{H}\\) (uppercase “eta”) context information theory expected value information content mean/average information content “event” compared possible outcomes.\nEncountering event low probability holds information encountering event high probability.Binary cross entropy measure determine similarity two (discrete) probability distributions \\(~(\\mathrm{true~distribution}), B~(\\mathrm{predicted~distribution})\\) according inherent information.(!) symmetric, general: \\(\\textbf{H}_{}(B) \\neq \\textbf{H}_{B}()\\).\nminimum value depends distribution \\(\\) entropy \\(\\):\n\\[\\mathrm{min}~\\textbf{H}_{}(B) = \\underset{B}{\\mathrm{min}}~\\textbf{H}_{}(B) = \\textbf{H}_{}(B = ) = \\textbf{H}_{}() = \\textbf{H}()\\]setup:Outcomes \\(y_{} \\\\{0, 1\\}\\) (labels).Predictions \\(\\hat{y}_{} \\[0, 1]\\) (probabilities).binary cross entropy log loss system outcomes/predictions defined follows:\n\\[\n  \\textbf{H}_{}(B) =\n  -\\frac{1}{N} \\sum_{= 1}^{N} y_{} \\cdot \\mathrm{log} \\left( p(y_{}) \\right) + (1 -y_{}) \\cdot \\mathrm{log} \\left( 1-p(y_{}) \\right) =\\\\\n  = -\\frac{1}{N} \\sum_{= 1}^{N} y_{} \\cdot \\mathrm{log} (\\hat{y}_{}) + (1 -y_{}) \\cdot \\mathrm{log} \\left( 1- \\hat{y}_{} \\right)\n\\]\nHigh predicted probabilities label originally labeled data (1) yield low loss well predicting low probability label originally unlabeled data (0). Mind properties probabilities logarithm.possible application generative adversarial networks create pictures look like real photographs e.g. https://thispersondoesnotexist.com/. Visit site (several times)!.\nHowever, application generative adversarial networks today much wider just creation data. example, generative adversarial networks can also used “augment” data, .e. create new data thereby improve fitted model.","code":""},{"path":"gan.html","id":"mnist---generative-adversarial-networks-based-on-deep-neural-networks","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.2.1 MNIST - Generative Adversarial Networks Based on Deep Neural Networks","text":"now explore MNIST data set.don’t need test set .need function sample images discriminator.Define generator model:also test generator model:discriminator, noise (random vector 100 values) passed network output corresponds number pixels one MNIST image (784). therefore define discriminator function now.also test discriminator function.also define loss functions networks.use already known binary cross entropy. However, encode real predicted values two networks individually.discriminator get two losses - one identifying fake images fake, one identifying real MNIST images real images.generator just get one loss - able deceive discriminator?network get optimizer (GAN networks treated independently):write training loop (use fit function).\niteration (batch) following (GradientTape records computations automatic differentiation):Sample noise.Generator creates images noise.Discriminator makes predictions fake images real images (response probability [0,1]).Calculate loss generator.Calculate loss discriminator.Calculate gradients weights loss.Update weights generator.Update weights discriminator.Return losses.Now can finally train networks training loop:Create networks.Get batch images.Run train_step function.Print losses.Repeat step 2-4 number epochs.","code":"\nlibrary(keras)\nlibrary(tensorflow)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nrotate = function(x){ t(apply(x, 2, rev)) }\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE,\n        main = paste0(\"Label: \", as.character(title)))\n}\ndata = dataset_mnist()\ntrain = data$train\ntrain_x = array((train$x-127.5)/127.5, c(dim(train$x)[1], 784L))\nbatch_size = 32L\ndataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, \"float32\"))\ndataset$batch(batch_size)\n#> <BatchDataset element_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name=None)>\nget_generator = function(){\n  generator = keras_model_sequential()\n  generator %>% \n  layer_dense(units = 200L, input_shape = c(100L)) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 200L) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 784L, activation = \"tanh\")\n  \n  return(generator)\n}\ngenerator = get_generator()\nsample = tf$random$normal(c(1L, 100L))\nimgPlot(array(generator(sample)$numpy(), c(28L, 28L)))\nget_discriminator = function(){\n  discriminator = keras_model_sequential()\n  discriminator %>% \n  layer_dense(units = 200L, input_shape = c(784L)) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 100L) %>% \n  layer_activation_leaky_relu() %>% \n  layer_dense(units = 1L, activation = \"sigmoid\")\n  \n  return(discriminator)\n}\ndiscriminator = get_discriminator()\ndiscriminator(generator(tf$random$normal(c(1L, 100L))))\n#> tf.Tensor([[0.5089391]], shape=(1, 1), dtype=float32)\nce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE)\n\nloss_discriminator = function(real, fake){\n  real_loss = ce(tf$ones_like(real), real)\n  fake_loss = ce(tf$zeros_like(fake), fake)\n  return(real_loss + fake_loss)\n}\n\nloss_generator = function(fake){\n  return(ce(tf$ones_like(fake), fake))\n}\ngen_opt = tf$keras$optimizers$RMSprop(1e-4)\ndisc_opt = tf$keras$optimizers$RMSprop(1e-4)\ngenerator = get_generator()\ndiscriminator = get_discriminator()\n\ntrain_step = function(images){\n  noise = tf$random$normal(c(128L, 100L))\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      gen_images = generator(noise)\n      fake_output = discriminator(gen_images)\n      real_output = discriminator(images)\n      gen_loss = loss_generator(fake_output)\n      disc_loss = loss_discriminator(real_output, fake_output)\n    }\n  )\n  \n  gen_grads = tape$gradient(gen_loss, generator$weights)\n  disc_grads = tape$gradient(disc_loss, discriminator$weights)\n  rm(tape)\n  gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights)))\n  disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights)))\n  \n  return(c(gen_loss, disc_loss))\n}\n\ntrain_step = tf$`function`(reticulate::py_func(train_step))\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nbatch_size = 128L\nepochs = 20L\nsteps = as.integer(nrow(train_x)/batch_size)\ncounter = 1\ngen_loss = c()\ndisc_loss = c()\n\ndataset2 = dataset$prefetch(tf$data$AUTOTUNE)\n\nfor(e in 1:epochs){\n  dat = reticulate::as_iterator(dataset2$batch(batch_size))\n  \n  coro::loop(\n    for(images in dat){\n      losses = train_step(images)\n      gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy())\n      disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy())\n    }\n  )\n   \n  if(epochs %% 5 == 0){ #Print output every 5 steps.\n    cat(\"Gen: \", mean(gen_loss), \" Disc: \", mean(disc_loss), \" \\n\")\n  }\n  noise = tf$random$normal(c(1L, 100L))\n  if(epochs %% 10 == 0){  #Plot image every 10 steps.\n    imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), \"Gen\")\n  }\n}\n#> Gen:  0.6884934  Disc:  1.085609#> Gen:  0.7152359  Disc:  1.162967#> Gen:  0.7213668  Disc:  1.175857#> Gen:  0.7423671  Disc:  1.166635#> Gen:  0.7599397  Disc:  1.165749#> Gen:  0.7712564  Disc:  1.181281#> Gen:  0.8222006  Disc:  1.106725#> Gen:  0.9214915  Disc:  1.115732#> Gen:  0.9310804  Disc:  1.18354#> Gen:  0.9413241  Disc:  1.22591#> Gen:  0.952243  Disc:  1.252399#> Gen:  0.9622479  Disc:  1.270034#> Gen:  0.9704371  Disc:  1.283249#> Gen:  0.9778271  Disc:  1.29094#> Gen:  0.9859265  Disc:  1.297595#> Gen:  0.9939315  Disc:  1.301899#> Gen:  1.004366  Disc:  1.304773#> Gen:  1.019454  Disc:  1.307742#> Gen:  1.029843  Disc:  1.310403#> Gen:  1.043565  Disc:  1.316857"},{"path":"gan.html","id":"flower---gan","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.2.2 Flower - GAN","text":"can now also flower data set. write completely following steps also done MNIST data set.Define generator model test :Define discriminator test :Define loss functions:Define optimizers batch function:Define functions generator discriminator:training:images:","code":"\nlibrary(keras)\nlibrary(tidyverse)\n#> ── Attaching packages ───────────────────────────────────────────────── tidyverse 1.3.1 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ──────────────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\nlibrary(tensorflow)\nlibrary(EcoData)\n\ndata = EcoData::dataset_flower()\ntrain = (data$train-127.5)/127.5\ntest = (data$test-127.5)/127.5\ntrain_x = abind::abind(list(train, test), along = 1L)\ndataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, \"float32\"))\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nget_generator = function(){\n  generator = keras_model_sequential()\n  generator %>% \n    layer_dense(units = 20L*20L*128L, input_shape = c(100L),\n                use_bias = FALSE) %>% \n    layer_activation_leaky_relu() %>% \n    layer_reshape(c(20L, 20L, 128L)) %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 256L, kernel_size = c(3L, 3L),\n                            padding = \"same\", strides = c(1L, 1L),\n                            use_bias = FALSE) %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 128L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(1L, 1L),\n                            use_bias = FALSE) %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 64L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(2L, 2L),\n                            use_bias = FALSE) %>%\n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d_transpose(filters = 3L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(2L, 2L),\n                            activation = \"tanh\", use_bias = FALSE)\n  return(generator)\n}\n\ngenerator = get_generator()\nimage = generator(tf$random$normal(c(1L,100L)))$numpy()[1,,,]\nimage = scales::rescale(image, to = c(0, 255))\nimage %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nget_discriminator = function(){\n  discriminator = keras_model_sequential()\n  discriminator %>% \n    layer_conv_2d(filters = 64L, kernel_size = c(5L, 5L),\n                  strides = c(2L, 2L), padding = \"same\",\n                  input_shape = c(80L, 80L, 3L)) %>%\n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d(filters = 128L, kernel_size = c(5L, 5L),\n                  strides = c(2L, 2L), padding = \"same\") %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_conv_2d(filters = 256L, kernel_size = c(3L, 3L),\n                  strides = c(2L, 2L), padding = \"same\") %>% \n    layer_activation_leaky_relu() %>% \n    layer_dropout(0.3) %>% \n    layer_flatten() %>% \n    layer_dense(units = 1L, activation = \"sigmoid\")\n  return(discriminator)\n}\n\ndiscriminator = get_discriminator()\ndiscriminator\n#> Model: \"sequential_7\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  conv2d_5 (Conv2D)                      (None, 40, 40, 64)                  4864          \n#>  leaky_re_lu_14 (LeakyReLU)             (None, 40, 40, 64)                  0             \n#>  dropout_6 (Dropout)                    (None, 40, 40, 64)                  0             \n#>  conv2d_4 (Conv2D)                      (None, 20, 20, 128)                 204928        \n#>  leaky_re_lu_13 (LeakyReLU)             (None, 20, 20, 128)                 0             \n#>  dropout_5 (Dropout)                    (None, 20, 20, 128)                 0             \n#>  conv2d_3 (Conv2D)                      (None, 10, 10, 256)                 295168        \n#>  leaky_re_lu_12 (LeakyReLU)             (None, 10, 10, 256)                 0             \n#>  dropout_4 (Dropout)                    (None, 10, 10, 256)                 0             \n#>  flatten_1 (Flatten)                    (None, 25600)                       0             \n#>  dense_15 (Dense)                       (None, 1)                           25601         \n#> ==========================================================================================\n#> Total params: 530,561\n#> Trainable params: 530,561\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\ndiscriminator(generator(tf$random$normal(c(1L, 100L))))\n#> tf.Tensor([[0.4999608]], shape=(1, 1), dtype=float32)\nce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE,\n                                        label_smoothing = 0.1)\n\nloss_discriminator = function(real, fake){\n  real_loss = ce(tf$ones_like(real), real)\n  fake_loss = ce(tf$zeros_like(fake), fake)\n  return(real_loss+fake_loss)\n}\n\nloss_generator = function(fake){\n  return(ce(tf$ones_like(fake), fake))\n}\ngen_opt = tf$keras$optimizers$RMSprop(1e-4)\ndisc_opt = tf$keras$optimizers$RMSprop(1e-4)\ngenerator = get_generator()\ndiscriminator = get_discriminator()\n\ntrain_step = function(images){\n  noise = tf$random$normal(c(32L, 100L))\n  \n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      gen_images = generator(noise)\n      \n      real_output = discriminator(images)\n      fake_output = discriminator(gen_images)\n      \n      gen_loss = loss_generator(fake_output)\n      disc_loss = loss_discriminator(real_output, fake_output)\n    }\n  )\n  \n  gen_grads = tape$gradient(gen_loss, generator$weights)\n  disc_grads = tape$gradient(disc_loss, discriminator$weights)\n  rm(tape)\n  \n  gen_opt$apply_gradients(purrr::transpose(list(gen_grads,\n                                                generator$weights)))\n  disc_opt$apply_gradients(purrr::transpose(list(disc_grads,\n                                                 discriminator$weights)))\n  \n  return(c(gen_loss, disc_loss))\n}\n\ntrain_step = tf$`function`(reticulate::py_func(train_step))\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nbatch_size = 32L\nepochs = 30L\nsteps = as.integer(dim(train_x)[1]/batch_size)\ncounter = 1\ngen_loss = c()\ndisc_loss = c()\n\ndataset = dataset$prefetch(tf$data$AUTOTUNE)\n\nfor(e in 1:epochs){\n  dat = reticulate::as_iterator(dataset$batch(batch_size))\n  \n  coro::loop(\n    for(images in dat){\n      losses = train_step(images)\n      gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy())\n      disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy())\n    }\n  )\n   \n  noise = tf$random$normal(c(1L, 100L))\n  image = generator(noise)$numpy()[1,,,]\n  image = scales::rescale(image, to = c(0, 255))\n  if(e %% 10 == 0){\n    image %>% \n      image_to_array() %>%\n        `/`(., 255) %>%\n        as.raster() %>%\n        plot()\n  }\n   \n  cat(\"Gen: \", mean(gen_loss), \" Disc: \", mean(disc_loss), \" \\n\")\n}\n#> Gen:  1.143774  Disc:  0.7957661  \n#> Gen:  1.497174  Disc:  0.7416704  \n#> Gen:  1.723195  Disc:  0.7223008  \n#> Gen:  1.830423  Disc:  0.73045  \n#> Gen:  1.842911  Disc:  0.753501  \n#> Gen:  1.8544  Disc:  0.7677898  \n#> Gen:  1.796528  Disc:  0.8014661  \n#> Gen:  1.728089  Disc:  0.8346844  \n#> Gen:  1.674429  Disc:  0.8570877#> Gen:  1.617773  Disc:  0.8798137  \n#> Gen:  1.569825  Disc:  0.8988503  \n#> Gen:  1.527545  Disc:  0.9151082  \n#> Gen:  1.493145  Disc:  0.9310999  \n#> Gen:  1.463344  Disc:  0.9436654  \n#> Gen:  1.434633  Disc:  0.9576464  \n#> Gen:  1.406997  Disc:  0.9707255  \n#> Gen:  1.382015  Disc:  0.9831676  \n#> Gen:  1.359031  Disc:  0.9945794  \n#> Gen:  1.33527  Disc:  1.007315#> Gen:  1.312788  Disc:  1.018794  \n#> Gen:  1.292069  Disc:  1.030401  \n#> Gen:  1.275383  Disc:  1.038811  \n#> Gen:  1.259953  Disc:  1.046686  \n#> Gen:  1.245397  Disc:  1.05389  \n#> Gen:  1.232108  Disc:  1.060455  \n#> Gen:  1.22031  Disc:  1.065932  \n#> Gen:  1.209019  Disc:  1.071359  \n#> Gen:  1.198599  Disc:  1.075818  \n#> Gen:  1.189716  Disc:  1.079902#> Gen:  1.181944  Disc:  1.082428\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nnoise = tf$random$normal(c(1L, 100L))\nimage = generator(noise)$numpy()[1,,,]\nimage = scales::rescale(image, to = c(0, 255))\nimage %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()"},{"path":"gan.html","id":"reinforcement-learning","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.3 Reinforcement learning","text":"just teaser, run/adapt like.Objective: Train neural network capable balancing pole.environment run local server, please install gym.go colab book.","code":"library(keras)\nlibrary(tensorflow)\nlibrary(gym)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nremote_base =´ \"http://127.0.0.1:5000\"\nclient = create_GymClient(remote_base)\nenv = gym::env_create(client, \"CartPole-v1\")\ngym::env_list_all(client)\nenv_reset(client, env)\n# action = env_action_space_sample(client, env)\nstep = env_step(client, env, 1)\n\nenv_reset(client, env)\ngoal_steps = 500\nscore_requirement = 60\nintial_games = 1000\n\nstate_size = 4L\naction_size = 2L\ngamma = 0.95\nepsilon = 0.95\nepsilon_min = 0.01\nepsilon_decay = 0.995\n\nmodel = keras_model_sequential()\nmodel %>% \n layer_dense(input_shape = c(4L), units = 20L, activation = \"relu\") %>% \n layer_dense(units = 20L, activation = \"relu\") %>% \n layer_dense(2L, activation = \"linear\")\n\nmodel %>% \n keras::compile(loss = loss_mean_squared_error, optimizer = optimizer_adamax())\n\nmemory = matrix(0, nrow = 8000L, 11L)\ncounter = 1\n\nremember = function(memory, state, action, reward, next_state, done){\n  memory[counter,] = as.numeric(c(state, action, reward, next_state, done))\n  counter <<- counter+1\n  return(memory)\n}\n\n# memory: state 1:4, action 5, reward 6, next_state 7:10, done 11\nact = function(state){\n if(runif(1) <= epsilon){ return(sample(0:1, 1)) }\n act_prob = predict(model, matrix(state, nrow = 1L))\n return(which.max(act_prob) - 1L)\n}\n\nreplay = function(batch_size = 25L, memory, counter){\n  indices = sample.int(counter, batch_size)\n  batch = memory[indices,,drop = FALSE]\n  \n  for(i in 1:nrow(batch)){\n    target = batch[i,6] # Reward.\n    action = batch[i,5] # Action.\n    state = matrix(memory[i, 1:4], nrow = 1L)\n    next_state = matrix(memory[i,7:10], nrow =1L)\n    if(!batch[i,11]){ # Not done.\n      target = (batch[i,6] + gamma * predict(model,\n                                            matrix(next_state, nrow = 1L)))[1,1]\n    }\n    \n    target_f = predict(model, matrix(state, nrow = 1L))\n    target_f[action+1L] = target\n    model$fit(x = state, y = target_f, epochs = 1L, verbose = 0L)\n    \n    if(epsilon > epsilon_min){ epsilon <<- epsilon_decay*epsilon }\n  }\n}\n\ndone = 0\n\nfor(e in 1:100){\n  state = unlist(env_reset(client, env))\n  \n  for(time in 1:500){\n    action = act(state)\n    response = env_step(client, env, action = action)\n    done = as.integer(response$done)\n    \n    if(!done){ reward = response$reward }\n    else{ reward = -10 }\n    \n    next_state = unlist(response$observation)\n    memory = remember(memory, state, action, reward, next_state, done)\n    \n    state = next_state\n    if(done){\n      cat(\"episode\", e/500, \" score: \", time, \" eps: \", epsilon, \"\\n\")\n      break()\n    }\n    \n    if(counter > 32L){ replay(32L, memory, counter-1L) }\n  }\n}"},{"path":"gan.html","id":"exercises-8","chapter":"8 Generative Modeling and Reinforcement Learning","heading":"8.4 Exercises","text":"Read section 8.1.3 variational autoencoders try transfer examples MNIST flower data set.\nSplit data:Build variational autoencoder:Compile train model:Go R examples generative adversarial networks (8.2) compare flower example MNIST example - differences - ?\nMNIST example uses “simple” deep neural network sufficient classification easy. flower example uses much expensive convolutional neural network classify images.","code":"\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(tfprobability)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = EcoData::dataset_flower()\ntest = data$test/255\ntrain = data$train/255\nrm(data)\nencoded = 10L\nprior = tfp$distributions$Independent(\n  tfp$distributions$Normal(loc=tf$zeros(encoded), scale = 1.),\n  reinterpreted_batch_ndims = 1L\n)\n\ndown_size_model = tf$keras$models$Sequential(list(\n  tf$keras$layers$InputLayer(input_shape = c(80L, 80L, 3L)),\n  tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 1L),\n  tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 2L),\n  tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 1L),\n  tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 2L),\n  tf$keras$layers$Conv2D(filters = 128L, activation = tf$nn$leaky_relu,\n                         kernel_size = 7L, strides = 1L),\n  tf$keras$layers$Flatten(),\n  tf$keras$layers$Dense(units = tfp$layers$MultivariateNormalTriL$params_size(encoded),\n                        activation = NULL),\n  tfp$layers$MultivariateNormalTriL(\n    encoded, \n    activity_regularizer = tfp$layers$KLDivergenceRegularizer(prior, weight = 0.0002)\n  )\n))\n\nup_size_model = tf$keras$models$Sequential(list(\n  tf$keras$layers$InputLayer(input_shape = encoded),\n  tf$keras$layers$Dense(units = 8192L, activation = \"relu\"),\n  tf$keras$layers$Reshape(target_shape =  c(8L, 8L, 128L)),\n  tf$keras$layers$Conv2DTranspose(filters = 128L, kernel_size = 7L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 2L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 2L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 3L, kernel_size = c(4L, 4L),\n                                  activation = \"sigmoid\", strides = c(1L, 1L),\n                                  use_bias = FALSE)\n))\n\nVAE = tf$keras$models$Model(inputs = down_size_model$inputs, \n                            outputs = up_size_model(down_size_model$outputs))\nsummary(VAE)\n#> Model: \"model_1\"\n#> __________________________________________________________________________________________\n#>  Layer (type)                           Output Shape                        Param #       \n#> ==========================================================================================\n#>  input_1 (InputLayer)                   [(None, 80, 80, 3)]                 0             \n#>  conv2d_9 (Conv2D)                      (None, 76, 76, 32)                  2432          \n#>  conv2d_10 (Conv2D)                     (None, 36, 36, 32)                  25632         \n#>  conv2d_11 (Conv2D)                     (None, 32, 32, 64)                  51264         \n#>  conv2d_12 (Conv2D)                     (None, 14, 14, 64)                  102464        \n#>  conv2d_13 (Conv2D)                     (None, 8, 8, 128)                   401536        \n#>  flatten_3 (Flatten)                    (None, 8192)                        0             \n#>  dense_18 (Dense)                       (None, 65)                          532545        \n#>  multivariate_normal_tri_l (Multivariat  ((None, 10),                       0             \n#>  eNormalTriL)                            (None, 10))                                      \n#>  sequential_11 (Sequential)             (None, 80, 80, 3)                   1278464       \n#> ==========================================================================================\n#> Total params: 2,394,337\n#> Trainable params: 2,394,337\n#> Non-trainable params: 0\n#> __________________________________________________________________________________________\nbe = function(true, pred){\n  return(tf$losses$binary_crossentropy(true, pred) * 80.0 * 80.0)\n}\n\nVAE$compile(loss = be,\n            optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.0003))\nVAE$fit(x = train, y = train, epochs = 50L, shuffle = TRUE, batch_size = 20L)\n#> <keras.callbacks.History object at 0x7fe56aba9b90>\n\ndist = down_size_model(train[1:10,,,])\nimages = up_size_model( dist$sample()[1:5,] )\n\noldpar = par(mfrow = c(3, 1), mar = rep(1, 4))\nscales::rescale(images[1,,,]$numpy(), to = c(0, 255)) %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\nscales::rescale(images[2,,,]$numpy(), to = c(0, 255)) %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\n\nscales::rescale(images[3,,,]$numpy(), to = c(0, 255)) %>% \n  image_to_array() %>%\n  `/`(., 255) %>%\n  as.raster() %>%\n  plot()\npar(oldpar)"},{"path":"datasets.html","id":"datasets","chapter":"9 Data sets","heading":"9 Data sets","text":"can download data sets use course (ignore browser warnings) installing EcoData package:","code":"\ndevtools::install_github(repo = \"florianhartig/EcoData\", subdir = \"EcoData\",\n                         dependencies = TRUE, build_vignettes = FALSE)"},{"path":"datasets.html","id":"titanic","chapter":"9 Data sets","heading":"9.1 Titanic","text":"data set collection Titanic passengers information age, class, sex, survival status. competition simple : Train machine learning model predict survival probability.Titanic data set well explored serves stepping stone many machine learning careers. inspiration data exploration notebooks, check kaggle competition.Response variable: “survived”minimal working example:Load data set:Impute missing values (response variable!):Split training test set:Train model:Predictions:Create submission csv:submit csv http://rhsbio7.uni-regensburg.de:8500.","code":"\nlibrary(EcoData)\n\ndata(titanic_ml)\ntitanic = titanic_ml\nsummary(titanic)\n#>      pclass         survived          name               sex           age         \n#>  Min.   :1.000   Min.   :0.0000   Length:1309        female:466   Min.   : 0.1667  \n#>  1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843   1st Qu.:21.0000  \n#>  Median :3.000   Median :0.0000   Mode  :character                Median :28.0000  \n#>  Mean   :2.295   Mean   :0.3853                                   Mean   :29.8811  \n#>  3rd Qu.:3.000   3rd Qu.:1.0000                                   3rd Qu.:39.0000  \n#>  Max.   :3.000   Max.   :1.0000                                   Max.   :80.0000  \n#>                  NA's   :655                                      NA's   :263      \n#>      sibsp            parch            ticket          fare        \n#>  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000  \n#>  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896  \n#>  Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454  \n#>  Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295  \n#>  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275  \n#>  Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329  \n#>                                   (Other) :1261   NA's   :1        \n#>              cabin      embarked      boat          body                      home.dest  \n#>                 :1014    :  2           :823   Min.   :  1.0                       :564  \n#>  C23 C25 C27    :   6   C:270    13     : 39   1st Qu.: 72.0   New York, NY        : 64  \n#>  B57 B59 B63 B66:   5   Q:123    C      : 38   Median :155.0   London              : 14  \n#>  G6             :   5   S:914    15     : 37   Mean   :160.8   Montreal, PQ        : 10  \n#>  B96 B98        :   4            14     : 33   3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n#>  C22 C26        :   4            4      : 31   Max.   :328.0   Paris, France       :  9  \n#>  (Other)        : 271            (Other):308   NA's   :1188    (Other)             :639\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\ntitanic_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)\ntitanic_imputed = missRanger::missRanger(data = titanic_imputed %>%\n                                           select(-survived))\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       age, fare, body\n#>   Variables used to impute:  pclass, sex, age, sibsp, parch, fare, embarked, body\n#> iter 1:  ...\n#> iter 2:  ...\n#> iter 3:  ...\n#> iter 4:  ...\ntitanic_imputed$survived = titanic$survived\ntrain = titanic_imputed[!is.na(titanic$survived), ]\ntest = titanic_imputed[is.na(titanic$survived), ]\nmodel = glm(survived~., data = train, family = binomial())\npreds = predict(model, data = test, type = \"response\")\nhead(preds)\n#>        561        321       1177       1098       1252       1170 \n#> 0.79511615 0.29231652 0.01461978 0.12323274 0.14130063 0.11847391\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")"},{"path":"datasets.html","id":"plant-pollinator-database","chapter":"9 Data sets","heading":"9.2 Plant-pollinator Database","text":"plant-pollinator database collection plant-pollinator interactions traits plants pollinators. idea pollinators interact plants traits fit (e.g. tongue bee needs match shape flower).\nexplored advantage machine learning algorithms traditional statistical models predicting species interactions paper. interested can look .Response variable: “interaction”minimal working example:Load data set:Impute missing values (response variable!)\nselect predictors (can work predictors course).Split training test set:Train model:Predictions:Create submission csv:","code":"\nlibrary(EcoData)\n\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n#>                    crop                       insect          type          \n#>  Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n#>  Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n#>  Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n#>  Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n#>  Daucus_carota       :  256   Andrena_gravida    :   80                     \n#>  Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n#>  (Other)             :18944   (Other)            :20000                     \n#>     season             diameter        corolla             colour         \n#>  Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n#>  Class :character   1st Qu.:  5.00   Class :character   Class :character  \n#>  Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n#>                     Mean   : 27.03                                        \n#>                     3rd Qu.: 25.00                                        \n#>                     Max.   :150.00                                        \n#>                     NA's   :9472                                          \n#>     nectar            b.system         s.pollination      inflorescence     \n#>  Length:20480       Length:20480       Length:20480       Length:20480      \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>   composite            guild               tongue            body      \n#>  Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n#>  Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n#>  Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n#>                                        Mean   : 8.104   Mean   :10.66  \n#>                                        3rd Qu.:10.500   3rd Qu.:13.00  \n#>                                        Max.   :26.400   Max.   :25.00  \n#>                                        NA's   :17040    NA's   :6160   \n#>   sociality           feeding          interaction \n#>  Length:20480       Length:20480       0   :14095  \n#>  Class :character   Class :character   1   :  595  \n#>  Mode  :character   Mode  :character   NA's: 5790  \n#>                                                    \n#>                                                    \n#>                                                    \n#> \nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nplant_poll_imputed = plant_poll %>% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction)\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>%\n                                              select(-interaction))\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       diameter, corolla, tongue, body\n#>   Variables used to impute:  diameter, corolla, tongue, body\n#> iter 1:  ....\n#> iter 2:  ....\n#> iter 3:  ....\n#> iter 4:  ....\nplant_poll_imputed$interaction = plant_poll$interaction\ntrain = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]\ntest = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ]\nmodel = glm(interaction~., data = train, family = binomial())\npreds = predict(model, newdata = test, type = \"response\")\nhead(preds)\n#>          1          2          3          4          5          6 \n#> 0.02887581 0.04972722 0.03670465 0.03670465 0.02713261 0.03996870\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")"},{"path":"datasets.html","id":"wine","chapter":"9 Data sets","heading":"9.3 Wine","text":"data set collection wines different quality. aim predict quality wine based physiochemical predictors.inspiration data exploration notebooks, check kaggle competition.\ninstance, check nice notebook removes problems data.Response variable: “quality”theoretically use regression model task stick classification model.minimal working example:Load data set:Impute missing values (response variable!).Split training test set:Train model:Predictions:Create submission csv:","code":"\nlibrary(EcoData)\n\ndata(wine)\nsummary(wine)\n#>  fixed.acidity    volatile.acidity  citric.acid     residual.sugar     chlorides      \n#>  Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900   Min.   :0.01200  \n#>  1st Qu.: 7.100   1st Qu.:0.3900   1st Qu.:0.0900   1st Qu.: 1.900   1st Qu.:0.07000  \n#>  Median : 7.900   Median :0.5200   Median :0.2600   Median : 2.200   Median :0.07900  \n#>  Mean   : 8.335   Mean   :0.5284   Mean   :0.2705   Mean   : 2.533   Mean   :0.08747  \n#>  3rd Qu.: 9.300   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600   3rd Qu.:0.09000  \n#>  Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500   Max.   :0.61100  \n#>  NA's   :70       NA's   :48       NA's   :41       NA's   :60       NA's   :37       \n#>  free.sulfur.dioxide total.sulfur.dioxide    density             pH       \n#>  Min.   : 1.00       Min.   :  6.00       Min.   :0.9901   Min.   :2.740  \n#>  1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956   1st Qu.:3.210  \n#>  Median :14.00       Median : 38.00       Median :0.9968   Median :3.310  \n#>  Mean   :15.83       Mean   : 46.23       Mean   :0.9968   Mean   :3.311  \n#>  3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9979   3rd Qu.:3.400  \n#>  Max.   :72.00       Max.   :289.00       Max.   :1.0037   Max.   :4.010  \n#>  NA's   :78          NA's   :78           NA's   :78       NA's   :25     \n#>    sulphates         alcohol         quality     \n#>  Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n#>  1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n#>  Median :0.6200   Median :10.20   Median :6.000  \n#>  Mean   :0.6572   Mean   :10.42   Mean   :5.596  \n#>  3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n#>  Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n#>  NA's   :51                       NA's   :905\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nwine_imputed = missRanger::missRanger(data = wine %>% select(-quality))\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates\n#>   Variables used to impute:  fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol\n#> iter 1:  ..........\n#> iter 2:  ..........\n#> iter 3:  ..........\n#> iter 4:  ..........\n#> iter 5:  ..........\n#> iter 6:  ..........\n#> iter 7:  ..........\nwine_imputed$quality = wine$quality\ntrain = wine_imputed[!is.na(wine$quality), ]\ntest = wine_imputed[is.na(wine$quality), ]\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(quality~., data = train, classification = TRUE)\npreds = predict(rf, data = test)$predictions\nhead(preds)\n#> [1] 6 5 5 7 6 6\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"},{"path":"datasets.html","id":"nasa","chapter":"9 Data sets","heading":"9.4 Nasa","text":"collection asteroids characteristics kaggle. aim predict whether asteroids hazardous .\ninspiration data exploration notebooks, check kaggle competition.Response variable: “Hazardous”Load data set:Impute missing values (response variable!):Split training test set:Train model:Predictions:Create submission csv:","code":"\nlibrary(EcoData)\n\ndata(nasa)\nsummary(nasa)\n#>  Neo.Reference.ID       Name         Absolute.Magnitude Est.Dia.in.KM.min.\n#>  Min.   :2000433   Min.   :2000433   Min.   :11.16      Min.   : 0.00101  \n#>  1st Qu.:3102682   1st Qu.:3102683   1st Qu.:20.10      1st Qu.: 0.03346  \n#>  Median :3514800   Median :3514800   Median :21.90      Median : 0.11080  \n#>  Mean   :3272675   Mean   :3273113   Mean   :22.27      Mean   : 0.20523  \n#>  3rd Qu.:3690987   3rd Qu.:3690385   3rd Qu.:24.50      3rd Qu.: 0.25384  \n#>  Max.   :3781897   Max.   :3781897   Max.   :32.10      Max.   :15.57955  \n#>  NA's   :53        NA's   :57        NA's   :36         NA's   :60        \n#>  Est.Dia.in.KM.max. Est.Dia.in.M.min.   Est.Dia.in.M.max.  Est.Dia.in.Miles.min.\n#>  Min.   : 0.00226   Min.   :    1.011   Min.   :    2.26   Min.   :0.00063      \n#>  1st Qu.: 0.07482   1st Qu.:   33.462   1st Qu.:   74.82   1st Qu.:0.02079      \n#>  Median : 0.24777   Median :  110.804   Median :  247.77   Median :0.06885      \n#>  Mean   : 0.45754   Mean   :  204.649   Mean   :  458.45   Mean   :0.12734      \n#>  3rd Qu.: 0.56760   3rd Qu.:  253.837   3rd Qu.:  567.60   3rd Qu.:0.15773      \n#>  Max.   :34.83694   Max.   :15579.552   Max.   :34836.94   Max.   :9.68068      \n#>  NA's   :23         NA's   :29          NA's   :46         NA's   :42           \n#>  Est.Dia.in.Miles.max. Est.Dia.in.Feet.min. Est.Dia.in.Feet.max. Close.Approach.Date\n#>  Min.   : 0.00140      Min.   :    3.32     Min.   :     7.41    2016-07-22:  18    \n#>  1st Qu.: 0.04649      1st Qu.:  109.78     1st Qu.:   245.49    2015-01-15:  17    \n#>  Median : 0.15395      Median :  363.53     Median :   812.88    2015-02-15:  16    \n#>  Mean   : 0.28486      Mean   :  670.44     Mean   :  1500.77    2007-11-08:  15    \n#>  3rd Qu.: 0.35269      3rd Qu.:  832.80     3rd Qu.:  1862.19    2012-01-15:  15    \n#>  Max.   :21.64666      Max.   :51114.02     Max.   :114294.42    (Other)   :4577    \n#>  NA's   :50            NA's   :21           NA's   :46           NA's      :  29    \n#>  Epoch.Date.Close.Approach Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr\n#>  Min.   :7.889e+11         Min.   : 0.3355              Min.   :  1208             \n#>  1st Qu.:1.016e+12         1st Qu.: 8.4497              1st Qu.: 30399             \n#>  Median :1.203e+12         Median :12.9370              Median : 46532             \n#>  Mean   :1.180e+12         Mean   :13.9848              Mean   : 50298             \n#>  3rd Qu.:1.356e+12         3rd Qu.:18.0774              3rd Qu.: 65068             \n#>  Max.   :1.473e+12         Max.   :44.6337              Max.   :160681             \n#>  NA's   :43                NA's   :27                   NA's   :28                 \n#>  Miles.per.hour    Miss.Dist..Astronomical. Miss.Dist..lunar.   Miss.Dist..kilometers.\n#>  Min.   :  750.5   Min.   :0.00018          Min.   :  0.06919   Min.   :   26610      \n#>  1st Qu.:18846.7   1st Qu.:0.13341          1st Qu.: 51.89874   1st Qu.:19964907      \n#>  Median :28893.7   Median :0.26497          Median :103.19415   Median :39685408      \n#>  Mean   :31228.0   Mean   :0.25690          Mean   : 99.91366   Mean   :38436154      \n#>  3rd Qu.:40436.9   3rd Qu.:0.38506          3rd Qu.:149.59244   3rd Qu.:57540318      \n#>  Max.   :99841.2   Max.   :0.49988          Max.   :194.45491   Max.   :74781600      \n#>  NA's   :38        NA's   :60               NA's   :30          NA's   :56            \n#>  Miss.Dist..miles.  Orbiting.Body    Orbit.ID             Orbit.Determination.Date\n#>  Min.   :   16535   Earth:4665    Min.   :  1.00   2017-06-21 06:17:20:   9       \n#>  1st Qu.:12454813   NA's :  22    1st Qu.:  9.00   2017-04-06 08:57:13:   8       \n#>  Median :24662435                 Median : 16.00   2017-04-06 09:24:24:   8       \n#>  Mean   :23885560                 Mean   : 28.34   2017-04-06 08:24:13:   7       \n#>  3rd Qu.:35714721                 3rd Qu.: 31.00   2017-04-06 08:26:19:   7       \n#>  Max.   :46467132                 Max.   :611.00   (Other)            :4622       \n#>  NA's   :27                       NA's   :33       NA's               :  26       \n#>  Orbit.Uncertainity Minimum.Orbit.Intersection Jupiter.Tisserand.Invariant\n#>  Min.   :0.000      Min.   :0.00000            Min.   :2.196              \n#>  1st Qu.:0.000      1st Qu.:0.01435            1st Qu.:4.047              \n#>  Median :3.000      Median :0.04653            Median :5.071              \n#>  Mean   :3.521      Mean   :0.08191            Mean   :5.056              \n#>  3rd Qu.:6.000      3rd Qu.:0.12150            3rd Qu.:6.017              \n#>  Max.   :9.000      Max.   :0.47789            Max.   :9.025              \n#>  NA's   :49         NA's   :137                NA's   :56                 \n#>  Epoch.Osculation   Eccentricity     Semi.Major.Axis   Inclination      \n#>  Min.   :2450164   Min.   :0.00752   Min.   :0.6159   Min.   : 0.01451  \n#>  1st Qu.:2458000   1st Qu.:0.24086   1st Qu.:1.0012   1st Qu.: 4.93290  \n#>  Median :2458000   Median :0.37251   Median :1.2422   Median :10.27694  \n#>  Mean   :2457723   Mean   :0.38267   Mean   :1.4009   Mean   :13.36159  \n#>  3rd Qu.:2458000   3rd Qu.:0.51256   3rd Qu.:1.6782   3rd Qu.:19.47848  \n#>  Max.   :2458020   Max.   :0.96026   Max.   :5.0720   Max.   :75.40667  \n#>  NA's   :60        NA's   :39        NA's   :53       NA's   :42        \n#>  Asc.Node.Longitude Orbital.Period   Perihelion.Distance Perihelion.Arg    \n#>  Min.   :  0.0019   Min.   : 176.6   Min.   :0.08074     Min.   :  0.0069  \n#>  1st Qu.: 83.1849   1st Qu.: 365.9   1st Qu.:0.63038     1st Qu.: 95.6430  \n#>  Median :172.6347   Median : 504.9   Median :0.83288     Median :189.7729  \n#>  Mean   :172.1717   Mean   : 635.5   Mean   :0.81316     Mean   :184.0185  \n#>  3rd Qu.:254.8804   3rd Qu.: 793.1   3rd Qu.:0.99718     3rd Qu.:271.9535  \n#>  Max.   :359.9059   Max.   :4172.2   Max.   :1.29983     Max.   :359.9931  \n#>  NA's   :60         NA's   :46       NA's   :22          NA's   :48        \n#>  Aphelion.Dist    Perihelion.Time    Mean.Anomaly       Mean.Motion       Equinox    \n#>  Min.   :0.8038   Min.   :2450100   Min.   :  0.0032   Min.   :0.08628   J2000:4663  \n#>  1st Qu.:1.2661   1st Qu.:2457815   1st Qu.: 87.0069   1st Qu.:0.45147   NA's :  24  \n#>  Median :1.6182   Median :2457972   Median :186.0219   Median :0.71137               \n#>  Mean   :1.9864   Mean   :2457726   Mean   :181.2882   Mean   :0.73732               \n#>  3rd Qu.:2.4497   3rd Qu.:2458108   3rd Qu.:276.6418   3rd Qu.:0.98379               \n#>  Max.   :8.9839   Max.   :2458839   Max.   :359.9180   Max.   :2.03900               \n#>  NA's   :38       NA's   :59        NA's   :40         NA's   :48                    \n#>    Hazardous    \n#>  Min.   :0.000  \n#>  1st Qu.:0.000  \n#>  Median :0.000  \n#>  Mean   :0.176  \n#>  3rd Qu.:0.000  \n#>  Max.   :1.000  \n#>  NA's   :4187\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nnasa_imputed = missRanger::missRanger(data = nasa %>% select(-Hazardous),\n                                      maxiter = 1, num.trees = 5L)\n#> \n#> Missing value imputation by random forests\n#> \n#>   Variables to impute:       Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox\n#>   Variables used to impute:  Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox\n#> iter 1:  .......................................\nnasa_imputed$Hazardous = nasa$Hazardous\ntrain = nasa_imputed[!is.na(nasa$Hazardous), ]\ntest = nasa_imputed[is.na(nasa$Hazardous), ]\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(Hazardous~., data = train, classification = TRUE,\n            probability = TRUE)\npreds = predict(rf, data = test)$predictions[,2]\nhead(preds)\n#> [1] 0.6454968 0.7883127 0.0050000 0.8353611 0.1265778 0.1251611\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"},{"path":"datasets.html","id":"flower","chapter":"9 Data sets","heading":"9.5 Flower","text":"collection 4000 flower images 5 plant species. data set kaggle downsampled images \\(320*240\\) \\(80*80\\) pixels.\ncan download data set .Notes:Check convolutional neural network notebooks kaggle (often written Python can still copy architectures), e.g. one.Last year’s winners used transfer learning approach (achieved around 70% accuracy), check notebook, see also section transfer learning 6.4.2.Response variable: “Plant species”Load data set:Let’s visualize flower:Build train model:Predictions:Create submission csv:","code":"\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ntrain = EcoData::dataset_flower()$train/255\ntest = EcoData::dataset_flower()$test/255\nlabels = EcoData::dataset_flower()$labels\ntrain[100,,,] %>%\n  image_to_array() %>%\n  as.raster() %>%\n  plot()\nmodel = keras_model_sequential()\nmodel %>% \n  layer_conv_2d(filters = 4L, kernel_size = 2L,\n                input_shape = list(80L, 80L, 3L)) %>% \n  layer_max_pooling_2d() %>% \n  layer_flatten() %>% \n  layer_dense(units = 5L, activation = \"softmax\")\n\n### Model fitting ###\n\nepochs = 50L\nbatch_size = 25L\nsteps = floor(dim(train)[1]/batch_size)\ngenerator = keras::flow_images_from_data(x = train,\n                                         y = keras::k_one_hot(labels, 5L),\n                                         batch_size = batch_size)\n\noptim = optimizer_adamax(learning_rate = 0.01)\nepoch_losses = c()\n\nfor(e in 1:epochs){\n  epoch_loss = c()\n  for(s in 1:steps){\n    batch = reticulate::iter_next(generator)\n    with(tf$GradientTape() %as% tape,\n      {\n        pred = model(batch[[1]])\n        loss = keras::loss_categorical_crossentropy(batch[[2]], pred)\n        loss = tf$reduce_mean(loss)\n      }\n    )\n    gradients = tape$gradient(target = loss, sources = model$weights)\n    optim$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n    epoch_loss = c(epoch_loss, loss$numpy())\n  }\n  epoch_losses = c(epoch_losses, epoch_loss)\n  cat(\"Epoch: \", e, \" Loss: \", mean(epoch_losses), \" \\n\")\n}\n# Prediction on training data:\npred = apply(model %>% predict(train), 1, which.max)\nMetrics::accuracy(pred - 1L, labels)\ntable(pred)\n\n# Prediction for the submission server:\npred = model %>% predict(test) %>% apply(1, which.max) - 1L\ntable(pred)\nwrite.csv(data.frame(y = pred), file = \"cnn.csv\")"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
