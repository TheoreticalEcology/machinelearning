[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Deep Learning with R",
    "section": "",
    "text": "Preface\nMachine learning (ML) is the process of building a predictive model that makes predictions about new data based on observations (training data). The goal of this course is to enable you to build a robust ML model, one that generalizes well to new observations and doesn’t “overfit” your training data. To do this, you will need to master a number of skills, in particular\nIn recent years, a new field within ML called Deep Learning has emerged and attracted a lot of attention. The reason for this is that DL incorporates many different but very flexible architectures that allow to natively model different types of data, e.g. Convolutional Neural Networks for images or Recurrent Neural Networks for time series. However, exploiting the flexibility of DL requires a deeper, more fundamental understanding of the frameworks in which they are implemented. To this end, the course will also cover common DL frameworks such as TensorFlow (and PyTorch) and:"
  },
  {
    "objectID": "index.html#before-the-course",
    "href": "index.html#before-the-course",
    "title": "Machine Learning and Deep Learning with R",
    "section": "Before the course",
    "text": "Before the course\n\nPlease read the following two reviews about Machine Learning in General (Pichler and Hartig 2023) and Deep Learning (Borowiec et al. 2022)\nPlease install all dependencies before the course because it will take some time, see Chapter 1 for installation instructions\nThis course assumes advanced knowledge of the R programming language. If you want to refresh your knowledge about R, you can find a crashcourse in R in the book of the advanced statistic course: R-Crash-Course\n\nAuthors:\nMaximilian Pichler: @_Max_Pichler\nFlorian Hartig: @florianhartig\nContributors:\nJohannes Oberpriller, Matthias Meier\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\n\n\n\n\nBorowiec, Marek L, Rebecca B Dikow, Paul B Frandsen, Alexander McKeeken, Gabriele Valentini, and Alexander E White. 2022. “Deep Learning as a Tool for Ecology and Evolution.” Methods in Ecology and Evolution 13 (8): 1640–60.\n\n\nPichler, Maximilian, and Florian Hartig. 2023. “Machine Learning and Deep Learning—a Review for Ecologists.” Methods in Ecology and Evolution 14 (4): 994–1016."
  },
  {
    "objectID": "A1-GettingStarted.html#r-system",
    "href": "A1-GettingStarted.html#r-system",
    "title": "1  Getting Started",
    "section": "1.1 R System",
    "text": "1.1 R System\nMake sure you have a recent version of R (>=4.0, ideally >=4.2) and RStudio on your computers. For Mac users, if you have already a M1/M2 Mac, please install the R-ARM version (see here (not the x86_64 version))"
  },
  {
    "objectID": "A1-GettingStarted.html#tensorflow-and-keras",
    "href": "A1-GettingStarted.html#tensorflow-and-keras",
    "title": "1  Getting Started",
    "section": "1.2 TensorFlow and Keras",
    "text": "1.2 TensorFlow and Keras\nIf you want to run the code on your own computers, you need to install TensorFlow / Keras for R. For this, the following should work for most people:\n\nWindowsLinuxMacOS-M1/M2MacOS-Intel\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\nIf you have already installed anaconda/miniconda, please uninstall it first!\n\nDownload the ARM miniconda version from here and install it\nOpen the terminal (e.g. by pressing cmd+whitespace and search for ‘terminal’)\nRun in the terminal (not in R!):\n\nconda env create -n \"r-reticulate\" python=3.10 \nconda install -c apple tensorflow-deps \npython -m pip install tensorflow-macos \npython -m pip install scipy\n\nOpen R, install the package ‘reticulate’ by running install.packages('reticulate') and run:\n\n\nreticulate::use_condaenv(\"r-reticulate\")\n\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\n\nThis should work on most computers, in particular if all software is recent. Sometimes, however, things don’t work well, especially the python distribution often makes problems. If the installation does not work for you, we can look at it together. Also, we will provide some virtual machines in case your computers / laptops are too old or you don’t manage to install TensorFlow."
  },
  {
    "objectID": "A1-GettingStarted.html#torch-for-r",
    "href": "A1-GettingStarted.html#torch-for-r",
    "title": "1  Getting Started",
    "section": "1.3 Torch for R",
    "text": "1.3 Torch for R\nWe may also use Torch for R. This is an R frontend for the popular PyTorch framework. To install Torch, type in R:\n\ninstall.packages(\"torch\")\nlibrary(torch)\ntorch::install_torch()"
  },
  {
    "objectID": "A1-GettingStarted.html#ecodata",
    "href": "A1-GettingStarted.html#ecodata",
    "title": "1  Getting Started",
    "section": "1.4 EcoData",
    "text": "1.4 EcoData\nWe use data sets from the EcoData package. To install the package, run:\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = TRUE, build_vignettes = TRUE)\n\nThe default installation will install a number of packages that are useful for statistics. Especially in Linux, this may take some time to install. If you are in a hurry and only want the data, you can also run\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = FALSE, build_vignettes = FALSE)"
  },
  {
    "objectID": "A1-GettingStarted.html#further-used-libraries",
    "href": "A1-GettingStarted.html#further-used-libraries",
    "title": "1  Getting Started",
    "section": "1.5 Further Used Libraries",
    "text": "1.5 Further Used Libraries\nWe will make huge use of different libraries. So take a coffee or two (that will take a while…) and install the following libraries. Please do this in the given order unless you know what you’re doing, because there are some dependencies between the packages.\n\ninstall.packages(\"abind\")\ninstall.packages(\"animation\")\ninstall.packages(\"ape\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"Rgraphviz\", \"graph\", \"RBGL\"))\ninstall.packages(\"coro\")\ninstall.packages(\"dbscan\")\ninstall.packages(\"dendextend\")\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"e1071\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"fields\")\ninstall.packages(\"forcats\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"gym\")\ninstall.packages(\"kknn\")\ninstall.packages(\"knitr\")\ninstall.packages(\"iml\")\ninstall.packages(\"lavaan\")\ninstall.packages(\"lmtest\")\ninstall.packages(\"magick\")\ninstall.packages(\"mclust\")\ninstall.packages(\"Metrics\")\ninstall.packages(\"microbenchmark\")\ninstall.packages(\"missRanger\")\ninstall.packages(\"mlbench\")\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\ninstall.packages(\"mlr3measures\")\ninstall.packages(\"mlr3pipelines\")\ninstall.packages(\"mlr3tuning\")\ninstall.packages(\"paradox\")\ninstall.packages(\"partykit\")\ninstall.packages(\"pcalg\")\ninstall.packages(\"piecewiseSEM\")\ninstall.packages(\"purrr\")\ninstall.packages(\"randomForest\")\ninstall.packages(\"ranger\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"scales\")\ninstall.packages(\"semPlot\")\ninstall.packages(\"stringr\")\n#install.packages(\"tfprobability\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"xgboost\")\n\ndevtools::install_github(\"andrie/deepviz\", dependencies = TRUE,\n                         upgrade = \"always\")\ndevtools::install_github('skinner927/reprtree')\ndevtools::install_version(\"lavaanPlot\", version = \"0.6.0\")\n\n# reticulate::conda_install(\"r-reticulate\", packages = \"scipy\", pip = TRUE)"
  },
  {
    "objectID": "A1-GettingStarted.html#linuxunix-systems-have-sometimes-to-fulfill-some-further-dependencies",
    "href": "A1-GettingStarted.html#linuxunix-systems-have-sometimes-to-fulfill-some-further-dependencies",
    "title": "1  Getting Started",
    "section": "1.6 Linux/UNIX systems have sometimes to fulfill some further dependencies",
    "text": "1.6 Linux/UNIX systems have sometimes to fulfill some further dependencies\nDebian based systems\nFor Debian based systems, we need:\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev"
  },
  {
    "objectID": "A1-GettingStarted.html#reminders-about-basic-operations-in-r",
    "href": "A1-GettingStarted.html#reminders-about-basic-operations-in-r",
    "title": "1  Getting Started",
    "section": "1.7 Reminders About Basic Operations in R",
    "text": "1.7 Reminders About Basic Operations in R\nBasic and advanced knowledge of R is required to successfully participate in this course. If you would like to refresh your knowledge of R, you can review the chapter ‘Reminder: R Basics’ from the advanced statistic course."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#overview",
    "href": "A2-MachineLearningTasks.html#overview",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nThere are three types of machine learning tasks:\n\nSupervised learning\nUnsupervised learning\nReinforcement learning\n\nIn supervised learning, you train algorithms using labeled data, what means that you already know the correct answer for a part of the data (the so called training data).\nUnsupervised learning in contrast is a technique, where one does not need to monitor the model or apply labels. Instead, you allow the model to work on its own to discover information.\nReinforcement learning is a technique that emulates a game-like situation. The algorithm finds a solution by trial and error and gets either rewards or penalties for every action. As in games, the goal is to maximize the rewards. We will talk more about this technique on the last day of the course.\nFor the moment, we will focus on the first two tasks, supervised and unsupervised learning. To do so, we will begin with a small example. But before you start with the code, here is a video to prepare you for what we will do in the class:\n\n\n\n2.1.1 Questions\n\nIn ML, predictors (or the explaining variables) are often called features: TRUEFALSE\nIn supervised learning the response (y) and the predictors (x) are known: TRUEFALSE\nIn unsupervised learning, only the predictors are known: TRUEFALSE\nIn reinforcement learning an agent (ML model) is trained by interacting with an environment: TRUEFALSE\nHave a look at the two textbooks on ML (Elements of statistical learning and introduction to statistical learning) in our further readings at the end of the GRIPS course - which of the following statements is true?\n\n Both books can be downloaded for free. Higher model complexity is always better for predicting."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#unsupervised-learning",
    "href": "A2-MachineLearningTasks.html#unsupervised-learning",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.2 Unsupervised Learning",
    "text": "2.2 Unsupervised Learning\nIn unsupervised learning, we want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider the iris data set. Here, we have 150 observations of 4 floral traits:\n\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4])\nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits,\n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\n\n\n\n\nTrait distributions of iris dataset\n\n\n\n\nThe observations are from 3 species and indeed those species tend to have different traits, meaning that the observations form 3 clusters.\n\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])\n\n\n\n\nScatterplots for trait-trait combinations.\n\n\n\n\nHowever, imagine we don’t know what species are, what is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does.\n\n2.2.1 Hierarchical Clustering\nA cluster refers to a collection of data points aggregated together because of certain similarities.\nIn hierarchical clustering, a hierarchy (tree) between data points is built.\n\nAgglomerative: Start with each data point in their own cluster, merge them up hierarchically.\nDivisive: Start with all data points in one cluster, and split hierarchically.\n\nMerges / splits are done according to linkage criterion, which measures distance between (potential) clusters. Cut the tree at a certain height to get clusters.\nHere an example\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nd = dist(traits)\nhc = hclust(d, method = \"complete\")\n\nplot(hc, main=\"\")\nrect.hclust(hc, k = 3)  # Draw rectangles around the branches.\n\n\n\n\nResults of hierarchical clustering. Red rectangle is drawn around the corresponding clusters.\n\n\n\n\nSame plot, but with colors for true species identity\n\nlibrary(ape)\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes.\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\n\nCalculate confusion matrix. Note we are switching labels here so that it fits to the species.\n\ntmp = hcRes3\ntmp[hcRes3 == 2] = 3\ntmp[hcRes3 == 3] = 2\nhcRes3 = tmp\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n27\n1\n\n\n0\n23\n49\n\n\n\n\n\nNote that results might change if you choose a different agglomeration method, distance metric or scale of your variables. Compare, e.g. to this example:\n\nhc = hclust(d, method = \"ward.D2\")\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes. Different agglomeration method\n\n\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\nWhich method is best? firstsecond\n\nlibrary(dendextend)\n\n\nset.seed(123)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n             \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\nout = dendlist()   # Create a dendlist object from several dendrograms.\nfor(method in methods){\n  res = hclust(d, method = method)\n  out = dendlist(out, as.dendrogram(res))\n}\nnames(out) = methods\nprint(out)\n\n$ward.D\n'dendrogram' with 2 branches and 150 members total, at height 199.6205 \n\n$single\n'dendrogram' with 2 branches and 150 members total, at height 1.640122 \n\n$complete\n'dendrogram' with 2 branches and 150 members total, at height 7.085196 \n\n$average\n'dendrogram' with 2 branches and 150 members total, at height 4.062683 \n\n$mcquitty\n'dendrogram' with 2 branches and 150 members total, at height 4.497283 \n\n$median\n'dendrogram' with 2 branches and 150 members total, at height 2.82744 \n\n$centroid\n'dendrogram' with 2 branches and 150 members total, at height 2.994307 \n\n$ward.D2\n'dendrogram' with 2 branches and 150 members total, at height 32.44761 \n\nattr(,\"class\")\n[1] \"dendlist\"\n\nget_ordered_3_clusters = function(dend){\n  # order.dendrogram function returns the order (index)\n  # or the \"label\" attribute for the leaves.\n  # cutree: Cut the tree (dendrogram) into groups of data.\n  cutree(dend, k = 3)[order.dendrogram(dend)]\n}\ndend_3_clusters = lapply(out, get_ordered_3_clusters)\n\n# Calculate Fowlkes-Mallows Index (determine the similarity between clusterings)\ncompare_clusters_to_iris = function(clus){\n  FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)\n}\n\nclusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\ndotchart(sort(clusters_performance), xlim = c(0.3, 1),\n         xlab = \"Fowlkes-Mallows index\",\n         main = \"Performance of linkage methods\n         in detecting the 3 species \\n in our example\",\n         pch = 19)\n\n\n\n\nWe might conclude that ward.D2 works best here. However, as we will learn later, optimizing the method without a hold-out for testing implies that our model may be overfitting. We should check this using cross-validation.\n\n\n2.2.2 K-means Clustering\nAnother example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms.\nTo start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the assumed or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere. Afterwards each data point is assigned to the respective cluster that raises the overall in-cluster sum of squares (variance) related to the distance to the centroid least of all. After the algorithm has placed all data points into a cluster the centroids get updated. By iterating this procedure until the assignment doesn’t change any longer, the algorithm can find the (locally) optimal centroids and the data points belonging to this cluster. Note that results might differ according to the initial positions of the centroids. Thus several (locally) optimal solutions might be found.\nThe “k” in K-means refers to the number of clusters and the ‘means’ refers to averaging the data-points to find the centroids.\nA typical pipeline for using k-means clustering looks the same as for other algorithms. After having visualized the data, we fit a model, visualize the results and have a look at the performance by use of the confusion matrix. By setting a fixed seed, we can ensure that results are reproducible.\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n\nK-means clustering with 3 clusters of sizes 50, 62, 38\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.006000    3.428000     1.462000    0.246000\n2     5.901613    2.748387     4.393548    1.433871\n3     6.850000    3.073684     5.742105    2.071053\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 15.15100 39.82097 23.87947\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nVisualizing the results. Color codes true species identity, symbol shows cluster result.\n\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\n\n\n\n\nWe see that there are are some discrepancies. Confusion matrix:\n\ntable(iris$Species, kc$cluster)\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36\n\n\nIf you want to animate the clustering process, you could run\n\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\n\nElbow technique to determine the probably best suited number of clusters:\n\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))\n\n\n\n\nOften, one is interested in sparse models. Furthermore, higher k than necessary tends to overfitting. At the kink in the picture, the sum of squares dropped enough and k is still low enough. But keep in mind, this is only a rule of thumb and might be wrong in some special cases.\n\n\n2.2.3 Density-based Clustering\nDetermine the affinity of a data point according to the affinity of its k nearest neighbors. This is a very general description as there are many ways to do so.\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nset.seed(123)\n\nkNNdistplot(traits, k = 4)   # Calculate and plot k-nearest-neighbor distances.\nabline(h = 0.4, lty = 2)\n\n\n\ndc = dbscan(traits, eps = 0.4, minPts = 6)\nprint(dc)\n\nDBSCAN clustering for 150 objects.\nParameters: eps = 0.4, minPts = 6\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 4 cluster(s) and 32 noise points.\n\n 0  1  2  3  4 \n32 46 36 14 22 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\n\n\nlibrary(factoextra)\n\n\nfviz_cluster(dc, traits, geom = \"point\", ggtheme = theme_light())\n\n\n\n\n\n\n2.2.4 Model-based Clustering\nThe last class of methods for unsupervised clustering are so-called model-based clustering methods.\n\nlibrary(mclust)\n\nPackage 'mclust' version 6.0.0\nType 'citation(\"mclust\")' for citing this R package in publications.\n\n\n\nmb = Mclust(traits)\n\nMclust automatically compares a number of candidate models (clusters, shape) according to BIC (The BIC is a criterion for classifying algorithms depending their prediction quality and their usage of parameters). We can look at the selected model via:\n\nmb$G # Two clusters.\n\n[1] 2\n\nmb$modelName # > Ellipsoidal, equal shape.\n\n[1] \"VEV\"\n\n\nWe see that the algorithm prefers having 2 clusters. For better comparability to the other 2 methods, we will override this by setting:\n\nmb3 = Mclust(traits, 3)\n\nResult in terms of the predicted densities for 3 clusters\n\nplot(mb3, \"density\")\n\n\n\n\nPredicted clusters:\n\nplot(mb3, what=c(\"classification\"), add = T)\n\n\n\n\nConfusion matrix:\n\ntable(iris$Species, mb3$classification)\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\n\n\n2.2.5 Ordination\nOrdination is used in explorative analysis and compared to clustering, similar objects are ordered together. So there is a relationship between clustering and ordination. Here a PCA ordination on on the iris data set.\n\npcTraits = prcomp(traits, center = TRUE, scale. = TRUE)\nbiplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25))\n\n\n\n\nYou can cluster the results of this ordination, ordinate before clustering, or superimpose one on the other.\n\n\n2.2.6 Exercise\n\n\n\n\n\n\nTask\n\n\n\nGo through the 4(5) algorithms above, and check if they are sensitive (i.e. if results change) if you scale the input features (= predictors), instead of using the raw data. Discuss in your group: Which is more appropriate for this analysis and/or in general: Scaling or not scaling?\n\n\nClick here to see the solution for hierarchical clustering\n\n\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)\n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n\n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n\n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\nIt seems that scaling is harmful for hierarchical clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for K-means\n\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n\n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n\n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n\n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\nIt seems that scaling is harmful for K-means clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for density-based clustering\n\n\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\nLevels: 1 2 3\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n\n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n\n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     3 47  0  0  0\n      2     5  0 38  3  4\n      3    17  0  0 33  0\n\nCorrect classified points:  118  /  150\nSum of noise points:  25 \n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     9 41  0  0  0\n      2    14  0 36  0  0\n      3    36  0  1  4  9\n\nCorrect classified points:  81  /  150\nSum of noise points:  59 \n\n\nIt seems that scaling is harmful for density based clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for model-based clustering\n\n\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  mb3 = Mclust(traits, 3)\n\n  tbl = table(iris$Species, mb3$classification)\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n\nFor model based clustering, scaling does not matter.\n\n\n\nClick here to see the solution for ordination\n\n\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n\n\n\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n\n\n\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")\n\n\n\n\nFor PCA ordination, scaling matters. Because we are interested in directions of maximal variance, all parameters should be scaled, or the one with the highest values might dominate all others."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#supervised-learning",
    "href": "A2-MachineLearningTasks.html#supervised-learning",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.3 Supervised Learning",
    "text": "2.3 Supervised Learning\nThe two most prominent branches of supervised learning are regression and classification. Fundamentally, classification is about predicting a label and regression is about predicting a quantity. The following video explains that in more depth:\n\n\n\n2.3.1 Regression\nThe random forest (RF) algorithm is possibly the most widely used machine learning algorithm and can be used for regression and classification. We will talk more about the algorithm later.\nFor the moment, we want to go through a typical workflow for a supervised regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. We will again use the iris data set that we used before. The goal is now to predict Sepal.Length based on the information about the other variables (including species).\nFitting the model:\n\nlibrary(randomForest)\nset.seed(123)\n\n\nm1 = randomForest(Sepal.Length ~ ., data = iris)   # ~.: Against all others.\n# str(m1)\n# m1$type\n# predict(m1)\nprint(m1)\n\n\nCall:\n randomForest(formula = Sepal.Length ~ ., data = iris) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1364625\n                    % Var explained: 79.97\n\n\nVisualization of the results:\n\noldpar = par(mfrow = c(1, 2))\nplot(predict(m1), iris$Sepal.Length, xlab = \"Predicted\", ylab = \"Observed\")\nabline(0, 1)\nvarImpPlot(m1)\n\n\n\npar(oldpar)\n\nTo understand the structure of a random forest in more detail, we can use a package from GitHub.\n\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\nHere, one of the regression trees is shown.\n\n\n2.3.2 Classification\nWith the random forest, we can also do classification. The steps are the same as for regression tasks, but we can additionally see how well it performed by looking at the confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represents the instances in the actual class. Thus the diagonals are the correctly predicted classes and the off-diagonal elements are the falsely classified elements.\nFitting the model:\n\nset.seed(123)\nlibrary(randomForest)\nm1 = randomForest(Species ~ ., data = iris)\nprint(m1)\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n\nvarImpPlot(m1)\n\n\n\n\nVisualizing one of the fitted models:\n\noldpar = par(mfrow = c(1, 2))\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\nVisualizing results ecologically:\n\npar(mfrow = c(1, 2))\nplot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = \"Observed\")\nplot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = \"Predicted\")\n\n\n\n\n\npar(oldpar)   #Reset par.\n\nConfusion matrix:\n\ntable(predict(m1), iris$Species)\n\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\nsetosa\n50\n0\n0\n\n\nversicolor\n0\n47\n4\n\n\nvirginica\n0\n3\n46\n\n\n\n\n\n\n\n2.3.3 Questions\nUsing a random forest on the iris dataset, which parameter would be more important (remember there is a function to check this) to predict Petal.Width?\n\n Species. Sepal.Width."
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#understanding-the-bias-variance-trade-off",
    "href": "A3-BiasVarianceTradeOff.html#understanding-the-bias-variance-trade-off",
    "title": "3  Bias-variance trade-off",
    "section": "3.1 Understanding the bias-variance trade-off",
    "text": "3.1 Understanding the bias-variance trade-off\n\n\nWhich of the following statements about the bias-variance trade-off is correct? (see figure above)\n\n The goal of considering the bias-variance trade-off is to realize that increasing complexity typically leads to more flexibility (allowing you to reduce bias) but at the cost of uncertainty (variance) in the estimated parameters. The goal of considering the bias-variance trade-off is to get the bias of the model as small as possible."
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#validating-of-the-ml-models",
    "href": "A3-BiasVarianceTradeOff.html#validating-of-the-ml-models",
    "title": "3  Bias-variance trade-off",
    "section": "3.2 Validating of the ML models",
    "text": "3.2 Validating of the ML models\n\n3.2.1 Error metrics\n\n\n3.2.2 Cross-validation"
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "href": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "title": "3  Bias-variance trade-off",
    "section": "3.3 Optimizing the bias-variance trade-off",
    "text": "3.3 Optimizing the bias-variance trade-off\n\n3.3.1 Feature selection\n\n\n3.3.2 Hyper-parameters\n\n\n3.3.3 Regularization\nRegularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a machine learning model. The most important distinction is between shrinkage estimators and estimators based on model averaging.\nShrikage estimators are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are “shrunk” to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penalty is proportional to the sum of absolute deviations (\\(L1\\) penalty), and the Tikhonov regularization aka Ridge regression, where the penalty is proportional to the sum of squared distances from the reference (\\(L2\\) penalty). Thus, the loss function that we optimize is given by\n\\[\nloss = fit - \\lambda \\cdot d\n\\]\nwhere fit refers to the standard loss function, \\(\\lambda\\) is the strength of the regularization, and \\(d\\) is the chosen metric, e.g. \\(L1\\) or\\(L2\\):\n\\[\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n\\] \\[\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n\\]\n\\(\\lambda\\) and possibly d are typically optimized under cross-validation. \\(L1\\) and \\(L2\\) can be also combined what is then called elastic net (see Zou and Hastie (2005)).\nModel averaging refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it is often more efficient having many simpler models and average them, than one “super model”. The reasons are complicated, and explained in more detail in Dormann et al. (2018).\nA particular important application of averaging is boosting, where the idea is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap (use random sampling with replacement ) the data, and average the bootstrapped predictions.\nTo see how these techniques work in practice, let’s first focus on LASSO and Ridge regularization for weights in neural networks. We can imagine that the LASSO and Ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunken) to zero.\nLASSO \\(\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)\\) and Ridge \\(\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)\\) have slightly different properties. They are best understood if we express those as the effective prior preference they create on the parameters:\n\n\n\n\n\nAs you can see, the LASSO creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the Ridge. For this reason, LASSO is often more interpreted as a model selection method.\nThe Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.\n\n\n\n\nDormann, Carsten F, Justin M Calabrese, Gurutzeta Guillera-Arroita, Eleni Matechou, Volker Bahn, Kamil Bartoń, Colin M Beale, et al. 2018. “Model Averaging in Ecology: A Review of Bayesian, Information-Theoretic, and Tactical Approaches for Predictive Inference.” Ecological Monographs 88 (4): 485–504.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20."
  },
  {
    "objectID": "A4-MLpipeline.html#the-standard-machine-learning-pipeline-at-the-example-of-the-titanic-data-set",
    "href": "A4-MLpipeline.html#the-standard-machine-learning-pipeline-at-the-example-of-the-titanic-data-set",
    "title": "4  Machine learning pipeline",
    "section": "4.1 The Standard Machine Learning Pipeline at the example of the Titanic Data set",
    "text": "4.1 The Standard Machine Learning Pipeline at the example of the Titanic Data set\nBefore we specialize on any tuning, it is important to understand that machine learning always consists of a pipeline of actions.\nThe typical machine learning workflow consist of:\n\nData cleaning and exploration (EDA = explorative data analysis) for example with tidyverse.\nPreprocessing and feature selection.\nSplitting data set into training and test set for evaluation.\nModel fitting.\nModel evaluation.\nNew predictions.\n\nHere is an (optional) video that explains the entire pipeline from a slightly different perspective:\n\n\nIn the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham. A video that explains the basics can be found here :\n\n\nAnother good reference is “R for data science” by Hadley Wickham: .\nFor this lecture you need the Titanic data set provided by us. You can find it in GRIPS (datasets.RData in the data set and submission section) or at http://rhsbio6.uni-regensburg.de:8500.\nWe have split the data set already into training and test/prediction data sets (the test/prediction split has one column less than the train split, as the result is not known a priori).\n\n4.1.1 Data Cleaning\nLoad necessary libraries:\n\nlibrary(tidyverse)\n\nLoad data set:\n\nlibrary(EcoData)\ndata(titanic_ml)\ndata = titanic_ml\n\nStandard summaries:\n\nstr(data)\n\n'data.frame':   1309 obs. of  14 variables:\n $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\nsummary(data)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n\nhead(data)\n\n     pclass survived                         name    sex  age sibsp parch\n561       2        1        Sinkkonen, Miss. Anna female 30.0     0     0\n321       1        1            Woolner, Mr. Hugh   male   NA     0     0\n1177      3        0     Sage, Mr. Douglas Bullen   male   NA     8     2\n1098      3        0  Palsson, Master. Paul Folke   male  6.0     3     1\n1252      3        0   Tomlin, Mr. Ernest Portage   male 30.5     0     0\n1170      3        0 Saether, Mr. Simon Sivertsen   male 38.5     0     0\n                 ticket   fare cabin embarked boat body\n561              250648 13.000              S   10   NA\n321               19947 35.500   C52        S    D   NA\n1177           CA. 2343 69.550              S        NA\n1098             349909 21.075              S        NA\n1252             364499  8.050              S        50\n1170 SOTON/O.Q. 3101262  7.250              S        32\n                    home.dest\n561  Finland / Washington, DC\n321           London, England\n1177                         \n1098                         \n1252                         \n1170                         \n\n\nThe name variable consists of 1309 unique factors (there are 1309 observations…):\n\nlength(unique(data$name))\n\n[1] 1307\n\n\nHowever, there is a title in each name. Let’s extract the titles:\n\nWe will extract all names and split each name after each comma “,”.\nWe will split the second split of the name after a point “.” and extract the titles.\n\n\nfirst_split = sapply(data$name,\n                     function(x) stringr::str_split(x, pattern = \",\")[[1]][2])\ntitles = sapply(first_split,\n                function(x) strsplit(x, \".\",fixed = TRUE)[[1]][1])\n\nWe get 18 unique titles:\n\ntable(titles)\n\ntitles\n         Capt           Col           Don          Dona            Dr \n            1             4             1             1             8 \n     Jonkheer          Lady         Major        Master          Miss \n            1             1             2            61           260 \n         Mlle           Mme            Mr           Mrs            Ms \n            2             1           757           197             2 \n          Rev           Sir  the Countess \n            8             1             1 \n\n\nA few titles have a very low occurrence rate:\n\ntitles = stringr::str_trim((titles))\ntitles %>%\n fct_count()\n\n# A tibble: 18 × 2\n   f                n\n   <fct>        <int>\n 1 Capt             1\n 2 Col              4\n 3 Don              1\n 4 Dona             1\n 5 Dr               8\n 6 Jonkheer         1\n 7 Lady             1\n 8 Major            2\n 9 Master          61\n10 Miss           260\n11 Mlle             2\n12 Mme              1\n13 Mr             757\n14 Mrs            197\n15 Ms               2\n16 Rev              8\n17 Sir              1\n18 the Countess     1\n\n\nWe will combine titles with low occurrences into one title, which we can easily do with the forcats package.\n\ntitles2 =\n  forcats::fct_collapse(titles,\n                        officer = c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\"),\n                        royal = c(\"Jonkheer\", \"Don\", \"Sir\",\n                                  \"the Countess\", \"Dona\", \"Lady\"),\n                        miss = c(\"Miss\", \"Mlle\"),\n                        mrs = c(\"Mrs\", \"Mme\", \"Ms\")\n                        )\n\nWe can count titles again to see the new number of titles:\n\ntitles2 %>%  \n   fct_count()\n\n# A tibble: 6 × 2\n  f           n\n  <fct>   <int>\n1 officer    23\n2 royal       6\n3 Master     61\n4 miss      262\n5 mrs       200\n6 Mr        757\n\n\nAdd new title variable to data set:\n\ndata =\n  data %>%\n    mutate(title = titles2)\n\nAs a second example, we will explore and clean the numeric “age” variable.\nExplore the variable:\n\nsummary(data)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest       title    \n Min.   :  1.0                       :564   officer: 23  \n 1st Qu.: 72.0   New York, NY        : 64   royal  :  6  \n Median :155.0   London              : 14   Master : 61  \n Mean   :160.8   Montreal, PQ        : 10   miss   :262  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9   mrs    :200  \n Max.   :328.0   Paris, France       :  9   Mr     :757  \n NA's   :1188    (Other)             :639                \n\nsum(is.na(data$age)) / nrow(data)\n\n[1] 0.2009167\n\n\n20% NAs! Either we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups. In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age….)) will be done within the specified groups.\n\ndata =\n  data %>%\n    group_by(sex, pclass, title) %>%\n    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %>%\n    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %>%\n    ungroup()\n\n\n\n4.1.2 Preprocessing and Feature Selection\nLater (tomorrow), we want to use Keras in our example, but it cannot handle factors and requires the data to be scaled.\nNormally, one would do this for all predictors, but as we only show the pipeline here, we have sub-selected a bunch of predictors and do this only for them. We first scale the numeric predictors and change the factors with only two groups/levels into integers (this can be handled by Keras).\n\ndata_sub =\n  data %>%\n    select(survived, sex, age2, fare2, title, pclass) %>%\n    mutate(age2 = scales::rescale(age2, c(0, 1)),\n           fare2 = scales::rescale(fare2, c(0, 1))) %>%\n    mutate(sex = as.integer(sex) - 1L,\n           title = as.integer(title) - 1L, pclass = as.integer(pclass - 1L))\n\nFactors with more than two levels should be one hot encoded (Make columns for every different factor level and write 1 in the respective column for every taken feature value and 0 else. For example: \\(\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}\\)):\n\none_title = model.matrix(~0+as.factor(title), data = data_sub)\ncolnames(one_title) = levels(data$title)\n\none_sex = model.matrix(~0+as.factor(sex), data = data_sub)\ncolnames(one_sex) = levels(data$sex)\n\none_pclass = model.matrix(~0+as.factor(pclass), data = data_sub)\ncolnames(one_pclass) = paste0(\"pclass\", 1:length(unique(data$pclass)))\n\nAnd we have to add the dummy encoded variables to the data set:\n\ndata_sub = cbind(data.frame(survived= data_sub$survived),\n                 one_title, one_sex, age = data_sub$age2,\n                 fare = data_sub$fare2, one_pclass)\nhead(data_sub)\n\n  survived officer royal Master miss mrs Mr female male        age       fare\n1        1       0     0      0    1   0  0      1    0 0.37369494 0.02537431\n2        1       0     0      0    0   0  1      0    1 0.51774510 0.06929139\n3        0       0     0      0    0   0  1      0    1 0.32359053 0.13575256\n4        0       0     0      1    0   0  0      0    1 0.07306851 0.04113566\n5        0       0     0      0    0   0  1      0    1 0.37995799 0.01571255\n6        0       0     0      0    0   0  1      0    1 0.48016680 0.01415106\n  pclass1 pclass2 pclass3\n1       0       1       0\n2       1       0       0\n3       0       0       1\n4       0       0       1\n5       0       0       1\n6       0       0       1\n\n\n\n\n4.1.3 Split Data\nThe splitting consists of two splits:\n\nAn outer split (the original split, remember we got a training and test split without the response “survived”).\nAn inner split (we will split the training data set further into another training and test split with known response). The inner split is important to assess the model’s performance and potential overfitting.\n\nOuter split:\n\ntrain = data_sub[!is.na(data_sub$survived),]\ntest = data_sub[is.na(data_sub$survived),]\n\nInner split:\n\nindices = sample.int(nrow(train), 0.7 * nrow(train))\nsub_train = train[indices,]\nsub_test = train[-indices,]\n\nWhat is the difference between the two splits? (Tip: have a look at the variable survived.)\n\n\n4.1.4 Training\nIn the next step we will fit a Random Forest on the training data of the inner split:\n\nlibrary(ranger)\nmodel = ranger(survived~., data = sub_train)\n\n\n\n4.1.5 Evaluation\nWe will predict the variable “survived” for the test set of the inner split and calculate the accuracy:\n\npred =\n  model %>%\n    predict(data = as.matrix(sub_test[,-1]))\n\npred = pred$predictions\n\npredicted = ifelse(pred < 0.5, 0, 1) \nobserved = sub_test[,1]\n(accuracy = mean(predicted == observed))  \n\n[1] 0.7360406\n\n# Let's calculate the AUC:\nMetrics::auc(observed, pred)\n\n[1] 0.790855\n\n\n\n\n4.1.6 Predictions and Submission\nWhen we are satisfied with the performance of our model in the inner split, we will create predictions for the test data of the outer split. To do so, we take all observations that belong to the outer test split (use the filter function) and remove the survived (NAs) columns:\n\nsubmit = \n  test %>% \n      select(-survived)\n\nWe cannot assess the performance on the test split because the true survival ratio is unknown, however, we can now submit our predictions to the submission server at http://rhsbio7.uni-regensburg.de:8500. To do so, we have to transform our survived probabilities into actual 0/1 predictions (probabilities are not allowed) and create a .csv file:\n\npred = model %>% \n  predict(as.matrix(submit))\n\nFor the submission it is critical to change the predictions into a data.frame, select the second column (the probability to survive), and save it with the write.csv function:\n\nwrite.csv(data.frame(y = pred[,2] ), file = \"Max_1.csv\")\n\nThe file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself.\n\n4.1.6.1 Exercises\n\n\n\n\n\n\nTask: Imrove predictions\n\n\n\nPlay around with the feature engineering and the hyperparameters of the random forest. Try to improve the AUC on the outer split (submission server).\n\n\n\n\n\n4.1.7 Hyperparameter optimization\nHyperparameters (configuration parameters of our ML algorithms that (mostly) control their complexity) are usually tuned (optimized) in an automatic / systematic way. A common procedure, called random search, is to sample random configuration combinations from the set of hyperparameters and test for each combination the prediction error.\nIf we test many different hyperparameter combinations, how do we ensure that a certain hyperparameter is not only good for our training dataset but also good for the new data (our outer split on the submission server)? You may have guessed it already, we need to do another CV within the previous CV to check whether a certain hyperparameter solution generalizes to the whole data.\nThe “double CV” approach is called nested CV. Let’s start with a 3CVx3CV and 10x different mtry values:\n\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 10\ncv_inner = 10\nhyper_mtry = seq(4, 13, by = 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  mtry = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  # inner split\n  for(j in 1:cv_inner) {\n    inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n    train_inner = train_outer[inner_split != j, ]\n    test_inner = train_outer[inner_split == j, ]\n    \n    tuning_results_inner = \n      sapply(1:length(hyper_mtry), function(k) {\n        model = ranger(survived~., data = train_inner, mtry = hyper_mtry[k])\n        return(Metrics::auc(test_inner$survived, predict(model, data = test_inner)$predictions))\n      })\n    best_mtry = hyper_mtry[which.max(tuning_results_inner)]\n  }\n  model = ranger(survived~., data = train_outer, mtry = best_mtry)\n  results[i, 1] = i\n  results[i, 2] = best_mtry\n  results[i, 3] = Metrics::auc(test_outer$survived, predict(model, data = test_outer)$predictions)\n}\n\nprint(results)\n\n   set mtry       AUC\n1    1    4 0.8442460\n2    2    7 0.8065302\n3    3    7 0.7657520\n4    4    6 0.8809756\n5    5   10 0.8964497\n6    6    6 0.7733333\n7    7    6 0.8590244\n8    8    6 0.8933398\n9    9    6 0.8582251\n10  10    4 0.8492647\n\n\nWe found different ‘good’ mtry values. We could now use either the mtry value with the highest AUC, or we could fit now for each value a RF and average the predictions (which will be next task):\n\n\n\n\n\n\nExercise: Which model is better\n\n\n\n\ndata_new = data_sub[is.na(data_sub$survived),]\n\nThe task is to make two predictions: 1. Fit RF on the obs_data with the mtry hyperparameter that has shown the highest AUC 2. Fit for each mtry which we have found a RF and combine (average) the predictions\nSubmit both predictions, which approach has a higher AUC?\n\n\nClick here to see the solution for the single model\n\n\n\n  set mtry       AUC\n5   5   10 0.8964497\n\n\n\n\n\nClick here to see the solution for the single model"
  },
  {
    "objectID": "A4-MLpipeline.html#mlr",
    "href": "A4-MLpipeline.html#mlr",
    "title": "4  Machine learning pipeline",
    "section": "4.2 Bonus - Machine Learning Pipelines with mlr3",
    "text": "4.2 Bonus - Machine Learning Pipelines with mlr3\nAs we have seen today, many of the machine learning algorithms are distributed over several packages but the general machine learning pipeline is very similar for all models: feature engineering, feature selection, hyperparameter tuning and cross-validation.\nThe idea of the mlr3 framework is now to provide a general machine learning interface which you can use to build reproducible and automatic machine learning pipelines. The key features of mlr3 are:\n\nAll common machine learning packages are integrated into mlr3, you can easily switch between different machine learning algorithms.\nA common ‘language’/workflow to specify machine learning pipelines.\nSupport for different cross-validation strategies.\nHyperparameter tuning for all supported machine learning algorithms.\nEnsemble models.\n\nUseful links:\n\nmlr3-book (still in work)\nmlr3 website\nmlr3 cheatsheet\n\n\n4.2.1 mlr3 - The Basic Workflow\nThe mlr3 package actually consists of several packages for different tasks (e.g. mlr3tuning for hyperparameter tuning, mlr3pipelines for data preparation pipes). But let’s start with the basic workflow:\n\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n\n'data.frame':   4687 obs. of  40 variables:\n $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n $ Orbital.Period              : num  419 1040 682 427 503 ...\n $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n\n\nLet’s drop time, name and ID variable and create a classification task:\n\ndata = nasa %>% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\n\nCreate a generic pipeline of data transformation (imputation \\(\\rightarrow\\) scaling \\(\\rightarrow\\) encoding of categorical variables):\n\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n\n                   Hazardous           Absolute.Magnitude \n                        4187                            0 \n               Aphelion.Dist           Asc.Node.Longitude \n                           0                            0 \n                Eccentricity    Epoch.Date.Close.Approach \n                           0                            0 \n            Epoch.Osculation         Est.Dia.in.Feet.max. \n                           0                            0 \n        Est.Dia.in.Feet.min.           Est.Dia.in.KM.max. \n                           0                            0 \n          Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n                           0                            0 \n           Est.Dia.in.M.min.        Est.Dia.in.Miles.max. \n                           0                            0 \n       Est.Dia.in.Miles.min.                  Inclination \n                           0                            0 \n Jupiter.Tisserand.Invariant                 Mean.Anomaly \n                           0                            0 \n                 Mean.Motion               Miles.per.hour \n                           0                            0 \n  Minimum.Orbit.Intersection     Miss.Dist..Astronomical. \n                           0                            0 \n      Miss.Dist..kilometers.            Miss.Dist..lunar. \n                           0                            0 \n           Miss.Dist..miles.                     Orbit.ID \n                           0                            0 \n          Orbit.Uncertainity               Orbital.Period \n                           0                            0 \n              Perihelion.Arg          Perihelion.Distance \n                           0                            0 \n             Perihelion.Time  Relative.Velocity.km.per.hr \n                           0                            0 \nRelative.Velocity.km.per.sec              Semi.Major.Axis \n                           0                            0 \n               Equinox.J2000             Equinox..MISSING \n                           0                            0 \n         Orbiting.Body.Earth       Orbiting.Body..MISSING \n                           0                            0 \n\n\nWe can even visualize the preprocessing graph:\n\npreprocessing$plot()\n\n\n\n\nNow, to test our model (random forest) 10-fold cross-validated, we will do:\n\nSpecify the missing target rows as validation so that they will be ignored.\nSpecify the cross-validation, the learner (the machine learning model we want to use), and the measurement (AUC).\nRun (benchmark) our model.\n\n\nset.seed(123)\n\ntransformed_task$data()\n\n      Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude\n   1:         0        -0.81322649   -0.38042005       -1.140837452\n   2:         0         0.02110348    0.94306517       -1.380254611\n   3:         0         0.68365964    0.10199889        0.044905370\n   4:         1        -0.10159210   -0.38415066        1.606769281\n   5:         1        -0.15067034   -0.29632490        0.151458877\n  ---                                                              \n4683:      <NA>        -0.32244415    0.69173184       -0.171022906\n4684:      <NA>         0.46280759   -0.24203066       -0.009803808\n4685:      <NA>         1.51798962   -0.56422744        1.514551982\n4686:      <NA>         0.16833819    0.14193044       -1.080452287\n4687:      <NA>        -0.05251387   -0.08643345       -0.013006704\n      Eccentricity Epoch.Date.Close.Approach Epoch.Osculation\n   1: -0.315605975                -4.7929881       0.14026773\n   2:  0.744287645                 1.1058704      -0.26325244\n   3: -0.068280074                 0.1591740      -7.76281014\n   4: -0.392030729                -0.7630231       0.24229559\n   5: -0.516897963                -0.6305034       0.24229559\n  ---                                                        \n4683:  1.043608082                 1.3635097       0.24229559\n4684: -0.006429588                 1.3635097       0.05711503\n4685: -1.045386877                 1.3635097       0.24229559\n4686:  0.017146757                 1.3635097       0.24229559\n4687: -0.579210554                 1.3635097       0.24229559\n      Est.Dia.in.Feet.max. Est.Dia.in.Feet.min. Est.Dia.in.KM.max.\n   1:          0.271417899          0.313407647        0.300713440\n   2:          0.032130074         -0.029173486       -0.020055639\n   3:         -0.012841645         -0.093558135       -0.080340934\n   4:          0.048493723         -0.005746146        0.001880088\n   5:          0.056169717          0.005243343        0.012169879\n  ---                                                             \n4683:          0.089353662          0.052751793        0.056653478\n4684:         -0.003481174         -0.080157032       -0.067793075\n4685:         -0.027260163         -0.114200690       -0.099669182\n4686:          0.016872584         -0.051017172       -0.040508543\n4687:          0.041493133         -0.015768679       -0.007504312\n      Est.Dia.in.KM.min. Est.Dia.in.M.max. Est.Dia.in.M.min.\n   1:        0.256568684       0.271095311       0.291624502\n   2:        0.057560696       0.031844946     -12.143577263\n   3:        0.020159164      -0.013119734      -0.060269734\n   4:        0.071169817       0.048206033       0.015659335\n   5:        0.077553695       0.055880826       0.025161701\n  ---                                                       \n4683:        0.105151714       0.089059576       0.066241198\n4684:        0.027943967      -0.003760728      -0.048682099\n4685:        0.008167747      -0.027535994      -0.078118891\n4686:        0.044871533       0.016589844      -0.023485512\n4687:        0.065347651       0.041206539       0.006993074\n      Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n   1:          2.620443e-01           0.258651038   0.5442288\n   2:          4.153888e-02           0.030928225  -0.5925952\n   3:          9.711407e-05         -10.258220292  -0.5164818\n   4:          5.661810e-02           0.046501003   0.8225188\n   5:          6.369158e-02           0.053806009  -0.6568722\n  ---                                                        \n4683:          9.427082e-02           0.085386142   0.8222493\n4684:          8.722856e-03          -0.002961897   1.9818623\n4685:         -1.318965e-02          -0.025591624  -0.5220442\n4686:          2.747899e-02           0.016408144  -0.5912988\n4687:          5.016700e-02           0.039838758   0.6181969\n      Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n   1:                   0.3840868  -1.02876096  0.31939530   -0.254130552\n   2:                  -0.7801632  -4.55056211 -0.71151122    0.009333354\n   3:                  -0.2872777  -4.55056211 -0.34600512   -0.866997591\n   4:                   0.3403535   1.02239674  0.28551117    0.039031045\n   5:                  -6.2415005   1.13265516  0.03164827   -0.995720084\n  ---                                                                    \n4683:                  -0.6412806   0.01560046 -0.51852041    1.403775544\n4684:                   0.1346891   1.08051799  0.17477591    0.970963141\n4685:                   0.4810091   0.89998250  0.36895738   -1.150527134\n4686:                  -0.3061894   0.22720275 -0.35895074   -0.705980518\n4687:                  -0.2665930   0.22740438 -0.31462613   -0.239696213\n      Minimum.Orbit.Intersection Miss.Dist..Astronomical.\n   1:                -5.45911858               -7.0769260\n   2:                 0.07077092               -0.6830928\n   3:                -0.11099960               -0.9035573\n   4:                -5.45911858               -0.7188386\n   5:                -0.02962490               -0.8013948\n  ---                                                    \n4683:                 0.30711241               -0.2728622\n4684:                -0.05962478               -0.7879458\n4685:                -0.10766868               -0.9303542\n4686:                 0.08529226               -0.7077555\n4687:                 0.50904764                0.1075071\n      Miss.Dist..kilometers. Miss.Dist..lunar. Miss.Dist..miles.   Orbit.ID\n   1:             0.25122963         0.2398625        0.23810770 -9.6514722\n   2:            -1.08492125        -1.1742128       -1.18860632 -0.2412680\n   3:            -1.40898698        -4.7878719       -1.53463694 -0.1803606\n   4:            -4.48402327        -1.2298206       -1.24471124 -0.1803606\n   5:            -1.25881601        -1.3582490       -1.37428752  1.0225620\n  ---                                                                      \n4683:            -0.48191427        -0.5360384       -0.54472804 -0.1194531\n4684:            -1.23904708        -1.3373272       -1.35317867 -0.3021755\n4685:            -1.44837625        -1.5588644       -1.57669598 -0.3326292\n4686:            -1.12117355        -1.2125793       -1.22731578 -0.1042262\n4687:             0.07719897         0.0556823        0.05228143 -0.2717218\n      Orbit.Uncertainity Orbital.Period Perihelion.Arg Perihelion.Distance\n   1:         -1.0070872     -0.3013135   -1.170536399         -0.01831583\n   2:          1.3770116      0.7811097    1.549452700          0.20604472\n   3:          0.7809869      0.1566040    1.470307933          0.61816146\n   4:         -1.0070872     -0.2866969    0.769006449          0.09005898\n   5:         -1.0070872     -0.1552813    0.006829799          0.52730977\n  ---                                                                     \n4683:         -0.7090748      0.3873214   -0.580282684         -0.65810123\n4684:          1.3770116     -0.2345610    0.839430173         -0.18350549\n4685:          0.7809869     -0.3216884   -1.168210857          0.62646993\n4686:          0.7809869      0.1712806    0.824836889          0.52899080\n4687:          0.4829746      0.1224733    0.016358127          1.22720096\n      Perihelion.Time Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec\n   1:      0.10526107                 -0.28167821                 -0.284140684\n   2:     -0.28203779                 -0.00604459                 -0.008343348\n   3:      0.20313227                 -0.92285430                 -0.925697621\n   4:     -7.86832915                  0.02502487                  0.022744569\n   5:      0.26755741                 -1.05752264                 -1.060445948\n  ---                                                                         \n4683:      0.03734532                  1.45280854                  1.451376301\n4684:      0.09156633                  1.00000402                  0.998302826\n4685:      0.27629790                 -1.21948041                 -1.222499918\n4686:      0.37994517                 -0.75439966                 -0.757142920\n4687:      0.37399573                 -0.26657713                 -0.269030636\n      Semi.Major.Axis Equinox.J2000 Equinox..MISSING Orbiting.Body.Earth\n   1:      -0.2791037             1                0                   1\n   2:      -7.3370940             1                0                   1\n   3:       0.2204883             0                1                   1\n   4:      -0.2617714             1                0                   1\n   5:      -0.1106954             1                0                   1\n  ---                                                                   \n4683:       0.4468886             1                0                   1\n4684:      -0.2008499             1                0                   1\n4685:      -0.3034586             1                0                   1\n4686:       0.2353030             1                0                   1\n4687:       0.1857979             1                0                   1\n      Orbiting.Body..MISSING\n   1:                      0\n   2:                      0\n   3:                      0\n   4:                      0\n   5:                      0\n  ---                       \n4683:                      0\n4684:                      0\n4685:                      0\n4686:                      0\n4687:                      0\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n\n\nresult = mlr3::resample(transformed_task,\n                        rf, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nVery cool! Preprocessing + 10-fold cross-validation model evaluation in a few lines of code!\nLet’s create the final predictions:\n\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n\nYou could now submit the predictions here.\nBut we are still not happy with the results, let’s do some hyperparameter tuning!\n\n\n4.2.2 mlr3 - Hyperparameter Tuning\nMachine learning algorithms have a varying number of hyperparameters which can (!) have a high impact on the predictive performance. To list a few hyperparameters:\nRandom Forest\n\nmtry\nMinimal node size\n\nK-nearest-neighbors classification\n\nKernel\nNumber of neighbors\nDistance metric\n\nBoosted Regression Tree\n\nnrounds\nMaximum depth\nalpha\nbooster\neta\ngamma\nlambda\n\nWith mlr3, we can easily extend the above example to do hyperparameter tuning within nested cross-validation (the tuning has its own inner cross-validation).\nPrint the hyperparameter space of our random forest learner:\n\nrf$param_set\n\n<ParamSet>\n                              id    class lower upper nlevels        default\n 1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5\n 2:       always.split.variables ParamUty    NA    NA     Inf <NoDefault[3]>\n 3:                class.weights ParamUty    NA    NA     Inf               \n 4:                      holdout ParamLgl    NA    NA       2          FALSE\n 5:                   importance ParamFct    NA    NA       4 <NoDefault[3]>\n 6:                   keep.inbag ParamLgl    NA    NA       2          FALSE\n 7:                    max.depth ParamInt     0   Inf     Inf               \n 8:                min.node.size ParamInt     1   Inf     Inf               \n 9:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1\n10:                      minprop ParamDbl  -Inf   Inf     Inf            0.1\n11:                         mtry ParamInt     1   Inf     Inf <NoDefault[3]>\n12:                   mtry.ratio ParamDbl     0     1     Inf <NoDefault[3]>\n13:            num.random.splits ParamInt     1   Inf     Inf              1\n14:                  num.threads ParamInt     1   Inf     Inf              1\n15:                    num.trees ParamInt     1   Inf     Inf            500\n16:                    oob.error ParamLgl    NA    NA       2           TRUE\n17:        regularization.factor ParamUty    NA    NA     Inf              1\n18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE\n19:                      replace ParamLgl    NA    NA       2           TRUE\n20:    respect.unordered.factors ParamFct    NA    NA       3         ignore\n21:              sample.fraction ParamDbl     0     1     Inf <NoDefault[3]>\n22:                  save.memory ParamLgl    NA    NA       2          FALSE\n23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE\n24:                    se.method ParamFct    NA    NA       2        infjack\n25:                         seed ParamInt  -Inf   Inf     Inf               \n26:         split.select.weights ParamUty    NA    NA     Inf               \n27:                    splitrule ParamFct    NA    NA       3           gini\n28:                      verbose ParamLgl    NA    NA       2           TRUE\n29:                 write.forest ParamLgl    NA    NA       2           TRUE\n                              id    class lower upper nlevels        default\n       parents value\n 1:                 \n 2:                 \n 3:                 \n 4:                 \n 5:                 \n 6:                 \n 7:                 \n 8:                 \n 9:                 \n10:                 \n11:                 \n12:                 \n13:  splitrule      \n14:                1\n15:                 \n16:                 \n17:                 \n18:                 \n19:                 \n20:                 \n21:                 \n22:                 \n23: importance      \n24:                 \n25:                 \n26:                 \n27:                 \n28:                 \n29:                 \n       parents value\n\n\nDefine the hyperparameter space of the random forest:\n\nlibrary(paradox)\n\nrf_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamInt$new(\"min.node.size\", lower = 1, upper = 30L),\n           paradox::ParamInt$new(\"mtry\", lower = 1, upper = 30L),\n           paradox::ParamLgl$new(\"regularization.usedepth\", default = TRUE)))\nprint(rf_pars)\n\n<ParamSet>\n                        id    class lower upper nlevels        default value\n1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n3: regularization.usedepth ParamLgl    NA    NA       2           TRUE      \n\n\nTo set up the tuning pipeline we need:\n\nInner cross-validation resampling object.\nTuning criterion (e.g. AUC).\nTuning method (e.g. random or block search).\nTuning terminator (When should we stop tuning? E.g. after \\(n\\) iterations).\n\n\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = rf, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = rf_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n\n<AutoTuner:classif.ranger.tuned>\n* Model: list\n* Search Space:\n<ParamSet>\n                        id    class lower upper nlevels        default value\n1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n3: regularization.usedepth ParamLgl    NA    NA       2           TRUE      \n* Packages: mlr3, mlr3tuning, mlr3learners, ranger\n* Predict Type: prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n\nNow we can wrap it normally into the 10-fold cross-validated setup as done previously:\n\nset.seed(123)\n\nouter3 = mlr3::rsmp(\"cv\", folds = 3L)\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = outer3, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nYeah, we were able to improve the performance!\nLet’s create the final predictions:\n\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n\n\n\n4.2.3 mlr3 - Hyperparameter Tuning with Oversampling\nLet’s go one step back, maybe you have noticed that our classes are unbalanced:\n\ntable(data$Hazardous)\n\n\n  0   1 \n412  88 \n\n\nMany machine learning algorithms have problems with unbalanced data because if the imbalance is too strong it is cheaper for the algorithm to focus on only one class (e.g. by predicting only 0s or 1s). You need to keep in mind that machine learning algorithms are greedy and their main focus is to minimize the loss function.\nThere are few techniques to correct for imbalance:\n\nOversampling (oversample the undersampled class).\nUndersampling (undersample the oversampled class).\nSMOTE Synthetic Minority Over-sampling Technique (very briefly, we will use a k-nearest-neighbors classification to create new samples around our undersampled class).\n\nHere, we will use oversampling which we can do by extending our random forest learner:\n\nset.seed(123)\n\nrf_over = po(\"classbalancing\", id = \"over\", adjust = \"minor\") %>>% rf\n\n# However rf_over is now a \"graph\",\n# but we can easily transform it back into a learner:\nrf_over_learner = GraphLearner$new(rf_over)\nprint(rf_over_learner)\n\n<GraphLearner:over.classif.ranger>\n* Model: -\n* Parameters: over.ratio=1, over.reference=all, over.adjust=minor,\n  over.shuffle=TRUE, classif.ranger.num.threads=1\n* Packages: mlr3, mlr3pipelines, mlr3learners, ranger\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, character, factor, ordered,\n  POSIXct\n* Properties: featureless, hotstart_backward, hotstart_forward,\n  importance, loglik, missings, multiclass, oob_error,\n  selected_features, twoclass, weights\n\n\nThe learner has now a new feature space:\n\nrf_over_learner$param_set\n\n<ParamSetCollection>\n                                             id    class lower upper nlevels\n 1:                        classif.ranger.alpha ParamDbl  -Inf   Inf     Inf\n 2:       classif.ranger.always.split.variables ParamUty    NA    NA     Inf\n 3:                classif.ranger.class.weights ParamUty    NA    NA     Inf\n 4:                      classif.ranger.holdout ParamLgl    NA    NA       2\n 5:                   classif.ranger.importance ParamFct    NA    NA       4\n 6:                   classif.ranger.keep.inbag ParamLgl    NA    NA       2\n 7:                    classif.ranger.max.depth ParamInt     0   Inf     Inf\n 8:                classif.ranger.min.node.size ParamInt     1   Inf     Inf\n 9:                     classif.ranger.min.prop ParamDbl  -Inf   Inf     Inf\n10:                      classif.ranger.minprop ParamDbl  -Inf   Inf     Inf\n11:                         classif.ranger.mtry ParamInt     1   Inf     Inf\n12:                   classif.ranger.mtry.ratio ParamDbl     0     1     Inf\n13:            classif.ranger.num.random.splits ParamInt     1   Inf     Inf\n14:                  classif.ranger.num.threads ParamInt     1   Inf     Inf\n15:                    classif.ranger.num.trees ParamInt     1   Inf     Inf\n16:                    classif.ranger.oob.error ParamLgl    NA    NA       2\n17:        classif.ranger.regularization.factor ParamUty    NA    NA     Inf\n18:      classif.ranger.regularization.usedepth ParamLgl    NA    NA       2\n19:                      classif.ranger.replace ParamLgl    NA    NA       2\n20:    classif.ranger.respect.unordered.factors ParamFct    NA    NA       3\n21:              classif.ranger.sample.fraction ParamDbl     0     1     Inf\n22:                  classif.ranger.save.memory ParamLgl    NA    NA       2\n23: classif.ranger.scale.permutation.importance ParamLgl    NA    NA       2\n24:                    classif.ranger.se.method ParamFct    NA    NA       2\n25:                         classif.ranger.seed ParamInt  -Inf   Inf     Inf\n26:         classif.ranger.split.select.weights ParamUty    NA    NA     Inf\n27:                    classif.ranger.splitrule ParamFct    NA    NA       3\n28:                      classif.ranger.verbose ParamLgl    NA    NA       2\n29:                 classif.ranger.write.forest ParamLgl    NA    NA       2\n30:                                 over.adjust ParamFct    NA    NA       7\n31:                                  over.ratio ParamDbl     0   Inf     Inf\n32:                              over.reference ParamFct    NA    NA       6\n33:                                over.shuffle ParamLgl    NA    NA       2\n                                             id    class lower upper nlevels\n           default                   parents value\n 1:            0.5                                \n 2: <NoDefault[3]>                                \n 3:                                               \n 4:          FALSE                                \n 5: <NoDefault[3]>                                \n 6:          FALSE                                \n 7:                                               \n 8:                                               \n 9:            0.1                                \n10:            0.1                                \n11: <NoDefault[3]>                                \n12: <NoDefault[3]>                                \n13:              1  classif.ranger.splitrule      \n14:              1                               1\n15:            500                                \n16:           TRUE                                \n17:              1                                \n18:          FALSE                                \n19:           TRUE                                \n20:         ignore                                \n21: <NoDefault[3]>                                \n22:          FALSE                                \n23:          FALSE classif.ranger.importance      \n24:        infjack                                \n25:                                               \n26:                                               \n27:           gini                                \n28:           TRUE                                \n29:           TRUE                                \n30: <NoDefault[3]>                           minor\n31: <NoDefault[3]>                               1\n32: <NoDefault[3]>                             all\n33: <NoDefault[3]>                            TRUE\n           default                   parents value\n\n\nWe can also tune the oversampling rate!\n\nset.seed(123)\n\nrf_pars_over = \n    paradox::ParamSet$new(\n      list(paradox::ParamInt$new(\"over.ratio\", lower = 1, upper = 7L),\n           paradox::ParamInt$new(\"classif.ranger.min.node.size\",\n                                 lower = 1, upper = 30L),\n           paradox::ParamInt$new(\"classif.ranger.mtry\", lower = 1,\n                                 upper = 30L),\n           paradox::ParamLgl$new(\"classif.ranger.regularization.usedepth\",\n                                 default = TRUE)))\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\n\nlearner_tuner_over = AutoTuner$new(learner = rf_over_learner, \n                                   measure = measurement, \n                                   tuner = tuner, \n                                   terminator = terminator,\n                                   search_space = rf_pars_over,\n                                   resampling = inner3)\nprint(learner_tuner)\n\n<AutoTuner:classif.ranger.tuned>\n* Model: list\n* Search Space:\n<ParamSet>\n                        id    class lower upper nlevels        default value\n1:           min.node.size ParamInt     1    30      30 <NoDefault[3]>      \n2:                    mtry ParamInt     1    30      30 <NoDefault[3]>      \n3: regularization.usedepth ParamLgl    NA    NA       2           TRUE      \n* Packages: mlr3, mlr3tuning, mlr3learners, ranger\n* Predict Type: prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n\n\nset.seed(123)\n\nouter3 = mlr3::rsmp(\"cv\", folds = 3L)\nresult = mlr3::resample(transformed_task, learner_tuner_over,\n                        resampling = outer3, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\n5 iterations in the hyperspace is not very much…\nLet’s create the final predictions:\n\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n\n  \n  Optional bonus task\nAfter reading the above chapter about the mlr package, try to transfer it to the titanic data set (use the titanic_ml data set, this has already NAs for the values to predict). Alternatively, you can also use other data sets from our challenge (e.g. the plant-pollinator data set, see the data set chapter @ref(datasets)).\n  \n    \n      Solution\n    \n    \n\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\nset.seed(123)\n\ndata(titanic_ml)\nstr(titanic_ml)\n\n'data.frame':   1309 obs. of  14 variables:\n $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\ndata = titanic_ml %>% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n\n survived       age      boat     cabin  embarked      fare home.dest     parch \n      655       263         0         0         0         0         0         0 \n   pclass       sex     sibsp \n        0         0         0 \n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %>>% po(\"scale\") %>>% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nrf = lrn(\"classif.ranger\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n\n# result = mlr3::resample(transformed_task, rf,\n#                         resampling = cv10, store_models = TRUE)\n# \n# # Calculate the average AUC of the holdouts.\n# result$aggregate(measurement)\n# \n# pred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\n# row_ids = (1:nrow(data))[is.na(data$survived)])$data$prob[, \"1\", drop = FALSE])\n# \n# dim(pred)\n# predictions = round(apply(pred, 1, mean))\n# \n# write.csv(data.frame(y = predictions), file = \"submission_RF.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#titanic",
    "href": "Appendix-Datasets.html#titanic",
    "title": "Appendix A — Datasets",
    "section": "A.1 Titanic",
    "text": "A.1 Titanic\nThe data set is a collection of Titanic passengers with information about their age, class, sex, and their survival status. The competition is simple here: Train a machine learning model and predict the survival probability.\nThe Titanic data set is very well explored and serves as a stepping stone in many machine learning careers. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “survived”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(titanic_ml)\ntitanic = titanic_ml\nsummary(titanic)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\ntitanic_imputed = titanic %>% select(-name, -ticket, -cabin, -boat, -home.dest)\ntitanic_imputed = missRanger::missRanger(data = titanic_imputed %>%\n                                           select(-survived))\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      age, fare, body\n  Variables used to impute: pclass, sex, age, sibsp, parch, fare, embarked, body\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\niter 2\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\niter 3\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n\ntitanic_imputed$survived = titanic$survived\n\n\nSplit into training and test set:\n\n\ntrain = titanic_imputed[!is.na(titanic$survived), ]\ntest = titanic_imputed[is.na(titanic$survived), ]\n\n\nTrain model:\n\n\nmodel = glm(survived~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, data = test, type = \"response\")\nhead(preds)\n\n       561        321       1177       1098       1252       1170 \n0.79095923 0.30597519 0.01400693 0.12310859 0.14099292 0.11768284 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")\n\nAnd submit the csv on http://rhsbio7.uni-regensburg.de:8500."
  },
  {
    "objectID": "Appendix-Datasets.html#plant-pollinator-database",
    "href": "Appendix-Datasets.html#plant-pollinator-database",
    "title": "Appendix A — Datasets",
    "section": "A.2 Plant-pollinator Database",
    "text": "A.2 Plant-pollinator Database\nThe plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look here.\n\n\n\n\n\nResponse variable: “interaction”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n\n                   crop                       insect          type          \n Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n Daucus_carota       :  256   Andrena_gravida    :   80                     \n Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n (Other)             :18944   (Other)            :20000                     \n    season             diameter        corolla             colour         \n Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n Class :character   1st Qu.:  5.00   Class :character   Class :character  \n Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n                    Mean   : 27.03                                        \n                    3rd Qu.: 25.00                                        \n                    Max.   :150.00                                        \n                    NA's   :9472                                          \n    nectar            b.system         s.pollination      inflorescence     \n Length:20480       Length:20480       Length:20480       Length:20480      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  composite            guild               tongue            body      \n Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n                                       Mean   : 8.104   Mean   :10.66  \n                                       3rd Qu.:10.500   3rd Qu.:13.00  \n                                       Max.   :26.400   Max.   :25.00  \n                                       NA's   :17040    NA's   :6160   \n  sociality           feeding          interaction \n Length:20480       Length:20480       0   :14095  \n Class :character   Class :character   1   :  595  \n Mode  :character   Mode  :character   NA's: 5790  \n                                                   \n                                                   \n                                                   \n                                                   \n\n\n\nImpute missing values (not our response variable!) We will select only a few predictors here (you can work with all predictors of course).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nplant_poll_imputed = plant_poll %>% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction)\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %>%\n                                              select(-interaction))\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      diameter, corolla, tongue, body\n  Variables used to impute: diameter, corolla, tongue, body\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\niter 2\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\niter 3\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\nplant_poll_imputed$interaction = plant_poll$interaction\n\n\nSplit into training and test set:\n\n\ntrain = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]\ntest = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ]\n\n\nTrain model:\n\n\nmodel = glm(interaction~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, newdata = test, type = \"response\")\nhead(preds)\n\n         1          2          3          4          5          6 \n0.02942746 0.05063489 0.03780247 0.03780247 0.02651142 0.04130643 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#wine",
    "href": "Appendix-Datasets.html#wine",
    "title": "Appendix A — Datasets",
    "section": "A.3 Wine",
    "text": "A.3 Wine\nThe data set is a collection of wines of different quality. The aim is to predict the quality of the wine based on physiochemical predictors.\nFor inspiration and data exploration notebooks, check out this kaggle competition. For instance, check out this very nice notebook which removes a few problems from the data.\nResponse variable: “quality”\nWe could theoretically use a regression model for this task but we will stick with a classification model.\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(wine)\nsummary(wine)\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900  \n 1st Qu.: 7.100   1st Qu.:0.3900   1st Qu.:0.0900   1st Qu.: 1.900  \n Median : 7.900   Median :0.5200   Median :0.2600   Median : 2.200  \n Mean   : 8.335   Mean   :0.5284   Mean   :0.2705   Mean   : 2.533  \n 3rd Qu.: 9.300   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600  \n Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500  \n NA's   :70       NA's   :48       NA's   :41       NA's   :60      \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  \n Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  \n Mean   :0.08747   Mean   :15.83       Mean   : 46.23       Mean   :0.9968  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9979  \n Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  \n NA's   :37        NA's   :78          NA's   :78           NA's   :78      \n       pH          sulphates         alcohol         quality     \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n 1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000  \n Mean   :3.311   Mean   :0.6572   Mean   :10.42   Mean   :5.596  \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n NA's   :25      NA's   :51                       NA's   :905    \n\n\n\nImpute missing values (not our response variable!).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nwine_imputed = missRanger::missRanger(data = wine %>% select(-quality))\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates\n  Variables used to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 2\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 3\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 4\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 5\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\n\nwine_imputed$quality = wine$quality\n\n\nSplit into training and test set:\n\n\ntrain = wine_imputed[!is.na(wine$quality), ]\ntest = wine_imputed[is.na(wine$quality), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(quality~., data = train, classification = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions\nhead(preds)\n\n[1] 6 5 5 7 6 6\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#nasa",
    "href": "Appendix-Datasets.html#nasa",
    "title": "Appendix A — Datasets",
    "section": "A.4 Nasa",
    "text": "A.4 Nasa\nA collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “Hazardous”\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(nasa)\nsummary(nasa)\n\n Neo.Reference.ID       Name         Absolute.Magnitude Est.Dia.in.KM.min.\n Min.   :2000433   Min.   :2000433   Min.   :11.16      Min.   : 0.00101  \n 1st Qu.:3102682   1st Qu.:3102683   1st Qu.:20.10      1st Qu.: 0.03346  \n Median :3514800   Median :3514800   Median :21.90      Median : 0.11080  \n Mean   :3272675   Mean   :3273113   Mean   :22.27      Mean   : 0.20523  \n 3rd Qu.:3690987   3rd Qu.:3690385   3rd Qu.:24.50      3rd Qu.: 0.25384  \n Max.   :3781897   Max.   :3781897   Max.   :32.10      Max.   :15.57955  \n NA's   :53        NA's   :57        NA's   :36         NA's   :60        \n Est.Dia.in.KM.max. Est.Dia.in.M.min.   Est.Dia.in.M.max. \n Min.   : 0.00226   Min.   :    1.011   Min.   :    2.26  \n 1st Qu.: 0.07482   1st Qu.:   33.462   1st Qu.:   74.82  \n Median : 0.24777   Median :  110.804   Median :  247.77  \n Mean   : 0.45754   Mean   :  204.649   Mean   :  458.45  \n 3rd Qu.: 0.56760   3rd Qu.:  253.837   3rd Qu.:  567.60  \n Max.   :34.83694   Max.   :15579.552   Max.   :34836.94  \n NA's   :23         NA's   :29          NA's   :46        \n Est.Dia.in.Miles.min. Est.Dia.in.Miles.max. Est.Dia.in.Feet.min.\n Min.   :0.00063       Min.   : 0.00140      Min.   :    3.32    \n 1st Qu.:0.02079       1st Qu.: 0.04649      1st Qu.:  109.78    \n Median :0.06885       Median : 0.15395      Median :  363.53    \n Mean   :0.12734       Mean   : 0.28486      Mean   :  670.44    \n 3rd Qu.:0.15773       3rd Qu.: 0.35269      3rd Qu.:  832.80    \n Max.   :9.68068       Max.   :21.64666      Max.   :51114.02    \n NA's   :42            NA's   :50            NA's   :21          \n Est.Dia.in.Feet.max. Close.Approach.Date Epoch.Date.Close.Approach\n Min.   :     7.41    2016-07-22:  18     Min.   :7.889e+11        \n 1st Qu.:   245.49    2015-01-15:  17     1st Qu.:1.016e+12        \n Median :   812.88    2015-02-15:  16     Median :1.203e+12        \n Mean   :  1500.77    2007-11-08:  15     Mean   :1.180e+12        \n 3rd Qu.:  1862.19    2012-01-15:  15     3rd Qu.:1.356e+12        \n Max.   :114294.42    (Other)   :4577     Max.   :1.473e+12        \n NA's   :46           NA's      :  29     NA's   :43               \n Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr Miles.per.hour   \n Min.   : 0.3355              Min.   :  1208              Min.   :  750.5  \n 1st Qu.: 8.4497              1st Qu.: 30399              1st Qu.:18846.7  \n Median :12.9370              Median : 46532              Median :28893.7  \n Mean   :13.9848              Mean   : 50298              Mean   :31228.0  \n 3rd Qu.:18.0774              3rd Qu.: 65068              3rd Qu.:40436.9  \n Max.   :44.6337              Max.   :160681              Max.   :99841.2  \n NA's   :27                   NA's   :28                  NA's   :38       \n Miss.Dist..Astronomical. Miss.Dist..lunar.   Miss.Dist..kilometers.\n Min.   :0.00018          Min.   :  0.06919   Min.   :   26610      \n 1st Qu.:0.13341          1st Qu.: 51.89874   1st Qu.:19964907      \n Median :0.26497          Median :103.19415   Median :39685408      \n Mean   :0.25690          Mean   : 99.91366   Mean   :38436154      \n 3rd Qu.:0.38506          3rd Qu.:149.59244   3rd Qu.:57540318      \n Max.   :0.49988          Max.   :194.45491   Max.   :74781600      \n NA's   :60               NA's   :30          NA's   :56            \n Miss.Dist..miles.  Orbiting.Body    Orbit.ID     \n Min.   :   16535   Earth:4665    Min.   :  1.00  \n 1st Qu.:12454813   NA's :  22    1st Qu.:  9.00  \n Median :24662435                 Median : 16.00  \n Mean   :23885560                 Mean   : 28.34  \n 3rd Qu.:35714721                 3rd Qu.: 31.00  \n Max.   :46467132                 Max.   :611.00  \n NA's   :27                       NA's   :33      \n        Orbit.Determination.Date Orbit.Uncertainity Minimum.Orbit.Intersection\n 2017-06-21 06:17:20:   9        Min.   :0.000      Min.   :0.00000           \n 2017-04-06 08:57:13:   8        1st Qu.:0.000      1st Qu.:0.01435           \n 2017-04-06 09:24:24:   8        Median :3.000      Median :0.04653           \n 2017-04-06 08:24:13:   7        Mean   :3.521      Mean   :0.08191           \n 2017-04-06 08:26:19:   7        3rd Qu.:6.000      3rd Qu.:0.12150           \n (Other)            :4622        Max.   :9.000      Max.   :0.47789           \n NA's               :  26        NA's   :49         NA's   :137               \n Jupiter.Tisserand.Invariant Epoch.Osculation   Eccentricity    \n Min.   :2.196               Min.   :2450164   Min.   :0.00752  \n 1st Qu.:4.047               1st Qu.:2458000   1st Qu.:0.24086  \n Median :5.071               Median :2458000   Median :0.37251  \n Mean   :5.056               Mean   :2457723   Mean   :0.38267  \n 3rd Qu.:6.017               3rd Qu.:2458000   3rd Qu.:0.51256  \n Max.   :9.025               Max.   :2458020   Max.   :0.96026  \n NA's   :56                  NA's   :60        NA's   :39       \n Semi.Major.Axis   Inclination       Asc.Node.Longitude Orbital.Period  \n Min.   :0.6159   Min.   : 0.01451   Min.   :  0.0019   Min.   : 176.6  \n 1st Qu.:1.0012   1st Qu.: 4.93290   1st Qu.: 83.1849   1st Qu.: 365.9  \n Median :1.2422   Median :10.27694   Median :172.6347   Median : 504.9  \n Mean   :1.4009   Mean   :13.36159   Mean   :172.1717   Mean   : 635.5  \n 3rd Qu.:1.6782   3rd Qu.:19.47848   3rd Qu.:254.8804   3rd Qu.: 793.1  \n Max.   :5.0720   Max.   :75.40667   Max.   :359.9059   Max.   :4172.2  \n NA's   :53       NA's   :42         NA's   :60         NA's   :46      \n Perihelion.Distance Perihelion.Arg     Aphelion.Dist    Perihelion.Time  \n Min.   :0.08074     Min.   :  0.0069   Min.   :0.8038   Min.   :2450100  \n 1st Qu.:0.63038     1st Qu.: 95.6430   1st Qu.:1.2661   1st Qu.:2457815  \n Median :0.83288     Median :189.7729   Median :1.6182   Median :2457972  \n Mean   :0.81316     Mean   :184.0185   Mean   :1.9864   Mean   :2457726  \n 3rd Qu.:0.99718     3rd Qu.:271.9535   3rd Qu.:2.4497   3rd Qu.:2458108  \n Max.   :1.29983     Max.   :359.9931   Max.   :8.9839   Max.   :2458839  \n NA's   :22          NA's   :48         NA's   :38       NA's   :59       \n  Mean.Anomaly       Mean.Motion       Equinox       Hazardous    \n Min.   :  0.0032   Min.   :0.08628   J2000:4663   Min.   :0.000  \n 1st Qu.: 87.0069   1st Qu.:0.45147   NA's :  24   1st Qu.:0.000  \n Median :186.0219   Median :0.71137                Median :0.000  \n Mean   :181.2882   Mean   :0.73732                Mean   :0.176  \n 3rd Qu.:276.6418   3rd Qu.:0.98379                3rd Qu.:0.000  \n Max.   :359.9180   Max.   :2.03900                Max.   :1.000  \n NA's   :40         NA's   :48                     NA's   :4187   \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nnasa_imputed = missRanger::missRanger(data = nasa %>% select(-Hazardous),\n                                      maxiter = 1, num.trees = 5L)\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox\n  Variables used to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\nnasa_imputed$Hazardous = nasa$Hazardous\n\n\nSplit into training and test set:\n\n\ntrain = nasa_imputed[!is.na(nasa$Hazardous), ]\ntest = nasa_imputed[is.na(nasa$Hazardous), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(Hazardous~., data = train, classification = TRUE,\n            probability = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions[,2]\nhead(preds)\n\n[1] 0.6348055556 0.7525960317 0.0008444444 0.7733373016 0.1404333333\n[6] 0.1509190476\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#flower",
    "href": "Appendix-Datasets.html#flower",
    "title": "Appendix A — Datasets",
    "section": "A.5 Flower",
    "text": "A.5 Flower\nA collection of over 4000 flower images of 5 plant species. The data set is from kaggle but we downsampled the images from \\(320*240\\) to \\(80*80\\) pixels. You can a) download the data set here or b) get it via the EcoData package.\nNotes:\n\nCheck out convolutional neural network notebooks on kaggle (they are often written in Python but you can still copy the architectures), e.g. this one.\nLast year’s winners have used a transfer learning approach (they achieved around 70% accuracy), check out this notebook, see also the section about transfer learning @ref(transfer).\n\nResponse variable: “Plant species”\n\nLoad data set:\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\ntrain = EcoData::dataset_flower()$train/255\ntest = EcoData::dataset_flower()$test/255\nlabels = EcoData::dataset_flower()$labels\n\nLet’s visualize a flower:\n\ntrain[100,,,] %>%\n  image_to_array() %>%\n  as.raster() %>%\n  plot()\n\n\n\n\n\nBuild and train model:\n\n\nmodel = keras_model_sequential()\nmodel %>% \n  layer_conv_2d(filters = 4L, kernel_size = 2L,\n                input_shape = list(80L, 80L, 3L)) %>% \n  layer_max_pooling_2d() %>% \n  layer_flatten() %>% \n  layer_dense(units = 5L, activation = \"softmax\")\n\n### Model fitting ###\n\nmodel %>% \n  compile(loss = loss_categorical_crossentropy, \n          optimizer = optimizer_adamax(learning_rate = 0.01))\n\nmodel %>% \n  fit(x = train, y = keras::k_one_hot(labels, 5L))\n\n\nPredictions:\n\n\n# Prediction on training data:\npred = apply(model %>% predict(train), 1, which.max)\nMetrics::accuracy(pred - 1L, labels)\ntable(pred)\n\n# Prediction for the submission server:\npred = model %>% predict(test) %>% apply(1, which.max) - 1L\ntable(pred)\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = pred), file = \"cnn.csv\")"
  }
]