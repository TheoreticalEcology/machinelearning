[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Deep Learning with R",
    "section": "",
    "text": "Preface\nMachine learning (ML) is the process of building a predictive model that makes predictions about new data based on observations (training data). The goal of this course is to enable you to build a robust ML model, one that generalizes well to new observations and doesn’t “overfit” your training data. To do this, you will need to master a number of skills, in particular\nIn recent years, a new field within ML called Deep Learning has emerged and attracted a lot of attention. The reason for this is that DL incorporates many different but very flexible architectures that allow to natively model different types of data, e.g. Convolutional Neural Networks for images or Recurrent Neural Networks for time series. However, exploiting the flexibility of DL requires a deeper, more fundamental understanding of the frameworks in which they are implemented. To this end, the course will also cover common DL frameworks such as TensorFlow (and PyTorch) and:"
  },
  {
    "objectID": "index.html#before-the-course",
    "href": "index.html#before-the-course",
    "title": "Machine Learning and Deep Learning with R",
    "section": "Before the course",
    "text": "Before the course\n\nPlease read the following two reviews about Machine Learning in General (Pichler and Hartig 2023) and Deep Learning (Borowiec et al. 2022)\nPlease install all dependencies before the course because it will take some time, see Chapter 1 for installation instructions\nThis course assumes advanced knowledge of the R programming language. If you want to refresh your knowledge about R, you can find a crashcourse in R in the book of the advanced statistic course: R-Crash-Course\n\nAuthors:\nMaximilian Pichler: @_Max_Pichler\nFlorian Hartig: @florianhartig\nContributors:\nJohannes Oberpriller, Matthias Meier\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\n\n\n\n\nBorowiec, Marek L, Rebecca B Dikow, Paul B Frandsen, Alexander McKeeken, Gabriele Valentini, and Alexander E White. 2022. “Deep Learning as a Tool for Ecology and Evolution.” Methods in Ecology and Evolution 13 (8): 1640–60.\n\n\nPichler, Maximilian, and Florian Hartig. 2023. “Machine Learning and Deep Learning—a Review for Ecologists.” Methods in Ecology and Evolution 14 (4): 994–1016."
  },
  {
    "objectID": "A1-GettingStarted.html#r-system",
    "href": "A1-GettingStarted.html#r-system",
    "title": "1  Getting Started",
    "section": "1.1 R System",
    "text": "1.1 R System\nMake sure you have a recent version of R (&gt;=4.0, ideally &gt;=4.2) and RStudio on your computers. For Mac users, if you have already a M1/M2 Mac, please install the R-ARM version (see here (not the x86_64 version))"
  },
  {
    "objectID": "A1-GettingStarted.html#tensorflow-and-keras",
    "href": "A1-GettingStarted.html#tensorflow-and-keras",
    "title": "1  Getting Started",
    "section": "1.2 TensorFlow and Keras",
    "text": "1.2 TensorFlow and Keras\nIf you want to run the code on your own computers, you need to install TensorFlow / Keras for R. For this, the following should work for most people:\n\nWindowsLinuxMacOS-M1/M2MacOS-Intel\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\nIf you have already installed anaconda/miniconda, please uninstall it first!\n\nDownload the ARM miniconda version from here and install it\nOpen the terminal (e.g. by pressing cmd+whitespace and search for ‘terminal’)\nRun in the terminal (not in R!):\n\nconda create -n \"r-reticulate\" python=3.10 \nconda install -c apple tensorflow-deps \npython -m pip install tensorflow-macos \npython -m pip install scipy\n\nOpen R, install the package ‘reticulate’ by running install.packages('reticulate') and run:\n\n\nreticulate::use_condaenv(\"r-reticulate\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf the installation has failed, run the following command:\npython -m pip install --upgrade numpy\n\n\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\n\nThis should work on most computers, in particular if all software is recent. Sometimes, however, things don’t work well, especially the python distribution often makes problems. If the installation does not work for you, we can look at it together. Also, we will provide some virtual machines in case your computers / laptops are too old or you don’t manage to install TensorFlow."
  },
  {
    "objectID": "A1-GettingStarted.html#torch-for-r",
    "href": "A1-GettingStarted.html#torch-for-r",
    "title": "1  Getting Started",
    "section": "1.3 Torch for R",
    "text": "1.3 Torch for R\nWe may also use Torch for R. This is an R frontend for the popular PyTorch framework. To install Torch, type in R:\n\ninstall.packages(\"torch\")\nlibrary(torch)\ntorch::install_torch()"
  },
  {
    "objectID": "A1-GettingStarted.html#ecodata",
    "href": "A1-GettingStarted.html#ecodata",
    "title": "1  Getting Started",
    "section": "1.4 EcoData",
    "text": "1.4 EcoData\nWe use data sets from the EcoData package. To install the package, run:\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = TRUE, build_vignettes = TRUE)\n\nThe default installation will install a number of packages that are useful for statistics. Especially in Linux, this may take some time to install. If you are in a hurry and only want the data, you can also run\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = FALSE, build_vignettes = FALSE)"
  },
  {
    "objectID": "A1-GettingStarted.html#further-used-libraries",
    "href": "A1-GettingStarted.html#further-used-libraries",
    "title": "1  Getting Started",
    "section": "1.5 Further Used Libraries",
    "text": "1.5 Further Used Libraries\nWe will make huge use of different libraries. So take a coffee or two (that will take a while…) and install the following libraries. Please do this in the given order unless you know what you’re doing, because there are some dependencies between the packages.\n\ninstall.packages(\"abind\")\ninstall.packages(\"animation\")\ninstall.packages(\"ape\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"Rgraphviz\", \"graph\", \"RBGL\"))\ninstall.packages(\"coro\")\ninstall.packages(\"dbscan\")\ninstall.packages(\"dendextend\")\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"e1071\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"fields\")\ninstall.packages(\"forcats\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"glmnetUtils\")\ninstall.packages(\"gym\")\ninstall.packages(\"kknn\")\ninstall.packages(\"knitr\")\ninstall.packages(\"iml\")\ninstall.packages(\"lavaan\")\ninstall.packages(\"lmtest\")\ninstall.packages(\"magick\")\ninstall.packages(\"mclust\")\ninstall.packages(\"Metrics\")\ninstall.packages(\"microbenchmark\")\ninstall.packages(\"missRanger\")\ninstall.packages(\"mlbench\")\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\ninstall.packages(\"mlr3measures\")\ninstall.packages(\"mlr3pipelines\")\ninstall.packages(\"mlr3tuning\")\ninstall.packages(\"paradox\")\ninstall.packages(\"partykit\")\ninstall.packages(\"pcalg\")\ninstall.packages(\"piecewiseSEM\")\ninstall.packages(\"purrr\")\ninstall.packages(\"randomForest\")\ninstall.packages(\"ranger\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"scales\")\ninstall.packages(\"semPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tfprobability\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"xgboost\")\ninstall.packages(\"tidymodels\")\n\ndevtools::install_github(\"andrie/deepviz\", dependencies = TRUE,\n                         upgrade = \"always\")\ndevtools::install_github('skinner927/reprtree')\ndevtools::install_version(\"lavaanPlot\", version = \"0.6.0\")\n\nreticulate::conda_install(\"r-reticulate\", packages = \"scipy\", pip = TRUE)\nreticulate::conda_install(\"r-reticulate\", packages = \"tensorflow_probability\", pip = TRUE)"
  },
  {
    "objectID": "A1-GettingStarted.html#linuxunix-systems-have-sometimes-to-fulfill-some-further-dependencies",
    "href": "A1-GettingStarted.html#linuxunix-systems-have-sometimes-to-fulfill-some-further-dependencies",
    "title": "1  Getting Started",
    "section": "1.6 Linux/UNIX systems have sometimes to fulfill some further dependencies",
    "text": "1.6 Linux/UNIX systems have sometimes to fulfill some further dependencies\nDebian based systems\nFor Debian based systems, we need:\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev"
  },
  {
    "objectID": "A1-GettingStarted.html#reminders-about-basic-operations-in-r",
    "href": "A1-GettingStarted.html#reminders-about-basic-operations-in-r",
    "title": "1  Getting Started",
    "section": "1.7 Reminders About Basic Operations in R",
    "text": "1.7 Reminders About Basic Operations in R\nBasic and advanced knowledge of R is required to successfully participate in this course. If you would like to refresh your knowledge of R, you can review the chapter ‘Reminder: R Basics’ from the advanced statistic course."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#overview",
    "href": "A2-MachineLearningTasks.html#overview",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nThere are three types of machine learning tasks:\n\nSupervised learning\nUnsupervised learning\nReinforcement learning\n\nIn supervised learning, you train algorithms using labeled data, what means that you already know the correct answer for a part of the data (the so called training data).\nUnsupervised learning in contrast is a technique, where one does not need to monitor the model or apply labels. Instead, you allow the model to work on its own to discover information.\nReinforcement learning is a technique that emulates a game-like situation. The algorithm finds a solution by trial and error and gets either rewards or penalties for every action. As in games, the goal is to maximize the rewards. We will talk more about this technique on the last day of the course.\nFor the moment, we will focus on the first two tasks, supervised and unsupervised learning. To do so, we will begin with a small example. But before you start with the code, here is a video to prepare you for what we will do in the class:\n\n\n\n2.1.1 Questions\n\nIn ML, predictors (or the explaining variables) are often called features: TRUEFALSE\nIn supervised learning the response (y) and the features (x) are known: TRUEFALSE\nIn unsupervised learning, only the features are known: TRUEFALSE\nIn reinforcement learning an agent (ML model) is trained by interacting with an environment: TRUEFALSE\nHave a look at the two textbooks on ML (Elements of statistical learning and introduction to statistical learning) in our further readings at the end of the GRIPS course - which of the following statements is true?\n\n Both books can be downloaded for free. Higher model complexity is always better for predicting."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#unsupervised-learning",
    "href": "A2-MachineLearningTasks.html#unsupervised-learning",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.2 Unsupervised Learning",
    "text": "2.2 Unsupervised Learning\nIn unsupervised learning, we want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider the iris data set. Here, we have 150 observations of 4 floral traits:\n\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4])\nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits,\n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\n\n\n\n\nTrait distributions of iris dataset\n\n\n\n\nThe observations are from 3 species and indeed those species tend to have different traits, meaning that the observations form 3 clusters.\n\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])\n\n\n\n\nScatterplots for trait-trait combinations.\n\n\n\n\nHowever, imagine we don’t know what species are, what is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does.\n\n2.2.1 Hierarchical Clustering\nA cluster refers to a collection of data points aggregated together because of certain similarities.\nIn hierarchical clustering, a hierarchy (tree) between data points is built.\n\nAgglomerative: Start with each data point in their own cluster, merge them up hierarchically.\nDivisive: Start with all data points in one cluster, and split hierarchically.\n\nMerges / splits are done according to linkage criterion, which measures distance between (potential) clusters. Cut the tree at a certain height to get clusters.\nHere an example\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nd = dist(traits)\nhc = hclust(d, method = \"complete\")\n\nplot(hc, main=\"\")\nrect.hclust(hc, k = 3)  # Draw rectangles around the branches.\n\n\n\n\nResults of hierarchical clustering. Red rectangle is drawn around the corresponding clusters.\n\n\n\n\nSame plot, but with colors for true species identity\n\nlibrary(ape)\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes.\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\n\nCalculate confusion matrix. Note we are switching labels here so that it fits to the species.\n\ntmp = hcRes3\ntmp[hcRes3 == 2] = 3\ntmp[hcRes3 == 3] = 2\nhcRes3 = tmp\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n27\n1\n\n\n0\n23\n49\n\n\n\n\n\nNote that results might change if you choose a different agglomeration method, distance metric or scale of your variables. Compare, e.g. to this example:\n\nhc = hclust(d, method = \"ward.D2\")\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes. Different agglomeration method\n\n\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\nWhich method is best? firstsecond\n\nlibrary(dendextend)\n\n\nset.seed(123)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n             \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\nout = dendlist()   # Create a dendlist object from several dendrograms.\nfor(method in methods){\n  res = hclust(d, method = method)\n  out = dendlist(out, as.dendrogram(res))\n}\nnames(out) = methods\nprint(out)\n\n$ward.D\n'dendrogram' with 2 branches and 150 members total, at height 199.6205 \n\n$single\n'dendrogram' with 2 branches and 150 members total, at height 1.640122 \n\n$complete\n'dendrogram' with 2 branches and 150 members total, at height 7.085196 \n\n$average\n'dendrogram' with 2 branches and 150 members total, at height 4.062683 \n\n$mcquitty\n'dendrogram' with 2 branches and 150 members total, at height 4.497283 \n\n$median\n'dendrogram' with 2 branches and 150 members total, at height 2.82744 \n\n$centroid\n'dendrogram' with 2 branches and 150 members total, at height 2.994307 \n\n$ward.D2\n'dendrogram' with 2 branches and 150 members total, at height 32.44761 \n\nattr(,\"class\")\n[1] \"dendlist\"\n\nget_ordered_3_clusters = function(dend){\n  # order.dendrogram function returns the order (index)\n  # or the \"label\" attribute for the leaves.\n  # cutree: Cut the tree (dendrogram) into groups of data.\n  cutree(dend, k = 3)[order.dendrogram(dend)]\n}\ndend_3_clusters = lapply(out, get_ordered_3_clusters)\n\n# Calculate Fowlkes-Mallows Index (determine the similarity between clusterings)\ncompare_clusters_to_iris = function(clus){\n  FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)\n}\n\nclusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\ndotchart(sort(clusters_performance), xlim = c(0.3, 1),\n         xlab = \"Fowlkes-Mallows index\",\n         main = \"Performance of linkage methods\n         in detecting the 3 species \\n in our example\",\n         pch = 19)\n\n\n\n\nWe might conclude that ward.D2 works best here. However, as we will learn later, optimizing the method without a hold-out for testing implies that our model may be overfitting. We should check this using cross-validation.\n\n\n2.2.2 K-means Clustering\nAnother example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms.\nTo start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the assumed or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere. Afterwards each data point is assigned to the respective cluster that raises the overall in-cluster sum of squares (variance) related to the distance to the centroid least of all. After the algorithm has placed all data points into a cluster the centroids get updated. By iterating this procedure until the assignment doesn’t change any longer, the algorithm can find the (locally) optimal centroids and the data points belonging to this cluster. Note that results might differ according to the initial positions of the centroids. Thus several (locally) optimal solutions might be found.\nThe “k” in K-means refers to the number of clusters and the ‘means’ refers to averaging the data-points to find the centroids.\nA typical pipeline for using k-means clustering looks the same as for other algorithms. After having visualized the data, we fit a model, visualize the results and have a look at the performance by use of the confusion matrix. By setting a fixed seed, we can ensure that results are reproducible.\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n\nK-means clustering with 3 clusters of sizes 50, 62, 38\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.006000    3.428000     1.462000    0.246000\n2     5.901613    2.748387     4.393548    1.433871\n3     6.850000    3.073684     5.742105    2.071053\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 15.15100 39.82097 23.87947\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nVisualizing the results. Color codes true species identity, symbol shows cluster result.\n\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\n\n\n\n\nWe see that there are are some discrepancies. Confusion matrix:\n\ntable(iris$Species, kc$cluster)\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36\n\n\nIf you want to animate the clustering process, you could run\n\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\n\nElbow technique to determine the probably best suited number of clusters:\n\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))\n\n\n\n\nOften, one is interested in sparse models. Furthermore, higher k than necessary tends to overfitting. At the kink in the picture, the sum of squares dropped enough and k is still low enough. But keep in mind, this is only a rule of thumb and might be wrong in some special cases.\n\n\n2.2.3 Density-based Clustering\nDetermine the affinity of a data point according to the affinity of its k nearest neighbors. This is a very general description as there are many ways to do so.\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nset.seed(123)\n\nkNNdistplot(traits, k = 4)   # Calculate and plot k-nearest-neighbor distances.\nabline(h = 0.4, lty = 2)\n\n\n\ndc = dbscan(traits, eps = 0.4, minPts = 6)\nprint(dc)\n\nDBSCAN clustering for 150 objects.\nParameters: eps = 0.4, minPts = 6\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 4 cluster(s) and 32 noise points.\n\n 0  1  2  3  4 \n32 46 36 14 22 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\n\n\nlibrary(factoextra)\n\n\nfviz_cluster(dc, traits, geom = \"point\", ggtheme = theme_light())\n\n\n\n\n\n\n2.2.4 Model-based Clustering\nThe last class of methods for unsupervised clustering are so-called model-based clustering methods.\n\nlibrary(mclust)\n\nPackage 'mclust' version 6.0.0\nType 'citation(\"mclust\")' for citing this R package in publications.\n\n\n\nmb = Mclust(traits)\n\nMclust automatically compares a number of candidate models (clusters, shape) according to BIC (The BIC is a criterion for classifying algorithms depending their prediction quality and their usage of parameters). We can look at the selected model via:\n\nmb$G # Two clusters.\n\n[1] 2\n\nmb$modelName # &gt; Ellipsoidal, equal shape.\n\n[1] \"VEV\"\n\n\nWe see that the algorithm prefers having 2 clusters. For better comparability to the other 2 methods, we will override this by setting:\n\nmb3 = Mclust(traits, 3)\n\nResult in terms of the predicted densities for 3 clusters\n\nplot(mb3, \"density\")\n\n\n\n\nPredicted clusters:\n\nplot(mb3, what=c(\"classification\"), add = T)\n\n\n\n\nConfusion matrix:\n\ntable(iris$Species, mb3$classification)\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\n\n\n2.2.5 Ordination\nOrdination is used in explorative analysis and compared to clustering, similar objects are ordered together. So there is a relationship between clustering and ordination. Here a PCA ordination on on the iris data set.\n\npcTraits = prcomp(traits, center = TRUE, scale. = TRUE)\nbiplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25))\n\n\n\n\nYou can cluster the results of this ordination, ordinate before clustering, or superimpose one on the other.\n\n\n2.2.6 Exercise\n\n\n\n\n\n\nTask\n\n\n\nGo through the 4(5) algorithms above, and check if they are sensitive (i.e. if results change) if you scale the input features (= predictors), instead of using the raw data. Discuss in your group: Which is more appropriate for this analysis and/or in general: Scaling or not scaling?\n\n\nClick here to see the solution for hierarchical clustering\n\n\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)\n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n\n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n\n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\nIt seems that scaling is harmful for hierarchical clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for K-means\n\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n\n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n\n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n\n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\nIt seems that scaling is harmful for K-means clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for density-based clustering\n\n\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\nLevels: 1 2 3\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n\n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n\n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     3 47  0  0  0\n      2     5  0 38  3  4\n      3    17  0  0 33  0\n\nCorrect classified points:  118  /  150\nSum of noise points:  25 \n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     9 41  0  0  0\n      2    14  0 36  0  0\n      3    36  0  1  4  9\n\nCorrect classified points:  81  /  150\nSum of noise points:  59 \n\n\nIt seems that scaling is harmful for density based clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for model-based clustering\n\n\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  mb3 = Mclust(traits, 3)\n\n  tbl = table(iris$Species, mb3$classification)\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n\nFor model based clustering, scaling does not matter.\n\n\n\nClick here to see the solution for ordination\n\n\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n\n\n\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n\n\n\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")\n\n\n\n\nFor PCA ordination, scaling matters. Because we are interested in directions of maximal variance, all parameters should be scaled, or the one with the highest values might dominate all others."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#supervised-learning",
    "href": "A2-MachineLearningTasks.html#supervised-learning",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.3 Supervised Learning",
    "text": "2.3 Supervised Learning\nThe two most prominent branches of supervised learning are regression and classification. Fundamentally, classification is about predicting a label and regression is about predicting a continuous variable. The following video explains that in more depth:\n\n\n\n2.3.1 Regression\nThe random forest (RF) algorithm is possibly the most widely used machine learning algorithm and can be used for regression and classification. We will talk more about the algorithm later.\nFor the moment, we want to go through a typical workflow for a supervised regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. We will again use the iris data set that we used before. The goal is now to predict Sepal.Length based on the information about the other variables (including species).\nFitting the model:\n\nlibrary(randomForest)\nset.seed(123)\n\nSepal.Length is a numerical variable:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nhist(iris$Sepal.Length)\n\n\n\n\nThe randomForest can be used similar to a linear regression model, we can specify the features using the formula syntax (~. means that all other variables should be used as features):\n\nm1 = randomForest(Sepal.Length ~ ., data = iris)   # ~.: Against all others.\nprint(m1)\n\n\nCall:\n randomForest(formula = Sepal.Length ~ ., data = iris) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1364625\n                    % Var explained: 79.97\n\n\nAs many other ML algorithms, the RF is not interpretable, so we don’t get coefficients that connect the variables to the response. But, at least we get the variable importance which is similar to an anova, telling us which variables were the most important ones:\n\nvarImpPlot(m1)\n\n\n\n\nAnd the finally, we can use the model to make predictions using the predict method:\n\nplot(predict(m1), iris$Sepal.Length, xlab = \"Predicted\", ylab = \"Observed\")\nabline(0, 1)\n\n\n\n\nTo understand the structure of a random forest in more detail, we can use a package from GitHub.\n\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\nHere, one of the regression trees is shown.\n\n\n2.3.2 Classification\nWith the random forest, we can also do classification. The steps are the same as for regression tasks, but we can additionally see how well it performed by looking at the confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represents the instances in the actual class. Thus the diagonals are the correctly predicted classes and the off-diagonal elements are the falsely classified elements.\nSpecies is a factor with three levels:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nFitting the model (syntax is the same as for the regression task):\n\nset.seed(123)\nlibrary(randomForest)\nm1 = randomForest(Species ~ ., data = iris)\nprint(m1)\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n\nvarImpPlot(m1)\n\n\n\n\nPredictions:\n\nhead(predict(m1))\n\n     1      2      3      4      5      6 \nsetosa setosa setosa setosa setosa setosa \nLevels: setosa versicolor virginica\n\n\nConfusion matrix:\n\ntable(predict(m1), as.integer(iris$Species))\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 47  4\n  virginica   0  3 46\n\n\nOur model made a few errors.\nVisualizing results ecologically:\n\nplot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = \"Observed\")\n\n\n\nplot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = \"Predicted\")\n\n\n\n\nVisualizing one of the fitted models:\n\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\nConfusion matrix:\n\nknitr::kable(table(predict(m1), iris$Species))\n\n\n\n2.3.3 Exercise\nUsing a random forest on the iris dataset, which parameter would be more important (remember there is a function to check this) to predict Petal.Width?\n\n Species. Sepal.Width.\n\n\n\n\n\n\n\nTask: First deep neural network\n\n\n\nDeep neural networks are currently the state of the art in unsupervised learning. Their ability to model different types of data (e.g. graphs, images) is one of the reasons for their rise in recent years. However, their use beyond tabular data (tabular data == features have specific meanings) requires extensive (programming) knowledge of the underlying deep learning frameworks (e.g. TensorFlow or PyTorch), which we will teach you in two days. For tabular data, we can use packages like cito, which work similarly to regression functions like lm and allow us to train deep neural networks in one line of code.\nA demonstration with the iris dataset:\n\nlibrary(cito)\n\n# always scale your features when using DNNs\niris_scaled = iris\niris_scaled[,1:4] = scale(iris_scaled[,1:4])\n\n# the default architecture is 3 hidden layers, each with 10 hidden nodes (we will talk on Wednesday more about the architecture)\n# Similar to a lm/glm we have to specify the response/loss family, for multi-target (3 species) we use the softmax loss function\nmodel = dnn(Species~., lr = 0.1,data = iris_scaled, loss = \"softmax\")\n\n\n\n\nDNNs are not interpretable, i.e. no coefficients (slopes) that tell us how the features affect the response, however, similar to the RF, we can calculate a ‘variable importance’ which is similar to an anova:\n\nsummary(model)\n\nDeep Neural Network Model summary\nModel generated on basis of: \nFeature Importance:\n      variable importance\n1 Sepal.Length   1.010446\n2  Sepal.Width   1.106239\n3 Petal.Length   1.435946\n4  Petal.Width   1.475746\n\n\nPredictions\n\nhead(predict(model))\n\n        setosa   versicolor    virginica\n[1,] 0.9994506 0.0005492222 1.225359e-07\n[2,] 0.9984201 0.0015795394 4.002443e-07\n[3,] 0.9993551 0.0006447551 1.658843e-07\n[4,] 0.9990097 0.0009900426 2.514017e-07\n[5,] 0.9996005 0.0003993943 8.292547e-08\n[6,] 0.9993541 0.0006458615 1.188908e-07\n\n\nWe get three columns, one for each species, and they are probabilities.\n\nplot(iris$Sepal.Length, iris$Sepal.Width, col = apply(predict(model), 1, which.max))\n\n\n\n\nPerformance:\n\ntable(apply(predict(model), 1, which.max), as.integer(iris$Species))\n\n   \n     1  2  3\n  1 50  0  0\n  2  0 50  7\n  3  0  0 43\n\n\nTask:\n\npredict Sepal.Length instead of Species (classification -&gt; regression)\nUse the ‘mse’ loss function\nPlot predicted vs observed\n\n\n\nClick here to see the solution\n\nRegression:\nlosses such as “mse” (mean squared error) or the “msa” (mean absolute error) are used for regression tasks\n\nmodel = dnn(Sepal.Length~., lr = 0.1,data = iris_scaled, loss = \"mse\")\n\n\n\n\n\nsummary(model)\n\nDeep Neural Network Model summary\nModel generated on basis of: \nFeature Importance:\n      variable importance\n1  Sepal.Width  1.3924243\n2 Petal.Length  9.7724334\n3  Petal.Width  0.9492258\n4      Species  1.4818581\n\n\n\nplot(iris_scaled$Sepal.Length, predict(model))\n\n\n\n\nCalculate \\(R^2\\):\n\ncor(iris_scaled$Sepal.Length, predict(model))**2\n\n          [,1]\n[1,] 0.8769866"
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#understanding-the-bias-variance-trade-off",
    "href": "A3-BiasVarianceTradeOff.html#understanding-the-bias-variance-trade-off",
    "title": "3  Bias-variance trade-off",
    "section": "3.1 Understanding the bias-variance trade-off",
    "text": "3.1 Understanding the bias-variance trade-off\n\n\nWhich of the following statements about the bias-variance trade-off is correct? (see figure above)\n\n The goal of considering the bias-variance trade-off is to realize that increasing complexity typically leads to more flexibility (allowing you to reduce bias) but at the cost of uncertainty (variance) in the estimated parameters. The goal of considering the bias-variance trade-off is to get the bias of the model as small as possible."
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "href": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "title": "3  Bias-variance trade-off",
    "section": "3.2 Optimizing the bias-variance trade-off",
    "text": "3.2 Optimizing the bias-variance trade-off\nOptimizing the bias-variance trade-off means adjusting the complexity of the model which can be achieved by:\n\nFeature selection (more features increases the flexibility of the model)\nRegularization\n\n\n3.2.1 Feature selection\nAdding features increases the flexibility of the model and the goodness of fit:\n\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(BostonHousing)\ndata = BostonHousing\n\nsummary(lm(medv~rm, data = data))\n\n\nCall:\nlm(formula = medv ~ rm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -34.671      2.650  -13.08   &lt;2e-16 ***\nrm             9.102      0.419   21.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,    Adjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~rm+dis, data = data))$r.squared\n\n[1] 0.4955246\n\nsummary(lm(medv~., data = data))$r.squared\n\n[1] 0.7406427\n\n# Main effects + all potential interactions:\nsummary(lm(medv~.^2, data = data))$r.squared\n\n[1] 0.9211876\n\n\nThe model with all features and their potential interactions has the highest \\(R^2\\), but it also has the highest uncertainty because there are on average only 5 observations for each parameter (92 parameters and 506 observations). So how do we decide which level of complexity is appropriate for our task? For the data we use to train the model, \\(R^2\\) will always get better with higher model complexity, so it is a poor decision criterion. We will show this in the Section 3.2.4 section. In short, the idea is that we need to split the data so that we have an evaluation (test) dataset that wasn’t used to train the model, which we can then use in turn to see if our model generalizes well to new data.\n\n\n3.2.2 Regularization\nRegularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a machine learning model. The most important distinction is between shrinkage estimators and estimators based on model averaging.\nShrikage estimators are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are “shrunk” to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penalty is proportional to the sum of absolute deviations (\\(L1\\) penalty), and the Tikhonov regularization aka Ridge regression, where the penalty is proportional to the sum of squared distances from the reference (\\(L2\\) penalty). Thus, the loss function that we optimize is given by\n\\[\nloss = fit - \\lambda \\cdot d\n\\]\nwhere fit refers to the standard loss function, \\(\\lambda\\) is the strength of the regularization, and \\(d\\) is the chosen metric, e.g. \\(L1\\) or\\(L2\\):\n\\[\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n\\]\n\\[\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n\\]\n\\(\\lambda\\) and possibly d are typically optimized under cross-validation. \\(L1\\) and \\(L2\\) can be also combined what is then called elastic net (see Zou and Hastie (2005)).\nModel averaging refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it is often more efficient having many simpler models and average them, than one “super model”. The reasons are complicated, and explained in more detail in Dormann et al. (2018).\nA particular important application of averaging is boosting, where the idea is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap (use random sampling with replacement ) the data, and average the bootstrapped predictions.\nTo see how these techniques work in practice, let’s first focus on LASSO and Ridge regularization for weights in neural networks. We can imagine that the LASSO and Ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunken) to zero.\nLASSO \\(\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)\\) and Ridge \\(\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)\\) have slightly different properties. They are best understood if we express those as the effective prior preference they create on the parameters:\n\n\n\n\n\nAs you can see, the LASSO creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the Ridge. For this reason, LASSO is often more interpreted as a model selection method.\nThe Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.\n\n3.2.2.1 Ridge - Example\nWe can use the glmnet package for Ridge, LASSO, and elastic-net regressions.\nWe want to predict the house prices of Boston (see help of the dataset):\n\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %&gt;% select(-medv, -chas) %&gt;% scale()\n\nhist(cor(X))\n\n\n\n\n\nm1 = glmnet(y = Y, x = X, alpha = 0)\n\nThe glmnet function automatically tries different values for lambda:\n\ncbind(coef(m1, s = 0.001), coef(m1, s = 100.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1          s1\n(Intercept) 22.53280632 22.53280632\ncrim        -0.79174957 -0.21113427\nzn           0.76313031  0.18846808\nindus       -0.17037817 -0.25120998\nnox         -1.32794787 -0.21314250\nrm           2.85780876  0.46463202\nage         -0.05389395 -0.18279762\ndis         -2.38716188  0.07906631\nrad          1.42772476 -0.17967948\ntax         -1.09026758 -0.24233282\nptratio     -1.93105019 -0.31587466\nb            0.86718037  0.18764060\nlstat       -3.43236617 -0.46055837\n\n\n\n\n3.2.2.2 LASSO - Example\nBy changing \\(alpha\\) to 1.0 we use a LASSO instead of a Ridge regression:\n\nm2 = glmnet(y = Y, x = X, alpha = 1.0)\ncbind(coef(m2, s = 0.001), coef(m2, s = 0.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1           s1\n(Intercept) 22.53280632 22.532806324\ncrim        -0.95543108 -0.135047323\nzn           1.06718108  .          \nindus        0.21519500  .          \nnox         -1.95945910 -0.000537715\nrm           2.71666891  2.998520195\nage          0.05184895  .          \ndis         -3.10566908 -0.244045205\nrad          2.73963771  .          \ntax         -2.20279273  .          \nptratio     -2.13052857 -1.644234575\nb            0.88420283  0.561686909\nlstat       -3.80177809 -3.682148016\n\n\n\n\n3.2.2.3 Elastic-net - Example\nBy setting \\(alpha\\) to a value between 0 and 1.0, we use a combination of LASSO and Rdige:\n\nm3 = glmnet(y = Y, x = X, alpha = 0.5)\ncbind(coef(m3, s = 0.001), coef(m3, s = 0.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1         s1\n(Intercept) 22.53280632 22.5328063\ncrim        -0.95716118 -0.3488473\nzn           1.06836343  0.1995842\nindus        0.21825187  .        \nnox         -1.96211736 -0.7613698\nrm           2.71859592  3.0137090\nage          0.05299551  .        \ndis         -3.10330132 -1.3011740\nrad          2.73321635  .        \ntax         -2.19638611  .        \nptratio     -2.13041090 -1.8051547\nb            0.88458269  0.6897165\nlstat       -3.79836182 -3.6136853\n\n\n\n\n\n3.2.3 Hyperparameters\nGenerally, parameters such as \\(\\lambda\\) and \\(\\alpha\\) that, for example, control the complexity or other parameters that control their learning or the optimization are called hyperparameters. Comming back to our glmnet example:\nWe can plot the effect of \\(\\lambda\\) on the effect estimates:\n\nplot(m1)\n\n\n\n\nSo which lambda should we choose now? If we calculate the model fit for different lambdas (e.g. using the RMSE):\n\nlambdas = seq(0.001, 1.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = X, s = l)\n    RMSE = Metrics::rmse(Y, prediction)\n    return(RMSE)\n    })\nplot(lambdas, RMSEs)\n\n\n\n\nWe see that the lowest lambda achieved the highest RMSE - which is not surprising because the unconstrained model, the most complex model, has the highest fit, so no bias but probably high variance (with respect to the bias-variance tradeoff).\n\n3.2.3.1 Split data into training and testing\nWe want a model that generalizes well to new data, which we need to “simulate” here by splitting of a holdout before the training and using the holdout then for testing our model:\n\nset.seed(1)\nlibrary(mlbench)\nlibrary(dplyr)\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %&gt;% select(-medv, -chas) %&gt;% scale()\n\n# Split data\nindices = sample.int(nrow(X), 0.2*nrow(X))\ntrain_X = X[indices,]\ntest_X = X[-indices,]\ntrain_Y = Y[indices]\ntest_Y = Y[-indices]\n\n# Train model on train data\nm1 = glmnet(y = train_Y, x = train_X, alpha = 0.2)\n\n# Test model on test data\npred = predict(m1, newx = test_X, s = 0.01)\n\n# Calculate performance on test data\nMetrics::rmse(test_Y, pred)\n\n[1] 5.062992\n\n\nLet’s do it again for different values of lambdas:\n\nlambdas = seq(0.0000001, 0.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = test_X, s = l, alpha = 1.0)\n    return(Metrics::rmse(test_Y, prediction))\n    })\nplot(lambdas, RMSEs, xlab = \"Lambda\", ylab = \"RMSE\", type = \"l\", las = 2)\nabline(v = lambdas[which.min(RMSEs)], col = \"red\", lwd = 1.5)\n\n\n\n\nHyperparameter tuning describes the process of finding the optimal set of hyperparameters for a certain task. They are usually data specific, so they have to tuned for each dataset.\nIf we do only one split it could happen that we only find a set of hyperparameters that are best suited for this specific split and thus we usally do several splits so that each observation is once an observation in the test dataset, cross-validation\n\n\n\n3.2.4 Cross-validation\nThe cv.glmnet function does per default a 10xCV (so 10 splits) and in each split different values for \\(\\lambda\\) are tested (based on the deviance (-2xlogLik)):\n\nm1 = glmnet::cv.glmnet(x = train_X, y = train_Y, alpha = 0.2)\nm1\n\n\nCall:  glmnet::cv.glmnet(x = train_X, y = train_Y, alpha = 0.2) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure    SE Nonzero\nmin  0.083    65   27.21 9.803      12\n1se  5.969    19   36.46 9.486       6\n\nplot(m1)\n\n\n\n\nSo low values of \\(\\lambda\\) seem to achieve the lowest Error, so the higehst predictive performance.\nIf we want to tune \\(\\alpha\\) and \\(\\lambda\\) simoustanously, we need the glmnetUtils package:\n\nlibrary(glmnetUtils)\n\n\nAttaching package: 'glmnetUtils'\n\n\nThe following objects are masked from 'package:glmnet':\n\n    cv.glmnet, glmnet\n\nm2 = cva.glmnet(x = X, y = Y, alpha = seq(0, 1.0, length.out = 10), lambdas = seq(0.0001, 0.2, length.out = 10), nfolds = 3)\n\nplot(m2)\n\n\n\n\n\n\n\n\nDormann, Carsten F, Justin M Calabrese, Gurutzeta Guillera-Arroita, Eleni Matechou, Volker Bahn, Kamil Bartoń, Colin M Beale, et al. 2018. “Model Averaging in Ecology: A Review of Bayesian, Information-Theoretic, and Tactical Approaches for Predictive Inference.” Ecological Monographs 88 (4): 485–504.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20."
  },
  {
    "objectID": "A4-MLpipeline.html#data-preparation",
    "href": "A4-MLpipeline.html#data-preparation",
    "title": "4  Machine learning pipeline",
    "section": "4.1 Data preparation",
    "text": "4.1 Data preparation\nLoad necessary libraries:\n\nlibrary(tidyverse)\n\nLoad data set:\n\nlibrary(EcoData)\ndata(titanic_ml)\ndata = titanic_ml\n\nStandard summaries:\n\nstr(data)\n\n'data.frame':   1309 obs. of  14 variables:\n $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\nsummary(data)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n\n\nThe name variable consists of 1309 unique factors (there are 1309 observations…) and could be now transformed. If you are interested in how to do that, take a look at the following box.\n\n\n\n\n\n\nFeature engineering of the name variable\n\n\n\n\n\n\nlength(unique(data$name))\n\n[1] 1307\n\n\nHowever, there is a title in each name. Let’s extract the titles:\n\nWe will extract all names and split each name after each comma “,”.\nWe will split the second split of the name after a point “.” and extract the titles.\n\n\nfirst_split = sapply(data$name,\n                     function(x) stringr::str_split(x, pattern = \",\")[[1]][2])\ntitles = sapply(first_split,\n                function(x) strsplit(x, \".\",fixed = TRUE)[[1]][1])\n\nWe get 18 unique titles:\n\ntable(titles)\n\ntitles\n         Capt           Col           Don          Dona            Dr \n            1             4             1             1             8 \n     Jonkheer          Lady         Major        Master          Miss \n            1             1             2            61           260 \n         Mlle           Mme            Mr           Mrs            Ms \n            2             1           757           197             2 \n          Rev           Sir  the Countess \n            8             1             1 \n\n\nA few titles have a very low occurrence rate:\n\ntitles = stringr::str_trim((titles))\ntitles %&gt;%\n fct_count()\n\n# A tibble: 18 × 2\n   f                n\n   &lt;fct&gt;        &lt;int&gt;\n 1 Capt             1\n 2 Col              4\n 3 Don              1\n 4 Dona             1\n 5 Dr               8\n 6 Jonkheer         1\n 7 Lady             1\n 8 Major            2\n 9 Master          61\n10 Miss           260\n11 Mlle             2\n12 Mme              1\n13 Mr             757\n14 Mrs            197\n15 Ms               2\n16 Rev              8\n17 Sir              1\n18 the Countess     1\n\n\nWe will combine titles with low occurrences into one title, which we can easily do with the forcats package.\n\ntitles2 =\n  forcats::fct_collapse(titles,\n                        officer = c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\"),\n                        royal = c(\"Jonkheer\", \"Don\", \"Sir\",\n                                  \"the Countess\", \"Dona\", \"Lady\"),\n                        miss = c(\"Miss\", \"Mlle\"),\n                        mrs = c(\"Mrs\", \"Mme\", \"Ms\")\n                        )\n\nWe can count titles again to see the new number of titles:\n\ntitles2 %&gt;%  \n   fct_count()\n\n# A tibble: 6 × 2\n  f           n\n  &lt;fct&gt;   &lt;int&gt;\n1 officer    23\n2 royal       6\n3 Master     61\n4 miss      262\n5 mrs       200\n6 Mr        757\n\n\nAdd new title variable to data set:\n\ndata =\n  data %&gt;%\n    mutate(title = titles2)\n\n\n\n\n\n4.1.1 Imputation\nNAs are a common problem in ML. For example, the age variable has 20% NAs:\n\nsummary(data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1667 21.0000 28.0000 29.8811 39.0000 80.0000     263 \n\nsum(is.na(data$age)) / nrow(data)\n\n[1] 0.2009167\n\n\nEither we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups. In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age….)) will be done within the specified groups.\n\ndata =\n  data %&gt;%\n    group_by(sex, pclass, title) %&gt;%\n    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;%\n    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %&gt;%\n    ungroup()\n\n\n\n4.1.2 Preprocessing and Feature Selection\nLater (tomorrow), we want to use Keras in our example, but it cannot handle factors and requires the data to be scaled.\nNormally, one would do this for all predictors, but as we only show the pipeline here, we have sub-selected a bunch of predictors and do this only for them. We first scale the numeric predictors and change the factors with only two groups/levels into integers (this can be handled by Keras).\n\ndata_sub =\n  data %&gt;%\n    select(survived, sex, age2, fare2, pclass) %&gt;%\n    mutate(age2 = scales::rescale(age2, c(0, 1)),\n           fare2 = scales::rescale(fare2, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\n\n\n\n\n\n\n\nTransforming factors with more than two levels\n\n\n\n\n\nFactors with more than two levels should be one hot encoded (Make columns for every different factor level and write 1 in the respective column for every taken feature value and 0 else. For example: \\(\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}\\)):\n\none_title = model.matrix(~0+as.factor(title), data = data)\ncolnames(one_title) = levels(data$title)\n\none_sex = model.matrix(~0+as.factor(sex), data = data)\ncolnames(one_sex) = levels(data$sex)\n\none_pclass = model.matrix(~0+as.factor(pclass), data = data)\ncolnames(one_pclass) = paste0(\"pclass\", 1:length(unique(data$pclass)))\n\nAnd we have to add the dummy encoded variables to the data set:\n\ndata = cbind(data.frame(survived= data$survived),\n                 one_title, one_sex, age = data$age2,\n                 fare = data$fare2, one_pclass)\nhead(data)\n\n  survived officer royal Master miss mrs Mr female male  age   fare pclass1\n1        1       0     0      0    1   0  0      1    0 30.0 13.000       0\n2        1       0     0      0    0   0  1      0    1 41.5 35.500       1\n3        0       0     0      0    0   0  1      0    1 26.0 69.550       0\n4        0       0     0      1    0   0  0      0    1  6.0 21.075       0\n5        0       0     0      0    0   0  1      0    1 30.5  8.050       0\n6        0       0     0      0    0   0  1      0    1 38.5  7.250       0\n  pclass2 pclass3\n1       1       0\n2       0       0\n3       0       1\n4       0       1\n5       0       1\n6       0       1\n\n\n\n\n\n\n\n4.1.3 Split data\nTo tune our hyperparameters and evaluate our models, we need to split the data as we learned in the CV section. Before doing so, however, we must split off the new observations in the data set :\n\nsummary(data_sub$survived)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.0000  0.0000  0.3853  1.0000  1.0000     655 \n\n\n655 observations have NAs in our response variable, these are the observations for which we want to make predictions at the end of our pipeline (we have no information about their actual values!).\n\ndata_new = data_sub[!is.na(data_sub$survived),]\ndata_obs = data_sub[is.na(data_sub$survived),]"
  },
  {
    "objectID": "A4-MLpipeline.html#modelling",
    "href": "A4-MLpipeline.html#modelling",
    "title": "4  Machine learning pipeline",
    "section": "4.2 Modelling",
    "text": "4.2 Modelling\n\n4.2.1 Training and evaluation\nNow, we can do a simple 10xCV with the observed_data:\n\nlibrary(glmnet)\nlibrary(glmnetUtils)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 10\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = 1:cv,\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = 0.2)\n  results[i, 2] = Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                            alpha = 0.2,\n                                                            s = 0.01,\n                                                            type = \"response\"))\n\n}\n\n\nprint(results)\n\n   set       AUC\n1    1 0.8690476\n2    2 0.8810916\n3    3 0.7718496\n4    4 0.7570732\n5    5 0.8964497\n6    6 0.7244444\n7    7 0.7878049\n8    8 0.8711390\n9    9 0.8820346\n10  10 0.7904412\n\nprint(mean(results$AUC))\n\n[1] 0.8231376"
  },
  {
    "objectID": "A4-MLpipeline.html#predictions-and-submission",
    "href": "A4-MLpipeline.html#predictions-and-submission",
    "title": "4  Machine learning pipeline",
    "section": "4.3 Predictions and Submission",
    "text": "4.3 Predictions and Submission\nWhen we are satisfied with the performance of our model, we will create predictions for the new observations on the submission server. But first we will train now our model on the full observed dataset:\n\nmodel = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = 0.2)\n\nAnd make the predictions for the new observations:\n\npred = model %&gt;% \n  predict(as.matrix(data_new), s = 0.01,alpha = 0.2, type = \"response\")\n\nWarning in (function (formula, data, weights = NULL, offset = NULL, subset =\nNULL, : input data was converted to data.frame\n\nhead(pred, n = 3)\n\n            s1\n[1,] 0.7525265\n[2,] 0.3697726\n[3,] 0.1376621\n\n\nWe cannot assess the performance on the test split because the true survival ratio is unknown, however, we can now submit our predictions to the submission server at http://rhsbio7.uni-regensburg.de:8500.\nFor the submission it is critical to change the predictions into a data.frame, select the second column (the probability to survive), and save it with the write.csv function:\n\nwrite.csv(data.frame(y = pred[,1] ), file = \"Max_1.csv\")\n\nThe file name is used as the ID on the submission server, so change it to whatever you want as long as you can identify yourself.\n\n4.3.1 Exercises\n\n\n\n\n\n\nTask: Improve predictions\n\n\n\nPlay around with the feature engineering and the hyperparameters (\\(\\lambda\\) and \\(\\alpha\\)). Try to improve the AUC for the new observations on the submission server."
  },
  {
    "objectID": "A4-MLpipeline.html#hyperparameter-optimization",
    "href": "A4-MLpipeline.html#hyperparameter-optimization",
    "title": "4  Machine learning pipeline",
    "section": "4.4 Hyperparameter optimization",
    "text": "4.4 Hyperparameter optimization\nWe did a simple 10xCV to evaluate our model but we didn’t tune our hyperparameters (\\(\\lambda\\) and \\(\\alpha\\)). If we want to tune them, we need do another CV within each split of the model evaulation CV, which is called nested CV.\nWe used only one split (the split for the submission server doesn’t count) to evaluate the performance of the model before we made the final predictions. If we test many different hyperparameter combinations, how do we ensure that a certain hyperparameter is not only good for our training dataset but also good for the new data (our outer split on the submission server)? You may have guessed it already, we need to do another CV within the previous CV to check whether a certain hyperparameter solution generalizes to the whole data. To tune \\(\\lambda\\), we would need to split the data another time (called nested CV).\nWhy is it important to tune hyperparameters? Hyperparameters (configuration parameters of our ML algorithms that (mostly) control their complexity) are usually tuned (optimized) in an automatic / systematic way. A common procedure, called random search, is to sample random configuration combinations from the set of hyperparameters and test for each combination the prediction error.\nLet’s implement manually a nested CV to tune the \\(\\alpha\\). Let’s start with a 5CVx5CV and 20x different alpha values:\n\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(20,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  # inner split\n  for(j in 1:cv_inner) {\n    inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n    train_inner = train_outer[inner_split != j, ]\n    test_inner = train_outer[inner_split == j, ]\n    \n    tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = 0.01,\n                                                         type = \"response\")))\n      })\n    best_alpha = hyper_alpha[which.max(tuning_results_inner)]\n  }\n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = 0.01, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n\n  set     alpha       AUC\n1   1 0.9148060 0.8779412\n2   2 0.6417455 0.7638128\n3   3 0.9148060 0.8201346\n4   4 0.6417455 0.8265602\n5   5 0.2861395 0.8166425\n\n\n\\(\\alpha = 0.915\\) has the highest AUC, let’s use it to train the model on the full dataset:\n\nmodel = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = 0.915)\n\nAnd again, make predictions and submit:\n\ndata_new = data_sub[is.na(data_sub$survived),]\nwrite.csv(data.frame(y = predict(model, data_new, alpha = 0.915, s = 0.01, type = \"response\")[,1] ), file = \"Max_1.csv\")\n\nWe have now used the \\(\\alpha\\) value with the highest AUC here, but our tuning has shown that the best value of \\(\\alpha\\) depends on the partitioning, so it would probably be better to build ten models and combine their predictions (e.g., by averaging the predictions):\n\nprediction_ensemble = \n  sapply(results$alpha, function(alpha) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = alpha)\n    return(predict(model, data_new, alpha = alpha, s = 0.01, type = \"response\")[,1])\n  })\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean), file = \"Max_1.csv\"))\n\n\n\n\n\n\n\nExercise: Tuning \\(\\alpha\\) and \\(\\lambda\\)\n\n\n\n\nExtend the code from above and tune \\(\\alpha\\) and \\(\\lambda\\)\nTrain the model with best set of hyperparameters and submit your predictions\nCompare the predictive performance from the single best model with the ensemble model\n\nSubmit both predictions (http://rhsbio7.uni-regensburg.de:8500/), which model has a higher AUC?\n\n\nClick here to see the solution for the single model\n\n\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(30,0, 1)\nhyper_lambda = runif(30,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  # inner split\n  for(j in 1:cv_inner) {\n    inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n    train_inner = train_outer[inner_split != j, ]\n    test_inner = train_outer[inner_split == j, ]\n    \n    tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\")))\n      })\n    best_alpha = hyper_alpha[which.max(tuning_results_inner)]\n    best_lambda = hyper_lambda[which.max(tuning_results_inner)]\n  }\n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = best_lambda\n  results[i, 4] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = 0.01, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n\n  set     alpha      lambda       AUC\n1   1 0.6417455 0.003948339 0.8799020\n2   2 0.6417455 0.003948339 0.7638128\n3   3 0.9346722 0.037431033 0.8198758\n4   4 0.7365883 0.007334147 0.8265602\n5   5 0.6417455 0.003948339 0.8173682\n\n\nPredictions:\n\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = results$alpha[i])\n    return(predict(model, data_new, alpha = results$alpha[i], s = results$lambda[i], type = \"response\")[,1])\n  })\n\n# Single predictions from the model with the highest AUC:\nwrite.csv(data.frame(y = prediction_ensemble[,which.max(results$AUC)], file = \"Max_titanic_best_model.csv\"))\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean), file = \"Max_titanic_ensemble.csv\"))"
  },
  {
    "objectID": "A4-MLpipeline.html#machine-learning-frameworks",
    "href": "A4-MLpipeline.html#machine-learning-frameworks",
    "title": "4  Machine learning pipeline",
    "section": "4.5 Machine learning frameworks",
    "text": "4.5 Machine learning frameworks\nAs we have seen today, many of the machine learning algorithms are distributed over several packages but the general machine learning pipeline is very similar for all models: feature engineering, feature selection, hyperparameter tuning and cross-validation.\nMachine learning frameworks such as mlr3 or tidymodels provide a general interface for the ML pipeline, in particular the training and the hyperparameter tuning with nested CV. They support most ML packages/algorithms.\n\n4.5.1 mlr3\nThe key features of mlr3 are:\n\nAll common machine learning packages are integrated into mlr3, you can easily switch between different machine learning algorithms.\nA common ‘language’/workflow to specify machine learning pipelines.\nSupport for different cross-validation strategies.\nHyperparameter tuning for all supported machine learning algorithms.\nEnsemble models.\n\nUseful links:\n\nmlr3-book (still in work)\nmlr3 website\nmlr3 cheatsheet\n\n\n4.5.1.1 mlr3 - The Basic Workflow\nThe mlr3 package actually consists of several packages for different tasks (e.g. mlr3tuning for hyperparameter tuning, mlr3pipelines for data preparation pipes). But let’s start with the basic workflow:\n\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n\n'data.frame':   4687 obs. of  40 variables:\n $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n $ Orbital.Period              : num  419 1040 682 427 503 ...\n $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n\n\nLet’s drop time, name and ID variable and create a classification task:\n\ndata = nasa %&gt;% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\n\nCreate a generic pipeline of data transformation (imputation \\(\\rightarrow\\) scaling \\(\\rightarrow\\) encoding of categorical variables):\n\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n\n                   Hazardous           Absolute.Magnitude \n                        4187                            0 \n               Aphelion.Dist           Asc.Node.Longitude \n                           0                            0 \n                Eccentricity    Epoch.Date.Close.Approach \n                           0                            0 \n            Epoch.Osculation         Est.Dia.in.Feet.max. \n                           0                            0 \n        Est.Dia.in.Feet.min.           Est.Dia.in.KM.max. \n                           0                            0 \n          Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n                           0                            0 \n           Est.Dia.in.M.min.        Est.Dia.in.Miles.max. \n                           0                            0 \n       Est.Dia.in.Miles.min.                  Inclination \n                           0                            0 \n Jupiter.Tisserand.Invariant                 Mean.Anomaly \n                           0                            0 \n                 Mean.Motion               Miles.per.hour \n                           0                            0 \n  Minimum.Orbit.Intersection     Miss.Dist..Astronomical. \n                           0                            0 \n      Miss.Dist..kilometers.            Miss.Dist..lunar. \n                           0                            0 \n           Miss.Dist..miles.                     Orbit.ID \n                           0                            0 \n          Orbit.Uncertainity               Orbital.Period \n                           0                            0 \n              Perihelion.Arg          Perihelion.Distance \n                           0                            0 \n             Perihelion.Time  Relative.Velocity.km.per.hr \n                           0                            0 \nRelative.Velocity.km.per.sec              Semi.Major.Axis \n                           0                            0 \n               Equinox.J2000             Equinox..MISSING \n                           0                            0 \n         Orbiting.Body.Earth       Orbiting.Body..MISSING \n                           0                            0 \n\n\nWe can even visualize the preprocessing graph:\n\npreprocessing$plot()\n\n\n\n\nTo test our model (glmnet) with 10-fold cross-validated, we will do:\n\nSpecify the missing target rows as validation so that they will be ignored.\nSpecify the cross-validation, the learner (the machine learning model we want to use), and the measurement (AUC).\nRun (benchmark) our model.\n\n\nset.seed(123)\n\ntransformed_task$data()\n\n      Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude\n   1:         0        -0.81322649   -0.38042005       -1.140837452\n   2:         0         0.02110348    0.94306517       -1.380254611\n   3:         0         0.68365964    0.10199889        0.044905370\n   4:         1        -0.10159210   -0.38415066        1.606769281\n   5:         1        -0.15067034   -0.29632490        0.151458877\n  ---                                                              \n4683:      &lt;NA&gt;        -0.32244415    0.69173184       -0.171022906\n4684:      &lt;NA&gt;         0.46280759   -0.24203066       -0.009803808\n4685:      &lt;NA&gt;         1.51798962   -0.56422744        1.514551982\n4686:      &lt;NA&gt;         0.16833819    0.14193044       -1.080452287\n4687:      &lt;NA&gt;        -0.05251387   -0.08643345       -0.013006704\n      Eccentricity Epoch.Date.Close.Approach Epoch.Osculation\n   1: -0.315605975                -4.7929881       0.14026773\n   2:  0.744287645                 1.1058704      -0.26325244\n   3: -0.068280074                 0.1591740      -7.76281014\n   4: -0.392030729                -0.7630231       0.24229559\n   5: -0.516897963                -0.6305034       0.24229559\n  ---                                                        \n4683:  1.043608082                 1.3635097       0.24229559\n4684: -0.006429588                 1.3635097       0.05711503\n4685: -1.045386877                 1.3635097       0.24229559\n4686:  0.017146757                 1.3635097       0.24229559\n4687: -0.579210554                 1.3635097       0.24229559\n      Est.Dia.in.Feet.max. Est.Dia.in.Feet.min. Est.Dia.in.KM.max.\n   1:          0.271417899          0.313407647        0.300713440\n   2:          0.032130074         -0.029173486       -0.020055639\n   3:         -0.012841645         -0.093558135       -0.080340934\n   4:          0.048493723         -0.005746146        0.001880088\n   5:          0.056169717          0.005243343        0.012169879\n  ---                                                             \n4683:          0.089353662          0.052751793        0.056653478\n4684:         -0.003481174         -0.080157032       -0.067793075\n4685:         -0.027260163         -0.114200690       -0.099669182\n4686:          0.016872584         -0.051017172       -0.040508543\n4687:          0.041493133         -0.015768679       -0.007504312\n      Est.Dia.in.KM.min. Est.Dia.in.M.max. Est.Dia.in.M.min.\n   1:        0.256568684       0.271095311       0.291624502\n   2:        0.057560696       0.031844946     -12.143577263\n   3:        0.020159164      -0.013119734      -0.060269734\n   4:        0.071169817       0.048206033       0.015659335\n   5:        0.077553695       0.055880826       0.025161701\n  ---                                                       \n4683:        0.105151714       0.089059576       0.066241198\n4684:        0.027943967      -0.003760728      -0.048682099\n4685:        0.008167747      -0.027535994      -0.078118891\n4686:        0.044871533       0.016589844      -0.023485512\n4687:        0.065347651       0.041206539       0.006993074\n      Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n   1:          2.620443e-01           0.258651038   0.5442288\n   2:          4.153888e-02           0.030928225  -0.5925952\n   3:          9.711407e-05         -10.258220292  -0.5164818\n   4:          5.661810e-02           0.046501003   0.8225188\n   5:          6.369158e-02           0.053806009  -0.6568722\n  ---                                                        \n4683:          9.427082e-02           0.085386142   0.8222493\n4684:          8.722856e-03          -0.002961897   1.9818623\n4685:         -1.318965e-02          -0.025591624  -0.5220442\n4686:          2.747899e-02           0.016408144  -0.5912988\n4687:          5.016700e-02           0.039838758   0.6181969\n      Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n   1:                   0.3840868  -1.02876096  0.31939530   -0.254130552\n   2:                  -0.7801632  -4.55056211 -0.71151122    0.009333354\n   3:                  -0.2872777  -4.55056211 -0.34600512   -0.866997591\n   4:                   0.3403535   1.02239674  0.28551117    0.039031045\n   5:                  -6.2415005   1.13265516  0.03164827   -0.995720084\n  ---                                                                    \n4683:                  -0.6412806   0.01560046 -0.51852041    1.403775544\n4684:                   0.1346891   1.08051799  0.17477591    0.970963141\n4685:                   0.4810091   0.89998250  0.36895738   -1.150527134\n4686:                  -0.3061894   0.22720275 -0.35895074   -0.705980518\n4687:                  -0.2665930   0.22740438 -0.31462613   -0.239696213\n      Minimum.Orbit.Intersection Miss.Dist..Astronomical.\n   1:                -5.45911858               -7.0769260\n   2:                 0.07077092               -0.6830928\n   3:                -0.11099960               -0.9035573\n   4:                -5.45911858               -0.7188386\n   5:                -0.02962490               -0.8013948\n  ---                                                    \n4683:                 0.30711241               -0.2728622\n4684:                -0.05962478               -0.7879458\n4685:                -0.10766868               -0.9303542\n4686:                 0.08529226               -0.7077555\n4687:                 0.50904764                0.1075071\n      Miss.Dist..kilometers. Miss.Dist..lunar. Miss.Dist..miles.   Orbit.ID\n   1:             0.25122963         0.2398625        0.23810770 -9.6514722\n   2:            -1.08492125        -1.1742128       -1.18860632 -0.2412680\n   3:            -1.40898698        -4.7878719       -1.53463694 -0.1803606\n   4:            -4.48402327        -1.2298206       -1.24471124 -0.1803606\n   5:            -1.25881601        -1.3582490       -1.37428752  1.0225620\n  ---                                                                      \n4683:            -0.48191427        -0.5360384       -0.54472804 -0.1194531\n4684:            -1.23904708        -1.3373272       -1.35317867 -0.3021755\n4685:            -1.44837625        -1.5588644       -1.57669598 -0.3326292\n4686:            -1.12117355        -1.2125793       -1.22731578 -0.1042262\n4687:             0.07719897         0.0556823        0.05228143 -0.2717218\n      Orbit.Uncertainity Orbital.Period Perihelion.Arg Perihelion.Distance\n   1:         -1.0070872     -0.3013135   -1.170536399         -0.01831583\n   2:          1.3770116      0.7811097    1.549452700          0.20604472\n   3:          0.7809869      0.1566040    1.470307933          0.61816146\n   4:         -1.0070872     -0.2866969    0.769006449          0.09005898\n   5:         -1.0070872     -0.1552813    0.006829799          0.52730977\n  ---                                                                     \n4683:         -0.7090748      0.3873214   -0.580282684         -0.65810123\n4684:          1.3770116     -0.2345610    0.839430173         -0.18350549\n4685:          0.7809869     -0.3216884   -1.168210857          0.62646993\n4686:          0.7809869      0.1712806    0.824836889          0.52899080\n4687:          0.4829746      0.1224733    0.016358127          1.22720096\n      Perihelion.Time Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec\n   1:      0.10526107                 -0.28167821                 -0.284140684\n   2:     -0.28203779                 -0.00604459                 -0.008343348\n   3:      0.20313227                 -0.92285430                 -0.925697621\n   4:     -7.86832915                  0.02502487                  0.022744569\n   5:      0.26755741                 -1.05752264                 -1.060445948\n  ---                                                                         \n4683:      0.03734532                  1.45280854                  1.451376301\n4684:      0.09156633                  1.00000402                  0.998302826\n4685:      0.27629790                 -1.21948041                 -1.222499918\n4686:      0.37994517                 -0.75439966                 -0.757142920\n4687:      0.37399573                 -0.26657713                 -0.269030636\n      Semi.Major.Axis Equinox.J2000 Equinox..MISSING Orbiting.Body.Earth\n   1:      -0.2791037             1                0                   1\n   2:      -7.3370940             1                0                   1\n   3:       0.2204883             0                1                   1\n   4:      -0.2617714             1                0                   1\n   5:      -0.1106954             1                0                   1\n  ---                                                                   \n4683:       0.4468886             1                0                   1\n4684:      -0.2008499             1                0                   1\n4685:      -0.3034586             1                0                   1\n4686:       0.2353030             1                0                   1\n4687:       0.1857979             1                0                   1\n      Orbiting.Body..MISSING\n   1:                      0\n   2:                      0\n   3:                      0\n   4:                      0\n   5:                      0\n  ---                       \n4683:                      0\n4684:                      0\n4685:                      0\n4686:                      0\n4687:                      0\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n\n\nresult = mlr3::resample(transformed_task,\n                        EN, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nVery cool! Preprocessing + 10-fold cross-validation model evaluation in a few lines of code!\nLet’s create the final predictions:\n\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n\nYou could now submit the predictions here.\nBut we are still not happy with the results, let’s do some hyperparameter tuning!\n\n\n4.5.1.2 mlr3 - Hyperparameter Tuning\nWith mlr3, we can easily extend the above example to do hyperparameter tuning within nested cross-validation (the tuning has its own inner cross-validation).\nPrint the hyperparameter space of our glmnet learner:\n\nEN$param_set\n\n&lt;ParamSet&gt;\n                      id    class lower upper nlevels        default parents\n 1:                alpha ParamDbl     0     1     Inf              1        \n 2:                  big ParamDbl  -Inf   Inf     Inf        9.9e+35        \n 3:               devmax ParamDbl     0     1     Inf          0.999        \n 4:                dfmax ParamInt     0   Inf     Inf &lt;NoDefault[3]&gt;        \n 5:                  eps ParamDbl     0     1     Inf          1e-06        \n 6:                epsnr ParamDbl     0     1     Inf          1e-08        \n 7:                exact ParamLgl    NA    NA       2          FALSE        \n 8:              exclude ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;        \n 9:                 exmx ParamDbl  -Inf   Inf     Inf            250        \n10:                 fdev ParamDbl     0     1     Inf          1e-05        \n11:                gamma ParamDbl  -Inf   Inf     Inf              1   relax\n12:            intercept ParamLgl    NA    NA       2           TRUE        \n13:               lambda ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n14:     lambda.min.ratio ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;        \n15:         lower.limits ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n16:                maxit ParamInt     1   Inf     Inf         100000        \n17:                mnlam ParamInt     1   Inf     Inf              5        \n18:                 mxit ParamInt     1   Inf     Inf            100        \n19:               mxitnr ParamInt     1   Inf     Inf             25        \n20:            newoffset ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n21:              nlambda ParamInt     1   Inf     Inf            100        \n22:               offset ParamUty    NA    NA     Inf                       \n23:       penalty.factor ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n24:                 pmax ParamInt     0   Inf     Inf &lt;NoDefault[3]&gt;        \n25:                 pmin ParamDbl     0     1     Inf          1e-09        \n26:                 prec ParamDbl  -Inf   Inf     Inf          1e-10        \n27:                relax ParamLgl    NA    NA       2          FALSE        \n28:                    s ParamDbl     0   Inf     Inf           0.01        \n29:          standardize ParamLgl    NA    NA       2           TRUE        \n30: standardize.response ParamLgl    NA    NA       2          FALSE        \n31:               thresh ParamDbl     0   Inf     Inf          1e-07        \n32:             trace.it ParamInt     0     1       2              0        \n33:        type.gaussian ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;        \n34:        type.logistic ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;        \n35:     type.multinomial ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;        \n36:         upper.limits ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n                      id    class lower upper nlevels        default parents\n    value\n 1:      \n 2:      \n 3:      \n 4:      \n 5:      \n 6:      \n 7:      \n 8:      \n 9:      \n10:      \n11:      \n12:      \n13:      \n14:      \n15:      \n16:      \n17:      \n18:      \n19:      \n20:      \n21:      \n22:      \n23:      \n24:      \n25:      \n26:      \n27:      \n28:      \n29:      \n30:      \n31:      \n32:      \n33:      \n34:      \n35:      \n36:      \n    value\n\n\nDefine the hyperparameter space of the random forest:\n\nlibrary(paradox)\n\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\nprint(EN_pars)\n\n&lt;ParamSet&gt;\n       id    class lower upper nlevels        default value\n1:  alpha ParamDbl     0   1.0     Inf &lt;NoDefault[3]&gt;      \n2: lambda ParamDbl     0   0.5     Inf &lt;NoDefault[3]&gt;      \n\n\nTo set up the tuning pipeline we need:\n\nInner cross-validation resampling object.\nTuning criterion (e.g. AUC).\nTuning method (e.g. random or block search).\nTuning terminator (When should we stop tuning? E.g. after \\(n\\) iterations).\n\n\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n\n&lt;AutoTuner:classif.glmnet.tuned&gt;\n* Model: list\n* Search Space:\n&lt;ParamSet&gt;\n       id    class lower upper nlevels        default value\n1:  alpha ParamDbl     0   1.0     Inf &lt;NoDefault[3]&gt;      \n2: lambda ParamDbl     0   0.5     Inf &lt;NoDefault[3]&gt;      \n* Packages: mlr3, mlr3tuning, mlr3learners, glmnet\n* Predict Type: prob\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass, weights\n\n\nNow we can wrap it normally into the 10-fold cross-validated setup as done previously:\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nclassif.auc \n  0.6767554 \n\n\nLet’s create the final predictions:\n\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)"
  },
  {
    "objectID": "A4-MLpipeline.html#exercises-1",
    "href": "A4-MLpipeline.html#exercises-1",
    "title": "4  Machine learning pipeline",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n\n\n\n\n\n\nQuestion: Use mlr3 for the titanic dataset\n\n\n\n\nUse mlr3 to tune glmnet for the titanic dataset using nested CV\nSubmit single predictions and multiple predictions\n\nIf you need help, take a look at the solution, go through it line by line and try to understand it.\n\n\nClick here to see the solution\n\nPrepare data\n\ndata = titanic_ml %&gt;% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n\n survived       age      boat     cabin  embarked      fare home.dest     parch \n      655       263         0         0         0         0         0         0 \n   pclass       sex     sibsp \n        0         0         0 \n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n\nHyperparameter tuning:\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\n\n\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = cv10, store_models = TRUE)\n\nEvaluation:\n\nmeasurement =  msr(\"classif.auc\")\nresult$aggregate(measurement)\n\nclassif.auc \n  0.9924459 \n\n\nPredictions:\nWe can extract a learner with optimized hyperparameters:\n\nmodel = result$learners[[1]]$learner$clone()\nmodel$param_set$values\n\n$alpha\n[1] 0.2353052\n\n$lambda\n[1] 0.03832512\n\n\nAnd we can fit it then on the full data set:\n\nmodel$train(transformed_task)\npredictions = model$predict(transformed_task, row_ids = transformed_task$row_roles$holdout)\npredictions = predictions$prob[,1]\nhead(predictions)\n\n[1] 0.94428932 0.07481128 0.21730284 0.86504280 0.94653467 0.95414457\n\n\nAnd submit to http://rhsbio7.uni-regensburg.de:8500\n\nwrite.csv(data.frame(y = predictions), file = \"glmnet.csv\")"
  },
  {
    "objectID": "B1-Trees.html#classification-and-regression-trees",
    "href": "B1-Trees.html#classification-and-regression-trees",
    "title": "5  Tree-based Algorithms",
    "section": "5.1 Classification and Regression Trees",
    "text": "5.1 Classification and Regression Trees\nTree-based models in general use a series of if-then rules to generate predictions from one or more decision trees. In this lecture, we will explore regression and classification trees by the example of the airquality data set. There is one important hyperparameter for regression trees: “minsplit”.\n\nIt controls the depth of tree (see the help of rpart for a description).\nIt controls the complexity of the tree and can thus also be seen as a regularization parameter.\n\nWe first prepare and visualize the data and afterwards fit a decision tree.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\ndata = airquality[complete.cases(airquality),]\n\nFit and visualize one(!) regression tree:\n\nrt = rpart(Ozone~., data = data, control = rpart.control(minsplit = 10))\nrpart.plot(rt)\n\n\n\n\nVisualize the predictions:\n\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\nThe angular form of the prediction line is typical for regression trees and is a weakness of it."
  },
  {
    "objectID": "B1-Trees.html#random-forest",
    "href": "B1-Trees.html#random-forest",
    "title": "5  Tree-based Algorithms",
    "section": "5.2 Random Forest",
    "text": "5.2 Random Forest\nTo overcome this weakness, a random forest uses an ensemble of regression/classification trees. Thus, the random forest is in principle nothing else than a normal regression/classification tree, but it uses the idea of the “wisdom of the crowd” : By asking many people (regression/classification trees) one can make a more informed decision (prediction/classification). When you want to buy a new phone for example you also wouldn’t go directly into the shop, but search in the internet and ask your friends and family.\nThere are two randomization steps with the random forest that are responsible for their success:\n\nBootstrap samples for each tree (we will sample observations with replacement from the data set. For the phone this is like not everyone has experience about each phone).\nAt each split, we will sample a subset of predictors that is then considered as potential splitting criterion (for the phone this is like that not everyone has the same decision criteria). Annotation: While building a decision tree (random forests consist of many decision trees), one splits the data at some point according to their features. For example if you have females and males, big and small people in a crowd, you con split this crowd by gender and then by size or by size and then by gender to build a decision tree.\n\nApplying the random forest follows the same principle as for the methods before: We visualize the data (we have already done this so often for the airquality data set, thus we skip it here), fit the algorithm and then plot the outcomes.\nFit a random forest and visualize the predictions:\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\npred = predict(rf, data)\nplot(Ozone~Temp, data = data)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\nOne advantage of random forests is that we will get an importance of variables. At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Thus the variable importance shows us how important a variable is averaged over all trees.\n\nrf$importance\n\n        IncNodePurity\nSolar.R      17969.59\nWind         31978.36\nTemp         34176.71\nMonth        10753.73\nDay          15436.47\n\n\nThere are several important hyperparameters in a random forest that we can tune to get better results:\n\nSimilar to the minsplit parameter in regression and classification trees, the hyperparameter “nodesize” controls for complexity \\(\\rightarrow\\) Minimum size of terminal nodes in the tree. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).\nmtry: Number of features randomly sampled as candidates at each split."
  },
  {
    "objectID": "B1-Trees.html#boosted-regression-trees",
    "href": "B1-Trees.html#boosted-regression-trees",
    "title": "5  Tree-based Algorithms",
    "section": "5.3 Boosted Regression Trees",
    "text": "5.3 Boosted Regression Trees\nRandom forests fit hundreds of trees independent of each other. Here, the idea of a boosted regression tree comes in. Maybe we could learn from the errors the previous weak learners made and thus enhance the performance of the algorithm.\nA boosted regression tree (BRT) starts with a simple regression tree (weak learner) and then sequentially fits additional trees to improve the results. There are two different strategies to do so:\n\nAdaBoost: Wrong classified observations (by the previous tree) will get a higher weight and therefore the next trees will focus on difficult/missclassified observations.\nGradient boosting (state of the art): Each sequential model will be fit on the residual errors of the previous model (strongly simplified, the actual algorithm is very complex).\n\nWe can fit a boosted regression tree using xgboost, but before we have to transform the data into a xgb.Dmatrix.\n\nlibrary(xgboost)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\nbrt = xgboost(data_xg, nrounds = 16L)\n\n[1] train-rmse:39.724624 \n[2] train-rmse:30.225761 \n[3] train-rmse:23.134840 \n[4] train-rmse:17.899179 \n[5] train-rmse:14.097785 \n[6] train-rmse:11.375457 \n[7] train-rmse:9.391276 \n[8] train-rmse:7.889690 \n[9] train-rmse:6.646586 \n[10]    train-rmse:5.804859 \n[11]    train-rmse:5.128437 \n[12]    train-rmse:4.456416 \n[13]    train-rmse:4.069464 \n[14]    train-rmse:3.674615 \n[15]    train-rmse:3.424578 \n[16]    train-rmse:3.191301 \n\n\nThe parameter “nrounds” controls how many sequential trees we fit, in our example this was 16. When we predict on new data, we can limit the number of trees used to prevent overfitting (remember: each new tree tries to improve the predictions of the previous trees).\nLet us visualize the predictions for different numbers of trees:\n\noldpar = par(mfrow = c(2, 2))\nfor(i in 1:4){\n  pred = predict(brt, newdata = data_xg, ntreelimit = i)\n  plot(data$Temp, data$Ozone, main = i)\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\n[15:02:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[15:02:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[15:02:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[15:02:54] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n\n\npar(oldpar)\n\nThere are also other ways to control for complexity of the boosted regression tree algorithm:\n\nmax_depth: Maximum depth of each tree.\nshrinkage (each tree will get a weight and the weight will decrease with the number of trees).\n\nWhen having specified the final model, we can obtain the importance of the variables like for random forests:\n\nxgboost::xgb.importance(model = brt)\n\n   Feature        Gain     Cover  Frequency\n1:    Temp 0.570072012 0.2958229 0.24836601\n2:    Wind 0.348230653 0.3419576 0.24183007\n3: Solar.R 0.058795502 0.1571072 0.30718954\n4:     Day 0.019529985 0.1779925 0.16993464\n5:   Month 0.003371847 0.0271197 0.03267974\n\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n\n[1] 17.89918\n\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\n\nOne important strength of xgboost is that we can directly do a cross-validation (which is independent of the boosted regression tree itself!) and specify its properties with the parameter “n-fold”:\n\nset.seed(123)\n\nbrt = xgboost(data_xg, nrounds = 5L)\n\n[1] train-rmse:39.724624 \n[2] train-rmse:30.225761 \n[3] train-rmse:23.134840 \n[4] train-rmse:17.899179 \n[5] train-rmse:14.097785 \n\nbrt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L,\n                         nrounds = 3L, nthreads = 4L)\n\n[1] train-rmse:39.895106+2.127355   test-rmse:40.685477+5.745327 \n[2] train-rmse:30.367660+1.728788   test-rmse:32.255812+5.572963 \n[3] train-rmse:23.446237+1.366757   test-rmse:27.282435+5.746244 \n\nprint(brt_cv)\n\n##### xgb.cv 3-folds\n iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n    1        39.89511       2.127355       40.68548      5.745327\n    2        30.36766       1.728788       32.25581      5.572963\n    3        23.44624       1.366757       27.28244      5.746244\n\n\nAnnotation: The original data set is randomly partitioned into \\(n\\) equal sized subsamples. Each time, the model is trained on \\(n - 1\\) subsets (training set) and tested on the left out set (test set) to judge the performance.\nIf we do three-folded cross-validation, we actually fit three different boosted regression tree models (xgboost models) on \\(\\approx 67\\%\\) of the data points. Afterwards, we judge the performance on the respective holdout. This now tells us how well the model performed.\n\n5.3.1 Exercises\n\n\n\n\n\n\nQuestion: Regression Trees\n\n\n\nWe will use the following code snippet to test the effect of mincut on trees.\n\nlibrary(tree)\nset.seed(123)\n\ndata = airquality\nrt = tree(Ozone~., data = data,\n          control = tree.control(mincut = 1L, nobs = nrow(data)))\n\nplot(rt)\ntext(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n\nTry different mincut parameters and see what happens. (Compare the root mean squared error for different mincut parameters and explain what you see. Compare predictions for different mincut parameters and explain what happens.) What was wrong in the snippet above?\n\n\nClick here to see the solution\n\n\nlibrary(tree)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\ndoTask = function(mincut){\n  rt = tree(Ozone~., data = data,\n            control = tree.control(mincut = mincut, nobs = nrow(data)))\n\n  pred = predict(rt, data)\n  plot(data$Temp, data$Ozone,\n       main = paste0(\n         \"mincut: \", mincut,\n         \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n  )\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\nfor(i in c(1, 2, 3, 5, 10, 15, 25, 50, 54, 55, 56, 57, 75, 100)){ doTask(i) }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproximately at mincut = 15, prediction is the best (mind overfitting). After mincut = 56, the prediction has no information at all and the RMSE stays constant.\nMind the complete cases of the airquality data set, that was the error.\n\n\n\n\n\n\n\n\n\nQuestion: Random forest\n\n\n\nWe will use the following code snippet to explore a random forest:\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n\n        IncNodePurity\nSolar.R      17969.59\nWind         31978.36\nTemp         34176.71\nMonth        10753.73\nDay          15436.47\n\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n\nRMSE:  9.507848 \n\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\nTry different values for the nodesize and mtry and describe how the predictions depend on these parameters.\n\n\nClick here to see the solution\n\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\nfor(nodesize in c(1, 5, 15, 50, 100)){\n  for(mtry in c(1, 3, 5)){\n    rf = randomForest(Ozone~., data = data, mtry = mtry, nodesize = nodesize)\n    \n    pred = predict(rf, data)\n    \n    plot(data$Temp, data$Ozone, main = paste0(\n        \"mtry: \", mtry, \"    nodesize: \", nodesize,\n        \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n    )\n    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher numbers for mtry smooth the prediction curve and yield less overfitting. The same holds for the nodesize. In other words: The bigger the nodesize, the smaller the trees and the more bias/less variance.\n\n\n\n\n\n\n\n\n\nQuestion: Boosted regression trees\n\n\n\n\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\n\nmodel = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                         nrounds = 500L, verbose = 0L)\npred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n               ntreelimit = 10L)\n\nsaveGIF(\n  {\n    for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n      pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                     ntreelimit = i)\n      image(matrix(pred, 100, 100), main = paste0(\"Trees: \", i),\n            axes = FALSE, las = 2)\n      axis(1, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1))\n      axis(2, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n    }\n  },\n  movie.name = \"boosting.gif\", autobrowse = FALSE\n)\n\n\nRun the above code and play with different parameters for xgboost (especially with parameters that control the complexity) and describe what you see!\nTip: have a look at the boosting.gif.\n\n\nClick here to see the solution\n\n\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\nfor(eta in c(.1, .3, .5, .7, .9)){\n  for(max_depth in c(3, 6, 10, 20)){\n    model = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                             max_depth = max_depth, eta = eta,\n                             nrounds = 500, verbose = 0L)\n  \n    saveGIF(\n      {\n        for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n          pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                         ntreelimit = i)\n          image(matrix(pred, 100, 100),\n                main = paste0(\"eta: \", eta,\n                              \"    max_depth: \", max_depth,\n                              \"    Trees: \", i),\n                axes = FALSE, las = 2)\n          axis(1, at = seq(0, 1, length.out = 10),\n               labels = round(seq(-3, 3, length.out = 10), 1))\n          axis(2, at = seq(0, 1, length.out = 10),\n               labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n        }\n      },\n      movie.name = paste0(\"boosting_\", max_depth, \"_\", eta, \".gif\"),\n      autobrowse = FALSE\n    )\n  }\n}\n\nAs the possibilities scale extremely, you must vary some parameters on yourself. You may use for example the following parameters: “eta”, “gamma”, “max_depth”, “min_child_weight”, “subsample”, “colsample_bytree”, “num_parallel_tree”, “monotone_constraints” or “interaction_constraints”. Or you look into the documentation:\n\n?xgboost::xgboost\n\nJust some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonus: Implement a BRT on your own!\n\n\n\nYou can easily implement a BRT or boosted linear model using the rpart package or the lm function.\n\n\nClick here to see the solution\n\nGo through the code line by line and try to understand it. Ask, if you have any questions you cannot solve.\nLet’s try it:\n\ndata = model.matrix(~. , data = airquality)\n\nmodel = get_boosting_model(x = data[,-2], y = data[,2], n_trees = 5L )\npred = predict(model, newdata = data[,-2])\nplot(data[,2], pred, xlab = \"observed\", ylab = \"predicted\")"
  },
  {
    "objectID": "B2-Distance.html#k-nearest-neighbor",
    "href": "B2-Distance.html#k-nearest-neighbor",
    "title": "6  Distance-based Algorithms",
    "section": "6.1 K-Nearest-Neighbor",
    "text": "6.1 K-Nearest-Neighbor\nK-nearest-neighbor (kNN) is a simple algorithm that stores all the available cases and classifies the new data based on a similarity measure. It is mostly used to classify a data point based on how its \\(k\\) nearest neighbors are classified.\nLet us first see an example:\n\nx = scale(iris[,1:4])\ny = iris[,5]\nplot(x[-100,1], x[-100, 3], col = y)\npoints(x[100,1], x[100, 3], col = \"blue\", pch = 18, cex = 1.3)\n\n\n\n\nWhich class would you decide for the blue point? What are the classes of the nearest points? Well, this procedure is used by the k-nearest-neighbors classifier and thus there is actually no “real” learning in a k-nearest-neighbors classification.\nFor applying a k-nearest-neighbors classification, we first have to scale the data set, because we deal with distances and want the same influence of all predictors. Imagine one variable has values from -10.000 to 10.000 and another from -1 to 1. Then the influence of the first variable on the distance to the other points is much stronger than the influence of the second variable. On the iris data set, we have to split the data into training and test set on our own. Then we will follow the usual pipeline.\n\ndata = iris\ndata[,1:4] = apply(data[,1:4],2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\nFit model and create predictions:\n\nlibrary(kknn)\nset.seed(123)\n\nknn = kknn(Species~., train = train, test = test)\nsummary(knn)\n\n\nCall:\nkknn(formula = Species ~ ., train = train, test = test)\n\nResponse: \"nominal\"\n          fit prob.setosa prob.versicolor prob.virginica\n1      setosa   1.0000000      0.00000000     0.00000000\n2      setosa   1.0000000      0.00000000     0.00000000\n3      setosa   1.0000000      0.00000000     0.00000000\n4      setosa   1.0000000      0.00000000     0.00000000\n5      setosa   1.0000000      0.00000000     0.00000000\n6      setosa   1.0000000      0.00000000     0.00000000\n7      setosa   1.0000000      0.00000000     0.00000000\n8      setosa   1.0000000      0.00000000     0.00000000\n9      setosa   1.0000000      0.00000000     0.00000000\n10     setosa   1.0000000      0.00000000     0.00000000\n11     setosa   0.7013345      0.29866548     0.00000000\n12     setosa   1.0000000      0.00000000     0.00000000\n13     setosa   1.0000000      0.00000000     0.00000000\n14 versicolor   0.0000000      0.95118552     0.04881448\n15 versicolor   0.0000000      0.76888014     0.23111986\n16 versicolor   0.0000000      1.00000000     0.00000000\n17 versicolor   0.0000000      0.91487300     0.08512700\n18 versicolor   0.0000000      1.00000000     0.00000000\n19 versicolor   0.0000000      1.00000000     0.00000000\n20  virginica   0.0000000      0.34747996     0.65252004\n21  virginica   0.0000000      0.49084569     0.50915431\n22 versicolor   0.0000000      0.89918140     0.10081860\n23 versicolor   0.0000000      1.00000000     0.00000000\n24 versicolor   0.0000000      1.00000000     0.00000000\n25 versicolor   0.0000000      1.00000000     0.00000000\n26 versicolor   0.0000000      1.00000000     0.00000000\n27 versicolor   0.0000000      0.91487300     0.08512700\n28 versicolor   0.0000000      1.00000000     0.00000000\n29 versicolor   0.0000000      1.00000000     0.00000000\n30  virginica   0.0000000      0.00000000     1.00000000\n31 versicolor   0.0000000      0.87421565     0.12578435\n32  virginica   0.0000000      0.00000000     1.00000000\n33  virginica   0.0000000      0.06450608     0.93549392\n34  virginica   0.0000000      0.23111986     0.76888014\n35  virginica   0.0000000      0.00000000     1.00000000\n36  virginica   0.0000000      0.01569160     0.98430840\n37  virginica   0.0000000      0.18857273     0.81142727\n38  virginica   0.0000000      0.33193846     0.66806154\n39  virginica   0.0000000      0.00000000     1.00000000\n40  virginica   0.0000000      0.04881448     0.95118552\n41  virginica   0.0000000      0.00000000     1.00000000\n42  virginica   0.0000000      0.00000000     1.00000000\n43  virginica   0.0000000      0.00000000     1.00000000\n44  virginica   0.0000000      0.12578435     0.87421565\n45 versicolor   0.0000000      0.54547694     0.45452306\n\ntable(test$Species, fitted(knn))\n\n            \n             setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         14         2\n  virginica       0          2        14"
  },
  {
    "objectID": "B2-Distance.html#support-vector-machines-svms",
    "href": "B2-Distance.html#support-vector-machines-svms",
    "title": "6  Distance-based Algorithms",
    "section": "6.2 Support Vector Machines (SVMs)",
    "text": "6.2 Support Vector Machines (SVMs)\nSupport vectors machines have a different approach. They try to divide the predictor space into sectors for each class. To do so, a support-vector machine fits the parameters of a hyperplane (a \\(n-1\\) dimensional subspace in a \\(n\\)-dimensional space) in the predictor space by optimizing the distance between the hyperplane and the nearest point from each class.\nFitting a support-vector machine:\n\nlibrary(e1071)\n\ndata = iris\ndata[,1:4] = apply(data[,1:4], 2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\nsm = svm(Species~., data = train, kernel = \"linear\")\npred = predict(sm, newdata = test)\n\n\noldpar = par(mfrow = c(1, 2))\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  pred, main = \"predicted\")\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  test$Species, main = \"observed\")\n\n\n\npar(oldpar)\n\nmean(pred == test$Species) # Accuracy.\n\n[1] 0.9777778\n\n\nSupport-vector machines can only work on linearly separable problems. (A problem is called linearly separable if there exists at least one line in the plane with all of the points of one class on one side of the hyperplane and all the points of the others classes on the other side).\nIf this is not possible, we however, can use the so called kernel trick, which maps the predictor space into a (higher dimensional) space in which the problem is linear separable. After having identified the boundaries in the higher-dimensional space, we can project them back into the original dimensions.\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nX = expand.grid(x1, x2)\ny = apply(X, 1, function(t) exp(-t[1]^2 - t[2]^2))\ny = ifelse(1/(1+exp(-y)) &lt; 0.62, 0, 1)\n\nimage(matrix(y, 100, 100))\nanimation::saveGIF(\n  {\n    for(i in c(\"truth\", \"linear\", \"radial\", \"sigmoid\")){\n      if(i == \"truth\"){\n        image(matrix(y, 100,100),\n        main = \"Ground truth\", axes = FALSE, las = 2)\n      }else{\n        sv = e1071::svm(x = x, y = factor(y), kernel = i)\n        image(matrix(as.numeric(as.character(predict(sv, x))), 100, 100),\n        main = paste0(\"Kernel: \", i), axes = FALSE, las = 2)\n        axis(1, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    }\n  },\n  movie.name = \"svm.gif\", autobrowse = FALSE, interval = 2\n)\n\n\n\n\n\n\nAs you have seen, this does not work with every kernel. Hence, the problem is to find the actual correct kernel, which is again an optimization procedure and can thus be approximated."
  },
  {
    "objectID": "B2-Distance.html#exercises",
    "href": "B2-Distance.html#exercises",
    "title": "6  Distance-based Algorithms",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\n\n\n\n\n\n\nQuestion: Sonar data\n\n\n\nWe will use the Sonar data set to explore support-vector machines and k-neartest-neighbor classifier.\n\nlibrary(mlbench)\nset.seed(123)\n\ndata(Sonar)\ndata = Sonar\nindices = sample.int(nrow(Sonar), 0.5 * nrow(Sonar))\n\nSplit the Sonar data set from the mlbench library into training- and testset with 50% in each group. Is this a useful split? The response variable is “class”. So you are trying to classify the class.\n\n\nClick here to see the solution\n\n\nlibrary(mlbench)\nset.seed(123)\n\ndata(Sonar)\ndata = Sonar\n#str(data)\n\n# Do not forget scaling! This may be done implicitly by most functions.\n# Here, it's done explicitly for teaching purposes.\ndata = cbind.data.frame(\n  scale(data[,-length(data)]),\n  \"class\" = data[,length(data)]\n)\n\nn = length(data[,1])\nindicesTrain = sample.int(n, (n+1) %/% 2) # Take (at least) 50 % of the data.\n\ntrain = data[indicesTrain,]\ntest = data[-indicesTrain,]\n\nlabelsTrain = train[,length(train)]\nlabelsTest = test[,length(test)]\n\nUntil you have strong reasons for that, 50/50 is no really good decision. You waste data/power. Do not forget scaling!\n\n\n\n\n\n\n\n\n\nQuestion: kNN and SVM\n\n\n\nFit a standard k-nearest-neighbor classifier and a support vector machine with a linear kernel (check help), and report what fitted better.\n\n\nClick here to see the solution\n\n\nlibrary(e1071)\nlibrary(kknn)\n\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"rectangular\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"linear\")\npredSVM = predict(sm, newdata = test)\n\n\n\nK-nearest-neighbor, standard (rectangular) kernel:\n\n\n       labelsTest\npredKNN  M  R\n      M 46 29\n      R  8 21\n\n\nCorrectly classified:  67  /  104\n\n\n\n\nSupport-vector machine, linear kernel:\n\n\n       labelsTest\npredSVM  M  R\n      M 41 15\n      R 13 35\n\n\nCorrectly classified:  76  /  104\n\n\nK-nearest neighbor fitted (slightly) better.\n\n\n\n\n\n\n\n\n\nQuestion: Reproducibility\n\n\n\n\nCalculate accuracies of both algorithms.\nTry the fit again with a different seed for training and test set generation.\n\n\n\nClick here to see the solution\n\n\n(accKNN = mean(predKNN == labelsTest))\n\n[1] 0.6442308\n\n(accSVM = mean(predSVM == labelsTest))\n\n[1] 0.7307692\n\n\n\nset.seed(42)\n\ndata = Sonar\ndata = cbind.data.frame(\n  scale(data[,-length(data)]),\n  \"class\" = data[,length(data)]\n)\n\nn = length(data[,1])\nindicesTrain = sample.int(n, (n+1) %/% 2)\n\ntrain = data[indicesTrain,]\ntest = data[-indicesTrain,]\n\nlabelsTrain = train[,length(train)]\nlabelsTest = test[,length(test)]\n\n#####\n\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"rectangular\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"linear\")\npredSVM = predict(sm, newdata = test)\n\n(accKNN = mean(predKNN == labelsTest))\n\n[1] 0.7115385\n\n(accSVM = mean(predSVM == labelsTest))\n\n[1] 0.75\n\n#####\n\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"optimal\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"radial\")\npredSVM = predict(sm, newdata = test)\n\n(accKNN = mean(predKNN == labelsTest))\n\n[1] 0.8557692\n\n(accSVM = mean(predSVM == labelsTest))\n\n[1] 0.8365385"
  },
  {
    "objectID": "B3-NeuralNetworks.html",
    "href": "B3-NeuralNetworks.html",
    "title": "7  Artificial Neural Networks",
    "section": "",
    "text": "Artificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\nlibrary(cito)"
  },
  {
    "objectID": "C1-TensorFlow.html#introduction-to-tensorflow",
    "href": "C1-TensorFlow.html#introduction-to-tensorflow",
    "title": "8  Introduction to TensorFlow and Keras",
    "section": "8.1 Introduction to TensorFlow",
    "text": "8.1 Introduction to TensorFlow\nOne of the most commonly used frameworks for machine learning is TensorFlow. TensorFlow is an open source linear algebra library with focus on neural networks, published by Google in 2015. TensorFlow supports several interesting features, in particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization.\nThese advantages are nicely explained in the following video:\n\n\nTo sum up the most important points of the video:\n\nTensorFlow is a math library which is highly optimized for neural networks.\nIf a GPU is available, computations can be easily run on the GPU but even on a CPU TensorFlow is still very fast.\nThe “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for NVIDIA GPUs).\nThe interface (the part of TensorFlow we use) is written in Python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend.\n\nAll operations in TensorFlow are written in C++ and are highly optimized. But don’t worry, we don’t have to use C++ to use TensorFlow because there are several bindings for other languages. TensorFlow officially supports a Python API, but meanwhile there are several community carried APIs for other languages:\n\nR\nGo\nRust\nSwift\nJavaScript\n\nIn this course we will use TensorFlow with the https://tensorflow.rstudio.com/ binding, that was developed and published 2017 by the RStudio team. First, they developed an R package (reticulate) for calling Python in R. Actually, we are using the Python TensorFlow module in R (more about this later).\nTensorFlow offers different levels of API. We could implement a neural network completely by ourselves or we could use Keras which is provided as a submodule by TensorFlow. Keras is a powerful module for building and training neural networks. It allows us building and training neural networks in a few lines of codes. Since the end of 2018, Keras and TensorFlow are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras for neural networks but also how we can use the TensorFlow’s automatic differenation for using complex objective functions.\nUseful links:\n\nTensorFlow documentation (This is for the Python API, but just replace the “.” with “$”.)\nRstudio TensorFlow website\n\n\n8.1.1 Data Containers\nTensorFlow has two data containers (structures):\n\nconstant (tf$constant): Creates a constant (immutable) value in the computation graph.\nvariable (tf$Variable): Creates a mutable value in the computation graph (used as parameter/weight in models).\n\nTo get started with TensorFlow, we have to load the library and check if the installation worked.\n\nlibrary(tensorflow)\nlibrary(keras)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n\n[1] TRUE\n\nimmutable = tf$constant(5.0)\nmutable = tf$constant(5.0)\n\nDon’t worry about weird messages (they will only appear once at the start of the session).\n\n\n8.1.2 Basic Operations\nWe now can define the variables and do some math with them:\n\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n\ntf.Tensor(5.0, shape=(), dtype=float32)\n\nprint(b)\n\ntf.Tensor(10.0, shape=(), dtype=float32)\n\nc = tf$add(a, b)\nprint(c)\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\nk_print_tensor(c) # Comes out of Keras!\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\n\nNormal R methods such as print() are provided by the R package “tensorflow”.\nThe TensorFlow library (created by the RStudio team) built R methods for all common operations:\n\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\nk_print_tensor(a+b)\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\n\nTheir operators also automatically transform R numbers into constant tensors when attempting to add a tensor to an R number:\n\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nTensorFlow containers are objects, what means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object). For instance, there is a method to transform the tensor object back to an R object:\n\nclass(d)\n\n[1] \"tensorflow.tensor\"                               \n[2] \"tensorflow.python.framework.ops.EagerTensor\"     \n[3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n[4] \"tensorflow.python.framework.ops.Tensor\"          \n[5] \"tensorflow.python.types.internal.NativeObject\"   \n[6] \"tensorflow.python.types.core.Tensor\"             \n[7] \"python.builtin.object\"                           \n\nclass(d$numpy())\n\n[1] \"numeric\"\n\n\n\n\n8.1.3 Data Types\nR uses dynamic typing, what means you can assign a number, character, function or whatever to a variable and the the type is automatically inferred. In other languages you have to state the type explicitly, e.g. in C:\n\nint a = 5;\nfloat a = 5.0;\nchar a = \"a\";\n\nWhile TensorFlow tries to infer the type dynamically, you must often state it explicitly. Common important types:\n\nfloat32 (floating point number with 32 bits, “single precision”)\nfloat64 (floating point number with 64 bits, “double precision”)\nint8 (integer with 8 bits)\n\nThe reason why TensorFlow is so explicit about types is that many GPUs (e.g. the NVIDIA GeForces) can handle only up to 32 bit numbers! (you do not need high precision in graphical modeling)\nBut let us see in practice what we have to do with these types and how to specifcy them:\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\n\nSo what went wrong here? We tried to divide a float32 by a float64 number, but we can only divide numbers of the same type!\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\n\nWe can also specify the type of the object by providing an object e.g. tf$float64.\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\n\nIn TensorFlow, arguments often require exact/explicit data types: TensorFlow often expects integers as arguments. In R however an integer is normally saved as float. Thus, we have to use an “L” after an integer to tell the R interpreter that it should be treated as an integer:\n\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()\n\nSkipping the “L” is one of the most common errors when using R-TensorFlow!\n\n\n8.1.4 Exercises\n\n\n\n\n\n\nQuestion: TensorFlow Operations\n\n\n\nTo run TensorFlow from R, note that you can access the different mathematical operations in TensorFlow via tf$…, e.g. there is a tf$math$… for all common math operations or the tf$linalg$… for different linear algebra operations. Tip: type tf$ and then hit the tab key to list all available options (sometimes you have to do this directly in the console).\nAn example: How to get the maximum value of a vector?\nAn example: How to get the maximum value of a vector?\n\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\nmax(y)  # Float!\ntf$math$reduce_max(y) # Float!\n\nRewrite the following expressions (a to g) in TensorFlow:\n\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n\n[1] 1\n\n# b)\nmean(x)\n\n[1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n\n[1] 1\n\n# d) \nwhich.min(x)\n\n[1] 100\n\n# e) Tip: Use Google! \norder(x)\n\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n\n[1] TRUE\n\n\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n\ntf.Tensor(1, shape=(), dtype=int32)\n\ntf$math$reduce_min(y) # Float!\n\ntf.Tensor(1.0, shape=(), dtype=float32)\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n\n[1] 50.5\n\nmean(y)\n\n[1] 50.5\n\ntf$math$reduce_mean(x)  # Integer!\n\ntf.Tensor(50, shape=(), dtype=int32)\n\ntf$math$reduce_mean(y)  # Float!\n\ntf.Tensor(50.5, shape=(), dtype=float32)\n\n# c)    which.max(x)\ntf$argmax(x)\n\ntf.Tensor(0, shape=(), dtype=int64)\n\ntf$argmax(y)\n\ntf.Tensor(0, shape=(), dtype=int64)\n\n# d)    which.min(x)\ntf$argmin(x)\n\ntf.Tensor(99, shape=(), dtype=int64)\n\n# e)    order(x)\ntf$argsort(x)\n\ntf.Tensor(\n[99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n  3  2  1  0], shape=(100), dtype=int32)\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n\ntf.Tensor(\n[[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n  10.433675  10.103772   9.608109   8.582045 ]\n [11.311237  11.200746  11.076511  10.934624  10.769221  10.570932\n  10.323349   9.993557   9.498147   8.473241 ]\n [11.186988  11.076511  10.952296  10.810434  10.645068  10.446829\n  10.199324   9.869672   9.374583   8.351139 ]\n [11.045079  10.934624  10.810434  10.668607  10.503285  10.305112\n  10.05771    9.728241   9.233568   8.212026 ]\n [10.87965   10.769221  10.645068  10.503285  10.338026  10.139942\n   9.892679   9.563459   9.069353   8.0503845]\n [10.68132   10.570932  10.446829  10.305112  10.139942   9.941987\n   9.694924   9.366061   8.872767   7.857481 ]\n [10.433675  10.323349  10.199324  10.05771    9.892679   9.694924\n   9.448175   9.119868   8.62784    7.6182513]\n [10.103772   9.993557   9.869672   9.728241   9.563459   9.366061\n   9.119868   8.79255    8.302762   7.30317  ]\n [ 9.608109   9.498147   9.374583   9.233568   9.069353   8.872767\n   8.62784    8.302762   7.818028   6.8405466]\n [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n   7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n\ntf.Tensor(True, shape=(), dtype=bool)\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Runtime\n\n\n\nThis exercise compares the speed of R to TensorFlow. The first exercise is to rewrite the following function in TensorFlow:\n\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n\nHere, we provide a skeleton for a TensorFlow function:\n\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n\nWe can compare the speed using the Microbenchmark package:\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nTry different matrix sizes for the test matrix and compare the speed.\nTip: Have a look at the the tf.reduce_mean documentation and the “axis” argument.\n\nCompare the following with different matrix sizes:\n\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nAlso try the following:\n\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\n\n\n\nClick here to see the solution\n\n\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\n\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nWarning in microbenchmark::microbenchmark(do_something_R(test),\ndo_something_TF(test)): less accurate nanosecond times to avoid potential\ninteger overflows\n\n\nUnit: microseconds\n                  expr     min       lq     mean   median       uq      max\n  do_something_R(test) 261.334 281.0345 307.6406 303.6050 313.5270 1085.680\n do_something_TF(test) 602.659 626.2750 695.9266 657.4965 676.0285 3533.708\n neval cld\n   100  a \n   100   b\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nUnit: milliseconds\n                  expr      min       lq     mean   median       uq       max\n  do_something_R(test) 5.164442 5.704391 7.705723 6.723959 8.316830 67.702521\n do_something_TF(test) 1.294575 1.577004 1.871683 1.734915 1.975093  5.039146\n neval cld\n   100  a \n   100   b\n\n\nWhy is R faster (the first time)?\n\n\nThe R functions we used (apply, mean, “-”) are also implemented in C.\n\n\nThe problem is not large enough and TensorFlow has an overhead.\n\n\n\n\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n\nUnit: milliseconds\n                                    expr       min         lq       mean\n tf$matmul(testTF, tf$transpose(testTF))   7.07619   7.375777   8.019691\n                        test %*% t(test) 161.22077 162.531503 164.182029\n     median         uq       max neval cld\n   7.577148   8.507972  11.41071   100  a \n 163.291725 164.431669 226.62451   100   b\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Linear Algebra\n\n\n\nGoogle to find out how to write the following expressions in TensorFlow:\n\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n\n[1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n# l)\neigen(A)\n\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n\n[1] 6\n\n\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n\ntf.Tensor(\n[[ 1.          0.         -0.66666667]\n [-1.          0.5        -0.16666667]\n [ 0.          0.          0.33333333]], shape=(3, 3), dtype=float64)\n\n# j)    diag(A)\ntf$linalg$diag_part(A)\n\ntf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n\ntf.Tensor(\n[[1. 0. 0.]\n [0. 2. 0.]\n [0. 0. 3.]], shape=(3, 3), dtype=float64)\n\n# l)    eigen(A)\ntf$linalg$eigh(A)\n\n[[1]]\ntf.Tensor([-0.56155281  3.          3.56155281], shape=(3), dtype=float64)\n\n[[2]]\ntf.Tensor(\n[[-0.78820544  0.         -0.61541221]\n [ 0.61541221  0.         -0.78820544]\n [ 0.          1.         -0.        ]], shape=(3, 3), dtype=float64)\n\n# m)    det(A)\ntf$linalg$det(A)\n\ntf.Tensor(6.0, shape=(), dtype=float64)\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Automatic differentation\n\n\n\nTensorFlow supports automatic differentiation (analytical and not numerical!). Let’s have a look at the function \\(f(x) = 5 x^2 + 3\\) with derivative \\(f'(x) = 10x\\). So for \\(f'(5)\\) we will get \\(10\\).\nLet’s do this in TensorFlow. Define the function:\n\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\n\nWe want to calculate the derivative for \\(x = 2.0\\):\n\nx = tf$constant(2.0)\n\nTo do automatic differentiation, we have to forward \\(x\\) through the function within the tf$GradientTape() environment. We have also have to tell TensorFlow which value to “watch”:\n\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n\nTo print the gradient:\n\n(tape$gradient(y, x))\n\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nWe can also calculate the second order derivative \\(f''(x) = 10\\):\n\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n\ntf.Tensor(10.0, shape=(), dtype=float32)\n\n\nWhat is happening here? Think about and discuss it.\nA more advanced example: Linear regression\nIn this case we first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows a normal distribution == error).\n\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\nIn R we would do the following to fit a linear regression model:\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   &lt;2e-16 ***\nx2          2.741690   0.025328 108.249   &lt;2e-16 ***\nx3          1.179181   0.023644  49.872   &lt;2e-16 ***\nx4          0.591873   0.025154  23.530   &lt;2e-16 ***\nx5          2.302417   0.022575 101.991   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,    Adjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights (\\(\\boldsymbol{w}\\)) of the regression model). We want our model to learn these weights.\nThe input (predictors, independent variables or features, \\(\\boldsymbol{X}\\)) and the observed (response, \\(\\boldsymbol{y}\\)) are constant and will not be learned/optimized.\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n\n&lt;tf.Variable 'Variable:0' shape=(5, 1) dtype=float64, numpy=\narray([[2.19290567],\n       [2.74534135],\n       [1.17146559],\n       [0.58811305],\n       [2.30174941]])&gt;\n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  2.217 2.719 1.165 0.593 2.303 \n\n\nDiscuss the code, go through the code line by line and try to understand it.\nAdditional exercise:\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, list(wTF, interceptTF))\n  optimizer$apply_gradients(purrr::transpose(list(grads, list(wTF, interceptTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n\n&lt;tf.Variable 'Variable:0' shape=(3, 1) dtype=float64, numpy=\narray([[2.48089253],\n       [2.47586968],\n       [1.00278615]])&gt;\n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  2.47 2.465 1.003 \n\nk_print_tensor(interceptTF, message = \"Resulting intercept:\\n\")\n\n&lt;tf.Variable 'Variable:0' shape=(1, 1) dtype=float64, numpy=array([[4.15394202]])&gt;\n\ncat(\"Original intercept: \", intercept, \"\\n\")\n\nOriginal intercept:  4.09"
  },
  {
    "objectID": "C1-TensorFlow.html#introduction-to-pytorch",
    "href": "C1-TensorFlow.html#introduction-to-pytorch",
    "title": "8  Introduction to TensorFlow and Keras",
    "section": "8.2 Introduction to PyTorch",
    "text": "8.2 Introduction to PyTorch\nPyTorch is another famous library for deep learning. Like TensorFlow, Torch itself is written in C++ with an API for Python. In 2020, the RStudio team released R-Torch, and while R-TensorFlow calls the Python API in the background, the R-Torch API is built directly on the C++ Torch library!\nUseful links:\n\nPyTorch documentation (This is for the Python API, bust just replace the “.” with “$”.)\nR-Torch website\n\nTo get started with Torch, we have to load the library and check if the installation worked.\n\nlibrary(torch)\n\n\n8.2.1 Data Containers\nUnlike TensorFlow, Torch doesn’t have two data containers for mutable and immutable variables. All variables are initialized via the torch_tensor function:\n\na = torch_tensor(1.)\n\nTo mark variables as mutable (and to track their operations for automatic differentiation) we have to set the argument ‘requires_grad’ to true in the torch_tensor function:\n\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)\n\n\n\n8.2.2 Basic Operations\nWe now can define the variables and do some math with them:\n\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n\nprint(b)\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n\nc = a$add(b)\nprint(c)\n\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n\n\nThe R-Torch package provides all common R methods (an advantage over TensorFlow).\n\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n\nprint(a/b)\n\ntorch_tensor\n 0.5000\n[ CPUFloatType{1} ]\n\nprint(a*b)\n\ntorch_tensor\n 50\n[ CPUFloatType{1} ]\n\n\nTheir operators also automatically transform R numbers into tensors when attempting to add a tensor to a R number:\n\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n\n\nAs for TensorFlow, we have to explicitly transform the tensors back to R:\n\nclass(d)\n\n[1] \"torch_tensor\" \"R7\"          \n\nclass(as.numeric(d))\n\n[1] \"numeric\"\n\n\n\n\n8.2.3 Data Types\nSimilar to TensorFlow:\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b \n\nBut here’s a difference! With TensorFlow we would get an error, but with R-Torch, m is automatically casted to a double (float64). However, this is still bad practice!\nDuring the course we will try to provide the corresponding PyTorch code snippets for all Keras/TensorFlow examples.\n\n\n8.2.4 Exercises\n\n\n\n\n\n\nQuestion: Torch Operations\n\n\n\nRewrite the following expressions (a to g) in torch:\n\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n\n[1] 1\n\n# b)\nmean(x)\n\n[1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n\n[1] 1\n\n# d) \nwhich.min(x)\n\n[1] 100\n\n# e) Tip: Use Google! \norder(x)\n\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n\n[1] TRUE\n\n\n\n\nClick here to see the solution\n\n\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\ntorch_min(y) # Float!\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n\n[1] 50.5\n\nmean(y)\n\n[1] 50.5\n\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n\ntorch_mean(y)  # Float!\n\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n\n# c)    which.max(x)\ntorch_argmax(x)\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\ntorch_argmax(y)\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\n# d)    which.min(x)\ntorch_argmin(x)\n\ntorch_tensor\n100\n[ CPULongType{} ]\n\n# e)    order(x)\ntorch_argsort(x)\n\ntorch_tensor\n 100\n  99\n  98\n  97\n  96\n  95\n  94\n  93\n  92\n  91\n  90\n  89\n  88\n  87\n  86\n  85\n  84\n  83\n  82\n  81\n  80\n  79\n  78\n  77\n  76\n  75\n  74\n  73\n  72\n  71\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{100} ]\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n\ntorch_tensor\n 4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n 4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n 4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n 4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n 4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n 3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n 3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n 3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n 2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n 2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n[ CPUFloatType{10,10} ]\n\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n\ntorch_tensor\n 1\n[ CPUBoolType{1} ]\n\n\n\n\n\n::: {.callout-caution icon=“false”} #### Question: Runtime\n\nWhat is the meaning of “An effect is not significant”?\nIs an effect with three *** more significant / certain than an effect with one *?\n\n\n\nClick here to see the solution\n\nThis exercise compares the speed of R to torch The first exercise is to rewrite the following function in torch:\n\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n\nHere, we provide a skeleton for a TensorFlow function:\n\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n\nWe can compare the speed using the Microbenchmark package:\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nTry different matrix sizes for the test matrix and compare the speed.\nTip: Have a look at the the torch_mean documentation and the “dim” argument.\n\nCompare the following with different matrix sizes:\n\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nAlso try the following:\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n\n\n\nClick here to see the solution\n\n\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\n\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nUnit: microseconds\n                     expr     min      lq     mean   median       uq      max\n     do_something_R(test) 260.473 276.627 294.3796 283.3510 291.4280 1100.932\n do_something_torch(test)  63.591  70.602 125.7531  75.4195  83.7425 3764.866\n neval cld\n   100  a \n   100   b\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nUnit: microseconds\n                     expr      min       lq     mean   median       uq\n     do_something_R(test) 5463.250 5757.138 7600.090 5965.705 9320.612\n do_something_torch(test)  944.517 1308.412 1737.009 1451.626 1690.041\n       max neval cld\n 28450.515   100  a \n  8954.482   100   b\n\n\nWhy is R faster (the first time)?\n\n\nThe R functions we used (apply, mean, “-”) are also implemented in C.\n\n\nThe problem is not large enough and torch has an overhead.\n\n\n\n\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n\nUnit: milliseconds\n                                   expr        min         lq       mean\n torch_matmul(testTorch, testTorch$t())   1.053618   1.250398   1.690949\n                       test %*% t(test) 163.971054 164.814608 169.907867\n     median         uq        max neval cld\n   1.384242   1.884298   5.823066   100  a \n 166.669326 169.065325 247.873946   100   b\n\n\n\n:::\n\n\n\n\n\n\nQuestion: Linear Algebra\n\n\n\nGoogle to find out how to write the following tasks in torch:\n\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n\n[1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n# l)\neigen(A)\n\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n\n[1] 6\n\n\n\n\nClick here to see the solution\n\n\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n\ntorch_tensor\n 1.0000 -0.0000 -0.6667\n-1.0000  0.5000 -0.1667\n 0.0000  0.0000  0.3333\n[ CPUFloatType{3,3} ]\n\n# j)    diag(A)\ntorch_diag(A)\n\ntorch_tensor\n 1\n 2\n 3\n[ CPUFloatType{3} ]\n\n# k)    diag(diag(A))\ntorch_diag(A)$diag()\n\ntorch_tensor\n 1  0  0\n 0  2  0\n 0  0  3\n[ CPUFloatType{3,3} ]\n\n# l)    eigen(A)\nlinalg_eigh(A)\n\n[[1]]\ntorch_tensor\n-0.5616\n 3.0000\n 3.5616\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n-0.7882  0.0000  0.6154\n 0.6154  0.0000  0.7882\n 0.0000  1.0000  0.0000\n[ CPUFloatType{3,3} ]\n\n# m)    det(A)\nlinalg_det(A)\n\ntorch_tensor\n6\n[ CPUFloatType{} ]\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Automatic differentation\n\n\n\nTorch supports automatic differentiation (analytical and not numerical!). Let’s have a look at the function \\(f(x) = 5 x^2 + 3\\) with derivative \\(f'(x) = 10x\\). So for \\(f'(5)\\) we will get \\(10\\).\nLet’s do this in torch Define the function:\n\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\n\nWe want to calculate the derivative for \\(x = 2.0\\):\n\nx = torch_tensor(2.0, requires_grad = TRUE)\n\nTo do automatic differentiation, we have to forward \\(x\\) through the function and call the $backward() method of the result:\n\ny = f(x)\ny$backward(retain_graph=TRUE )\n\nTo print the gradient:\n\nx$grad\n\ntorch_tensor\n 20\n[ CPUFloatType{1} ]\n\n\nWe can also calculate the second order derivative \\(f''(x) = 10\\):\n\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ][ grad_fn = &lt;MulBackward0&gt; ]\n\n\nWhat is happening here? Think about and discuss it.\nA more advanced example: Linear regression\nIn this case we first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows a normal distribution == error).\n\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\nIn R we would do the following to fit a linear regression model:\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   &lt;2e-16 ***\nx2          2.741690   0.025328 108.249   &lt;2e-16 ***\nx3          1.179181   0.023644  49.872   &lt;2e-16 ***\nx4          0.591873   0.025154  23.530   &lt;2e-16 ***\nx5          2.302417   0.022575 101.991   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,    Adjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights (\\(\\boldsymbol{w}\\)) of the regression model). We want our model to learn these weights.\nThe input (predictors, independent variables or features, \\(\\boldsymbol{X}\\)) and the observed (response, \\(\\boldsymbol{y}\\)) are constant and will not be learned/optimized.\n\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n\n[1] \"Loss: 4.4065318107605\"\n[1] \"Loss: 2.37926030158997\"\n[1] \"Loss: 0.901207685470581\"\n[1] \"Loss: 0.403193712234497\"\n[1] \"Loss: 0.296265542507172\"\n[1] \"Loss: 0.268377959728241\"\n[1] \"Loss: 0.232994809746742\"\n[1] \"Loss: 0.219554677605629\"\n[1] \"Loss: 0.215328559279442\"\n[1] \"Loss: 0.213282078504562\"\n\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n\nInferred weights:  0.701 3.089 1.801 1.123 3.452 \n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  0.67 3.085 1.787 1.121 3.455 \n\n\nDiscuss the code, go through the code line by line and try to understand it.\nAdditional exercise:\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\nClick here to see the solution\n\n\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n\n[1] \"Loss: 3.51533484458923\"\n[1] \"Loss: 1.74870145320892\"\n[1] \"Loss: 0.41416934132576\"\n[1] \"Loss: 0.518697261810303\"\n[1] \"Loss: 0.293963462114334\"\n[1] \"Loss: 0.263338804244995\"\n[1] \"Loss: 0.258341491222382\"\n[1] \"Loss: 0.254723280668259\"\n[1] \"Loss: 0.252453774213791\"\n[1] \"Loss: 0.25116890668869\"\n\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n\nInferred weights:  3.118 -0.349 2.107 \n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  3.131 -0.353 2.11 \n\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n\nInferred intercept:  2.836 \n\ncat(\"Original intercept: \", intercept, \"\\n\")\n\nOriginal intercept:  2.832"
  },
  {
    "objectID": "C1-TensorFlow.html#kerastorch-framework",
    "href": "C1-TensorFlow.html#kerastorch-framework",
    "title": "8  Introduction to TensorFlow and Keras",
    "section": "8.3 Keras/Torch Framework",
    "text": "8.3 Keras/Torch Framework\nWe have seen that we can use TensorFlow directly out of R, and we could use this knowledge to implement a neural network in TensorFlow directly in R. However, this can be quite cumbersome. For simple problems, it is usually faster to use a higher-level API that helps us with implementing the machine learning models in TensorFlow. The most common of those is Keras.\nKeras is a powerful framework for building and training neural networks with a few lines of codes. Since the end of 2018, Keras and TensorFlow are completely interoperable, allowing us to utilize the best of both.\nThe objective of this lesson is to familiarize yourself with Keras. If you have installed TensorFlow, Keras can be found within TensorFlow: tf.keras. However, the RStudio team has built an R package on top of tf.keras, and it is more convenient to use this. To load the Keras package, type\n\nlibrary(keras)\n# or library(torch)\n\n\n8.3.1 Example workflow in Keras / Torch\nTo show how Keras works, we will now build a small classifier to predict the three species of the iris data set. Load the necessary packages and data sets:\n\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(torch)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nFor neural networks, it is beneficial to scale the predictors (scaling = centering and standardization, see ?scale). We also split our data into predictors (X) and response (Y = the three species).\n\nX = scale(iris[,1:4])\nY = iris[,5]\n\nAdditionally, Keras/TensorFlow cannot handle factors and we have to create contrasts (one-hot encoding). To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like Python and C++, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Keep this in mind.\n\nY = to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    1    0    0\n[3,]    1    0    0\n[4,]    1    0    0\n[5,]    1    0    0\n[6,]    1    0    0\n\n\nAfter having prepared the data, we will now see a typical workflow to specify a model in Keras/Torch.\n1. Initialize a sequential model in Keras:\n\nKerasTorch\n\n\n\nmodel = keras_model_sequential()\n\n\n\nTorch users can skip this step.\n\n\n\nA sequential Keras model is a higher order type of model within Keras and consists of one input and one output model.\n2. Add hidden layers to the model (we will learn more about hidden layers during the next days).\nWhen specifying the hidden layers, we also have to specify the shape and a so called activation function. You can think of the activation function as decision for what is forwarded to the next neuron (but we will learn more about it later). If you want to know this topic in even more depth, consider watching the videos presented in section @ref(basicMath).\nThe shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).\n\nKerasTorch\n\n\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 3L, activation = \"softmax\") \n\n\n\nThe Torch syntax is very similar, we will give a list of layers to the “nn_sequential” function. Here, we have to specify the softmax activation function as an extra layer:\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\n\n\n\n\n\nsoftmax scales a potential multidimensional vector to the interval \\((0, 1]\\) for each component. The sum of all components equals 1. This might be very useful for example for handling probabilities. Ensure ther the labels start at 0! Otherwise the softmax function does not work well!\n\n3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).\nWe will learn about other options later, so for now, do not worry about the “learning_rate” (“lr” in Torch or earlier in TensorFlow) argument, cross entropy or the optimizer.\n\nKerasTorch\n\n\n\nmodel %&gt;%\n  compile(loss = loss_categorical_crossentropy,\n          keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_3 (Dense)                    (None, 20)                      100         \n dense_2 (Dense)                    (None, 20)                      420         \n dense_1 (Dense)                    (None, 20)                      420         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 1,003\nTrainable params: 1,003\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\nSpecify optimizer and the parameters which will be trained (in our case the parameters of the network):\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n\n\n\n\n4. Fit model in 30 iterations (epochs)\n\nKerasTorch\n\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel_history =\n  model %&gt;%\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\n\n\n\nIn Torch, we jump directly to the training loop, however, here we have to write our own training loop:\n\nGet a batch of data.\nPredict on batch.\nCcalculate loss between predictions and true labels.\nBackpropagate error.\nUpdate weights.\nGo to step 1 and repeat.\n\n\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n\n\n\n\n5. Plot training history:\n\nKerasTorch\n\n\n\nplot(model_history)\n\n\n\n\n\n\n\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\n\n\n\n\n\n\n\n6. Create predictions:\n\nKerasTorch\n\n\n\npredictions = predict(model, X) # Probabilities for each class.\n\nGet probabilities:\n\nhead(predictions) # Quasi-probabilities for each species.\n\n          [,1]        [,2]         [,3]\n[1,] 0.9915600 0.006817889 0.0016221496\n[2,] 0.9584184 0.037489697 0.0040918575\n[3,] 0.9910416 0.007848956 0.0011094128\n[4,] 0.9813542 0.016901711 0.0017440914\n[5,] 0.9949830 0.004031503 0.0009855649\n[6,] 0.9905725 0.006884387 0.0025430375\n\n\nFor each plant, we want to know for which species we got the highest probability:\n\npreds = apply(predictions, 1, which.max) \nprint(preds)\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 2 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\n\n\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\n\n\n7. Calculate Accuracy (how often we have been correct):\n\nKerasTorch\n\n\n\nmean(preds == as.integer(iris$Species))\n\n[1] 0.9066667\n\n\n\n\n\nmean(preds_torch == as.integer(iris$Species))\n\n[1] 0.98\n\n\n\n\n\n8. Plot predictions, to see if we have done a good job:\n\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\n\n\n\npar(oldpar)   # Reset par.\n\nSo you see, building a neural network is very easy with Keras or Torch and you can already do it on your own.\n\n\n8.3.2 Exercises\n  \n  1. Task\nWe will now build a regression for the airquality data set with Keras/Torch. We want to predict the variable “Ozone”.\nTasks: 1. Complete the steps and the code chunk so that the model is successfully trained! 2. Try different learning rates and neural network sizes (increase/decrease number of hidden layers and neurons (units) in each layer). What happes?\n\n\nKeras\n\nLoad and prepare the data set:\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\n\nExplore the data with summary() and plot():\n\nsummary(data)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\nplot(data)\n\n\n\n\n\nThere are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don’t know how to remove NAs from a data.frame, use Google (e.g. with the query: “remove-rows-with-all-or-some-nas-missing-values-in-data-frame”).\nSplit the data in predictors (\\(\\boldsymbol{X}\\)) and response (\\(\\boldsymbol{y}\\), Ozone) and scale the \\(\\boldsymbol{X}\\) matrix.\nBuild a sequential Keras model.\nAdd hidden layers (input and output layer are already specified, you have to add hidden layers between them):\n\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %&gt;%\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\n\n\nWhy do we use 5L as input shape?\nWhy only one output node and “linear” activation layer?\n\n\nCompile model.\n\n\nmodel %&gt;%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nWhat is the “mean_squared_error” loss?\n\nFit model:\n\nTip: Only matrices are accepted for \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{y}\\) by Keras. R often drops a one column matrix into a vector (change it back to a matrix!)\n\nPlot training history.\nCreate predictions.\nCompare your Keras model with a linear model:\n\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\nprint(rmse_keras)\n\n\n\nTorch\n\nLoad and prepare the data set:\n\n\nlibrary(torch)\n\ndata = airquality\n\nExplore the data with summary() and plot():\n\nsummary(data)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\nplot(data)\n\n\n\n\n\nThere are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don’t know how to remove NAs from a data.frame, use Google (e.g. with the query: “remove-rows-with-all-or-some-nas-missing-values-in-data-frame”).\nSplit the data in predictors (\\(\\boldsymbol{X}\\)) and response (\\(\\boldsymbol{y}\\), Ozone) and scale the \\(\\boldsymbol{X}\\) matrix.\nPass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):\n\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    ...\n    nn_linear(20L, 1L),\n  )\n\n\nWhy do we use 5L as input shape?\nWhy only one output node and no activation layer?\n\n\nCreate optimizer\n\nWe have to pass the network’s parameters to the optimizer (how is this different to keras?)\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n\n\nFit model\n\nIn torch we have to write the trainings loop on our own. Complete the trainings loop:\nTips:\n\nNumber of training $ steps = Number of rows / batchsize * Epochs $\nSearch torch::nnf_… for the correct loss function (mse…)\nMake sure that X_torch and Y_torch have the same data type! (you can set the dtype via torch_tensor(…, dtype = …)) _ Check the dimension of Y_torch, we need a matrix!\n\n\n# Calculate number of training steps.\nepochs = ...\nbatch_size = 32\nsteps = ...\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, ...) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = ...\n  Y_batch = ...\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = ...\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n\n\nPlot training history.\nCreate predictions.\nCompare your Torch model with a linear model:\n\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\nprint(rmse_torch)\n\n\n\n  \n    \n      Solution\n    \n    \n\ndata = airquality\n\n1. There are NAs in the data, which we have to remove because Keras and Torch cannot handle NAs!\n\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n\n\n2. Split the data in predictors and response and scale the matrix.\n\nx = scale(data[,2:6])\ny = data[,1]\n\n\n\nKeras\n3. Build sequential Keras model.\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\n4. Add hidden layers (input and output layer are already specified, you have to add hidden layers between them).\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 1L, activation = \"linear\")\n\nWe use 5L as input shape, because we have 5 predictors. Analogously, we use 1L for our 1d response. Because we do not want any compression, dilation or other nonlinear effects, we use the simple linear layer (equal to no activation function at all). For more about activation functions, look for example here. Or wait for the next days. You may also have seen the previously shown link about activation functions in more detail.\n5. Compile model.\n\nmodel %&gt;%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nThe mean_squared_error is the ordinary least squares approach in regression analysis.\n6. Fit model.\n\nmodel_history =\n  model %&gt;%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\n7. Plot training history.\n\nplot(model_history)\n\n\n\nmodel %&gt;%\n  evaluate(x, y)\n\n    loss \n147.5745 \n\n\n8. Create predictions.\n\npred_keras = predict(model, x)\n\n9. Compare Keras model with a linear model.\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_keras)\n\n[1] 9.067499\n\n\n\n\nTorch\n3. Pass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):\n\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\nWe use 5L as input shape, because we have 5 predictors. Analogously, we use 1L for our 1d response. Because we do not want any compression, dilation or other nonlinear effects, we use the simple linear layer (equal to no activation function at all). For more about activation functions, look for example here. Or wait for the next days. You may also have seen the previously shown link about activation functions in more detail.\n4. Create optimizer\nWe have to pass the network’s parameters to the optimizer (how is this different to keras?)\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n\nIn keras we use the compile function to pass a optimizer and a loss function to the model whereas in torch we have to pass the network’s parameters to the optimizer.\n5. Fit model\nIn torch we have to write the trainings loop on our own. Complete the trainings loop:\n\n# Calculate number of training steps.\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n\n6. Plot training history.\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\n7. Create predictions.\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\n8. Compare your Torch model with a linear model:\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\nprint(rmse_torch)\n\n\n\nLook at this slightly more complex model and compare the loss plot and the accuracy in contrast to the former.\n\n\nKeras\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %&gt;%\n  layer_dense(units = 20L, activation = \"relu\") %&gt;%\n  layer_dense(units = 30L, activation = \"relu\") %&gt;%\n  layer_dense(units = 20L, activation = \"relu\") %&gt;%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %&gt;%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nmodel_history =\n  model %&gt;%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\n\n\nmodel %&gt;%\n  evaluate(x, y)\n\n    loss \n210.4453 \n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_keras)\n\n[1] 10.51122\n\n\n\n\nTorch\n\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\n\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_torch)\n\n[1] 5.12053\n\n\n\n\nYou see, the more complex model works better, because it can learn the functional form between the features and the response better (if necessary). But keep the overfitting problem in mind!\nLook at the little change in learning rates for the next 2 models and compare the loss plot and the accuracy in contrast to the former.\n\n\nKeras\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %&gt;%\n  layer_dense(units = 20L, activation = \"relu\") %&gt;%\n  layer_dense(units = 30L, activation = \"relu\") %&gt;%\n  layer_dense(units = 20L, activation = \"relu\") %&gt;%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %&gt;%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.1))\n\nmodel_history =\n  model %&gt;%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\n\n\nmodel %&gt;%\n  evaluate(x, y)\n\n    loss \n56.70872 \n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_keras)\n\n[1] 5.661808\n\n\n\n\nTorch\n\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.1)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\n\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_torch)\n\n[1] 6.461597\n\n\n\n\nYou can see, the higher learning rate yields a little bit worse results. The optimum is jumped over.\n\n\nKeras\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel = keras_model_sequential()\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %&gt;%\n  layer_dense(units = 20L, activation = \"relu\") %&gt;%\n  layer_dense(units = 30L, activation = \"relu\") %&gt;%\n  layer_dense(units = 20L, activation = \"relu\") %&gt;%\n  layer_dense(units = 1L, activation = \"linear\")\n\nmodel %&gt;%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.001))\n\nmodel_history =\n  model %&gt;%\n  fit(x = x, y = matrix(y, ncol = 1L), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\nplot(model_history)\n\n\n\nmodel %&gt;%\n  evaluate(x, y)\n\n    loss \n340.8205 \n\npred_keras = predict(model, x)\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\n\nrmse_keras = mean(sqrt((y - pred_keras)^2))\n\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_keras)\n\n[1] 13.18632\n\n\n\n\nTorch\n\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 30L),\n    nn_relu(),\n    nn_linear(30L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\nmodel_torch$train()\nlog_losses = NULL\nfor(i in 1:steps){\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  log_losses[i] = as.numeric(loss)\n}\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\n\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_torch)\n\n[1] 12.48754\n\n\n\n\nYou can see, that for the lower learning rate, the optimum (compared to the run with learning rate 0.05) is not yet reached (to few epochs have gone by). But also here, mind the overfitting problem. For too many epochs, things might get worse!\n    \n  \n  \n  \n  2. Task\nThe next task differs for Torch and Keras users. Keras users will learn more about the inner working of training while Torch users will learn how to simplify and generalize the training loop.\n\n\nKeras\nSimilar to Torch, here we will write the training loop ourselves in the following. The training loop consists of several steps:\n\nSample batches of X and Y data\nOpen the gradientTape to create a computational graph (autodiff)\nMake predictions and calculate loss\nUpdate parameters based on the gradients at the loss (go back to 1. and repeat)\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n\n$bX\n        Solar.R        Wind        Temp      Month        Day\n87  -1.13877323 -0.37654514  0.44147123 -0.1467431  1.1546835\n117  0.58361881 -1.83815816  0.33653910  0.5319436  1.0398360\n129 -1.01809608  1.56290291  0.65133550  1.2106304 -1.1422676\n121  0.44100036 -2.14734553  1.70065685  0.5319436  1.4992262\n91   0.74817856 -0.71384046  0.54640337 -0.1467431  1.6140738\n137 -1.76410028  0.26993754 -0.71278225  1.2106304 -0.2234871\n21  -1.93963068 -0.06735777 -1.97196786 -1.5041165  0.5804458\n141 -1.73118833  0.10128988 -0.18812157  1.2106304  0.2359031\n78   0.97856221  0.10128988  0.44147123 -0.1467431  0.1210555\n15  -1.31430363  0.91642022 -2.07689999 -1.5041165 -0.1086396\n38  -0.63412333 -0.06735777  0.44147123 -0.8254298 -1.0274200\n49  -1.62148183 -0.20789749 -1.34237505 -0.8254298  0.2359031\n123  0.03508631 -1.02302783  1.70065685  0.5319436  1.7289213\n136  0.58361881 -1.02302783 -0.08318944  1.2106304 -0.3383347\n120  0.19964606 -0.06735777  2.01545325  0.5319436  1.3843787\n114 -1.63245248  1.22560759 -0.60785011  0.5319436  0.6952933\n145 -1.87380678 -0.20789749 -0.71278225  1.2106304  0.6952933\n140  0.43002971  1.08506788 -1.13251078  1.2106304  0.1210555\n64   0.56167751 -0.20789749  0.33653910 -0.1467431 -1.4868103\n118  0.33129386 -0.54519280  0.86119977  0.5319436  1.1546835\n128 -0.98518413 -0.71384046  0.96613190  1.2106304 -1.2571152\n62   0.92370896 -1.64140257  0.65133550 -0.1467431 -1.7165054\n125  0.13382216 -1.36032314  1.49079258  1.2106304 -1.6016578\n4    1.40641756  0.43858520 -1.65717146 -1.5041165 -1.3719627\n79   1.09923936 -1.02302783  0.65133550 -0.1467431  0.2359031\n82  -1.95060133 -0.85438017 -0.39798584 -0.1467431  0.5804458\n149  0.08993956 -0.85438017 -0.81771438  1.2106304  1.1546835\n17   1.34059366  0.57912491 -1.23744292 -1.5041165  0.1210555\n48   1.08826871  3.02451593 -0.60785011 -0.8254298  0.1210555\n130  0.73720791  0.26993754  0.23160696  1.2106304 -1.0274200\n132  0.49585361  0.26993754 -0.29305371  1.2106304 -0.7977249\n30   0.41905906 -1.19167548  0.12667483 -1.5041165  1.6140738\n\n$bY\n [1]  20 168  32 118  64   9   1  13  35  18  29  20  85  28  76   9  23  18  32\n[20]  73  47 135  78  18  61  16  30  34  37  20  21 115\n\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n\nLoss:  1444.033 \nLoss:  488.953 \nLoss:  270.0465 \nLoss:  450.0282 \nLoss:  138.2488 \nLoss:  227.6001 \nLoss:  216.2361 \nLoss:  109.9781 \nLoss:  352.7486 \nLoss:  239.2065 \nLoss:  234.0703 \nLoss:  224.0462 \nLoss:  227.475 \nLoss:  336.5538 \nLoss:  348.1582 \nLoss:  158.787 \nLoss:  209.5738 \nLoss:  321.0661 \nLoss:  232.6139 \nLoss:  289.6932 \n\n\n\n\nTorch\nKeras and Torch use dataloaders to generate the data batches. Dataloaders are objects that return batches of data infinetly. Keras create the dataloader object automatically in the fit function, in Torch we have to write them ourselves:\n\nDefine a dataset object. This object informs the dataloader function about the inputs, outputs, length (nrow), and how to sample from it.\nCreate an instance of the dataset object by calling it and passing the actual data to it\nPass the initiated dataset to the dataloader function\n\n\nlibrary(torch)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = matrix(data[,1], ncol = 1L)\n\n\ntorch_dataset = torch::dataset(\n    name = \"airquality\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\nOur dataloader is again an object which has to be initiated. The initiated object returns a list of two elements, batch x and batch y. The initated object stops returning batches when the dataset was completly transversed (no worries, we don’t have to all of this ourselves).\nOur training has also changed now:\n\nmodel_torch = nn_sequential(\n  nn_linear(5L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 1L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n\nLoss at epoch 10: 387.950073\nLoss at epoch 20: 282.698288\nLoss at epoch 30: 257.855043\nLoss at epoch 40: 244.420750\nLoss at epoch 50: 217.362108\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n\n\n\n\nThe previous sampling wasn’t ideal, why?\n\n\nNow change the code from above for the iris data set. Tip: In tf$keras$losses$… you can find various loss functions.\n  \n    \n      Solution\n    \n    \n\n\nKeras\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n\nLoss:  0.002633849 \nLoss:  0.0005500487 \nLoss:  0.001006462 \nLoss:  0.0001315936 \nLoss:  0.0004843124 \nLoss:  0.0004023896 \nLoss:  0.0004356128 \nLoss:  0.000235351 \nLoss:  4.823796e-05 \nLoss:  0.0001512702 \nLoss:  0.0002624761 \nLoss:  0.0001274793 \nLoss:  7.111725e-05 \nLoss:  0.0001509234 \nLoss:  0.0002024032 \nLoss:  0.0001532886 \nLoss:  9.489701e-05 \nLoss:  0.0001040314 \nLoss:  7.334561e-05 \nLoss:  2.743953e-05 \nLoss:  9.655961e-05 \nLoss:  2.361947e-05 \nLoss:  6.918395e-05 \nLoss:  1.603245e-05 \nLoss:  1.772152e-05 \nLoss:  2.512357e-05 \n\n\n\n\nTorch\n\nlibrary(torch)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = as.integer(iris$Species)\n\n\ntorch_dataset = torch::dataset(\n    name = \"iris\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(4L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 3L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n\nLoss at epoch 10: 16.298814\nLoss at epoch 20: 8.492696\nLoss at epoch 30: 5.744957\nLoss at epoch 40: 4.344102\nLoss at epoch 50: 3.493563\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n\n\n\n\n\n\nRemarks:\n\nMind the different input and output layer numbers.\nThe loss function increases randomly, because different subsets of the data were drawn. This is a downside of stochastic gradient descent.\nA positive thing about stochastic gradient descent is, that local valleys or hills may be left and global ones can be found instead."
  },
  {
    "objectID": "C1-TensorFlow.html#basicMath",
    "href": "C1-TensorFlow.html#basicMath",
    "title": "8  Introduction to TensorFlow and Keras",
    "section": "8.4 Underlying mathematical concepts - optional",
    "text": "8.4 Underlying mathematical concepts - optional\nIf are not yet familiar with the underlying concepts of neural networks and want to know more about that, it is suggested to read / view the following videos / sites. Consider the Links and videos with descriptions in parentheses as optional bonus.\nThis might be useful to understand the further concepts in more depth.\n\n(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially the animated graphic is interesting).)\nhttps://en.wikipedia.org/wiki/Gradient_descent#Description\nNeural networks (Backpropagation, etc.).\nActivation functions in detail (requires the above as prerequisite).\n\nVideos about the topic:\n\nGradient descent explained\n\n\n\n\n(Stochastic gradient descent explained)\n\n\n\n\n(Entropy explained)\n\n\n\n\nShort explanation of entropy, cross entropy and Kullback–Leibler divergence\n\n\n\n\nDeep Learning (chapter 1)\n\n\n\n\nHow neural networks learn - Deep Learning (chapter 2)\n\n\n\n\nBackpropagation - Deep Learning (chapter 3)\n\n\n\n\nAnother video about backpropagation (extends the previous one) - Deep Learning (chapter 4)\n\n\n\n\n8.4.1 Caveats of neural network optimization\nDepending on activation functions, it might occur that the network won’t get updated, even with high learning rates (called vanishing gradient, especially for “sigmoid” functions). Furthermore, updates might overshoot (called exploding gradients) or activation functions will result in many zeros (especially for “relu”, dying relu).\nIn general, the first layers of a network tend to learn (much) more slowly than subsequent ones."
  },
  {
    "objectID": "C2-DeepNeuralNetworks.html",
    "href": "C2-DeepNeuralNetworks.html",
    "title": "9  Deep Neural Networks",
    "section": "",
    "text": "Artificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\nlibrary(cito)"
  },
  {
    "objectID": "C3-ConvolutionalNeuralNetworks.html",
    "href": "C3-ConvolutionalNeuralNetworks.html",
    "title": "10  Convolutional Neural Networks",
    "section": "",
    "text": "Artificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\nlibrary(cito)"
  },
  {
    "objectID": "C4-RecurrentNeuralNetworks.html",
    "href": "C4-RecurrentNeuralNetworks.html",
    "title": "11  Recurrent Neural Networks",
    "section": "",
    "text": "Artificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\n\nlibrary(cito)"
  },
  {
    "objectID": "Appendix-Datasets.html#titanic",
    "href": "Appendix-Datasets.html#titanic",
    "title": "Appendix A — Datasets",
    "section": "A.1 Titanic",
    "text": "A.1 Titanic\nThe data set is a collection of Titanic passengers with information about their age, class, sex, and their survival status. The competition is simple here: Train a machine learning model and predict the survival probability.\nThe Titanic data set is very well explored and serves as a stepping stone in many machine learning careers. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “survived”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(titanic_ml)\ntitanic = titanic_ml\nsummary(titanic)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\ntitanic_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest)\ntitanic_imputed = missRanger::missRanger(data = titanic_imputed %&gt;%\n                                           select(-survived))\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      age, fare, body\n  Variables used to impute: pclass, sex, age, sibsp, parch, fare, embarked, body\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\niter 2\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\niter 3\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |======================================================================| 100%\n\ntitanic_imputed$survived = titanic$survived\n\n\nSplit into training and test set:\n\n\ntrain = titanic_imputed[!is.na(titanic$survived), ]\ntest = titanic_imputed[is.na(titanic$survived), ]\n\n\nTrain model:\n\n\nmodel = glm(survived~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, data = test, type = \"response\")\nhead(preds)\n\n       561        321       1177       1098       1252       1170 \n0.79095923 0.30597519 0.01400693 0.12310859 0.14099292 0.11768284 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")\n\nAnd submit the csv on http://rhsbio7.uni-regensburg.de:8500."
  },
  {
    "objectID": "Appendix-Datasets.html#plant-pollinator-database",
    "href": "Appendix-Datasets.html#plant-pollinator-database",
    "title": "Appendix A — Datasets",
    "section": "A.2 Plant-pollinator Database",
    "text": "A.2 Plant-pollinator Database\nThe plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look here.\n\n\n\n\n\nResponse variable: “interaction”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n\n                   crop                       insect          type          \n Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n Daucus_carota       :  256   Andrena_gravida    :   80                     \n Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n (Other)             :18944   (Other)            :20000                     \n    season             diameter        corolla             colour         \n Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n Class :character   1st Qu.:  5.00   Class :character   Class :character  \n Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n                    Mean   : 27.03                                        \n                    3rd Qu.: 25.00                                        \n                    Max.   :150.00                                        \n                    NA's   :9472                                          \n    nectar            b.system         s.pollination      inflorescence     \n Length:20480       Length:20480       Length:20480       Length:20480      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  composite            guild               tongue            body      \n Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n                                       Mean   : 8.104   Mean   :10.66  \n                                       3rd Qu.:10.500   3rd Qu.:13.00  \n                                       Max.   :26.400   Max.   :25.00  \n                                       NA's   :17040    NA's   :6160   \n  sociality           feeding          interaction \n Length:20480       Length:20480       0   :14095  \n Class :character   Class :character   1   :  595  \n Mode  :character   Mode  :character   NA's: 5790  \n                                                   \n                                                   \n                                                   \n                                                   \n\n\n\nImpute missing values (not our response variable!) We will select only a few predictors here (you can work with all predictors of course).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nplant_poll_imputed = plant_poll %&gt;% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction)\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %&gt;%\n                                              select(-interaction))\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      diameter, corolla, tongue, body\n  Variables used to impute: diameter, corolla, tongue, body\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\niter 2\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\niter 3\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\nplant_poll_imputed$interaction = plant_poll$interaction\n\n\nSplit into training and test set:\n\n\ntrain = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]\ntest = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ]\n\n\nTrain model:\n\n\nmodel = glm(interaction~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, newdata = test, type = \"response\")\nhead(preds)\n\n         1          2          3          4          5          6 \n0.02942746 0.05063489 0.03780247 0.03780247 0.02651142 0.04130643 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#wine",
    "href": "Appendix-Datasets.html#wine",
    "title": "Appendix A — Datasets",
    "section": "A.3 Wine",
    "text": "A.3 Wine\nThe data set is a collection of wines of different quality. The aim is to predict the quality of the wine based on physiochemical predictors.\nFor inspiration and data exploration notebooks, check out this kaggle competition. For instance, check out this very nice notebook which removes a few problems from the data.\nResponse variable: “quality”\nWe could theoretically use a regression model for this task but we will stick with a classification model.\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(wine)\nsummary(wine)\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900  \n 1st Qu.: 7.100   1st Qu.:0.3900   1st Qu.:0.0900   1st Qu.: 1.900  \n Median : 7.900   Median :0.5200   Median :0.2600   Median : 2.200  \n Mean   : 8.335   Mean   :0.5284   Mean   :0.2705   Mean   : 2.533  \n 3rd Qu.: 9.300   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600  \n Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500  \n NA's   :70       NA's   :48       NA's   :41       NA's   :60      \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  \n Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  \n Mean   :0.08747   Mean   :15.83       Mean   : 46.23       Mean   :0.9968  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9979  \n Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  \n NA's   :37        NA's   :78          NA's   :78           NA's   :78      \n       pH          sulphates         alcohol         quality     \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n 1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000  \n Mean   :3.311   Mean   :0.6572   Mean   :10.42   Mean   :5.596  \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n NA's   :25      NA's   :51                       NA's   :905    \n\n\n\nImpute missing values (not our response variable!).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nwine_imputed = missRanger::missRanger(data = wine %&gt;% select(-quality))\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates\n  Variables used to impute: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 2\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 3\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 4\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\niter 5\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\n\nwine_imputed$quality = wine$quality\n\n\nSplit into training and test set:\n\n\ntrain = wine_imputed[!is.na(wine$quality), ]\ntest = wine_imputed[is.na(wine$quality), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(quality~., data = train, classification = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions\nhead(preds)\n\n[1] 6 5 5 7 6 6\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#nasa",
    "href": "Appendix-Datasets.html#nasa",
    "title": "Appendix A — Datasets",
    "section": "A.4 Nasa",
    "text": "A.4 Nasa\nA collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “Hazardous”\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(nasa)\nsummary(nasa)\n\n Neo.Reference.ID       Name         Absolute.Magnitude Est.Dia.in.KM.min.\n Min.   :2000433   Min.   :2000433   Min.   :11.16      Min.   : 0.00101  \n 1st Qu.:3102682   1st Qu.:3102683   1st Qu.:20.10      1st Qu.: 0.03346  \n Median :3514800   Median :3514800   Median :21.90      Median : 0.11080  \n Mean   :3272675   Mean   :3273113   Mean   :22.27      Mean   : 0.20523  \n 3rd Qu.:3690987   3rd Qu.:3690385   3rd Qu.:24.50      3rd Qu.: 0.25384  \n Max.   :3781897   Max.   :3781897   Max.   :32.10      Max.   :15.57955  \n NA's   :53        NA's   :57        NA's   :36         NA's   :60        \n Est.Dia.in.KM.max. Est.Dia.in.M.min.   Est.Dia.in.M.max. \n Min.   : 0.00226   Min.   :    1.011   Min.   :    2.26  \n 1st Qu.: 0.07482   1st Qu.:   33.462   1st Qu.:   74.82  \n Median : 0.24777   Median :  110.804   Median :  247.77  \n Mean   : 0.45754   Mean   :  204.649   Mean   :  458.45  \n 3rd Qu.: 0.56760   3rd Qu.:  253.837   3rd Qu.:  567.60  \n Max.   :34.83694   Max.   :15579.552   Max.   :34836.94  \n NA's   :23         NA's   :29          NA's   :46        \n Est.Dia.in.Miles.min. Est.Dia.in.Miles.max. Est.Dia.in.Feet.min.\n Min.   :0.00063       Min.   : 0.00140      Min.   :    3.32    \n 1st Qu.:0.02079       1st Qu.: 0.04649      1st Qu.:  109.78    \n Median :0.06885       Median : 0.15395      Median :  363.53    \n Mean   :0.12734       Mean   : 0.28486      Mean   :  670.44    \n 3rd Qu.:0.15773       3rd Qu.: 0.35269      3rd Qu.:  832.80    \n Max.   :9.68068       Max.   :21.64666      Max.   :51114.02    \n NA's   :42            NA's   :50            NA's   :21          \n Est.Dia.in.Feet.max. Close.Approach.Date Epoch.Date.Close.Approach\n Min.   :     7.41    2016-07-22:  18     Min.   :7.889e+11        \n 1st Qu.:   245.49    2015-01-15:  17     1st Qu.:1.016e+12        \n Median :   812.88    2015-02-15:  16     Median :1.203e+12        \n Mean   :  1500.77    2007-11-08:  15     Mean   :1.180e+12        \n 3rd Qu.:  1862.19    2012-01-15:  15     3rd Qu.:1.356e+12        \n Max.   :114294.42    (Other)   :4577     Max.   :1.473e+12        \n NA's   :46           NA's      :  29     NA's   :43               \n Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr Miles.per.hour   \n Min.   : 0.3355              Min.   :  1208              Min.   :  750.5  \n 1st Qu.: 8.4497              1st Qu.: 30399              1st Qu.:18846.7  \n Median :12.9370              Median : 46532              Median :28893.7  \n Mean   :13.9848              Mean   : 50298              Mean   :31228.0  \n 3rd Qu.:18.0774              3rd Qu.: 65068              3rd Qu.:40436.9  \n Max.   :44.6337              Max.   :160681              Max.   :99841.2  \n NA's   :27                   NA's   :28                  NA's   :38       \n Miss.Dist..Astronomical. Miss.Dist..lunar.   Miss.Dist..kilometers.\n Min.   :0.00018          Min.   :  0.06919   Min.   :   26610      \n 1st Qu.:0.13341          1st Qu.: 51.89874   1st Qu.:19964907      \n Median :0.26497          Median :103.19415   Median :39685408      \n Mean   :0.25690          Mean   : 99.91366   Mean   :38436154      \n 3rd Qu.:0.38506          3rd Qu.:149.59244   3rd Qu.:57540318      \n Max.   :0.49988          Max.   :194.45491   Max.   :74781600      \n NA's   :60               NA's   :30          NA's   :56            \n Miss.Dist..miles.  Orbiting.Body    Orbit.ID     \n Min.   :   16535   Earth:4665    Min.   :  1.00  \n 1st Qu.:12454813   NA's :  22    1st Qu.:  9.00  \n Median :24662435                 Median : 16.00  \n Mean   :23885560                 Mean   : 28.34  \n 3rd Qu.:35714721                 3rd Qu.: 31.00  \n Max.   :46467132                 Max.   :611.00  \n NA's   :27                       NA's   :33      \n        Orbit.Determination.Date Orbit.Uncertainity Minimum.Orbit.Intersection\n 2017-06-21 06:17:20:   9        Min.   :0.000      Min.   :0.00000           \n 2017-04-06 08:57:13:   8        1st Qu.:0.000      1st Qu.:0.01435           \n 2017-04-06 09:24:24:   8        Median :3.000      Median :0.04653           \n 2017-04-06 08:24:13:   7        Mean   :3.521      Mean   :0.08191           \n 2017-04-06 08:26:19:   7        3rd Qu.:6.000      3rd Qu.:0.12150           \n (Other)            :4622        Max.   :9.000      Max.   :0.47789           \n NA's               :  26        NA's   :49         NA's   :137               \n Jupiter.Tisserand.Invariant Epoch.Osculation   Eccentricity    \n Min.   :2.196               Min.   :2450164   Min.   :0.00752  \n 1st Qu.:4.047               1st Qu.:2458000   1st Qu.:0.24086  \n Median :5.071               Median :2458000   Median :0.37251  \n Mean   :5.056               Mean   :2457723   Mean   :0.38267  \n 3rd Qu.:6.017               3rd Qu.:2458000   3rd Qu.:0.51256  \n Max.   :9.025               Max.   :2458020   Max.   :0.96026  \n NA's   :56                  NA's   :60        NA's   :39       \n Semi.Major.Axis   Inclination       Asc.Node.Longitude Orbital.Period  \n Min.   :0.6159   Min.   : 0.01451   Min.   :  0.0019   Min.   : 176.6  \n 1st Qu.:1.0012   1st Qu.: 4.93290   1st Qu.: 83.1849   1st Qu.: 365.9  \n Median :1.2422   Median :10.27694   Median :172.6347   Median : 504.9  \n Mean   :1.4009   Mean   :13.36159   Mean   :172.1717   Mean   : 635.5  \n 3rd Qu.:1.6782   3rd Qu.:19.47848   3rd Qu.:254.8804   3rd Qu.: 793.1  \n Max.   :5.0720   Max.   :75.40667   Max.   :359.9059   Max.   :4172.2  \n NA's   :53       NA's   :42         NA's   :60         NA's   :46      \n Perihelion.Distance Perihelion.Arg     Aphelion.Dist    Perihelion.Time  \n Min.   :0.08074     Min.   :  0.0069   Min.   :0.8038   Min.   :2450100  \n 1st Qu.:0.63038     1st Qu.: 95.6430   1st Qu.:1.2661   1st Qu.:2457815  \n Median :0.83288     Median :189.7729   Median :1.6182   Median :2457972  \n Mean   :0.81316     Mean   :184.0185   Mean   :1.9864   Mean   :2457726  \n 3rd Qu.:0.99718     3rd Qu.:271.9535   3rd Qu.:2.4497   3rd Qu.:2458108  \n Max.   :1.29983     Max.   :359.9931   Max.   :8.9839   Max.   :2458839  \n NA's   :22          NA's   :48         NA's   :38       NA's   :59       \n  Mean.Anomaly       Mean.Motion       Equinox       Hazardous    \n Min.   :  0.0032   Min.   :0.08628   J2000:4663   Min.   :0.000  \n 1st Qu.: 87.0069   1st Qu.:0.45147   NA's :  24   1st Qu.:0.000  \n Median :186.0219   Median :0.71137                Median :0.000  \n Mean   :181.2882   Mean   :0.73732                Mean   :0.176  \n 3rd Qu.:276.6418   3rd Qu.:0.98379                3rd Qu.:0.000  \n Max.   :359.9180   Max.   :2.03900                Max.   :1.000  \n NA's   :40         NA's   :48                     NA's   :4187   \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nnasa_imputed = missRanger::missRanger(data = nasa %&gt;% select(-Hazardous),\n                                      maxiter = 1, num.trees = 5L)\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox\n  Variables used to impute: Neo.Reference.ID, Name, Absolute.Magnitude, Est.Dia.in.KM.min., Est.Dia.in.KM.max., Est.Dia.in.M.min., Est.Dia.in.M.max., Est.Dia.in.Miles.min., Est.Dia.in.Miles.max., Est.Dia.in.Feet.min., Est.Dia.in.Feet.max., Close.Approach.Date, Epoch.Date.Close.Approach, Relative.Velocity.km.per.sec, Relative.Velocity.km.per.hr, Miles.per.hour, Miss.Dist..Astronomical., Miss.Dist..lunar., Miss.Dist..kilometers., Miss.Dist..miles., Orbiting.Body, Orbit.ID, Orbit.Determination.Date, Orbit.Uncertainity, Minimum.Orbit.Intersection, Jupiter.Tisserand.Invariant, Epoch.Osculation, Eccentricity, Semi.Major.Axis, Inclination, Asc.Node.Longitude, Orbital.Period, Perihelion.Distance, Perihelion.Arg, Aphelion.Dist, Perihelion.Time, Mean.Anomaly, Mean.Motion, Equinox\n\niter 1\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\nnasa_imputed$Hazardous = nasa$Hazardous\n\n\nSplit into training and test set:\n\n\ntrain = nasa_imputed[!is.na(nasa$Hazardous), ]\ntest = nasa_imputed[is.na(nasa$Hazardous), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(Hazardous~., data = train, classification = TRUE,\n            probability = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions[,2]\nhead(preds)\n\n[1] 0.6348055556 0.7525960317 0.0008444444 0.7733373016 0.1404333333\n[6] 0.1509190476\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#flower",
    "href": "Appendix-Datasets.html#flower",
    "title": "Appendix A — Datasets",
    "section": "A.5 Flower",
    "text": "A.5 Flower\nA collection of over 4000 flower images of 5 plant species. The data set is from kaggle but we downsampled the images from \\(320*240\\) to \\(80*80\\) pixels. You can a) download the data set here or b) get it via the EcoData package.\nNotes:\n\nCheck out convolutional neural network notebooks on kaggle (they are often written in Python but you can still copy the architectures), e.g. this one.\nLast year’s winners have used a transfer learning approach (they achieved around 70% accuracy), check out this notebook, see also the section about transfer learning @ref(transfer).\n\nResponse variable: “Plant species”\n\nLoad data set:\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\ntrain = EcoData::dataset_flower()$train/255\ntest = EcoData::dataset_flower()$test/255\nlabels = EcoData::dataset_flower()$labels\n\nLet’s visualize a flower:\n\ntrain[100,,,] %&gt;%\n  image_to_array() %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\n\n\nBuild and train model:\n\n\nmodel = keras_model_sequential()\nmodel %&gt;% \n  layer_conv_2d(filters = 4L, kernel_size = 2L,\n                input_shape = list(80L, 80L, 3L)) %&gt;% \n  layer_max_pooling_2d() %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 5L, activation = \"softmax\")\n\n### Model fitting ###\n\nmodel %&gt;% \n  compile(loss = loss_categorical_crossentropy, \n          optimizer = optimizer_adamax(learning_rate = 0.01))\n\nmodel %&gt;% \n  fit(x = train, y = keras::k_one_hot(labels, 5L))\n\n\nPredictions:\n\n\n# Prediction on training data:\npred = apply(model %&gt;% predict(train), 1, which.max)\nMetrics::accuracy(pred - 1L, labels)\ntable(pred)\n\n# Prediction for the submission server:\npred = model %&gt;% predict(test) %&gt;% apply(1, which.max) - 1L\ntable(pred)\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = pred), file = \"cnn.csv\")"
  }
]