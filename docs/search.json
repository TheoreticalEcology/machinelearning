[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Deep Learning with R",
    "section": "",
    "text": "Preface\nMachine learning (ML) is the process of building a predictive model that makes predictions about new data based on observations (training data). The goal of this course is to enable you to build a robust ML model, one that generalizes well to new observations and does not “overfit” your training data. To do this, you will need to master a number of skills, in particular\nIn recent years, a new field within ML called Deep Learning (DL) has emerged and attracted a lot of attention. The reason for this is that DL incorporates many different but very flexible architectures that allow to natively model different types of data, e.g. Convolutional Neural Networks for images or Recurrent Neural Networks for time series. However, exploiting the flexibility of DL requires a deeper, more fundamental understanding of the frameworks in which they are implemented. To this end, the course will also cover common DL frameworks such as torch (and TensorFlow) and:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#before-the-course",
    "href": "index.html#before-the-course",
    "title": "Machine Learning and Deep Learning with R",
    "section": "Before the course",
    "text": "Before the course\n\nPlease read the following two reviews about Machine Learning in General (Pichler and Hartig 2023) and Deep Learning (Borowiec et al. 2022)\nPlease install all dependencies before the course because it will take some time, see 1  Getting Started for installation instructions\nThis course assumes advanced knowledge of the R programming language. If you want to refresh your knowledge about R, you can find a crashcourse in R in the book of the advanced statistic course: R-Crash-Course\n\nAuthors:\nMaximilian Pichler: @_Max_Pichler\nFlorian Hartig: @florianhartig\nContributors:\nJohannes Oberpriller, Matthias Meier\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\n\n\n\n\nBorowiec, Marek L, Rebecca B Dikow, Paul B Frandsen, Alexander McKeeken, Gabriele Valentini, and Alexander E White. 2022. “Deep Learning as a Tool for Ecology and Evolution.” Methods in Ecology and Evolution 13 (8): 1640–60.\n\n\nPichler, Maximilian, and Florian Hartig. 2023. “Machine Learning and Deep Learning—a Review for Ecologists.” Methods in Ecology and Evolution 14 (4): 994–1016.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "A1-GettingStarted.html",
    "href": "A1-GettingStarted.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "1.1 Organization of this book\nThe book is organized on five parts chapters:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "A1-GettingStarted.html#organization-of-this-book",
    "href": "A1-GettingStarted.html#organization-of-this-book",
    "title": "1  Getting Started",
    "section": "",
    "text": "ML basics\nClassical ML algorithms\nDeep learning\nxAI and causal ML\nGenerative AI",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "A1-GettingStarted.html#software-requirements",
    "href": "A1-GettingStarted.html#software-requirements",
    "title": "1  Getting Started",
    "section": "1.2 Software requirements",
    "text": "1.2 Software requirements\n\n1.2.1 R System\nMake sure you have a recent version of R (&gt;=4.2, ideally &gt;=4.3) and RStudio on your computers. For Mac users, if you have already a M1-M3 Mac, please install the R-ARM version (see here (not the x86_64 version))\n\n\n1.2.2 TensorFlow and Keras\nIf you want to run the code on your own computers, you need to install TensorFlow / Keras for R. For this, the following should work for most people:\n\ninstall.packages(\"keras3\", dependencies = TRUE)\nkeras3::install_keras(backend=\"tensorflow\")\n\nThis should work on most computers, in particular if all software is recent. Sometimes, however, things don’t work well, especially the python distribution often makes problems. If the installation does not work for you, we can look at it together. Also, we will provide some virtual machines in case your computers / laptops are too old or you don’t manage to install TensorFlow.\n\n\n1.2.3 Torch for R\nWe may also use Torch for R. This is an R frontend for the popular PyTorch framework. To install Torch, type in R:\n\ninstall.packages(\"torch\")\nlibrary(torch)\ntorch::install_torch()\n\n\n\n1.2.4 EcoData\nWe use data sets from the EcoData package. To install the package, run:\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = TRUE, build_vignettes = TRUE)\n\nThe default installation will install a number of packages that are useful for statistics. Especially in Linux, this may take some time to install. If you are in a hurry and only want the data, you can also run\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = FALSE, build_vignettes = FALSE)\n\n\n\n1.2.5 Additional Libraries\nThere are a number of additional libraries that we may use during the course. So take a coffee or two (that will take a while…) and install the following libraries. Please do this in the given order unless you know what you’re doing, because there are some dependencies between the packages.\n\ninstall.packages(\"abind\")\ninstall.packages(\"animation\")\ninstall.packages(\"ape\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"Rgraphviz\", \"graph\", \"RBGL\"))\ninstall.packages(\"coro\")\ninstall.packages(\"cito\")\ninstall.packages(\"dbscan\")\ninstall.packages(\"dendextend\")\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"e1071\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"fields\")\ninstall.packages(\"forcats\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"glmnetUtils\")\ninstall.packages(\"gym\")\ninstall.packages(\"kknn\")\ninstall.packages(\"knitr\")\ninstall.packages(\"iml\")\ninstall.packages(\"lavaan\")\ninstall.packages(\"lmtest\")\ninstall.packages(\"magick\")\ninstall.packages(\"mclust\")\ninstall.packages(\"Metrics\")\ninstall.packages(\"microbenchmark\")\ninstall.packages(\"missRanger\")\ninstall.packages(\"mlbench\")\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\ninstall.packages(\"mlr3measures\")\ninstall.packages(\"mlr3pipelines\")\ninstall.packages(\"mlr3tuning\")\ninstall.packages(\"paradox\")\ninstall.packages(\"partykit\")\ninstall.packages(\"pcalg\")\ninstall.packages(\"piecewiseSEM\")\ninstall.packages(\"purrr\")\ninstall.packages(\"randomForest\")\ninstall.packages(\"ranger\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"scales\")\ninstall.packages(\"semPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tfprobability\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"xgboost\")\ninstall.packages(\"tidymodels\")\n\ndevtools::install_github(\"andrie/deepviz\", dependencies = TRUE,\n                         upgrade = \"always\")\ndevtools::install_github('skinner927/reprtree')\ndevtools::install_version(\"lavaanPlot\", version = \"0.6.0\")\n\nreticulate::virtualenv_install(\"r-keras\", packages = \"scipy\")\nreticulate::virtualenv_install(\"r-keras\", packages = \"tensorflow_probability\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "A1-GettingStarted.html#linuxunix",
    "href": "A1-GettingStarted.html#linuxunix",
    "title": "1  Getting Started",
    "section": "1.3 Linux/UNIX",
    "text": "1.3 Linux/UNIX\nLinux/UNIX systems have sometimes to fulfill some further dependencies\nDebian based systems\nFor Debian based systems, we need:\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "A1-GettingStarted.html#assumed-r-knowledge",
    "href": "A1-GettingStarted.html#assumed-r-knowledge",
    "title": "1  Getting Started",
    "section": "1.4 Assumed R knowledge",
    "text": "1.4 Assumed R knowledge\nBasic knowledge of R is required to successfully participate in this course. In particular, you should be able to transform and subselect (slice) data. Have a look at this section from the advanced statistic course which provides you with a short tests as well as with further links to read up on background!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "A2-MachineLearningTasks.html",
    "href": "A2-MachineLearningTasks.html",
    "title": "2  Introduction to Machine Learning",
    "section": "",
    "text": "2.1 Machine Learning Tasks\nMachine Learning (ML) is about training an algorithm that can perform certain tasks. The general steps to get the trained model include:\nThe goal of this course is that you can answer the following questions:\nTypically we can define roughly three types of ML tasks:\nSupervised learning, you train algorithms to predict something (classes = classification or values = regression) from some other data (= features), and you provide it with correct examples of the execution of the task (called training data). A linear regression is an example of supervised learning. Given \\(y = f(x)\\) with \\(x\\) our input feature (e.g. precipitation), \\(y\\) our response (growth), and \\(f\\) an unknown function that maps \\(x \\rightarrow y\\) . The goal of supervised learning is to train a ML algorithm to approximate \\(f\\) given observed \\((x_i, y_i)\\) pairs.\nUnsupervised learning, on the other hand, is when you provide the features, but no examples of the correct execution of the task. Clustering techniques are examples of unsupervised learning. (In the example above, \\(y\\) would be unknown).\nReinforcement learning is a technique that mimics a game-like situation. The algorithm finds a solution through trial and error, receiving either rewards or penalties for each action. As in games, the goal is to maximize the rewards. We will talk more about this technique on the last day of the course.\nFor now, we will focus on the first two tasks, supervised and unsupervised learning (here a YouTube video explaining again the difference).",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "A2-MachineLearningTasks.html#machine-learning-tasks",
    "href": "A2-MachineLearningTasks.html#machine-learning-tasks",
    "title": "2  Introduction to Machine Learning",
    "section": "",
    "text": "Supervised learning\nUnsupervised learning\nReinforcement learning\n\n\n\n\n\n\n2.1.1 Test questions\n\nIn ML, predictors (or the explaining variables) are often called features: TRUEFALSE\nIn supervised learning the response (y) and the features (x) are known: TRUEFALSE\nIn unsupervised learning, only the features are known: TRUEFALSE\nIn reinforcement learning an agent (ML model) is trained by interacting with an environment: TRUEFALSE\nHave a look at the two textbooks on ML (Elements of statistical learning and introduction to statistical learning) in our further readings at the end of the GRIPS course - which of the following statements is true?\n\n Both books can be downloaded for free. Higher model complexity is always better for predicting.",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "A2-MachineLearningTasks.html#sec-unsupervised",
    "href": "A2-MachineLearningTasks.html#sec-unsupervised",
    "title": "2  Introduction to Machine Learning",
    "section": "2.2 Unsupervised Learning",
    "text": "2.2 Unsupervised Learning\nIn unsupervised learning, we want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider the iris data set. Here, we have 150 observations of 4 floral traits:\n\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4])\nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits,\n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\n\n\n\n\nTrait distributions of iris dataset\n\n\n\n\nThe observations are from 3 species and indeed those species tend to have different traits, meaning that the observations form 3 clusters.\n\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])\n\n\n\n\n\n\n\n\nHowever, imagine we don’t know what species are, what is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does.\n\n2.2.1 K-means Clustering\nAn example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms (see more on this in section “distance based algorithms”).\nTo start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the assumed or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere. Afterwards each data point is assigned to the respective cluster that raises the overall in-cluster sum of squares (variance) related to the distance to the centroid least of all. After the algorithm has placed all data points into a cluster the centroids get updated. By iterating this procedure until the assignment doesn’t change any longer, the algorithm can find the (locally) optimal centroids and the data points belonging to this cluster. Note that results might differ according to the initial positions of the centroids. Thus several (locally) optimal solutions might be found.\nThe “k” in K-means refers to the number of clusters and the ‘means’ refers to averaging the data-points to find the centroids.\nA typical pipeline for using k-means clustering looks the same as for other algorithms. After having visualized the data, we fit a model, visualize the results and have a look at the performance by use of the confusion matrix. By setting a fixed seed, we can ensure that results are reproducible.\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n\nK-means clustering with 3 clusters of sizes 50, 62, 38\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.006000    3.428000     1.462000    0.246000\n2     5.901613    2.748387     4.393548    1.433871\n3     6.850000    3.073684     5.742105    2.071053\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 15.15100 39.82097 23.87947\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nVisualizing the results. Color codes true species identity, symbol shows cluster result.\n\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\n\n\n\n\n\n\n\n\nWe see that there are are some discrepancies. Confusion matrix:\n\ntable(iris$Species, kc$cluster)\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36\n\n\nIf you want to animate the clustering process, you could run\n\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\n\nElbow technique to determine the probably best suited number of clusters:\n\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))\n\n\n\n\n\n\n\n\nOften, one is interested in sparse models. Furthermore, higher k than necessary tends to overfitting. At the kink in the picture, the sum of squares dropped enough and k is still low enough. But keep in mind, this is only a rule of thumb and might be wrong in some special cases.\n\n\nInformation criteria such as AIC or BIC can be also used to select the number of clusters and control complexity.",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "A2-MachineLearningTasks.html#supervised-learning",
    "href": "A2-MachineLearningTasks.html#supervised-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.3 Supervised Learning",
    "text": "2.3 Supervised Learning\nThe two most prominent branches of supervised learning are regression and classification. The basic distinction between the two is that classification is about predicting a categorical variable, and regression is about predicting a continuous variable.\n\n2.3.1 Regression\nThe random forest (RF) algorithm is possibly the most widely used machine learning algorithm and can be used for regression and classification. We will talk more about the algorithm later.\nFor the moment, we want to go through a typical workflow for a supervised regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. We will again use the iris data set that we used before. The goal is now to predict Sepal.Length based on the information about the other variables (including species).\nFitting the model:\n\nlibrary(randomForest)\nset.seed(123)\n\nSepal.Length is a numerical variable:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nhist(iris$Sepal.Length)\n\n\n\n\n\n\n\n\nThe randomForest can be used similar to a linear regression model, we can specify the features using the formula syntax (~. means that all other variables should be used as features):\n\nm1 = randomForest(Sepal.Length ~ ., data = iris)   # ~.: Against all others.\nprint(m1)\n\n\nCall:\n randomForest(formula = Sepal.Length ~ ., data = iris) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1364625\n                    % Var explained: 79.97\n\n\n\n\nIn statistics we would use a linear regression model:\n\nmLM = lm(Sepal.Length~., data = iris)\n\nAs many other ML algorithms, the RF is not interpretable, so we don’t get coefficients that connect the variables to the response. But, at least we get the variable importance which is similar to an anova, telling us which variables were the most important ones:\n\nvarImpPlot(m1)\n\n\n\n\n\n\n\n\n\n\nOur liner model would report linear effects, however, the lm cannot keep up with the flexibility of a random forest!\n\ncoef(mLM)\n\n      (Intercept)       Sepal.Width      Petal.Length       Petal.Width \n        2.1712663         0.4958889         0.8292439        -0.3151552 \nSpeciesversicolor  Speciesvirginica \n       -0.7235620        -1.0234978 \n\n\nAnd the finally, we can use the model to make predictions using the predict method:\n\nplot(predict(m1), iris$Sepal.Length, xlab = \"Predicted\", ylab = \"Observed\")\nabline(0, 1)\n\n\n\n\n\n\n\n\nTo understand the structure of a random forest in more detail, we can use a package from GitHub.\n\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\n\n\n\n\nHere, one of the regression trees is shown.\n\n\n2.3.2 Classification\nWith the random forest, we can also do classification. The steps are the same as for regression tasks, but we can additionally see how well it performed by looking at the confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represents the instances in the actual class. Thus the diagonals are the correctly predicted classes and the off-diagonal elements are the falsely classified elements.\nSpecies is a factor with three levels:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nFitting the model (syntax is the same as for the regression task):\n\nset.seed(123)\nlibrary(randomForest)\nm1 = randomForest(Species ~ ., data = iris)\nprint(m1)\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n\nvarImpPlot(m1)\n\n\n\n\n\n\n\n\nPredictions:\n\nhead(predict(m1))\n\n     1      2      3      4      5      6 \nsetosa setosa setosa setosa setosa setosa \nLevels: setosa versicolor virginica\n\n\nConfusion matrix:\n\ntable(predict(m1), as.integer(iris$Species))\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 47  4\n  virginica   0  3 46\n\n\nOur model made a few errors.\nVisualizing results ecologically:\n\nplot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = \"Observed\")\n\n\n\n\n\n\n\nplot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = \"Predicted\")\n\n\n\n\n\n\n\n\nVisualizing one of the fitted models:\n\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\n\n\n\n\nConfusion matrix:\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\nsetosa\n50\n0\n0\n\n\nversicolor\n0\n47\n4\n\n\nvirginica\n0\n3\n46",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "A2-MachineLearningTasks.html#exercise---supervised-learning",
    "href": "A2-MachineLearningTasks.html#exercise---supervised-learning",
    "title": "2  Introduction to Machine Learning",
    "section": "2.4 Exercise - Supervised Learning",
    "text": "2.4 Exercise - Supervised Learning\n\nUsing a random forest on the iris dataset, which parameter would be more important (remember there is a function to check this) to predict Petal.Width?\n\n Species. Sepal.Width.\n\n\n\n\n\n\n\nTask: Fit random forest\n\n\n\nA demonstration with the iris dataset:\n\nlibrary(randomForest)\n\n# scale your features if possible (some ML algorithms converge faster with scaled features)\niris_scaled = iris\niris_scaled[,1:4] = scale(iris_scaled[,1:4])\n\n\nmodel = randomForest(Species~., data = iris_scaled)\n\nRandomForest is not based on a specific data generating model and thus we will not get effect estimates that tell us how the input features affect the response:\n\n# no summary method available\nprint(model) \n\n\nCall:\n randomForest(formula = Species ~ ., data = iris_scaled) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          3        47        0.06\n\n\nThe confusion matrix explains where (for which species) the model makes wrong predictions / classifications on the OOB splits (OOB = out of bag). Each tree in the random forest is trained on a bootstrap of the data (bootstrap = sample with replacement from the original data, on average, each bootstrap will have 66% of the original data). Observations not used in a specific bootstrap are then used to validate the specific tree, bootstrap errors are at the end averaged for the n trees in the random forest.\nWhile we don’t get effect estimates as in a lm, we get the variable importance which reports how important the specific predictors are:\n\nvarImpPlot(model)\n\n\n\n\n\n\n\n\nPredictions\n\nhead(predict(model))\n\n     1      2      3      4      5      6 \nsetosa setosa setosa setosa setosa setosa \nLevels: setosa versicolor virginica\n\n\nThe model predicts the species class for each observation\nPerformance:\n\ntable(predict(model), as.integer(iris$Species))\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 47  3\n  virginica   0  3 47\n\n\nTask:\n\npredict Sepal.Length instead of Species (classification -&gt; regression)\nPlot predicted vs observed (usually used to asses the goodness of the predictions, if the model is good, predicted and observed values should be on one diagonal line)\n\n\n\n\n\nClick here to see the solution\n\nRegression:\nRandom Forest automatically infers the type of the task, so we don’t have to change much:\n\nmodel = randomForest(Sepal.Length~., data = iris_scaled)\n\nThe OOB error is now “% Var explained” which is very similar to a \\(R^2\\):\n\nprint(model)\n\n\nCall:\n randomForest(formula = Sepal.Length ~ ., data = iris_scaled) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.2031826\n                    % Var explained: 79.55\n\n\nPlot observed vs predicted:\n\nplot(iris_scaled$Sepal.Length, predict(model), xlim = c(-3, 3), ylim = c(-3, 3))\nabline(a = c(0, 1))\n\n\n\n\n\n\n\n\nCalculate \\(R^2\\):\n\ncor(iris_scaled$Sepal.Length, predict(model))**2\n\n[1] 0.7987894",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Machine Learning</span>"
    ]
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html",
    "href": "A3-BiasVarianceTradeOff.html",
    "title": "3  Managing algorithmic complexity",
    "section": "",
    "text": "3.1 Estimating error on the validation data\nYou probably remember from statistics that a more complex model always fits the training data better. The decisive question, however, is if it also works better on new (independent) data. Technically, we call this the out-of-sample error, as opposed to the in-sample error, which is the error on the training data.\nError can be measured in different ways, but usually we calculate some kind of accuracy (especially for classification tasks) or how much variance is explained by our model (regression tasks). We also distinguish between the error used to train the model and the error used to validate the model. The error used internally by the ML algorithms to train the model is what we usually call the loss. The smaller the loss, the smaller the error of the model, and vice versa for larger losses.\nWhile we can use losses to validate the model, losses are often not interpretable as they are often in the range \\([0, \\infty[\\) and they cannot be generalized to other datasets because they are often data specific. Therefore, in practice, we usually use interpretable losses, validation metrics, during validation that can be also used to compare the models over different datasets (Model A achieves 80% accuarcy on dataset A and 70% accuracy on dataset B), here is an overview of some common validation metrics and their interpretation:\nValidation metrics for classification tasks:\nValidation metrics for regression tasks:",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Managing algorithmic complexity</span>"
    ]
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#estimating-error-on-the-validation-data",
    "href": "A3-BiasVarianceTradeOff.html#estimating-error-on-the-validation-data",
    "title": "3  Managing algorithmic complexity",
    "section": "",
    "text": "Validation Metric\nRange\nClassification Types\nExplanation\n\n\n\n\nArea Under the Curve (AUC)\n\\([0, 1]\\)\nBinary Classification Tasks (e.g. Titanic Dataset, survived or died)\nThe ability of our models to distinguish between 0 and 1. Requires probability predictions. An AUC of 0.5 means that the algorithm is making random predictions. Lower than 0.5 –&gt; worse than random\n\n\nAccuracy\n\\([0, 1]\\)\nAll types of classifications (including multiclass tasks)\nThe accuracy of our models, how many of the predicted classes are correct. The baseline accuracy depends on the distributions of the classes (if one class occurs 99% in the data, a random model that will only predict this class, will achieve already a very high accuracy\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Metric\nRange\nExplanation\n\n\n\n\n\\(R^2\\)\n\\([0, 1]\\)\nHow much variance is explained by our model. We usually use the sum of squares \\(R^2\\)\n\n\nCorrelation factors (Pearson or Spearman)\n\\([-1, 1]\\)\nMeasures correlation between predictions and observations. Spearman (rank correlation factor) can be useful for skewed distributed responses (or non-normal distributed responses, such as count data).\n\n\nRoot mean squared error (RMSE)\n\\([0, \\infty[\\)\nRMSE is not a really interpretable but it is still used as a common validation metrics (is also used as a loss to train models). The RMSE reports how much variance is unexplained (so smaller RMSE is better). However, RMSE is not really comparable between different data sets.\n\n\n\n\n3.1.1 Splitting off validation data\nTo check the out-of-sample error, we usually split out some part of the data for later model validation. Let’s look at this at the example of a supervised regression, trying to predict house prices in Boston.\n\nlibrary(mlbench)\n\ndata(BostonHousing)\ndata = BostonHousing\nset.seed(123)\n\nCreating a split by deciding randomly for each data point if it is used for training or validation\n\nn = nrow(BostonHousing)\ntrain = sample.int(n, size = round(0.7*n))\n\nFitting two lms, one with a few predictors, one with a lot of predictors (all interaction up to 3-way)\n\nm1 = lm(medv~., data = data[train,])\nm2 = lm(medv~.^3, data = data[train,])\n\nTesting predictive ability on training data (in-sample error)\n\ncor(predict(m1), data[train,]$medv)\n\n[1] 0.8561528\n\ncor(predict(m2), data[train,]$medv)\n\n[1] 0.9971297\n\n\nConclusion: m2 (more complex) is much better on training. As a next step, we are testing the predictive ability on hold-out (aka valikation, out-of-sample error).\n\ncor(predict(m1, newdata = data[-train,] ), \n    data[-train,]$medv)\n\n[1] 0.8637908\n\ncor(predict(m2, newdata = data[-train,] ), \n    data[-train,]$medv)\n\nWarning in predict.lm(m2, newdata = data[-train, ]): prediction from\nrank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\n\n[1] -0.04036532\n\n\nNow, m2 is much worse!\n\n\n3.1.2 Overfitting vs. underfitting\nThe phenomenon that the predictive error drops significantly when going from the training to the validation data signals overfitting, i.e. a too complex model!\nWhat about m1 - is m1 just complex enough, or is it too simple? Underfitting cannot be directly diagnosed, you just have to try around if making the model more complex can improve results on the validation data. Let’s try a random forest\n\nlibrary(randomForest)\nm3 = randomForest(medv~., data = data[train,])\ncor(predict(m3), data[train,]$medv)\n\n[1] 0.9345165\n\ncor(predict(m3, newdata = data[-train,] ), \n    data[-train,]$medv)\n\n[1] 0.9380828\n\n\nNo drop on validation data (i.e. no overfitting), but error on training and validation is much better than for m1 - so this seems to be a better model, and m1 was probably underfitting, i.e. it was not complex enough to get good performance!\n\n\n3.1.3 Validation vs. cross-validation\nA problem with the validation split is that we test only on a certain fraction of the data (say: 20% in a 80/20 split).\nIf computationally possible, a better method to estimate error is cross-validation. The idea of cross-validation is to perform the train/validation split again and again until all data was used for the validation, and then average the validation error over this data.\nHere an example of a k-fold cross-validation, which is akin to 5x an 80/20 split.\n\nk = 5 # folds\nsplit = sample.int(k, n, replace = T)\npred = rep(NA, n)\n\nfor(i in 1:k){\n  m1 = randomForest(medv~., data = data[split != i,])\n  pred[split == i] = predict(m1, newdata = data[split == i,])\n}\n\ncor(pred, data$medv)\n\n[1] 0.9368875",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Managing algorithmic complexity</span>"
    ]
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "href": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "title": "3  Managing algorithmic complexity",
    "section": "3.2 Optimizing the bias-variance trade-off",
    "text": "3.2 Optimizing the bias-variance trade-off\n\n3.2.1 The bias-variance trade-off\nWhat we have just seen in the previous chapter is an example of the bias-variance trade-off. The idea is that we look at the error of the model on new test data. The total error comes from 2 contributions:\n\nBias = systematic error that comes from the fact that the model is not flexible enough, related to underfitting\nVariance = statistical error that comes from that fact that estimates of the model parameters get more uncertain when we add complexity\n\nOptimizing the bias-variance trade-off means adjusting the complexity of the model which can be achieved by:\n\nFeature selection (more features increases the flexibility of the model)\nRegularization\n\n\n\nWhich of the following statements about the bias-variance trade-off is correct? (see figure above)\n\n The goal of considering the bias-variance trade-off is to realize that increasing complexity typically leads to more flexibility (allowing you to reduce bias) but at the cost of uncertainty (variance) in the estimated parameters. The goal of considering the bias-variance trade-off is to get the bias of the model as small as possible.\n\n\n\n\n3.2.2 Feature selection\nAdding features increases the flexibility of the model and the goodness of fit:\n\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(BostonHousing)\ndata = BostonHousing\n\nsummary(lm(medv~rm, data = data))\n\n\nCall:\nlm(formula = medv ~ rm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -34.671      2.650  -13.08   &lt;2e-16 ***\nrm             9.102      0.419   21.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,    Adjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~rm+dis, data = data))$r.squared\n\n[1] 0.4955246\n\nsummary(lm(medv~., data = data))$r.squared\n\n[1] 0.7406427\n\n# Main effects + all potential interactions:\nsummary(lm(medv~.^2, data = data))$r.squared\n\n[1] 0.9211876\n\n\nThe model with all features and their potential interactions has the highest \\(R^2\\), but it also has the highest uncertainty because there are on average only 5 observations for each parameter (92 parameters and 506 observations). So how do we decide which level of complexity is appropriate for our task? For the data we use to train the model, \\(R^2\\) will always get better with higher model complexity, so it is a poor decision criterion. We will show this in the Section 3.1.3 section. In short, the idea is that we need to split the data so that we have an evaluation (test) dataset that wasn’t used to train the model, which we can then use in turn to see if our model generalizes well to new data.\n\n\n3.2.3 Regularization\nRegularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a machine learning model. The most important distinction is between shrinkage estimators and estimators based on model averaging.\nShrinkage estimators are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are “shrunk” to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penalty is proportional to the sum of absolute deviations (\\(L1\\) penalty), and the Tikhonov regularization aka Ridge regression, where the penalty is proportional to the sum of squared distances from the reference (\\(L2\\) penalty). Thus, the loss function that we optimize is given by\n\\[\nloss = fit - \\lambda \\cdot d\n\\]\nwhere fit refers to the standard loss function, \\(\\lambda\\) is the strength of the regularization, and \\(d\\) is the chosen metric, e.g. \\(L1\\) or\\(L2\\):\n\\[\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n\\]\n\\[\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n\\]\n\\(\\lambda\\) and possibly d are typically optimized under cross-validation. \\(L1\\) and \\(L2\\) can be also combined what is then called elastic net (see Zou and Hastie (2005)).\nModel averaging refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it is often more efficient having many simpler models and average them, than one “super model”. The reasons are complicated, and explained in more detail in Dormann et al. (2018).\nA particular important application of averaging is boosting, where the idea is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap (use random sampling with replacement ) the data, and average the bootstrapped predictions.\nTo see how these techniques work in practice, let’s first focus on LASSO and Ridge regularization for weights in neural networks. We can imagine that the LASSO and Ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunken) to zero.\nLASSO \\(\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)\\) and Ridge \\(\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)\\) have slightly different properties. They are best understood if we express those as the effective prior preference they create on the parameters:\n\n\n\n\n\n\n\n\n\nAs you can see, the LASSO creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the Ridge. For this reason, LASSO is often more interpreted as a model selection method.\nThe Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.\n\n3.2.3.1 Ridge - Example\nWe can use the glmnet package for Ridge, LASSO, and elastic-net regressions.\nWe want to predict the house prices of Boston (see help of the dataset):\n\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %&gt;% select(-medv, -chas) %&gt;% scale()\n\nhist(cor(X))\n\n\n\n\n\n\n\n\n\nm1 = glmnet(y = Y, x = X, alpha = 0)\n\nThe glmnet function automatically tests different values for lambda:\n\ncbind(coef(m1, s = 0.001), coef(m1, s = 100.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1          s1\n(Intercept) 22.53280632 22.53280632\ncrim        -0.79174957 -0.21113427\nzn           0.76313031  0.18846808\nindus       -0.17037817 -0.25120998\nnox         -1.32794787 -0.21314250\nrm           2.85780876  0.46463202\nage         -0.05389395 -0.18279762\ndis         -2.38716188  0.07906631\nrad          1.42772476 -0.17967948\ntax         -1.09026758 -0.24233282\nptratio     -1.93105019 -0.31587466\nb            0.86718037  0.18764060\nlstat       -3.43236617 -0.46055837\n\n\n\n\n3.2.3.2 LASSO - Example\nBy changing \\(alpha\\) to 1.0 we use a LASSO instead of a Ridge regression:\n\nm2 = glmnet(y = Y, x = X, alpha = 1.0)\ncbind(coef(m2, s = 0.001), coef(m2, s = 0.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1           s1\n(Intercept) 22.53280632 22.532806324\ncrim        -0.95543108 -0.135047323\nzn           1.06718108  .          \nindus        0.21519500  .          \nnox         -1.95945910 -0.000537715\nrm           2.71666891  2.998520195\nage          0.05184895  .          \ndis         -3.10566908 -0.244045205\nrad          2.73963771  .          \ntax         -2.20279273  .          \nptratio     -2.13052857 -1.644234575\nb            0.88420283  0.561686909\nlstat       -3.80177809 -3.682148016\n\n\n\n\n3.2.3.3 Elastic-net - Example\nBy setting \\(alpha\\) to a value between 0 and 1.0, we use a combination of LASSO and Rdige:\n\nm3 = glmnet(y = Y, x = X, alpha = 0.5)\ncbind(coef(m3, s = 0.001), coef(m3, s = 0.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1         s1\n(Intercept) 22.53280632 22.5328063\ncrim        -0.95716118 -0.3488473\nzn           1.06836343  0.1995842\nindus        0.21825187  .        \nnox         -1.96211736 -0.7613698\nrm           2.71859592  3.0137090\nage          0.05299551  .        \ndis         -3.10330132 -1.3011740\nrad          2.73321635  .        \ntax         -2.19638611  .        \nptratio     -2.13041090 -1.8051547\nb            0.88458269  0.6897165\nlstat       -3.79836182 -3.6136853",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Managing algorithmic complexity</span>"
    ]
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#hyperparameter-tuning",
    "href": "A3-BiasVarianceTradeOff.html#hyperparameter-tuning",
    "title": "3  Managing algorithmic complexity",
    "section": "3.3 Hyperparameter tuning",
    "text": "3.3 Hyperparameter tuning\n\n3.3.1 What is a hyperparameter?\nGenerally, parameters such as \\(\\lambda\\) and \\(\\alpha\\) that, for example, control the complexity of the model or other model features such as learning or the optimization are called hyperparameters.\nHyperparameter tuning describes the process of finding the optimal set of hyperparameters for a certain task. They are usually data specific, so they have to tuned for each dataset.\nLet’s have a look at this using our glmnet example - we can plot the effect of \\(\\lambda\\) on the effect estimates:\n\nplot(m1)\n\n\n\n\n\n\n\n\nSo which lambda should we choose now? If we calculate the model fit for different lambdas (e.g. using the RMSE):\n\nlambdas = seq(0.001, 1.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = X, s = l)\n    RMSE = Metrics::rmse(Y, prediction)\n    return(RMSE)\n    })\nplot(lambdas, RMSEs)\n\n\n\n\n\n\n\n\nWe see that the lowest lambda achieved the highest RMSE - which is not surprising because the unconstrained model, the most complex model, has the highest fit, so no bias but probably high variance (with respect to the bias-variance tradeoff).\n\n\n3.3.2 Tuning with a train / test split\nWe want a model that generalizes well to new data, which we need to “simulate” here by splitting of a holdout before the training and using the holdout then for testing our model. This split is often called the train / test split.\n\nset.seed(1)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(glmnet)\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %&gt;% select(-medv, -chas) %&gt;% scale()\n\n# Split data\nindices = sample.int(nrow(X), 0.2*nrow(X))\ntrain_X = X[indices,]\ntest_X = X[-indices,]\ntrain_Y = Y[indices]\ntest_Y = Y[-indices]\n\n# Train model on train data\nm1 = glmnet(y = train_Y, x = train_X, alpha = 0.5)\n\n# Test model on test data\npred = predict(m1, newx = test_X, s = 0.01)\n\n# Calculate performance on test data\nMetrics::rmse(test_Y, pred)\n\n[1] 5.063774\n\n\nLet’s do it again for different values of lambdas:\n\nlambdas = seq(0.0000001, 0.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = test_X, s = l)\n    return(Metrics::rmse(test_Y, prediction))\n    })\nplot(lambdas, RMSEs, xlab = \"Lambda\", ylab = \"RMSE\", type = \"l\", las = 2)\nabline(v = lambdas[which.min(RMSEs)], col = \"red\", lwd = 1.5)\n\n\n\n\n\n\n\n\nAlternatively, you automatically run a CV to determine the hyperparameters for glmnet, using the cv.glmnet function which does per default a 5xCV (so 5 splits) and in each split different values for \\(\\lambda\\) are tested\n\nm1 = glmnet::cv.glmnet(x = X, y = Y, alpha = 0.5, nfolds = 5)\nm1\n\n\nCall:  glmnet::cv.glmnet(x = X, y = Y, nfolds = 5, alpha = 0.5) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure    SE Nonzero\nmin 0.0105    78   23.80 3.247      12\n1se 0.6905    33   26.88 4.014       8\n\nplot(m1)\n\n\n\n\n\n\n\nm1$lambda.min\n\n[1] 0.01049538\n\n\nSo low values of \\(\\lambda\\) seem to achieve the lowest error, thus the highest predictive performance.\n\n\n3.3.3 Nested (cross)-validation\nIn the previous example, we have used the train/test split to find the best model. However, we have not done a validation split yet to see how the finally selected model would do on new data. This is absolutely necessary, because else you will overfit with your model selection to the test data.\nIf we have several nested splits, we talk about a nested validation / cross-validation. For each level, you can in principle switch between validation and cross-validation. Here, and example of tuning with a inner cross-validation and an outer validation.\n\n# outer split\nvalidation = sample.int(n, round(0.2*n))\ndat = data[-validation,]\n\n# inner split\nnI = nrow(dat)\nhyperparameter = data.frame(mtry = c(3,5))\nm = nrow(hyperparameter)\nk = 5 # folds\nsplit = sample.int(k, nI, replace = T)\n\n\n# making predictions for all hyperparameters / splits\npred = matrix(NA, nI, m)\nfor(l in 1:m){\n  for(i in 1:k){\n    m1 = randomForest(medv~., data = dat[split != i,], mtry = hyperparameter$mtry[l])\n    pred[split == i,l] = predict(m1, newdata = dat[split == i,])\n  }\n}\n\n# getting best hyperparameter option on test\ninnerLoss = function(x) cor(x, dat$medv)\nres = apply(pred, 2, innerLoss)\nchoice = which.max(res) \n\n# fitting model again with best hyperparameters \n# and all test / validation data \nmFinal = randomForest(medv~., data = dat, mtry = hyperparameter$mtry[choice])\n\n# testing final prediction on validation data \nfinalPred = predict(mFinal, newdata = data[validation,])\n\ncor(finalPred, \n    data[validation,]$medv)\n\n[1] 0.93926",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Managing algorithmic complexity</span>"
    ]
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#exercise---predicting-survival-rate-of-titanic-passengers",
    "href": "A3-BiasVarianceTradeOff.html#exercise---predicting-survival-rate-of-titanic-passengers",
    "title": "3  Managing algorithmic complexity",
    "section": "3.4 Exercise - Predicting survival rate of titanic passengers",
    "text": "3.4 Exercise - Predicting survival rate of titanic passengers\n\nThe titanic dataset is a collection of data about the titanic passengers and their survival status. The goal is to train a model that can predict whether a passenger survives or not based on their features (e.g. their passenger class):\n\nResponse variable: survived survival status of the passengers\n\n\n\nFeatures: all other variables\n\nYou can also find a small explanation of the dataset in the Appendix of the book.\nIn the following exercise we will also use a new technique called data imputation:\nMost ML algorithms (including statistical models) cannot handle missing data, observations with NAs would normally be dropped from the dataset. To prevent that, we use data imputation to fill NAs in the dataset. In short, we use an algorithm, e.g. random forest, loop over the columns, treat these columns as response variables, train the RF on this column and then make predictions for the NAs in this column. By doing this, we can fill in the NAs in our data set. However, exclude the actual response variable (here survival) from the data imputation, otherwise it would be data leakage!\n\n\n\n\n\n\nTask: Tune random forest model for the Titanic dataset\n\n\n\nTune the nodesize hyperparameter, from the randomForest help:\nnodesize = Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).\nNodesize determines the complexity of the individual trees (we will talk about the exact working tomorrow)\n1. Prepare data\n\nlibrary(randomForest) # alternative faster random forest implementation\nlibrary(EcoData)\n\ndata(titanic_ml)\ntitanic_df = titanic_ml\nsummary(titanic_df)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      body      \n Min.   :  0.000                  :1014    :  2    Min.   :  1.0  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    1st Qu.: 72.0  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    Median :155.0  \n Mean   : 33.295   G6             :   5   S:914    Mean   :160.8  \n 3rd Qu.: 31.275   B96 B98        :   4            3rd Qu.:256.0  \n Max.   :512.329   C22 C26        :   4            Max.   :328.0  \n NA's   :1         (Other)        : 271            NA's   :1188   \n                home.dest  \n                     :564  \n New York, NY        : 64  \n London              : 14  \n Montreal, PQ        : 10  \n Cornwall / Akron, OH:  9  \n Paris, France       :  9  \n (Other)             :639  \n\n# data imputation - fill NA using the missRanger package column 2 is our response variable\ntitanic_df[,-2] = missRanger::missRanger(titanic_df[,-2], verbose = 0)\n\n# remove name column, too many levels\ntitanic_df = subset(titanic_df, select = c(-name, -home.dest, -ticket, -cabin))\n\n# change response to factor\ntitanic_df$survived = as.factor(titanic_df$survived)\n\n# remove NAs\ndf = titanic_df[complete.cases(titanic_df),] # remove NAs\n\n\n# Example:\nrf = randomForest(survived~., \n                  data = df[1:300,], \n                  min.node.size = 20) # we want our model to predict probabilities!\n\n# the predict function of the ranger will return an object, the actual predictions\n# are inside a matrix with the name predictions\npred = predict(rf, newdata = df[-(1:300),], type = \"prob\")[,2]\nMetrics::auc(as.integer(df[-(1:300),]$survived)-1, pred)\n\n[1] 0.8349267\n\n\n2. Create an outer split\n3. Tune nodesize under nested Cross-Validation on the training split from step 2\n4. Create submissions\nWe separated data from the original dataset. Observations with NA in the survived column are held back by us to simulate a real-world scenario where you have training data to train your model, and then use the model in production on new data where you have no information about the response variable.\nSo, 654 observations will serve as training data:\n\n# Training data data:\nsum(!is.na(titanic_ml$survived))\n\n[1] 654\n\n\nAnd 655 observations will serve as validation data:\n\n# Outer test/validation data:\nsum(is.na(titanic_ml$survived))\n\n[1] 655\n\n\nThe goal is to tune/train your model (under k-Cross Validation) on the training data and make predictions for the validation data;\nAfter tuning your model on the training data (again, where the response variable is not NA) and you are happy with your model, you can make predictions for the observations where the response is unknown and upload the predictions to our server (http://rhsbio7.uni-regensburg.de:8500/, ignore the unsecure warning and UR VPN is required). The server will report your final performance and compare it with other predictions):\nHow to create your submission file:\n\nnewdata = titanic_df[is.na(titanic_df$survived), ]\npredictions = predict(rf, newdata = newdata, type = \"prob\")[,2]\n\nwrite.csv(data.frame(y = predictions), file = \"rf_max.csv\")\n\nImportant:\n\nThe predictions must be probabilities\nPredictions must the same number of observations as in the raw titanic_ml$response == NA column, this is why we impute the NA in the features, otherwise these observations would be dropped. So nrow(data.frame(y = predictions)) == 655\n\n\n\n\n\nClick here to see the solution\n\n\nset.seed(42)\nn = nrow(df)\n# outer split\nvalidation = sample.int(n, round(0.2*n))\ndat = df[-validation,]\n\n# inner split\nnI = nrow(dat)\nhyperparameter = data.frame(nodesize = seq(10, 500, by = 25))\nm = nrow(hyperparameter)\nk = 5 # folds\nsplit = sample.int(k, nI, replace = T)\n\n\n# making predictions for all hyperparameters / splits\npred = matrix(NA, nI, m)\nfor(l in 1:m){\n  # loop over the hyperparameters and do CV for each hyperparameter\n  for(i in 1:k){\n    m1 = randomForest(survived~., data = dat[split != i,], nodesize = hyperparameter$nodesize[l])\n    pred[split == i,l] = predict(m1, newdata = dat[split == i,], type = \"prob\")[,2]\n  }\n}\n\n# getting best hyperparameter option on test\ninnerLoss = function(x) Metrics::auc(dat$survived, x)\nres = apply(pred, 2, innerLoss)\nchoice = which.max(res) \n\n# fitting model again with best hyperparameters \n# and all test / validation data \nmFinal = randomForest(survived~., data = dat, nodesize = hyperparameter$nodesize[choice])\n\n# testing final prediction on validation data \nfinalPred = predict(mFinal, newdata = df[validation,], type = \"prob\")[,2]\n\nMetrics::auc(df[validation,]$survived, finalPred)\n\n[1] 0.8265476\n\n\nCreate submissions:\n\nnewdata = titanic_df[is.na(titanic_df$survived), ]\npredictions = predict(rf, newdata = newdata, type = \"prob\")[,2]\n\nwrite.csv(data.frame(y = predictions), file = \"rf_max.csv\")\n\nAnd upload the csv file\nImportant:\n\nThe predictions must be probabilities\nPredictions must the same number of observations as in the raw titanic_ml$response == NA column, this is why we impute the NA in the features, otherwise these observations would be dropped.",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Managing algorithmic complexity</span>"
    ]
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#references",
    "href": "A3-BiasVarianceTradeOff.html#references",
    "title": "3  Managing algorithmic complexity",
    "section": "References",
    "text": "References\n\n\n\n\nDormann, Carsten F, Justin M Calabrese, Gurutzeta Guillera-Arroita, Eleni Matechou, Volker Bahn, Kamil Bartoń, Colin M Beale, et al. 2018. “Model Averaging in Ecology: A Review of Bayesian, Information-Theoretic, and Tactical Approaches for Predictive Inference.” Ecological Monographs 88 (4): 485–504.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20.",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Managing algorithmic complexity</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline.html",
    "href": "A4-MLpipeline.html",
    "title": "4  Machine learning pipeline",
    "section": "",
    "text": "4.1 Data preparation\nThe Standard Machine Learning Pipeline using the Titanic Data set\nBefore we specialize on any tuning, it is important to understand that machine learning always consists of a pipeline of actions.\nThe typical machine learning workflow consist of:\nIn the following example, we use tidyverse, a collection of R packages for data science / data manipulation mainly developed by Hadley Wickham.\nFor this lecture you need the Titanic data set provided by us (via the EcoData package).\nLoad necessary libraries:\nlibrary(tidyverse)\nLoad data set:\nlibrary(EcoData)\ndata(titanic_ml)\ndata = titanic_ml\nStandard summaries:\nstr(data)\n\n'data.frame':   1309 obs. of  13 variables:\n $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\nsummary(data)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      body      \n Min.   :  0.000                  :1014    :  2    Min.   :  1.0  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    1st Qu.: 72.0  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    Median :155.0  \n Mean   : 33.295   G6             :   5   S:914    Mean   :160.8  \n 3rd Qu.: 31.275   B96 B98        :   4            3rd Qu.:256.0  \n Max.   :512.329   C22 C26        :   4            Max.   :328.0  \n NA's   :1         (Other)        : 271            NA's   :1188   \n                home.dest  \n                     :564  \n New York, NY        : 64  \n London              : 14  \n Montreal, PQ        : 10  \n Cornwall / Akron, OH:  9  \n Paris, France       :  9  \n (Other)             :639\nThe name variable consists of 1309 unique factors (there are 1309 observations…) and could be now transformed. If you are interested in how to do that, take a look at the following box.",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine learning pipeline</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline.html#data-preparation",
    "href": "A4-MLpipeline.html#data-preparation",
    "title": "4  Machine learning pipeline",
    "section": "",
    "text": "Feature engineering of the name variable\n\n\n\n\n\n\nlength(unique(data$name))\n\n[1] 1307\n\n\nHowever, there is a title in each name. Let’s extract the titles:\n\nWe will extract all names and split each name after each comma “,”.\nWe will split the second split of the name after a point “.” and extract the titles.\n\n\nfirst_split = sapply(data$name,\n                     function(x) stringr::str_split(x, pattern = \",\")[[1]][2])\ntitles = sapply(first_split,\n                function(x) strsplit(x, \".\",fixed = TRUE)[[1]][1])\n\nWe get 18 unique titles:\n\ntable(titles)\n\ntitles\n         Capt           Col           Don          Dona            Dr \n            1             4             1             1             8 \n     Jonkheer          Lady         Major        Master          Miss \n            1             1             2            61           260 \n         Mlle           Mme            Mr           Mrs            Ms \n            2             1           757           197             2 \n          Rev           Sir  the Countess \n            8             1             1 \n\n\nA few titles have a very low occurrence rate:\n\ntitles = stringr::str_trim((titles))\ntitles %&gt;%\n fct_count()\n\n# A tibble: 18 × 2\n   f                n\n   &lt;fct&gt;        &lt;int&gt;\n 1 Capt             1\n 2 Col              4\n 3 Don              1\n 4 Dona             1\n 5 Dr               8\n 6 Jonkheer         1\n 7 Lady             1\n 8 Major            2\n 9 Master          61\n10 Miss           260\n11 Mlle             2\n12 Mme              1\n13 Mr             757\n14 Mrs            197\n15 Ms               2\n16 Rev              8\n17 Sir              1\n18 the Countess     1\n\n\nWe will combine titles with low occurrences into one title, which we can easily do with the forcats package.\n\ntitles2 =\n  forcats::fct_collapse(titles,\n                        officer = c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\"),\n                        royal = c(\"Jonkheer\", \"Don\", \"Sir\",\n                                  \"the Countess\", \"Dona\", \"Lady\"),\n                        miss = c(\"Miss\", \"Mlle\"),\n                        mrs = c(\"Mrs\", \"Mme\", \"Ms\")\n                        )\n\nWe can count titles again to see the new number of titles:\n\ntitles2 %&gt;%  \n   fct_count()\n\n# A tibble: 6 × 2\n  f           n\n  &lt;fct&gt;   &lt;int&gt;\n1 officer    23\n2 royal       6\n3 Master     61\n4 miss      262\n5 mrs       200\n6 Mr        757\n\n\nAdd new title variable to data set:\n\ndata =\n  data %&gt;%\n    mutate(title = titles2)\n\n\n\n\n\n4.1.1 Imputation\nNAs are a common problem in ML and most ML algorithms cannot handle NAs. For example, the age variable has 20% NAs:\n\nsummary(data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1667 21.0000 28.0000 29.8811 39.0000 80.0000     263 \n\nsum(is.na(data$age)) / nrow(data)\n\n[1] 0.2009167\n\n\nThere are few options how to handle NAs:\n\nDrop observations with NAs, however, we may lose many observations (not what we want!)\nImputation, fill the missing values\n\nWe impute (fill) the missing values, for example with the median age. However, age itself might depend on other variables such as sex, class and title. Thus, instead of filling the NAs with the overall median of the passengers, we want to fill the NAs with the median age of these groups so that the associations with the other groups are preserved (or in other words, that the new values are hopefully closer to the unknown true values).\nIn tidyverse we can “group” the data, i.e. we will nest the observations within categorical variables for which we assume that there may be an association with age (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age....)) will be done within the specified groups (to get better estimates of these missing NAs).\n\ndata =\n  data %&gt;%\n    select(survived, sex, age, fare, pclass) %&gt;% \n    group_by(sex, pclass) %&gt;%\n    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;%\n    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %&gt;%\n    ungroup()\n\n\n\n4.1.2 Preprocessing and Feature Selection\nLater (tomorrow), we want to use Keras in our example, but it cannot handle factors and requires the data to be scaled.\nNormally, one would do this for all predictors, but as we only show the pipeline here, we have sub-selected a bunch of predictors and do this only for them. We first scale the numeric predictors and change the factors with only two groups/levels into integers (this can be handled by Keras).\n\ndata_sub =\n  data %&gt;%\n    select(survived, sex, age2, fare2, pclass) %&gt;%\n    mutate(age2 = scales::rescale(age2, c(0, 1)),\n           fare2 = scales::rescale(fare2, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\n\n\n\n\n\n\n\nTransforming factors with more than two levels\n\n\n\n\n\nFactors with more than two levels should be one hot encoded (Make columns for every different factor level and write 1 in the respective column for every taken feature value and 0 else. For example: \\(\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}\\)):\n\none_title = model.matrix(~0+as.factor(title), data = data)\ncolnames(one_title) = levels(data$title)\n\none_sex = model.matrix(~0+as.factor(sex), data = data)\ncolnames(one_sex) = levels(data$sex)\n\none_pclass = model.matrix(~0+as.factor(pclass), data = data)\ncolnames(one_pclass) = paste0(\"pclass\", 1:length(unique(data$pclass)))\n\nAnd we have to add the dummy encoded variables to the data set:\n\ndata = cbind(data.frame(survived= data$survived),\n                 one_title, one_sex, age = data$age2,\n                 fare = data$fare2, one_pclass)\nhead(data)",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine learning pipeline</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline.html#modelling",
    "href": "A4-MLpipeline.html#modelling",
    "title": "4  Machine learning pipeline",
    "section": "4.2 Modelling",
    "text": "4.2 Modelling\n\n4.2.1 Split data for final predictions\nTo tune our hyperparameters and evaluate our models, we split the data into the training and testing data. The testing data are the observations where the response is NA:\n\nsummary(data_sub$survived)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.0000  0.0000  0.3853  1.0000  1.0000     655 \n\n\n655 observations have NAs in our response variable, these are the observations for which we want to make predictions at the end of our pipeline.\n\ndata_new = data_sub[is.na(data_sub$survived),]\ndata_obs = data_sub[!is.na(data_sub$survived),]\n\n\n\n4.2.2 Hyperparameter optimization\nWe want to tune our hyperparameters (\\(\\lambda\\) and \\(\\alpha\\)). Normally, we should do a nested CV on our training data (data_obs), however, we assume that the test data on the submission server is our outer split, so we can tune our hyperparameters using a n-fold Cross-Validation which serves as our inner CV.\n\n\nAgain, why is it important to tune hyperparameters? Hyperparameters (configuration parameters of our ML algorithms that (mostly) control their complexity) are usually tuned (optimized) in an automatic / systematic way. A common procedure, called random search, is to sample random configuration combinations from the set of hyperparameters and test for each combination the prediction error.\nWe implement manually a CV to tune the learning rate. We start with a 3xCV and 10x different learning rates:\n\nlibrary(cito)\nset.seed(42)\nmodel = dnn(survived~.,\n            data = data_obs, \n            loss = \"binomial\",\n            lr = tune(0.001, 0.1),\n            tuning = config_tuning(CV = 3, steps = 10)\n            )\n\nRegistered S3 methods overwritten by 'reformulas':\n  method       from\n  head.call    cito\n  head.formula cito\n  head.name    cito\n\n\nStarting hyperparameter tuning...\nFitting final model...\n\nmodel$tuning\n\n# A tibble: 10 × 5\n   steps  test train models      lr\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt;\n 1     1  329.     0 NA     0.0394 \n 2     2  319.     0 NA     0.0623 \n 3     3  325.     0 NA     0.0627 \n 4     4  326.     0 NA     0.0804 \n 5     5  326.     0 NA     0.0658 \n 6     6  315.     0 NA     0.0683 \n 7     7  321.     0 NA     0.0417 \n 8     8  317.     0 NA     0.0667 \n 9     9  346.     0 NA     0.00368\n10    10  333.     0 NA     0.0818",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine learning pipeline</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline.html#predictions-and-submission",
    "href": "A4-MLpipeline.html#predictions-and-submission",
    "title": "4  Machine learning pipeline",
    "section": "4.3 Predictions and Submission",
    "text": "4.3 Predictions and Submission\nWhen we are satisfied with the performance of our model, we will create predictions for the new observations on the submission server. cito directly returns the best model so we do not have to fit the final model.\nWe submit our predictions to the submission server at http://rhsbio7.uni-regensburg.de:8500.\nFor the submission it is critical to change the predictions into a data.frame, select the second column (the probability to survive), and save it with the write.csv function:\n\ndata_new = data_sub[is.na(data_sub$survived),]\npredictions = predict(model, data_new, type = \"response\")[,1] \nwrite.csv(data.frame(y = predictions), file = \"Max_1.csv\")",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine learning pipeline</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline.html#exercises",
    "href": "A4-MLpipeline.html#exercises",
    "title": "4  Machine learning pipeline",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning rf\n\n\n\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\nmtry\nSubset of features randomly selected in each node (from which the algorithm can select the feature that will be used to split the data).\n\n\nminimum node size\nMinimal number of observations allowed in a node (before the branching is canceled)\n\n\nmax depth\nMaximum number of tree depth\n\n\n\nComing back to the titanic dataset from the morning, we want to optimize nodesize, max depth, and mtry in our RF using a simple CV.\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data |&gt; select(survived, sex, age, fare, pclass)\n\n# data imputation without the response variable!\ndata[,-1] = missRanger(data[,-1], verbose = 0) \n\ndata$survived = as.factor(data$survived)\n\ndata_sub =\n  data |&gt; \n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) |&gt; \n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\ndata_sub$survived = as.factor(data_sub$survived)\ndata_obs$survived = as.factor(data_obs$survived)\n\nHints:\n\nadjust the ‘type’ argument in the predict(…) method (the default is to predict classes)\nwhen predicting probabilities, the randomForest will return a matrix, a column for each class, we are interested in the probability of surviving (so the second column)\ntune nodesize, mtry, and maxnodes\nuse more features and do feature engineering!\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(randomForest)\ndata_obs = data_sub[!is.na(data_sub$survived),] \nset.seed(42)\n\ncv = 3\nhyper_minnodesize = ... # \n\ntuning_results =\n    sapply(1:length(hyper_minnodesize), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n          train_inner = data_obs[inner_split != j, ]\n          test_inner = data_obs[inner_split == j, ]\n          \n          model = randomForest(survived~.,data = train_inner, nodesize = hyper_minnodesize[k])\n          predictions = predict(model, newdata=test_inner, type = \"prob\")[,2]\n          \n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions)\n        }\n      return(mean(auc_inner))\n    })\n\nresults = data.frame(minnodesize = hyper_minnodesize, AUC = tuning_results)\n\nprint(results)\n\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\ndata_obs = data_sub[!is.na(data_sub$survived),] \nset.seed(42)\n\ncv = 3\nhyper_minnodesize = sample(300, 20)\nhyper_mtry = sample(4, 20, replace = TRUE)\n\ntuning_results =\n    sapply(1:length(hyper_minnodesize), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n          train_inner = data_obs[inner_split != j, ]\n          test_inner = data_obs[inner_split == j, ]\n          model = randomForest(survived~.,data = train_inner, \n                               nodesize = hyper_minnodesize[k], \n                               mtry = hyper_mtry[k])\n          predictions = predict(model, test_inner, type=\"prob\")[,2]\n          \n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions)\n        }\n      return(mean(auc_inner))\n    })\n\nresults = data.frame(minnodesize = hyper_minnodesize, mtry = hyper_mtry, AUC = tuning_results)\n\nprint(results)\n\n   minnodesize mtry       AUC\n1           49    2 0.8207934\n2          153    3 0.8054506\n3           74    3 0.8169019\n4          228    2 0.8051004\n5          146    2 0.8082379\n6          122    4 0.7948290\n7          300    4 0.7615291\n8          128    4 0.7905482\n9           24    2 0.8294439\n10          89    2 0.8175564\n11         165    1 0.7951680\n12         110    4 0.7898386\n13          20    1 0.8266383\n14         291    4 0.7444037\n15         283    2 0.8017500\n16         109    4 0.8004647\n17           5    2 0.8319759\n18         212    3 0.7996817\n19         259    4 0.7444037\n20         292    3 0.7970839\n\n# highest AUC / best hyperparameters\nbest_hyper = results[which.max(results$AUC),]\nprint(best_hyper)\n\n   minnodesize mtry       AUC\n17           5    2 0.8319759\n\n\nMake predictions for the submission server:\n\nmodel = randomForest(survived~.,data = data_obs, \n                     nodesize = best_hyper[1,1], \n                     mtry = best_hyper[1,2])\n\nwrite.csv(data.frame(y = predict(model, newdata=data_new, type=\"prob\")[,2]), file = \"Max_titanic_rf.csv\")",
    "crumbs": [
      "Machine Learning Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine learning pipeline</span>"
    ]
  },
  {
    "objectID": "B1-Trees.html",
    "href": "B1-Trees.html",
    "title": "5  Tree-based Algorithms",
    "section": "",
    "text": "5.1 Classification and Regression Trees\nTree-based algorithms use a series of if-then rules to generate predictions from one or more decision trees. In this lecture, we will explore regression and classification trees by the example of the airquality data set. There is one important hyperparameter for regression trees: “minsplit”.\nlibrary(rpart)\nlibrary(rpart.plot)\n\ndata = airquality[complete.cases(airquality),]\nFit and visualize one(!) regression tree:\nrt = rpart(Ozone~., data = data, control = rpart.control(minsplit = 10))\nrpart.plot(rt)\nVisualize the predictions:\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nThe angular form of the prediction line is typical for regression trees and is a weakness of it.",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B1-Trees.html#classification-and-regression-trees",
    "href": "B1-Trees.html#classification-and-regression-trees",
    "title": "5  Tree-based Algorithms",
    "section": "",
    "text": "It controls the depth of tree (see the help of rpart for a description).\nIt controls the complexity of the tree and can thus also be seen as a regularization parameter.",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B1-Trees.html#random-forest",
    "href": "B1-Trees.html#random-forest",
    "title": "5  Tree-based Algorithms",
    "section": "5.2 Random Forest",
    "text": "5.2 Random Forest\nTo overcome this weakness, a random forest uses an ensemble of regression/classification trees. In principle, a random forest is nothing more than a normal regression/classification tree, but it uses the idea of “wisdom of the crowd”: by asking many people (regression/classification trees), you can make a more informed decision (prediction/classification). For example, if you wanted to buy a new phone, you wouldn’t go directly to the store, but you would search the Internet and ask your friends and family.\nThere are two randomization steps with the random forest that are responsible for their success:\n\nBootstrap samples for each tree (we will sample observations with replacement from the data set. For the phone this is like not everyone has experience about each phone).\nAt each split, we will sample a subset of predictors that is then considered as potential splitting criterion (for the phone this is like that not everyone has the same decision criteria). Annotation: While building a decision tree (random forests consist of many decision trees), one splits the data at some point according to their features. For example if you have females and males, big and small people in a crowd, you con split this crowd by gender and then by size or by size and then by gender to build a decision tree.\n\nApplying the random forest follows the same principle as for the methods before: We visualize the data (we have already done this so often for the airquality data set, thus we skip it here), fit the algorithm and then plot the outcomes.\nFit a random forest and visualize the predictions:\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\npred = predict(rf, data)\nplot(Ozone~Temp, data = data)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\n\n\n\n\nOne advantage of random forest is that we get an importance of the variables. For each split in each tree, the improvement in the split criterion is the measure of importance attributed to the split variable, and is accumulated over all trees in the forest separately for each variable. Thus, the variable importance tells us how important a variable is averaged across all trees.\n\nrf$importance\n\n        IncNodePurity\nSolar.R      17969.59\nWind         31978.36\nTemp         34176.71\nMonth        10753.73\nDay          15436.47\n\n\nThere are several important hyperparameters in a random forest that we can tune to get better results:\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\nmtry\nSubset of features randomly selected in each node (from which the algorithm can select the feature that will be used to split the data).\n\n\nminimum node size\nMinimal number of observations allowed in a node (before the branching is canceled)\n\n\nmax depth\nMaximum number of tree depth",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B1-Trees.html#boosted-regression-trees",
    "href": "B1-Trees.html#boosted-regression-trees",
    "title": "5  Tree-based Algorithms",
    "section": "5.3 Boosted Regression Trees",
    "text": "5.3 Boosted Regression Trees\nA boosted regression tree (BRT) starts with a simple regression tree (weak learner) and then sequentially fits additional trees to improve the results. There are two different strategies to do this:\n\nAdaBoost: Wrong classified observations (by the previous tree) will get a higher weight and therefore the next trees will focus on difficult/missclassified observations.\nGradient boosting (state of the art): Each sequential model will be fit on the residual errors of the previous model (strongly simplified, the actual algorithm is complex).\n\nWe can fit a boosted regression tree using xgboost, but before we have to transform the data into a xgb.Dmatrix (which is a xgboost specific data type, the package sadly doesn’t support R matrices or data.frames).\n\nlibrary(xgboost)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\nbrt = xgboost(data_xg, nrounds = 16L)\n\n[1] train-rmse:39.724624 \n[2] train-rmse:30.225761 \n[3] train-rmse:23.134840 \n[4] train-rmse:17.899179 \n[5] train-rmse:14.097785 \n[6] train-rmse:11.375457 \n[7] train-rmse:9.391276 \n[8] train-rmse:7.889690 \n[9] train-rmse:6.646586 \n[10]    train-rmse:5.804859 \n[11]    train-rmse:5.128437 \n[12]    train-rmse:4.456416 \n[13]    train-rmse:4.069464 \n[14]    train-rmse:3.674615 \n[15]    train-rmse:3.424578 \n[16]    train-rmse:3.191301 \n\n\nThe parameter “nrounds” controls how many sequential trees we fit, in our example this was 16. When we predict on new data, we can limit the number of trees used to prevent overfitting (remember: each new tree tries to improve the predictions of the previous trees).\nLet us visualize the predictions for different numbers of trees:\n\noldpar = par(mfrow = c(2, 2))\nfor(i in 1:4){\n  pred = predict(brt, newdata = data_xg, ntreelimit = i)\n  plot(data$Temp, data$Ozone, main = i)\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\n[10:05:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[10:05:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[10:05:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[10:05:51] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n\n\n\n\n\n\npar(oldpar)\n\nThere are also other ways to control for complexity of the boosted regression tree algorithm:\n\nmax_depth: Maximum depth of each tree.\nshrinkage (each tree will get a weight and the weight will decrease with the number of trees).\n\nWhen having specified the final model, we can obtain the importance of the variables like for random forests:\n\nxgboost::xgb.importance(model = brt)\n\n   Feature        Gain     Cover  Frequency\n    &lt;char&gt;       &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:    Temp 0.570072012 0.2958229 0.24836601\n2:    Wind 0.348230653 0.3419576 0.24183007\n3: Solar.R 0.058795502 0.1571072 0.30718954\n4:     Day 0.019529985 0.1779925 0.16993464\n5:   Month 0.003371847 0.0271197 0.03267974\n\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n\n[1] 17.89918\n\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\n\nOne important strength of xgboost is that we can directly do a cross-validation (which is independent of the boosted regression tree itself!) and specify its properties with the parameter “n-fold”:\n\nset.seed(123)\n\nbrt = xgboost(data_xg, nrounds = 5L)\n\n[1] train-rmse:39.724624 \n[2] train-rmse:30.225761 \n[3] train-rmse:23.134840 \n[4] train-rmse:17.899179 \n[5] train-rmse:14.097785 \n\nbrt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L,\n                         nrounds = 3L, nthreads = 4L)\n\n[1] train-rmse:39.895106+2.127355   test-rmse:40.685477+5.745327 \n[2] train-rmse:30.367660+1.728788   test-rmse:32.255812+5.572963 \n[3] train-rmse:23.446237+1.366757   test-rmse:27.282435+5.746244 \n\nprint(brt_cv)\n\n##### xgb.cv 3-folds\n  iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n &lt;num&gt;           &lt;num&gt;          &lt;num&gt;          &lt;num&gt;         &lt;num&gt;\n     1        39.89511       2.127355       40.68548      5.745327\n     2        30.36766       1.728788       32.25581      5.572963\n     3        23.44624       1.366757       27.28244      5.746244\n\n\nAnnotation: The original data set is randomly partitioned into \\(n\\) equal sized subsamples. Each time, the model is trained on \\(n - 1\\) subsets (training set) and tested on the left out set (test set) to judge the performance.\nIf we do three-folded cross-validation, we actually fit three different boosted regression tree models (xgboost models) on \\(\\approx 67\\%\\) of the data points. Afterwards, we judge the performance on the respective holdout. This now tells us how well the model performed.\nImportant hyperparameters:\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\neta\nlearning rate (weighting of the sequential trees)\n\n\nmax depth\nmaximal depth in the trees (small = low complexity, large = high complexity)\n\n\nsubsample\nsubsample ratio of the data (bootstrap ratio)\n\n\nlambda\nregularization strength of the individual trees\n\n\nmax tree\nmaximal number of trees in the ensemble",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B1-Trees.html#exercise---trees",
    "href": "B1-Trees.html#exercise---trees",
    "title": "5  Tree-based Algorithms",
    "section": "5.4 Exercise - Trees",
    "text": "5.4 Exercise - Trees\n\n\n\n\n\n\nQuestion: Understanding complexity in Regression Trees\n\n\n\nThe goal of this exercise is to understand how the hyperparameter mincut (minsplit) affects the complexity of regression trees.\n\nlibrary(tree)\nset.seed(123)\n\ndata = airquality\nrt = tree(Ozone~., data = data,\n          control = tree.control(mincut = 1L, nobs = nrow(data)))\n\nplot(rt)\ntext(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n\nTasks:\n\nThe code snippet above returns NA for the RMSE, what is wrong in the snippet?\nRead the tree.control documentation, what does the mincut parameter do?\nTry different mincut values and check how the predictions (and the RMSE) change. What was wrong in the snippet above?\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(tree)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\ndoTask = function(mincut){\n  rt = tree(Ozone~., data = data,\n            control = tree.control(mincut = mincut, nobs = nrow(data)))\n\n  pred = predict(rt, data)\n  plot(data$Temp, data$Ozone,\n       main = paste0(\n         \"mincut: \", mincut,\n         \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n  )\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\nfor(i in c(1, 2, 3, 5, 10, 15, 25, 50, 54, 55, 56, 57, 75, 100)){ doTask(i) }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproximately at mincut = 15, prediction is the best (mind overfitting). After mincut = 56, the prediction has no information at all and the RMSE stays constant.\nMind the complete cases of the airquality data set, that was the error.\n\n\n\n\n\n\n\nQuestion: Understanding complexity in Random forest\n\n\n\nThe goal of this exercise is to understand how the hyperparameter nodesize affects the complexity of random forest.\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n\n        IncNodePurity\nSolar.R      17969.59\nWind         31978.36\nTemp         34176.71\nMonth        10753.73\nDay          15436.47\n\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n\nRMSE:  9.507848 \n\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\n\n\n\n\nTasks:\n\nCheck the documentation of the randomForest function and read the description of the nodesize parameter\nTry different nodesize values and describe how the predictions change\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\nfor(nodesize in c(1, 15, 50, 100)){\n  for(mtry in c(1, 3, 5)){\n    rf = randomForest(Ozone~., data = data, nodesize = nodesize)\n    \n    pred = predict(rf, data)\n    \n    plot(data$Temp, data$Ozone, main = paste0(\n        \"    nodesize: \", nodesize,\n        \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n    )\n    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodesize affects the complexity. In other words: The bigger the nodesize, the smaller the trees and the more bias/less variance.\n\n\n\n\n\n\n\nQuestion: Understanding complexity in Boosted regression trees\n\n\n\nThe goal of this exercise is to understand how complexity in BRT affects predictions. For that, we will simulate data with two predictors x1 and x2 and the y response variable will be an interaction of the two predictors:\n\\[y = e^{-x_1^2 - x_2^2} \\] We can visualize the simulated data as an image (x1 and x2 on the x and y axis, and the y values as colors)\n\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\n\nmodel = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                         nrounds = 500L, verbose = 0L)\npred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n               ntreelimit = 10L)\n\nsaveGIF(\n  {\n    for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n      pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                     ntreelimit = i)\n      image(matrix(pred, 100, 100), main = paste0(\"Trees: \", i),\n            axes = FALSE, las = 2)\n      axis(1, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1))\n      axis(2, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n    }\n  },\n  movie.name = \"boosting.gif\", autobrowse = FALSE\n)\n\n\nTasks:\n\nRun the code above and try different max_depth values and describe what you see!\n\nTip: have a look at the boosting.gif.\n\n\n\n\nClick here to see the solution\n\n\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\nfor(max_depth in c(3, 6, 10, 20)){\n  model = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                           max_depth = max_depth,\n                           nrounds = 500, verbose = 0L)\n\n  saveGIF(\n    {\n      for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n        pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                       ntreelimit = i)\n        image(matrix(pred, 100, 100),\n              main = paste0(\"eta: \", eta,\n                            \"    max_depth: \", max_depth,\n                            \"    Trees: \", i),\n              axes = FALSE, las = 2)\n        axis(1, at = seq(0, 1, length.out = 10),\n             labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0, 1, length.out = 10),\n             labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    },\n    movie.name = paste0(\"boosting_\", max_depth, \"_\", eta, \".gif\"),\n    autobrowse = FALSE\n  )\n}\n\nWe see that for high values of max_depth, the predictions “smooth out” faster. On the other hand, with a low max_depth (low complexity of the individual trees), more trees are required in the ensemble to achieve a smooth prediction surface.\n\n?xgboost::xgboost\n\nJust some examples:\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning of boosted regression trees\n\n\n\nImportant hyperparameters:\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\neta\nlearning rate (weighting of the sequential trees)\n\n\nmax depth\nmaximal depth in the trees (small = low complexity, large = high complexity)\n\n\nsubsample\nsubsample ratio of the data (bootstrap ratio)\n\n\nlambda\nregularization strength of the individual trees\n\n\nmax tree\nmaximal number of trees in the ensemble\n\n\n\nThe goal of this exercise is to tune a BRT on the titanic_ml dataset and beat yesterday’s RF predictions.\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:xgboost':\n\n    slice\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\nTasks:\n\nTune eta and max depth!\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(xgboost)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nsplit = sample.int(cv, nrow(data_obs), replace = T)\n\n# sample minnodesize values (must be integers)\nhyper_depth = ...\nhyper_eta = ...\n\ntuning_results =\n    sapply(1:length(hyper_minnodesize), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          split = outer_split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          \n          data_xg = xgb.DMatrix(data = as.matrix(train_inner[,-1]), label = train_inner$survived)\n          \n          model = xgboost(data_xg, nrounds = 16L, eta = hyper_eta[k], max_depth = hyper_depth[k], objective = \"reg:logistic\")\n          predictions = predict(model, newdata = as.matrix(test_inner)[,-1])\n          \n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions)\n        }\n      return(mean(auc_inner))\n    })\n\nresults = data.frame(depth = hyper_depth, eta = hyper_eta, AUC = tuning_results)\n\nprint(results)\n\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(xgboost)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nsplit = sample.int(cv, nrow(data_obs), replace = T)\n\n# sample minnodesize values (must be integers)\nhyper_depth = sample(200, 20)\nhyper_eta = runif(20, 0, 1)\n\n\ntuning_results =\n    sapply(1:length(hyper_depth), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          \n          data_xg = xgb.DMatrix(data = as.matrix(train_inner[,-1]), label = train_inner$survived)\n          \n          model = xgboost(data_xg, nrounds = 16L, eta = hyper_eta[k], max_depth = hyper_depth[k], objective = \"reg:logistic\", verbose = 0)\n          predictions = predict(model, newdata = as.matrix(test_inner)[,-1])\n          \n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions)\n        }\n      return(mean(auc_inner))\n    })\n\nresults = data.frame(depth = hyper_depth, eta = hyper_eta, AUC = tuning_results)\n\nprint(results)\n\n   depth        eta       AUC\n1     63 0.81336270 0.8201865\n2    190 0.87043248 0.8288359\n3     19 0.01916128 0.8150229\n4     21 0.82474818 0.8181561\n5    107 0.40126850 0.8222465\n6    191 0.67205829 0.8187190\n7     98 0.11867222 0.8221003\n8    116 0.47642434 0.8197725\n9    135 0.92533996 0.8179209\n10   177 0.57483934 0.8203259\n11    68 0.91275158 0.8230057\n12   173 0.75530192 0.8241578\n13   115 0.63099992 0.8209234\n14    65 0.79442065 0.8289604\n15    92 0.92354646 0.8134317\n16    81 0.16446881 0.8252634\n17   164 0.19288394 0.8242471\n18   162 0.09764267 0.8177379\n19   155 0.82910940 0.8159333\n20   154 0.80586396 0.8215947\n\n\nMake predictions:\n\ndata_xg = xgb.DMatrix(data = as.matrix(data_obs[,-1]), label = data_obs$survived)\n\nmodel = xgboost(data_xg, nrounds = 16L, eta = results[which.max(results$AUC), 2], max_depth = results[which.max(results$AUC), 1], objective = \"reg:logistic\")\n\npredictions = predict(model, newdata = as.matrix(data_new)[,-1])\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = predictions), file = \"Max_titanic_xgboost.csv\")\n\n\n\n\n\n\n\n\nBonus: Implement a BRT on your own!\n\n\n\nYou can easily implement a BRT or boosted linear model using the rpart package or the lm function.\n\n\n\n\nClick here to see the solution\n\nGo through the code line by line and try to understand it. Ask, if you have any questions:\nLet’s try it:\n\ndata = model.matrix(~. , data = airquality)\n\nmodel = get_boosting_model(x = data[,-2], y = data[,2], n_trees = 5L )\npred = predict(model, newdata = data[,-2])\nplot(data[,2], pred, xlab = \"observed\", ylab = \"predicted\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning of random forest\n\n\n\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\nmtry\nSubset of features randomly selected in each node (from which the algorithm can select the feature that will be used to split the data).\n\n\nminimum node size\nMinimal number of observations allowed in a node (before the branching is canceled)\n\n\nmax depth\nMaximum number of tree depth\n\n\n\nCombing back to the titanic dataset from the morning, we want to optimize min node size in our RF using a simple CV.\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\ndata_sub$survived = as.factor(data_sub$survived)\ndata_obs$survived = as.factor(data_obs$survived)\n\nHints:\n\nadjust the ‘type’ argument in the predict(…) method (the default is to predict classes)\nwhen predicting probabilities, the randomForest will return a matrix, a column for each class, we are interested in the probability of surviving (so the second column)\n\nBonus:\n\ntune min node size (and mtry)\nuse more features\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(randomForest)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ndata_obs$survived = as.factor(data_obs$survived)\n\ncv = 3\nhyper_minnodesize = ...\nsplit = ...\n\ntuning_results =\n    sapply(1:length(hyper_minnodesize), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          model = randomForest(survived~.,data = train_inner, nodesize = hyper_minnodesize[k])\n          predictions = predict(model, test_inner, type = \"prob\")[,2]\n          \n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions)\n        }\n      return(mean(auc_inner))\n    })\n\nresults = data.frame(minnodesize = hyper_minnodesize, AUC = tuning_results)\n\nprint(results)\n\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(randomForest)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ndata_obs$survived = as.factor(data_obs$survived)\n\ncv = 3\nhyper_minnodesize = sample(100, 20)\nsplit = sample.int(cv, nrow(data_obs), replace = T)\n\ntuning_results =\n    sapply(1:length(hyper_minnodesize), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          model = randomForest(survived~.,data = train_inner, nodesize = hyper_minnodesize[k])\n          predictions = predict(model, test_inner, type = \"prob\")[,2]\n          \n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions)\n        }\n      return(mean(auc_inner))\n    })\n\nresults = data.frame(minnodesize = hyper_minnodesize, AUC = tuning_results)\n\nprint(results)\n\n   minnodesize       AUC\n1           49 0.8157904\n2           65 0.8097477\n3           25 0.8222759\n4           74 0.8135160\n5           18 0.8257731\n6          100 0.8086002\n7           47 0.8153151\n8           24 0.8246248\n9           71 0.8181533\n10          89 0.8101602\n11          37 0.8247795\n12          20 0.8239917\n13          26 0.8263038\n14           3 0.8309202\n15          41 0.8212540\n16          27 0.8239286\n17          36 0.8251308\n18           5 0.8274042\n19          34 0.8214008\n20          87 0.8148782\n\n\nMake predictions:\n\nmodel = randomForest(survived~.,data = data_obs, nodesize = results[which.max(results$AUC),1])\n\nwrite.csv(data.frame(y = predict(model, data_new, type = \"prob\")[,2]), file = \"Max_titanic_rf.csv\")",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tree-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B2-Distance.html",
    "href": "B2-Distance.html",
    "title": "6  Distance-based Algorithms",
    "section": "",
    "text": "6.1 K-Nearest-Neighbor\nIn this chapter, we introduce support-vector machines (SVM) and other distance-based methods Hint: Distance-based models need scaling!\nK-nearest-neighbor (kNN) is a simple algorithm that stores all the available cases and classifies the new data based on a similarity measure. It is mostly used to classify a data point based on how its \\(k\\) nearest neighbors are classified.\nLet us first see an example:\nx = scale(iris[,1:4])\ny = iris[,5]\nplot(x[-100,1], x[-100, 3], col = y)\npoints(x[100,1], x[100, 3], col = \"blue\", pch = 18, cex = 1.3)\nWhich class would you decide for the blue point? What are the classes of the nearest points? Well, this procedure is used by the k-nearest-neighbors classifier and thus there is actually no “real” learning in a k-nearest-neighbors classification.\nFor applying a k-nearest-neighbors classification, we first have to scale the data set, because we deal with distances and want the same influence of all predictors. Imagine one variable has values from -10.000 to 10.000 and another from -1 to 1. Then the influence of the first variable on the distance to the other points is much stronger than the influence of the second variable. On the iris data set, we have to split the data into training and test set on our own. Then we will follow the usual pipeline.\ndata = iris\ndata[,1:4] = apply(data[,1:4],2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\nFit model and create predictions:\nlibrary(kknn)\nset.seed(123)\n\nknn = kknn(Species~., train = train, test = test)\n\nWarning in model.matrix.default(mt2, test, contrasts.arg = contrasts.arg):\nvariable 'Species' is absent, its contrast will be ignored\n\nsummary(knn)\n\n\nCall:\nkknn(formula = Species ~ ., train = train, test = test)\n\nResponse: \"nominal\"\n          fit prob.setosa prob.versicolor prob.virginica\n1      setosa           1      0.00000000      0.0000000\n2      setosa           1      0.00000000      0.0000000\n3      setosa           1      0.00000000      0.0000000\n4      setosa           1      0.00000000      0.0000000\n5      setosa           1      0.00000000      0.0000000\n6      setosa           1      0.00000000      0.0000000\n7      setosa           1      0.00000000      0.0000000\n8      setosa           1      0.00000000      0.0000000\n9      setosa           1      0.00000000      0.0000000\n10     setosa           1      0.00000000      0.0000000\n11     setosa           1      0.00000000      0.0000000\n12     setosa           1      0.00000000      0.0000000\n13     setosa           1      0.00000000      0.0000000\n14     setosa           1      0.00000000      0.0000000\n15     setosa           1      0.00000000      0.0000000\n16     setosa           1      0.00000000      0.0000000\n17 versicolor           0      0.98430840      0.0156916\n18 versicolor           0      1.00000000      0.0000000\n19 versicolor           0      0.91487300      0.0851270\n20 versicolor           0      0.82540117      0.1745988\n21 versicolor           0      1.00000000      0.0000000\n22 versicolor           0      0.91487300      0.0851270\n23 versicolor           0      0.98430840      0.0156916\n24 versicolor           0      0.63493866      0.3650613\n25 versicolor           0      1.00000000      0.0000000\n26 versicolor           0      1.00000000      0.0000000\n27 versicolor           0      1.00000000      0.0000000\n28 versicolor           0      1.00000000      0.0000000\n29 versicolor           0      1.00000000      0.0000000\n30 versicolor           0      0.91487300      0.0851270\n31 versicolor           0      1.00000000      0.0000000\n32 versicolor           0      1.00000000      0.0000000\n33  virginica           0      0.04881448      0.9511855\n34  virginica           0      0.08512700      0.9148730\n35  virginica           0      0.00000000      1.0000000\n36  virginica           0      0.00000000      1.0000000\n37  virginica           0      0.14147595      0.8585240\n38  virginica           0      0.00000000      1.0000000\n39  virginica           0      0.31624686      0.6837531\n40  virginica           0      0.25800813      0.7419919\n41  virginica           0      0.00000000      1.0000000\n42  virginica           0      0.00000000      1.0000000\n43 versicolor           0      0.64309579      0.3569042\n44  virginica           0      0.00000000      1.0000000\n45  virginica           0      0.00000000      1.0000000\n\ntable(test$Species, fitted(knn))\n\n            \n             setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         16         0\n  virginica       0          1        12",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B2-Distance.html#k-nearest-neighbor",
    "href": "B2-Distance.html#k-nearest-neighbor",
    "title": "6  Distance-based Algorithms",
    "section": "",
    "text": "Hyperparameter\nExplanation\n\n\n\n\nkernel\nKernel that should be used. Kernel is used to bring the features into a feature space where the problem/task is easier to solve\n\n\nk\nNumber of neighbors used to calculate the response",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B2-Distance.html#support-vector-machines-svms",
    "href": "B2-Distance.html#support-vector-machines-svms",
    "title": "6  Distance-based Algorithms",
    "section": "6.2 Support Vector Machines (SVMs)",
    "text": "6.2 Support Vector Machines (SVMs)\nSupport vectors machines have a different approach. They try to divide the predictor space into sectors for each class. To do so, a support-vector machine fits the parameters of a hyperplane (a \\(n-1\\) dimensional subspace in a \\(n\\)-dimensional space) in the predictor space by optimizing the distance between the hyperplane and the nearest point from each class.\nFitting a support-vector machine:\n\nlibrary(e1071)\n\ndata = iris\ndata[,1:4] = apply(data[,1:4], 2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\nsm = svm(Species~., data = train, kernel = \"linear\")\npred = predict(sm, newdata = test)\n\n\noldpar = par(mfrow = c(1, 2))\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  pred, main = \"predicted\")\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  test$Species, main = \"observed\")\n\n\n\n\n\n\n\npar(oldpar)\n\nmean(pred == test$Species) # Accuracy.\n\n[1] 0.9777778\n\n\nSupport-vector machines can only work on linearly separable problems. (A problem is called linearly separable if there exists at least one line in the plane with all of the points of one class on one side of the hyperplane and all the points of the others classes on the other side).\nIf this is not possible, we however, can use the so called kernel trick, which maps the predictor space into a (higher dimensional) space in which the problem is linear separable. After having identified the boundaries in the higher-dimensional space, we can project them back into the original dimensions.\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nX = expand.grid(x1, x2)\ny = apply(X, 1, function(t) exp(-t[1]^2 - t[2]^2))\ny = ifelse(1/(1+exp(-y)) &lt; 0.62, 0, 1)\n\nimage(matrix(y, 100, 100))\nanimation::saveGIF(\n  {\n    for(i in c(\"truth\", \"linear\", \"radial\", \"sigmoid\")){\n      if(i == \"truth\"){\n        image(matrix(y, 100,100),\n        main = \"Ground truth\", axes = FALSE, las = 2)\n      }else{\n        sv = e1071::svm(x = X, y = factor(y), kernel = i)\n        image(matrix(as.numeric(as.character(predict(sv, X))), 100, 100),\n        main = paste0(\"Kernel: \", i), axes = FALSE, las = 2)\n        axis(1, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    }\n  },\n  movie.name = \"svm.gif\", autobrowse = FALSE, interval = 2\n)\n\n\n\n\n\n\n\n\n\n\nAs you have seen, this does not work with every kernel. Hence, the problem is to find the actual correct kernel, which is again an optimization procedure and can thus be approximated.\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\nkernel\nKernel that should be used. Kernel is used to bring the features into a feature space where the problem/task is easier to solve / linear separable\n\n\ncost\nregularization term",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B2-Distance.html#clustering-methods",
    "href": "B2-Distance.html#clustering-methods",
    "title": "6  Distance-based Algorithms",
    "section": "6.3 Clustering methods",
    "text": "6.3 Clustering methods\nIn unsupervised learning, we want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider the iris data set. Here, we have 150 observations of 4 floral traits:\n\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4])\nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits,\n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\n\n\n\n\nTrait distributions of iris dataset\n\n\n\n\nThe observations are from 3 species and indeed those species tend to have different traits, meaning that the observations form 3 clusters.\n\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])\n\n\n\n\nScatterplots for trait-trait combinations.\n\n\n\n\nHowever, imagine we don’t know what species are, what is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does.\n\n6.3.1 Hierarchical Clustering\nA cluster refers to a collection of data points aggregated together because of certain similarities.\nIn hierarchical clustering, a hierarchy (tree) between data points is built.\n\nAgglomerative: Start with each data point in their own cluster, merge them up hierarchically.\nDivisive: Start with all data points in one cluster, and split hierarchically.\n\nMerges / splits are done according to linkage criterion, which measures distance between (potential) clusters. Cut the tree at a certain height to get clusters.\nHere an example\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nd = dist(traits)\nhc = hclust(d, method = \"complete\")\n\nplot(hc, main=\"\")\nrect.hclust(hc, k = 3)  # Draw rectangles around the branches.\n\n\n\n\nResults of hierarchical clustering. Red rectangle is drawn around the corresponding clusters.\n\n\n\n\nSame plot, but with colors for true species identity\n\nlibrary(ape)\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes.\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\n\nCalculate confusion matrix. Note we are switching labels here so that it fits to the species.\n\ntmp = hcRes3\ntmp[hcRes3 == 2] = 3\ntmp[hcRes3 == 3] = 2\nhcRes3 = tmp\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n27\n1\n\n\n0\n23\n49\n\n\n\n\n\nNote that results might change if you choose a different agglomeration method, distance metric or scale of your variables. Compare, e.g. to this example:\n\nhc = hclust(d, method = \"ward.D2\")\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes. Different agglomeration method\n\n\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\nWhich method is best? firstsecond\n\nlibrary(dendextend)\n\n\nset.seed(123)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n             \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\nout = dendlist()   # Create a dendlist object from several dendrograms.\nfor(method in methods){\n  res = hclust(d, method = method)\n  out = dendlist(out, as.dendrogram(res))\n}\nnames(out) = methods\nprint(out)\n\n$ward.D\n'dendrogram' with 2 branches and 150 members total, at height 199.6205 \n\n$single\n'dendrogram' with 2 branches and 150 members total, at height 1.640122 \n\n$complete\n'dendrogram' with 2 branches and 150 members total, at height 7.085196 \n\n$average\n'dendrogram' with 2 branches and 150 members total, at height 4.062683 \n\n$mcquitty\n'dendrogram' with 2 branches and 150 members total, at height 4.497283 \n\n$median\n'dendrogram' with 2 branches and 150 members total, at height 2.82744 \n\n$centroid\n'dendrogram' with 2 branches and 150 members total, at height 2.994307 \n\n$ward.D2\n'dendrogram' with 2 branches and 150 members total, at height 32.44761 \n\nattr(,\"class\")\n[1] \"dendlist\"\n\nget_ordered_3_clusters = function(dend){\n  # order.dendrogram function returns the order (index)\n  # or the \"label\" attribute for the leaves.\n  # cutree: Cut the tree (dendrogram) into groups of data.\n  cutree(dend, k = 3)[order.dendrogram(dend)]\n}\ndend_3_clusters = lapply(out, get_ordered_3_clusters)\n\n# Calculate Fowlkes-Mallows Index (determine the similarity between clusterings)\ncompare_clusters_to_iris = function(clus){\n  FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)\n}\n\nclusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\ndotchart(sort(clusters_performance), xlim = c(0.3, 1),\n         xlab = \"Fowlkes-Mallows index\",\n         main = \"Performance of linkage methods\n         in detecting the 3 species \\n in our example\",\n         pch = 19)\n\n\n\n\n\n\n\n\nWe might conclude that ward.D2 works best here. However, as we will learn later, optimizing the method without a hold-out for testing implies that our model may be overfitting. We should check this using cross-validation.\n\n\n6.3.2 K-means Clustering\nAnother example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms.\nTo start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the assumed or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere. Afterwards each data point is assigned to the respective cluster that raises the overall in-cluster sum of squares (variance) related to the distance to the centroid least of all. After the algorithm has placed all data points into a cluster the centroids get updated. By iterating this procedure until the assignment doesn’t change any longer, the algorithm can find the (locally) optimal centroids and the data points belonging to this cluster. Note that results might differ according to the initial positions of the centroids. Thus several (locally) optimal solutions might be found.\nThe “k” in K-means refers to the number of clusters and the ‘means’ refers to averaging the data-points to find the centroids.\nA typical pipeline for using k-means clustering looks the same as for other algorithms. After having visualized the data, we fit a model, visualize the results and have a look at the performance by use of the confusion matrix. By setting a fixed seed, we can ensure that results are reproducible.\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n\nK-means clustering with 3 clusters of sizes 50, 62, 38\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.006000    3.428000     1.462000    0.246000\n2     5.901613    2.748387     4.393548    1.433871\n3     6.850000    3.073684     5.742105    2.071053\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 15.15100 39.82097 23.87947\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nVisualizing the results. Color codes true species identity, symbol shows cluster result.\n\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\n\n\n\n\n\n\n\n\nWe see that there are are some discrepancies. Confusion matrix:\n\ntable(iris$Species, kc$cluster)\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36\n\n\nIf you want to animate the clustering process, you could run\n\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\n\nElbow technique to determine the probably best suited number of clusters:\n\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))\n\n\n\n\n\n\n\n\nOften, one is interested in sparse models. Furthermore, higher k than necessary tends to overfitting. At the kink in the picture, the sum of squares dropped enough and k is still low enough. But keep in mind, this is only a rule of thumb and might be wrong in some special cases.\n\n\n6.3.3 Density-based Clustering\nDetermine the affinity of a data point according to the affinity of its k nearest neighbors. This is a very general description as there are many ways to do so.\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nset.seed(123)\n\nkNNdistplot(traits, k = 4)   # Calculate and plot k-nearest-neighbor distances.\nabline(h = 0.4, lty = 2)\n\n\n\n\n\n\n\ndc = dbscan(traits, eps = 0.4, minPts = 6)\nprint(dc)\n\nDBSCAN clustering for 150 objects.\nParameters: eps = 0.4, minPts = 6\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 4 cluster(s) and 32 noise points.\n\n 0  1  2  3  4 \n32 46 36 14 22 \n\nAvailable fields: cluster, eps, minPts, metric, borderPoints\n\n\n\nlibrary(factoextra)\n\n\nfviz_cluster(dc, traits, geom = \"point\", ggtheme = theme_light())\n\n\n\n\n\n\n\n\n\n\n6.3.4 Model-based Clustering\nThe last class of methods for unsupervised clustering are so-called model-based clustering methods.\n\nlibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\n\n\nmb = Mclust(traits)\n\nMclust automatically compares a number of candidate models (clusters, shape) according to BIC (The BIC is a criterion for classifying algorithms depending their prediction quality and their usage of parameters). We can look at the selected model via:\n\nmb$G # Two clusters.\n\n[1] 2\n\nmb$modelName # &gt; Ellipsoidal, equal shape.\n\n[1] \"VEV\"\n\n\nWe see that the algorithm prefers having 2 clusters. For better comparability to the other 2 methods, we will override this by setting:\n\nmb3 = Mclust(traits, 3)\n\nResult in terms of the predicted densities for 3 clusters\n\nplot(mb3, \"density\")\n\n\n\n\n\n\n\n\nPredicted clusters:\n\nplot(mb3, what=c(\"classification\"), add = T)\n\n\n\n\n\n\n\n\nConfusion matrix:\n\ntable(iris$Species, mb3$classification)\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\n\n\n6.3.5 Ordination\nOrdination is used in explorative analysis and compared to clustering, similar objects are ordered together. So there is a relationship between clustering and ordination. Here a PCA ordination on on the iris data set.\n\npcTraits = prcomp(traits, center = TRUE, scale. = TRUE)\nbiplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25))\n\n\n\n\n\n\n\n\nYou can cluster the results of this ordination, ordinate before clustering, or superimpose one on the other.",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B2-Distance.html#exercise---knn-and-svm",
    "href": "B2-Distance.html#exercise---knn-and-svm",
    "title": "6  Distance-based Algorithms",
    "section": "6.4 Exercise - kNN and SVM",
    "text": "6.4 Exercise - kNN and SVM\n\n\n\n\n\n\nQuestion: Hyperparameter tuning of kNN\n\n\n\nWe want to optimize the number of neighbors (k) and the kernel of the kNN:\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:ape':\n\n    where\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\nHints:\n\ncheck the help of the kNN function to understand the hyperparameters\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(kknn)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ndata_obs$survived = as.factor(data_obs$survived)\ncv = 3\nsteps = 10\nsplit = ...\n\nhyper_k = ...\nhyper_kernel = ...\n\ntuning_results =\n    sapply(1:length(hyper_kernel), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          predictions = kknn(survived~., train = train_inner, test = test_inner, k = hyper_k[k], scale = FALSE, kernel = hyper_kernel[k])\n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions$prob[,2])\n        }\n      return(mean(auc_inner))\n    })\n\n\nresults = data.frame(k = hyper_k, kernel = hyper_kernel, AUC = tuning_results)\n\n\nprint(results)\n\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(kknn)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ndata_obs$survived = as.factor(data_obs$survived)\ncv = 3\nsteps = 10\nsplit = sample.int(cv, nrow(data_obs), replace = T)\n\nhyper_k = sample(10, 10)\nhyper_kernel = sample(c(\"triangular\", \"inv\", \"gaussian\", \"rank\"), 10, replace = TRUE)\n\ntuning_results =\n    sapply(1:length(hyper_kernel), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          predictions = kknn(survived~., train = train_inner, test = test_inner, k = hyper_k[k], scale = FALSE, kernel = hyper_kernel[k])\n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions$prob[,2])\n        }\n      return(mean(auc_inner))\n    })\n\n\nresults = data.frame(k = hyper_k, kernel = hyper_kernel, AUC = tuning_results)\n\n\nprint(results)\n\n    k     kernel       AUC\n1   3 triangular 0.7766805\n2   5   gaussian 0.7940078\n3  10 triangular 0.8071778\n4   8       rank 0.8010088\n5   4 triangular 0.7857645\n6   1       rank 0.7164612\n7   2 triangular 0.7520952\n8   7        inv 0.8020953\n9   9   gaussian 0.8076482\n10  6 triangular 0.7984275\n\n\nMake predictions:\n\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n    predictions = kknn(as.factor(survived)~., train = data_obs, test = data_new, k = results$k[i], scale = FALSE, kernel = results$kernel[i])\n    return(predictions$prob[,2])\n  })\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning of SVM\n\n\n\nWe want to optimize the kernel and the cost parameters\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\nHints:\n\ncheck the help of the kNN function to understand the hyperparameters\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(e1071)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ndata_obs$survived = as.factor(data_obs$survived)\ncv = 3\nsteps = 10\nsplit = ...\n\nhyper_k = ...\nhyper_kernel = ...\n\ntuning_results =\n    sapply(1:length(hyper_kernel), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          predictions = kknn(survived~., train = train_inner, test = test_inner, k = hyper_k[k], scale = FALSE, kernel = hyper_kernel[k])\n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions$prob[,2])\n        }\n      return(mean(auc_inner))\n    })\n\n\nresults = data.frame(k = hyper_k, kernel = hyper_kernel, AUC = tuning_results)\n\n\nprint(results)\n\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(e1071)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ndata_obs$survived = as.factor(data_obs$survived)\ncv = 3\nsteps = 40\nsplit = sample.int(cv, nrow(data_obs), replace = T)\n\nhyper_cost = runif(10, 0, 2)\nhyper_kernel = sample(c(\"linear\", \"polynomial\", \"radial\", \"sigmoid\"), 10, replace = TRUE)\n\ntuning_results =\n    sapply(1:length(hyper_kernel), function(k) {\n        auc_inner = NULL\n        for(j in 1:cv) {\n          inner_split = split == j\n          train_inner = data_obs[!inner_split, ]\n          test_inner = data_obs[inner_split, ]\n          model = svm(survived~., data = train_inner, cost = hyper_cost[k], kernel = hyper_kernel[k], probability = TRUE)\n          predictions = attr(predict(model, newdata = test_inner, probability = TRUE), \"probabilities\")[,1]\n          auc_inner[j]= Metrics::auc(test_inner$survived, predictions)\n        }\n      return(mean(auc_inner))\n    })\n\n\nresults = data.frame(cost = hyper_cost, kernel = hyper_kernel, AUC = tuning_results)\n\n\nprint(results)\n\n        cost  kernel       AUC\n1  0.2753503 sigmoid 0.5526128\n2  0.5526653  radial 0.6159725\n3  0.7036870  linear 0.5865394\n4  1.6256105 sigmoid 0.5178411\n5  0.3430827  linear 0.5643597\n6  1.0423242  radial 0.6035677\n7  1.5292457  linear 0.5559287\n8  0.5775113 sigmoid 0.5368064\n9  0.8735566  linear 0.6131153\n10 1.3389015 sigmoid 0.5189292\n\n\nMake predictions:\n\nmodel = svm(survived~., data = data_obs, cost = results[which.max(results$AUC),1], kernel = results[which.max(results$AUC),2], probability = TRUE)\npredictions = attr(predict(model, newdata = data_new[,-1], probability = TRUE), \"probabilities\")[,1]\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B2-Distance.html#exercise---unsupervised-learning",
    "href": "B2-Distance.html#exercise---unsupervised-learning",
    "title": "6  Distance-based Algorithms",
    "section": "6.5 Exercise - Unsupervised learning",
    "text": "6.5 Exercise - Unsupervised learning\n\n\n\n\n\n\nTask\n\n\n\nGo through the 4(5) unsupervised algorithms from the supervised chapter Section 2.2, and check\n\nif they are sensitive (i.e. if results change)\nif you scale the input features (= predictors), instead of using the raw data.\n\nDiscuss in your group: Which is more appropriate for this analysis and/or in general: Scaling or not scaling?\n\n\n\n\nClick here to see the solution for hierarchical clustering\n\n\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)\n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n\n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n\n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\n\n\n\n\nIt seems that scaling is harmful for hierarchical clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for K-means\n\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n\n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n\n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n\n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\n\n\n\n\nIt seems that scaling is harmful for K-means clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for density-based clustering\n\n\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\nLevels: 1 2 3\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n\n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n\n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     3 47  0  0  0\n      2     5  0 38  3  4\n      3    17  0  0 33  0\n\nCorrect classified points:  118  /  150\nSum of noise points:  25 \n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     9 41  0  0  0\n      2    14  0 36  0  0\n      3    36  0  1  4  9\n\nCorrect classified points:  81  /  150\nSum of noise points:  59 \n\n\nIt seems that scaling is harmful for density based clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for model-based clustering\n\n\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  mb3 = Mclust(traits, 3)\n\n  tbl = table(iris$Species, mb3$classification)\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n\nFor model based clustering, scaling does not matter.\n\n\n\nClick here to see the solution for ordination\n\n\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n\n\n\n\n\n\n\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n\n\n\n\n\n\n\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")\n\n\n\n\n\n\n\n\nFor PCA ordination, scaling matters. Because we are interested in directions of maximal variance, all parameters should be scaled, or the one with the highest values might dominate all others.",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distance-based Algorithms</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html",
    "href": "B3-NeuralNetworks.html",
    "title": "7  Artificial Neural Networks",
    "section": "",
    "text": "7.1 Fitting (deep) neural networks with the cito package\nArtificial neural networks are biologically inspired, the idea is that inputs are processed by weights, the neurons, the signals then accumulate at hidden nodes (axioms), and only if the sum of activations of several neurons exceed a certain threshold, the signal will be passed on.\nDeep neural networks are currently the state of the art in unsupervised learning. Their ability to model different types of data (e.g. graphs, images) is one of the reasons for their rise in recent years. However, requires extensive (programming) knowledge of the underlying deep learning frameworks (e.g. TensorFlow or PyTorch), which we will teach you in two days. For tabular data, we can use packages like cito, which work similarly to regression functions like lm and allow us to train deep neural networks in one line of code:\nlibrary(cito)\nnn.fit&lt;- dnn(Species~., data = datasets::iris, loss = \"softmax\", verbose = FALSE, plot = FALSE)\n\nRegistered S3 methods overwritten by 'reformulas':\n  method       from\n  head.call    cito\n  head.formula cito\n  head.name    cito\ncito also supports many of the S3 methods that are available for statistical models, e.g. the summary function:\nsummary(nn.fit)\n\nSummary of Deep Neural Network Model\n\nFeature Importance:\n      variable importance_1\n1 Sepal.Length    1.6094284\n2  Sepal.Width    0.2381193\n3 Petal.Length   45.3715753\n4  Petal.Width   10.4426141\n\nAverage Conditional Effects:\n               Response_1  Response_2  Response_3\nSepal.Length  0.001933703  0.08236437 -0.08429806\nSepal.Width   0.005151204  0.07330558 -0.07845679\nPetal.Length -0.009846135 -0.14471930  0.15456542\nPetal.Width  -0.006189441 -0.17347164  0.17966107\n\nStandard Deviation of Conditional Effects:\n              Response_1 Response_2 Response_3\nSepal.Length 0.003113749  0.1736472  0.1730299\nSepal.Width  0.008174312  0.1627127  0.1606906\nPetal.Length 0.014838127  0.3025878  0.2983690\nPetal.Width  0.009500972  0.3541783  0.3519059\nVariable importance can also be computed for non-tree algorithms (although it is slightly different, more on that on Thursday). The feature importance reports the importance of the features for distinguishing the three species, the average conditional effects are an approximation of the linear effects, and the standard deviation of the conditional effects is a measure of the non-linearity of these three variables.\nWe can also plot the underlying neural network:\nplot(nn.fit)\nThe network starts with the input layer, 4 nodes for our four features, then two hidden layers, each with 50 nodes, and a final layer with 3 output nodes for our three levels of the species variable. The lines between the nodes are the connections between the nodes, and the lines actually represent the weights optimised during training (some connections are almost invisible, meaning they are close to 0).",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html#loss",
    "href": "B3-NeuralNetworks.html#loss",
    "title": "7  Artificial Neural Networks",
    "section": "7.2 Loss",
    "text": "7.2 Loss\nTasks such as regression and classification are fundamentally different; the former has continuous responses, while the latter has a discrete response. In ML algorithms, these different tasks can be represented by different loss functions (Classical ML algorithms also use loss functions but often they are automatically inferred, also neural networks are much more versatile, supporting more loss functions). Moreover, the tasks can differ even within regression or classification (e.g., in classification, we have binary classification (0 or 1) or multi-class classification (0, 1, or 2)). As a result, especially in DL, we have different specialized loss functions available for specific response types. The table below shows a list of supported loss functions in cito:\n\n\n\n\n\n\n\n\nLoss\nType\nExample\n\n\n\n\nmse (mean squared error)\nRegression\nNumeric values\n\n\nmae (mean absolute error)\nRegression\nNumeric values, often used for skewed data\n\n\nsoftmax\nClassification, multi-label\nSpecies\n\n\ncross-entropy\nClassification, binary or multi-class\nSurvived/non-survived, Multi-species/communities\n\n\nbinomial\nClassification, binary or multi-class\nBinomial likelihood\n\n\npoisson\nRegression\nCount data\n\n\n\nIn the iris data, we model Species which has 3 response levels, so this is was what we call multilabel and it requires a softmax link and a cross-entropy loss function, in cito we specify that by using the softmax loss:\n\nlibrary(cito)\nmodel&lt;- dnn(Species~., data = datasets::iris, loss = \"softmax\", verbose = FALSE)\n\n\n\n\n\n\n\nhead(predict(model, type = \"response\"))\n\n        setosa  versicolor    virginica\n[1,] 0.9956357 0.004364315 1.738729e-10\n[2,] 0.9927422 0.007257805 5.387814e-10\n[3,] 0.9942655 0.005734487 4.055749e-10\n[4,] 0.9882450 0.011755005 1.823194e-09\n[5,] 0.9955537 0.004446283 1.991508e-10\n[6,] 0.9933258 0.006674126 3.934524e-10",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html#validation",
    "href": "B3-NeuralNetworks.html#validation",
    "title": "7  Artificial Neural Networks",
    "section": "7.3 Validation",
    "text": "7.3 Validation\nA holdout, or validation data, is important for detecting (and preventing) overfitting. In cito, we can directly tell the dnn function to automatically use a random subset of the data as validation data, which is validated after each epoch (each iteration of the optimization), allowing us to monitor the training:\n\ndata = airquality[complete.cases(airquality),] # DNN cannot handle NAs!\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            validation = 0.2,\n            loss = \"mse\",data = data, verbose = FALSE)\n\n\n\n\n\n\n\n\nThe validation argument ranges from 0 and 1 is the percent of the data that should be used for validation\n\n7.3.1 Baseline loss\nSince training DNNs can be quite challenging, we provide in cito a baseline loss that is computed from an intercept-only model (e.g., just the mean of the response). And the absolute minimum performance our DNN should achieve is to outperform the baseline model!",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html#trainings-parameter",
    "href": "B3-NeuralNetworks.html#trainings-parameter",
    "title": "7  Artificial Neural Networks",
    "section": "7.4 Trainings parameter",
    "text": "7.4 Trainings parameter\nIn DL, the optimization (the training of the DNN) is challenging as we have to optimize up to millions of parameters (which are not really identifiable, it is accepted that the optimization does not find a global minimum but just a good local minimum). We have a few important hyperparameters that affect only the optimization:\n\n\n\n\n\n\n\n\nHyperparameter\nMeaning\nRange\n\n\n\n\nlearning rate\nthe step size of the parameter updating in the iterative optimization routine, if too high, the optimizer will step over good local optima, if too small, the optimizer will be stuck in a bad local optima\n[0.00001, 0.5]\n\n\nbatch size\nNNs are optimized via stochastic gradient descent, i.e. only a batch of the data is used to update the parameters at a time\nDepends on the data:\n10-250\n\n\nepoch\nthe data is fed into the optimization in batches, once the entire data set has been used in the optimization, the epoch is complete (so e.g. n = 100, batch size = 20, it takes 5 steps to complete an epoch)\n100+ (use early stopping)\n\n\n\n\n7.4.1 Learning rate\ncito visualizes the training (see graphic). The reason for this is that the training can easily fail if the learning rate (lr) is poorly chosen. If the lr is too high, the optimizer “jumps” over good local optima, while it gets stuck in local optima if the lr is too small:\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.4, data = data, epochs = 150L, verbose = FALSE)\n\nIf too high, the training will either directly fail (because the loss jumps to infinity) or the loss will be very wiggly and doesn’t decrease over the number of epochs.\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", lr = 0.0001, data = data, epochs = 150L, verbose = FALSE)\n\n\n\n\n\n\n\n\nIf too low, the loss will be very wiggly but doesn’t decrease.\n\n\n\n\n\n\nLearning rate scheduler\n\n\n\nAdjusting / reducing the learning rate during training is a common approach in neural networks. The idea is to start with a larger learning rate and then steadily decrease it during training (either systematically or based on specific properties):\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.1,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html#architecture",
    "href": "B3-NeuralNetworks.html#architecture",
    "title": "7  Artificial Neural Networks",
    "section": "7.5 Architecture",
    "text": "7.5 Architecture\nThe architecture of the NN can be specified by the hidden argument, it is a vector where the length corresponds to the number of hidden layers and value of entry to the number of hidden neurons in each layer (and the same applies for the activation argument that specifies the activation functions in the hidden layers). It is hard to make recommendations about the architecture, a kind of general rule is that the width of the hidden layers is more important than the depth of the NN.\nExample:\n\ndata = airquality[complete.cases(airquality),] # DNN cannot handle NAs!\ndata = scale(data)\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), # Architecture, number of hidden layers and nodes in each layer\n            activation = c(\"selu\", \"selu\"), # activation functions for the specific hidden layer\n            loss = \"mse\", lr = 0.01, data = data, epochs = 150L, verbose = FALSE)\n\n\n\n\n\n\n\nplot(model)\n\n\n\n\n\n\n\nsummary(model)\n\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R    0.2873814\n2     Wind    1.7571817\n3     Temp    2.4734383\n4    Month    0.1119469\n5      Day    0.1012945\n\nAverage Conditional Effects:\n        Response_1\nSolar.R  0.1625627\nWind    -0.3793555\nTemp     0.4532442\nMonth   -0.1140413\nDay      0.1017338\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.07120892\nWind    0.30708902\nTemp    0.22510422\nMonth   0.06714132\nDay     0.05358164",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html#regularization",
    "href": "B3-NeuralNetworks.html#regularization",
    "title": "7  Artificial Neural Networks",
    "section": "7.6 Regularization",
    "text": "7.6 Regularization\nWe can use \\(\\lambda\\) and \\(\\alpha\\) to set L1 and L2 regularization on the weights in our NN:\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.01,\n            lambda = 0.01, # regularization strength\n            alpha = 0.5,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n\n\n\n\n\n\n\nsummary(model)\n\nSummary of Deep Neural Network Model\n\nFeature Importance:\n  variable importance_1\n1  Solar.R   0.19268358\n2     Wind   0.84137686\n3     Temp   1.79677616\n4    Month   0.11496549\n5      Day   0.03293281\n\nAverage Conditional Effects:\n        Response_1\nSolar.R  0.1447125\nWind    -0.3445336\nTemp     0.5221033\nMonth   -0.1053451\nDay      0.0620252\n\nStandard Deviation of Conditional Effects:\n        Response_1\nSolar.R 0.05401614\nWind    0.08637159\nTemp    0.09301890\nMonth   0.07021153\nDay     0.06142853\n\n\nBe careful that you don’t accidentally set all weights to 0 because of a too high regularization. We check the weights of the first layer:\n\nfields::image.plot(coef(model)[[1]][[1]]) # weights of the first layer",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html#hyperparameter-tuning",
    "href": "B3-NeuralNetworks.html#hyperparameter-tuning",
    "title": "7  Artificial Neural Networks",
    "section": "7.7 Hyperparameter tuning",
    "text": "7.7 Hyperparameter tuning\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\nif you pass the function tune(...) to a hyperparameter, this hyperparameter will be automatically tuned\nin the tuning = config_tuning(...) argument, you can specify the cross-validation strategy and the number of hyperparameters that shoudl be tested\nafter the tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~., \n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L),\n                  verbose = FALSE\n                  )\n\nStarting hyperparameter tuning...\nFitting final model...\n\n# tuning results\nmodel_tuned$tuning\n\n# A tibble: 20 × 5\n   steps  test train models  lambda\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt;\n 1     1  13.6     0 NA     0.00181\n 2     2  72.2     0 NA     0.0744 \n 3     3  72.9     0 NA     0.0675 \n 4     4  45.7     0 NA     0.0319 \n 5     5 165.      0 NA     0.195  \n 6     6  77.9     0 NA     0.0919 \n 7     7  73.2     0 NA     0.0832 \n 8     8  18.3     0 NA     0.0110 \n 9     9 165.      0 NA     0.170  \n10    10  55.0     0 NA     0.0472 \n11    11  32.0     0 NA     0.0238 \n12    12 165.      0 NA     0.151  \n13    13 139.      0 NA     0.141  \n14    14  21.2     0 NA     0.0136 \n15    15  83.9     0 NA     0.120  \n16    16  49.0     0 NA     0.0373 \n17    17  75.4     0 NA     0.0804 \n18    18 111.      0 NA     0.117  \n19    19 165.      0 NA     0.156  \n20    20 165.      0 NA     0.194  \n\n# model_tuned is now already the best model!",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "B3-NeuralNetworks.html#exercise",
    "href": "B3-NeuralNetworks.html#exercise",
    "title": "7  Artificial Neural Networks",
    "section": "7.8 Exercise",
    "text": "7.8 Exercise\n\n\n\n\n\n\nQuestion: Hyperparameter tuning dnn - Titanic dataset\n\n\n\nTune architecture\n\nTune training parameters (learning rate, batch size) and regularization\n\nHints\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\npassing tune(...) to a hyperparameter will tell cito to tune this specific hyperparameter\nthe tuning = config_tuning(...) let you specify the cross-validation strategy and the number of hyperparameters that should be tested (steps = number of hyperparameter combinations that should be tried)\nafter tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\nlibrary(cito)\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~.,\n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L),\n                  burnin = Inf\n                  )\n\n# tuning results\nmodel_tuned$tuning\n\n\n# model_tuned is now already the best model!\n\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata =\n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\n\nmodel = dnn(survived~.,\n          hidden = c(10L, 10L), # change\n          activation = c(\"selu\", \"selu\"), # change\n          loss = \"binomial\",\n          lr = 0.05, #change\n          validation = 0.2,\n          lambda = 0.001, # change\n          alpha = 0.1, # change\n          burnin = Inf,\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = data_obs, epochs = 40L, verbose = FALSE, plot= TRUE)\n\n# Predictions:\n\npredictions = predict(model, newdata = data_new, type = \"response\") # change prediction type to response so that cito predicts probabilities\n\nwrite.csv(data.frame(y = predictions[,1]), file = \"Max_titanic_dnn.csv\")\n\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning - Plant-pollinator dataset\n\n\n\nThe plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look here.\nsee Section A.3 for more information about the dataset.\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n\n                   crop                       insect          type          \n Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n Daucus_carota       :  256   Andrena_gravida    :   80                     \n Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n (Other)             :18944   (Other)            :20000                     \n    season             diameter        corolla             colour         \n Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n Class :character   1st Qu.:  5.00   Class :character   Class :character  \n Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n                    Mean   : 27.03                                        \n                    3rd Qu.: 25.00                                        \n                    Max.   :150.00                                        \n                    NA's   :9472                                          \n    nectar            b.system         s.pollination      inflorescence     \n Length:20480       Length:20480       Length:20480       Length:20480      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  composite            guild               tongue            body      \n Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n                                       Mean   : 8.104   Mean   :10.66  \n                                       3rd Qu.:10.500   3rd Qu.:13.00  \n                                       Max.   :26.400   Max.   :25.00  \n                                       NA's   :17040    NA's   :6160   \n  sociality           feeding          interaction \n Length:20480       Length:20480       0   :14095  \n Class :character   Class :character   1   :  595  \n Mode  :character   Mode  :character   NA's: 5790  \n                                                   \n                                                   \n                                                   \n                                                   \n\n# scale numeric features\nplant_poll[, sapply(plant_poll, is.numeric)] = scale(plant_poll[, sapply(plant_poll, is.numeric)])\n\n# remove NAs\ndf = plant_poll[complete.cases(plant_poll),] # remove NAs\n\n# remove factors with only one level \ndata_obs = df %&gt;% select(-crop, -insect, -season, -colour, -guild, -feeding, -composite)\n\n# change response to integer (because cito wants integer 0/1 for binomial data)\ndata_obs$interaction = as.integer(data_obs$interaction) - 1 \n\n\n\n# prepare the test data\nnewdata = plant_poll[is.na(plantPollinator_df$interaction), ]\nnewdata_imputed = missRanger::missRanger(data = newdata[,-ncol(newdata)], verbose = 0) # fill NAs\nnewdata_imputed$interaction = NA\n\nMinimal example in cito:\n\nlibrary(cito)\nset.seed(42)\nmodel = dnn(interaction~., \n    hidden = c(50, 50), \n    activation = \"selu\", \n    loss = \"binomial\", \n    lr = tune(values = seq(0.0001, 0.03, length.out = 10)),\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    data = data_obs, \n    epochs = 200L, \n    burnin = Inf,\n    tuning = config_tuning(CV = 3, steps = 10))\n\n\nprint(model$tuning)\n\n# make final predictions\npredictions = predict(model, newdata_imputed, type = \"response\")[,1]\n\n# prepare submissions\nwrite.csv(data.frame(y = predictions), file = \"my_submission.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Tasks:\n\nUse cito to tune learning parameters and the regularization\nSubmit your predictions to http://rhsbio7.uni-regensburg.de:8500/\n\n\n\n\n\nClick here to see the solution\n\nMinimal example:\n\nlibrary(cito)\nset.seed(42)\nmodel = dnn(interaction~., \n    hidden = c(50, 50), \n    activation = \"selu\", \n    loss = \"binomial\", \n    lr = tune(values = seq(0.0001, 0.03, length.out = 10)),\n    lambda = tune(values = seq(0.0001, 0.1, length.out = 10)),\n    alpha = tune(),\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    data = data_obs, \n    epochs = 100L, \n    burnin = Inf,\n    tuning = config_tuning(CV = 3, steps = 15))\n\nStarting hyperparameter tuning...\nFitting final model...\n\nprint(model$tuning)\n\n# A tibble: 15 × 7\n   steps  test train models lambda  alpha      lr\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     1  387.     0 NA     0.0889 0.197  0.00674\n 2     2  355.     0 NA     0.1    0.501  0.0134 \n 3     3  331.     0 NA     0.0334 0.825  0.0200 \n 4     4  334.     0 NA     0.0112 0.0921 0.0134 \n 5     5  338.     0 NA     0.0778 0.470  0.0234 \n 6     6  332.     0 NA     0.0445 0.881  0.0134 \n 7     7  358.     0 NA     0.0445 0.436  0.00674\n 8     8  341.     0 NA     0.0667 0.277  0.0200 \n 9     9  562.     0 NA     0.1    0.0571 0.0001 \n10    10  320.     0 NA     0.0112 0.885  0.0234 \n11    11  332.     0 NA     0.0112 0.148  0.0234 \n12    12  344.     0 NA     0.0667 0.317  0.0167 \n13    13  338.     0 NA     0.0001 0.681  0.00342\n14    14  543.     0 NA     0.1    0.283  0.0001 \n15    15  356.     0 NA     0.0556 0.425  0.0101 \n\n\nMake predictions:\n\npredictions = predict(model, newdata_imputed, type = \"response\")[,1]\n\nwrite.csv(data.frame(y = predictions), file = \"Max_plant_.csv\")",
    "crumbs": [
      "Understanding ML Algorithms",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Artificial Neural Networks</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html",
    "href": "D2-explainableAI.html",
    "title": "8  Explainable AI",
    "section": "",
    "text": "8.1 A Practical Example\nThe goal of explainable AI (xAI, aka interpretable machine learning) is to explain why a fitted machine learning model makes certain predictions. A typical example is to understand how important different variables are for predictions. The incentives for doing so range from a better technical understanding of the models over understanding which data is important for improving predictions to questions of fairness and discrimination (e.g. to understand if an algorithm uses skin color to make a decision).\nIn this lecture we will work with an African Elephant occurrence dataset.\nWe will fit a random forest and use the iml package for xAI, see https://christophm.github.io/interpretable-ml-book/.\nlibrary(iml)\nlibrary(ranger) # different random Forest package!\nlibrary(EcoData)\nlibrary(cito)\nset.seed(123)\n\n\ndata = EcoData::elephant$occurenceData\nhead(data)\n?EcoData::elephant\nMeaning of the bioclim variables:\nrf = ranger(as.factor(Presence) ~ ., data = data, probability = TRUE)\nThe cito package has quite extensive xAI functionalities. However, ranger, as most other machine learning packages, has no extensive xAI functionalities. Thus, to do xAI with ranger, we have to use a generic xAI package that can handle almost all machine learning models.\nWhen we want to use such a generic package, we first have to create a predictor object, that holds the model and the data. The iml package uses R6 classes, that means new objects can be created by calling Predictor$new(). (Do not worry if you do not know what R6 classes are, just use the command.)\nWe often have to warp our predict function inside a so called wrapper function so that the output of the predict function fits to iml (iml expects that the predict function returns a vector of predictions:\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions[,2]\n\npredictor = Predictor$new(rf, data = data[,-1], y = data[,1], predict.function = predict_wrapper)\npredictor$task = \"classif\" # set task to classification\n# \"Predictor\" is an object generator.",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#a-practical-example",
    "href": "D2-explainableAI.html#a-practical-example",
    "title": "8  Explainable AI",
    "section": "",
    "text": "Bioclim variable\nMeaning\n\n\n\n\nbio1\nAnnual Mean Temperature\n\n\nbio2\nMean Diurnal Range (Mean of monthly (max temp - min temp))\n\n\nbio3\nIsothermality (BIO2/BIO7) (×100)\n\n\nbio4\nTemperature Seasonality (standard deviation ×100)\n\n\nbio5\nMax Temperature of Warmest Month\n\n\nbio6\nMin Temperature of Coldest Month\n\n\nbio7\nTemperature Annual Range (BIO5-BIO6)\n\n\nbio8\nMean Temperature of Wettest Quarter\n\n\nbio9\nMean Temperature of Driest Quarter\n\n\nbio10\nMean Temperature of Warmest Quarter\n\n\nbio11\nMean Temperature of Coldest Quarter\n\n\nbio12\nAnnual Precipitation\n\n\nbio13\nPrecipitation of Wettest Month\n\n\nbio14\nPrecipitation of Driest Month\n\n\nbio15\nPrecipitation Seasonality (Coefficient of Variation)\n\n\nbio16\nPrecipitation of Wettest Quarter\n\n\nbio17\nPrecipitation of Driest Quarter\n\n\nbio18\nPrecipitation of Warmest Quarter\n\n\nbio19\nPrecipitation of Coldest Quarter",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#feature-importance",
    "href": "D2-explainableAI.html#feature-importance",
    "title": "8  Explainable AI",
    "section": "8.2 Feature Importance",
    "text": "8.2 Feature Importance\nFeature importance should not be mistaken with the random forest variable importance though they are related. It tells us how important the individual variables are for predictions, can be calculated for all machine learning models and is based on a permutation approach (have a look at the book):\n\nimp = FeatureImp$new(predictor, loss = \"ce\")\nplot(imp)\n\nbio9 (Precipitation of the wettest Quarter) is the most important variable.",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#partial-dependencies",
    "href": "D2-explainableAI.html#partial-dependencies",
    "title": "8  Explainable AI",
    "section": "8.3 Partial Dependencies",
    "text": "8.3 Partial Dependencies\nPartial dependencies are similar to allEffects plots for normal regressions. The idea is to visualize “marginal effects” of predictors (with the “feature” argument we specify the variable we want to visualize):\n\neff = FeatureEffect$new(predictor, feature = \"bio9\", method = \"pdp\",\n                        grid.size = 30)\nplot(eff)\n\nOne disadvantage of partial dependencies is that they are sensitive to correlated predictors. Accumulated local effects can be used for accounting for correlation of predictors.",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#accumulated-local-effects",
    "href": "D2-explainableAI.html#accumulated-local-effects",
    "title": "8  Explainable AI",
    "section": "8.4 Accumulated Local Effects",
    "text": "8.4 Accumulated Local Effects\nAccumulated local effects (ALE) are basically partial dependencies plots but try to correct for correlations between predictors.\n\nale = FeatureEffect$new(predictor, feature = \"bio9\", method = \"ale\")\nale$plot()\n\nIf there is no collinearity, you shouldn’t see much difference between partial dependencies and ALE plots.",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#friedmans-h-statistic",
    "href": "D2-explainableAI.html#friedmans-h-statistic",
    "title": "8  Explainable AI",
    "section": "8.5 Friedman’s H-statistic",
    "text": "8.5 Friedman’s H-statistic\nThe H-statistic can be used to find interactions between predictors. However, again, keep in mind that the H-statistic is sensible to correlation between predictors:\n\ninteract = Interaction$new(predictor, \"bio9\",grid.size = 5L)\nplot(interact)",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#global-explainer---simplifying-the-machine-learning-model",
    "href": "D2-explainableAI.html#global-explainer---simplifying-the-machine-learning-model",
    "title": "8  Explainable AI",
    "section": "8.6 Global Explainer - Simplifying the Machine Learning Model",
    "text": "8.6 Global Explainer - Simplifying the Machine Learning Model\nAnother idea is simplifying the machine learning model with another simpler model such as a decision tree. We create predictions with the machine learning model for a lot of different input values and then we fit a decision tree on these predictions. We can then interpret the easier model.\n\nlibrary(partykit)\n\ntree = TreeSurrogate$new(predictor, maxdepth = 2)\nplot(tree$tree)",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#local-explainer---lime-explaining-single-instances-observations",
    "href": "D2-explainableAI.html#local-explainer---lime-explaining-single-instances-observations",
    "title": "8  Explainable AI",
    "section": "8.7 Local Explainer - LIME Explaining Single Instances (observations)",
    "text": "8.7 Local Explainer - LIME Explaining Single Instances (observations)\nThe global approach is to simplify the entire machine learning-black-box model via a simpler model, which is then interpretable.\nHowever, sometimes we are only interested in understanding how single predictions are generated. The LIME (Local interpretable model-agnostic explanations) approach explores the feature space around one observation and based on this locally fits a simpler model (e.g. a linear model):\n\nlime.explain = LocalModel$new(predictor, x.interest = data[1,-1])\nlime.explain$results\nplot(lime.explain)",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#local-explainer---shapley",
    "href": "D2-explainableAI.html#local-explainer---shapley",
    "title": "8  Explainable AI",
    "section": "8.8 Local Explainer - Shapley",
    "text": "8.8 Local Explainer - Shapley\nThe Shapley method computes the so called Shapley value, feature contributions for single predictions, and is based on an approach from cooperative game theory. The idea is that each feature value of the instance is a “player” in a game, where the prediction is the reward. The Shapley value tells us how to fairly distribute the reward among the features.\n\nshapley = Shapley$new(predictor, x.interest = data[1,-1])\nshapley$plot()",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#uncertainties---the-bootstrap",
    "href": "D2-explainableAI.html#uncertainties---the-bootstrap",
    "title": "8  Explainable AI",
    "section": "8.9 Uncertainties - the bootstrap",
    "text": "8.9 Uncertainties - the bootstrap\nStandard xAI method do not provide reliable uncertainties on the fitted curves. If you want uncertainties or p-values, the most common method is the bootstrap.\nIn a bootstrap, is instead of splitting up the data in test / validation, we sample from the data with replacement and fit the models repeatedly. The idea is to get an estimate about the variability we would expect if we created another dataset of the same size.\n\nk = 10 # bootstrap samples\nn = nrow(data)\nerror = rep(NA, k)\n\nfor(i in 1:k){\n  bootSample = sample.int(n, n, replace = TRUE)\n  rf = ranger(as.factor(Presence) ~ ., data = data[bootSample,], probability = TRUE)\n  error[i] = rf$prediction.error\n}\n\nhist(error, main = \"uncertainty of in-sample error\")\n\nNote that the distinction between bootstrap and validation / cross-validation is as follows:\n\nValidation / cross-validation estimates out-of-sample predictive error\nBootstrap estimates uncertainty / confidence interval on all model outputs (could be prediction and inference).",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D2-explainableAI.html#exercises",
    "href": "D2-explainableAI.html#exercises",
    "title": "8  Explainable AI",
    "section": "8.10 Exercises",
    "text": "8.10 Exercises\n\n\n\n\n\n\nxAI in cito\n\n\n\nData preparation\n\nlibrary(iml)\nlibrary(cito)\nlibrary(EcoData)\nlibrary(cito)\n\n\ndata = EcoData::elephant$occurenceData\nhead(data)\n?EcoData::elephant\n\n# we will subsample data (absences) to reduce runtime\ndata_sub = data[sample.int(nrow(data), 500),]\n\nCito includes a lot of xAI methods directly out of the box\n\nmodel = dnn(Presence~., data = data_sub, batchsize = 200L,loss = \"binomial\", verbose = FALSE, lr = 0.15, epochs = 300)\n\nTry the following commands:\n\nsummary(dnn, n_permute = 10)\nPDP(dnn)\nALE(dnn)\n\nMoreover, try to refit the model with the option bootstrap = 5. This may take a short while. Observe how the xAI options change.\n\n\n\n\nClick here to see the solution for cito\n\n\nmodel = dnn(Presence~., data = data_sub, batchsize = 200L, bootstrap = 5L, loss = \"binomial\", verbose = FALSE, lr = 0.15, epochs = 300)\n\n\nsummary(model, n_permute = 10L)\n\n\nPDP(model)\nALE(model)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse the Titanic_ml dataset and fit a random forest, dnn or a BRT using xgboost. Explore / interpret the fitted model using iml (see also the book: https://christophm.github.io/interpretable-ml-book/).\nTip:\nIf you use iml, you need to provide a proper prediction function wrapper:\n\n# random Forest (ranger), regression:\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions\n\n# random Forest (ranger), classification:\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions[,2]\n\n# xgboost:\npredict_wrapper = function(model, newdata) predict(model, as.matrix(newdata))\n\nPrepare the data\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger) # for imputation\n\n\ndata = titanic_ml\n\n# feature selection\ndata = data %&gt;% select(survived, sex, age, fare, pclass)# play around with the features\n\n# imputation - remove response variable!\nhead(data)\ndata_imputed = data\ndata_imputed[,-1] = missRanger(data_imputed[,-1])\nsummary(data_imputed)\n\ndata_imputed = data_imputed %&gt;% \n  mutate(age = (age - mean(age))/sd(age), fare = (fare - mean(fare))/sd(fare),\n         sex = as.integer(sex), pclass = as.integer(pclass))\n\ndata_obs = data_imputed[!is.na(data_imputed$survived), ]\ndata_new = data_imputed[is.na(data_imputed$survived), ]\n\n\n\n\n\nClick here to see the solution for RF\n\n\nlibrary(ranger)\nlibrary(\"iml\")\nset.seed(1234)\ndata_obs$survived = as.factor(data_obs$survived)\n\nrf = ranger(survived ~ ., data = data_obs, importance = \"impurity\", probability = TRUE)\n\n# For submission:\n#write.csv(data.frame(y=predict(rf, data_new)$predictions[,2]), file = \"wine_RF.csv\")\n\n# Standard depiction of importance:\nranger::importance(rf)\n\n# Setup wrapper\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions[,2]\n\n\n# IML:\npredictor = Predictor$new(\n    rf, data = data_obs[,which(names(data_obs) != \"survived\")], y = as.integer(data_obs$survived)-1,\n    predict.function = predict_wrapper\n    )\n\n# Mind: This is stochastical!\nimportance = FeatureImp$new(predictor, loss = \"logLoss\")\n\nplot(importance)\n\n# Comparison between standard importance and IML importance:\nimportanceRf = names(rf$variable.importance)[order(rf$variable.importance, decreasing = TRUE)]\nimportanceIML = importance$results[1]\ncomparison = cbind(importanceIML, importanceRf)\ncolnames(comparison) = c(\"IML\", \"RF\")\nas.matrix(comparison)\n\nMind that feature importance, and the random forest’s variable importance are related but not equal! Variable importance is a measure for determining importance while creating the forest (i.e. for fitting). Feature importance is a measure for how important a variable is for prediction.\nMaybe you want to see other explanation methods as well. Surely you can use the other techniques of this section on your own.\n\n\n\nClick here to see the solution for xgboost\n\n\nlibrary(xgboost)\nlibrary(\"iml\")\nset.seed(1234)\n\n\ndata_xg = xgb.DMatrix(\n  data = as.matrix(data_obs[,which(names(data_obs) != \"survived\")]),\n  label = as.integer(data_obs$survived)-1\n)\nbrt = xgboost(data_xg, nrounds = 24, objective = \"reg:logistic\")\n\n\n# For submission:\n#write.csv(round(predict(brt, data_new)), file = \"wine_RF.csv\")\n\n# Standard depiction of importance:\nxgboost::xgb.importance(model = brt)\n\n# Setup wrapper\npredict_wrapper = function(model, newdata) predict(model, as.matrix(newdata))\n\n\n# IML:\npredictor = Predictor$new(\n    brt, data = data_obs[,which(names(data_obs) != \"survived\")], y = as.integer(data_obs$survived)-1,\n    predict.function = predict_wrapper\n    )\n\n# Mind: This is stochastical!\nimportance = FeatureImp$new(predictor, loss = \"logLoss\")\n\nplot(importance)\n\n\n\n\nClick here to see the solution for cito\n\n\nlibrary(cito)\ndata_obs$survived = as.integer(data_obs$survived) - 1\nnn = dnn(survived~., data = data_obs, loss = \"binomial\", lr= 0.03, epochs = 300)\n\nsummary(nn)",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Explainable AI</span>"
    ]
  },
  {
    "objectID": "D1-causality.html",
    "href": "D1-causality.html",
    "title": "9  Causal Inference and Machine Learning",
    "section": "",
    "text": "9.1 Causal Inference on Static Data\nxAI aims at explaining how predictions are being made. In general, xAI != causality. xAI methods measure which variables are used for predictions by the algorithm, or how far variables improve predictions. The important point to note here: If a variable causes something, we could also expect that it helps predicting the very thing. The opposite, however, is not generally true - very often it is possible that a variable that doesn’t cause anything can predict something.\nIn statistics courses (in particular our course: Advanced Biostatistics), we discuss the issue of causality at full length. Here, we don’t want to go into the details, but again, you should in general resist to interpret indicators of importance in xAI as causal effects. They tell you something about what’s going on in the algorithm, not about what’s going on in reality.\nMethods for causal inference depend on whether we have dynamic or static data. The latter is the more common case. With static data, the problem is confounding. If you have several correlated predictors, you can get spurious correlations between a given predictor and the response, although there is no causal effect in general.\nMultiple regression and few other methods are able to correct for other predictors and thus isolate the causal effect. The same is not necessarily true for machine learning algorithms and xAI methods. This is not a bug, but a feature - for making good predictions, it is often no problem, but rather an advantage to also use non-causal predictors.\nHere an example for the indicators of variable importance in the random forest algorithm. The purpose of this script is to show that random forest variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are separable and would be correctly separated by an lm / ANOVA.\nWe first simulate a data set with 2 predictors that are strongly correlated, but only one of them has an effect on the response.\nlibrary(randomForest)\nset.seed(123)\n\n# Simulation parameters.\nn = 1000\ncol = 0.7\n\n# Create collinear predictors.\nx1 = runif(n)\nx2 = col * x1 + (1-col) * runif(n)\n\n# Response is only influenced by x1.\ny = x1 + rnorm(n)\nlm / anova correctly identify \\(x1\\) as causal variable.\nsummary(lm(y ~ x1 + x2))\nFit random forest and show variable importance:\nset.seed(123)\n\nfit = randomForest(y ~ x1 + x2, importance = TRUE)\nvarImpPlot(fit)\nVariable importance is now split nearly evenly.\nTask: understand why this is - remember:",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference and Machine Learning</span>"
    ]
  },
  {
    "objectID": "D1-causality.html#causalInference",
    "href": "D1-causality.html#causalInference",
    "title": "9  Causal Inference and Machine Learning",
    "section": "",
    "text": "How the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built.\nRemember that as \\(x1 \\propto x2\\), we can use \\(x2\\) as a replacement for \\(x1\\).\nRemember that the variable importance measures the average contributions of the different variables in the trees of the forest.",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference and Machine Learning</span>"
    ]
  },
  {
    "objectID": "D1-causality.html#structural-equation-models",
    "href": "D1-causality.html#structural-equation-models",
    "title": "9  Causal Inference and Machine Learning",
    "section": "9.2 Structural Equation Models",
    "text": "9.2 Structural Equation Models\nIf causal relationships get more complicated, it will not be possible to adjust correctly with a simple lm. In this case, in statistics, we will usually use structural equation models (SEMs). Structural equation models are designed to estimate entire causal diagrams. There are two main SEM packages in R: For anything that is non-normal, you will currently have to estimate the directed acyclic graph (that depicts causal relations) piece-wise with CRAN package piecewiseSEM. Example for a vegetation data set:\n\nlibrary(piecewiseSEM)\n\nmod = psem(\n lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover,\n    data = keeley),\n lm(firesev ~ elev + age + cover, data = keeley),\n lm(cover ~ age + elev + hetero + abiotic, data = keeley)\n)\nsummary(mod)\nplot(mod)\n\nFor linear structural equation models, we can estimate the entire directed acyclic graph at once. This also allows having unobserved variables in the directed acyclic graph. One of the most popular packages for this is lavaan.\n\nlibrary(lavaan)\n\nmod = \"\n rich ~ distance + elev + abiotic + age + hetero + firesev + cover\n firesev ~ elev + age + cover\n cover ~ age + elev + abiotic\n\"\nfit = sem(mod, data = keeley)\nsummary(fit)\n\nThe default plot options are not so nice as before.\n\nlibrary(lavaanPlot)\n\nlavaanPlot(model = fit)\n\nAnother plotting option is using semPlot.\n\nlibrary(semPlot)\n\nsemPaths(fit)",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference and Machine Learning</span>"
    ]
  },
  {
    "objectID": "D1-causality.html#automatic-causal-discovery",
    "href": "D1-causality.html#automatic-causal-discovery",
    "title": "9  Causal Inference and Machine Learning",
    "section": "9.3 Automatic Causal Discovery",
    "text": "9.3 Automatic Causal Discovery\nBut how to get the causal graph? In statistics, it is common to “guess” it and afterwards do residual checks, in the same way as we guess the structure of a regression. For more complicated problems, however, this is unsatisfying. Some groups therefore work on so-called causal discovery algorithms, i.e. algorithms that automatically generate causal graphs from data. One of the most classic algorithms of this sort is the PC algorithm. Here an example using the pcalg package:\n\nlibrary(pcalg)\n\nLoading the data:\n\ndata(\"gmG\", package = \"pcalg\") # Loads data sets gmG and gmG8.\nsuffStat = list(C = cor(gmG8$x), n = nrow(gmG8$x))\nvarNames = gmG8$g@nodes\n\nFirst, the skeleton algorithm creates a basic graph without connections (a skeleton of the graph).\n\nskel.gmG8 = skeleton(suffStat, indepTest = gaussCItest,\nlabels = varNames, alpha = 0.01)\nRgraphviz::plot(skel.gmG8@graph)\n\nWhat is missing here is the direction of the errors. The PC algorithm now makes tests for conditional independence, which allows fixing a part (but typically not all) of the directions of the causal arrows.\n\npc.gmG8 = pc(suffStat, indepTest = gaussCItest,\nlabels = varNames, alpha = 0.01)\nRgraphviz::plot(pc.gmG8@graph )",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference and Machine Learning</span>"
    ]
  },
  {
    "objectID": "D1-causality.html#causal-inference-on-dynamic-data",
    "href": "D1-causality.html#causal-inference-on-dynamic-data",
    "title": "9  Causal Inference and Machine Learning",
    "section": "9.4 Causal Inference on Dynamic Data",
    "text": "9.4 Causal Inference on Dynamic Data\nWhen working with dynamic data, we can use an additional piece of information - the cause usually precedes the effect, which means that we can test for a time-lag between cause and effect to determine the direction of causality. This way of testing for causality is known as Granger causality, or Granger methods. Here an example:\n\nlibrary(lmtest)\n\n## What came first: the chicken or the egg?\ndata(ChickEgg)\ngrangertest(egg ~ chicken, order = 3, data = ChickEgg)\ngrangertest(chicken ~ egg, order = 3, data = ChickEgg)",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference and Machine Learning</span>"
    ]
  },
  {
    "objectID": "D1-causality.html#outlook-for-machine-learning",
    "href": "D1-causality.html#outlook-for-machine-learning",
    "title": "9  Causal Inference and Machine Learning",
    "section": "9.5 Outlook for Machine Learning",
    "text": "9.5 Outlook for Machine Learning\nAs we have seen, there are already a few methods / algorithms for discovering causality from large data sets, but the systematic transfer of these concepts to machine learning, in particular deep learning, is still at its infancy. At the moment, this field is actively researched and changes extremely fast, so we recommend using Google to see what is currently going on. Particular in business and industry, there is a large interest in learning about causal effect from large data sets. In our opinion, a great topic for young scientists to specialize on.",
    "crumbs": [
      "Explainable AI and causal ML",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Causal Inference and Machine Learning</span>"
    ]
  },
  {
    "objectID": "Appendix-Datasets.html",
    "href": "Appendix-Datasets.html",
    "title": "Appendix A — Datasets",
    "section": "",
    "text": "A.1 Machine learning pipeline / workflow\nYou can download the data sets we use in the course here (ignore browser warnings) or by installing the EcoData package:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "Appendix-Datasets.html#machine-learning-pipeline-workflow",
    "href": "Appendix-Datasets.html#machine-learning-pipeline-workflow",
    "title": "Appendix A — Datasets",
    "section": "",
    "text": "Machine Learning pipeline",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "Appendix-Datasets.html#titanic",
    "href": "Appendix-Datasets.html#titanic",
    "title": "Appendix A — Datasets",
    "section": "A.2 Titanic",
    "text": "A.2 Titanic\nThe data set is a collection of Titanic passengers with information about their age, class, sex, and their survival status. The competition is simple here: Train a machine learning model and predict the survival probability.\nThe Titanic data set is very well explored and serves as a stepping stone in many machine learning careers. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “survived”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(titanic_ml)\ntitanic = titanic_ml\nsummary(titanic)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\ntitanic_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest)\ntitanic_imputed = missRanger::missRanger(data = titanic_imputed %&gt;%\n                                           select(-survived), verbose = 0)\ntitanic_imputed$survived = titanic$survived\n\n\nSplit into training and test set:\n\n\ntrain = titanic_imputed[!is.na(titanic$survived), ]\ntest = titanic_imputed[is.na(titanic$survived), ]\n\n\nTrain model:\n\n\nmodel = glm(survived~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, data = test, type = \"response\")\nhead(preds)\n\n       561        321       1177       1098       1252       1170 \n0.79095923 0.30597519 0.01400693 0.12310859 0.14099292 0.11768284 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")\n\nAnd submit the csv on http://rhsbio7.uni-regensburg.de:8500.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "Appendix-Datasets.html#sec-plantpoll",
    "href": "Appendix-Datasets.html#sec-plantpoll",
    "title": "Appendix A — Datasets",
    "section": "A.3 Plant-pollinator Database",
    "text": "A.3 Plant-pollinator Database\nThe plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look here.\n\n\n\n\n\n\n\n\n\nResponse variable: “interaction”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n\n                   crop                       insect          type          \n Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n Daucus_carota       :  256   Andrena_gravida    :   80                     \n Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n (Other)             :18944   (Other)            :20000                     \n    season             diameter        corolla             colour         \n Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n Class :character   1st Qu.:  5.00   Class :character   Class :character  \n Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n                    Mean   : 27.03                                        \n                    3rd Qu.: 25.00                                        \n                    Max.   :150.00                                        \n                    NA's   :9472                                          \n    nectar            b.system         s.pollination      inflorescence     \n Length:20480       Length:20480       Length:20480       Length:20480      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  composite            guild               tongue            body      \n Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n                                       Mean   : 8.104   Mean   :10.66  \n                                       3rd Qu.:10.500   3rd Qu.:13.00  \n                                       Max.   :26.400   Max.   :25.00  \n                                       NA's   :17040    NA's   :6160   \n  sociality           feeding          interaction \n Length:20480       Length:20480       0   :14095  \n Class :character   Class :character   1   :  595  \n Mode  :character   Mode  :character   NA's: 5790  \n                                                   \n                                                   \n                                                   \n                                                   \n\n\n\nImpute missing values (not our response variable!) We will select only a few predictors here (you can work with all predictors of course).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nplant_poll_imputed = plant_poll %&gt;% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction)\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %&gt;%\n                                              select(-interaction), verbose = 0)\nplant_poll_imputed$interaction = plant_poll$interaction\n\n\nSplit into training and test set:\n\n\ntrain = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]\ntest = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ]\n\n\nTrain model:\n\n\nmodel = glm(interaction~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, newdata = test, type = \"response\")\nhead(preds)\n\n         1          2          3          4          5          6 \n0.02942746 0.05063489 0.03780247 0.03780247 0.02651142 0.04130643 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "Appendix-Datasets.html#wine",
    "href": "Appendix-Datasets.html#wine",
    "title": "Appendix A — Datasets",
    "section": "A.4 Wine",
    "text": "A.4 Wine\nThe data set is a collection of wines of different quality. The aim is to predict the quality of the wine based on physiochemical predictors.\nFor inspiration and data exploration notebooks, check out this kaggle competition. For instance, check out this very nice notebook which removes a few problems from the data.\nResponse variable: “quality”\nWe could theoretically use a regression model for this task but we will stick with a classification model.\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(wine)\nsummary(wine)\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900  \n 1st Qu.: 7.100   1st Qu.:0.3900   1st Qu.:0.0900   1st Qu.: 1.900  \n Median : 7.900   Median :0.5200   Median :0.2600   Median : 2.200  \n Mean   : 8.335   Mean   :0.5284   Mean   :0.2705   Mean   : 2.533  \n 3rd Qu.: 9.300   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600  \n Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500  \n NA's   :70       NA's   :48       NA's   :41       NA's   :60      \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  \n Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  \n Mean   :0.08747   Mean   :15.83       Mean   : 46.23       Mean   :0.9968  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9979  \n Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  \n NA's   :37        NA's   :78          NA's   :78           NA's   :78      \n       pH          sulphates         alcohol         quality     \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n 1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000  \n Mean   :3.311   Mean   :0.6572   Mean   :10.42   Mean   :5.596  \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n NA's   :25      NA's   :51                       NA's   :905    \n\n\n\nImpute missing values (not our response variable!).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nwine_imputed = missRanger::missRanger(data = wine %&gt;% select(-quality), verbose = 0)\nwine_imputed$quality = wine$quality\n\n\nSplit into training and test set:\n\n\ntrain = wine_imputed[!is.na(wine$quality), ]\ntest = wine_imputed[is.na(wine$quality), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(quality~., data = train, classification = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions\nhead(preds)\n\n[1] 6 5 5 7 6 6\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "Appendix-Datasets.html#nasa",
    "href": "Appendix-Datasets.html#nasa",
    "title": "Appendix A — Datasets",
    "section": "A.5 Nasa",
    "text": "A.5 Nasa\nA collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “Hazardous”\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(nasa)\nsummary(nasa)\n\n Neo.Reference.ID       Name         Absolute.Magnitude Est.Dia.in.KM.min.\n Min.   :2000433   Min.   :2000433   Min.   :11.16      Min.   : 0.00101  \n 1st Qu.:3102682   1st Qu.:3102683   1st Qu.:20.10      1st Qu.: 0.03346  \n Median :3514800   Median :3514800   Median :21.90      Median : 0.11080  \n Mean   :3272675   Mean   :3273113   Mean   :22.27      Mean   : 0.20523  \n 3rd Qu.:3690987   3rd Qu.:3690385   3rd Qu.:24.50      3rd Qu.: 0.25384  \n Max.   :3781897   Max.   :3781897   Max.   :32.10      Max.   :15.57955  \n NA's   :53        NA's   :57        NA's   :36         NA's   :60        \n Est.Dia.in.KM.max. Est.Dia.in.M.min.   Est.Dia.in.M.max. \n Min.   : 0.00226   Min.   :    1.011   Min.   :    2.26  \n 1st Qu.: 0.07482   1st Qu.:   33.462   1st Qu.:   74.82  \n Median : 0.24777   Median :  110.804   Median :  247.77  \n Mean   : 0.45754   Mean   :  204.649   Mean   :  458.45  \n 3rd Qu.: 0.56760   3rd Qu.:  253.837   3rd Qu.:  567.60  \n Max.   :34.83694   Max.   :15579.552   Max.   :34836.94  \n NA's   :23         NA's   :29          NA's   :46        \n Est.Dia.in.Miles.min. Est.Dia.in.Miles.max. Est.Dia.in.Feet.min.\n Min.   :0.00063       Min.   : 0.00140      Min.   :    3.32    \n 1st Qu.:0.02079       1st Qu.: 0.04649      1st Qu.:  109.78    \n Median :0.06885       Median : 0.15395      Median :  363.53    \n Mean   :0.12734       Mean   : 0.28486      Mean   :  670.44    \n 3rd Qu.:0.15773       3rd Qu.: 0.35269      3rd Qu.:  832.80    \n Max.   :9.68068       Max.   :21.64666      Max.   :51114.02    \n NA's   :42            NA's   :50            NA's   :21          \n Est.Dia.in.Feet.max. Close.Approach.Date Epoch.Date.Close.Approach\n Min.   :     7.41    2016-07-22:  18     Min.   :7.889e+11        \n 1st Qu.:   245.49    2015-01-15:  17     1st Qu.:1.016e+12        \n Median :   812.88    2015-02-15:  16     Median :1.203e+12        \n Mean   :  1500.77    2007-11-08:  15     Mean   :1.180e+12        \n 3rd Qu.:  1862.19    2012-01-15:  15     3rd Qu.:1.356e+12        \n Max.   :114294.42    (Other)   :4577     Max.   :1.473e+12        \n NA's   :46           NA's      :  29     NA's   :43               \n Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr Miles.per.hour   \n Min.   : 0.3355              Min.   :  1208              Min.   :  750.5  \n 1st Qu.: 8.4497              1st Qu.: 30399              1st Qu.:18846.7  \n Median :12.9370              Median : 46532              Median :28893.7  \n Mean   :13.9848              Mean   : 50298              Mean   :31228.0  \n 3rd Qu.:18.0774              3rd Qu.: 65068              3rd Qu.:40436.9  \n Max.   :44.6337              Max.   :160681              Max.   :99841.2  \n NA's   :27                   NA's   :28                  NA's   :38       \n Miss.Dist..Astronomical. Miss.Dist..lunar.   Miss.Dist..kilometers.\n Min.   :0.00018          Min.   :  0.06919   Min.   :   26610      \n 1st Qu.:0.13341          1st Qu.: 51.89874   1st Qu.:19964907      \n Median :0.26497          Median :103.19415   Median :39685408      \n Mean   :0.25690          Mean   : 99.91366   Mean   :38436154      \n 3rd Qu.:0.38506          3rd Qu.:149.59244   3rd Qu.:57540318      \n Max.   :0.49988          Max.   :194.45491   Max.   :74781600      \n NA's   :60               NA's   :30          NA's   :56            \n Miss.Dist..miles.  Orbiting.Body    Orbit.ID     \n Min.   :   16535   Earth:4665    Min.   :  1.00  \n 1st Qu.:12454813   NA's :  22    1st Qu.:  9.00  \n Median :24662435                 Median : 16.00  \n Mean   :23885560                 Mean   : 28.34  \n 3rd Qu.:35714721                 3rd Qu.: 31.00  \n Max.   :46467132                 Max.   :611.00  \n NA's   :27                       NA's   :33      \n        Orbit.Determination.Date Orbit.Uncertainity Minimum.Orbit.Intersection\n 2017-06-21 06:17:20:   9        Min.   :0.000      Min.   :0.00000           \n 2017-04-06 08:57:13:   8        1st Qu.:0.000      1st Qu.:0.01435           \n 2017-04-06 09:24:24:   8        Median :3.000      Median :0.04653           \n 2017-04-06 08:24:13:   7        Mean   :3.521      Mean   :0.08191           \n 2017-04-06 08:26:19:   7        3rd Qu.:6.000      3rd Qu.:0.12150           \n (Other)            :4622        Max.   :9.000      Max.   :0.47789           \n NA's               :  26        NA's   :49         NA's   :137               \n Jupiter.Tisserand.Invariant Epoch.Osculation   Eccentricity    \n Min.   :2.196               Min.   :2450164   Min.   :0.00752  \n 1st Qu.:4.047               1st Qu.:2458000   1st Qu.:0.24086  \n Median :5.071               Median :2458000   Median :0.37251  \n Mean   :5.056               Mean   :2457723   Mean   :0.38267  \n 3rd Qu.:6.017               3rd Qu.:2458000   3rd Qu.:0.51256  \n Max.   :9.025               Max.   :2458020   Max.   :0.96026  \n NA's   :56                  NA's   :60        NA's   :39       \n Semi.Major.Axis   Inclination       Asc.Node.Longitude Orbital.Period  \n Min.   :0.6159   Min.   : 0.01451   Min.   :  0.0019   Min.   : 176.6  \n 1st Qu.:1.0012   1st Qu.: 4.93290   1st Qu.: 83.1849   1st Qu.: 365.9  \n Median :1.2422   Median :10.27694   Median :172.6347   Median : 504.9  \n Mean   :1.4009   Mean   :13.36159   Mean   :172.1717   Mean   : 635.5  \n 3rd Qu.:1.6782   3rd Qu.:19.47848   3rd Qu.:254.8804   3rd Qu.: 793.1  \n Max.   :5.0720   Max.   :75.40667   Max.   :359.9059   Max.   :4172.2  \n NA's   :53       NA's   :42         NA's   :60         NA's   :46      \n Perihelion.Distance Perihelion.Arg     Aphelion.Dist    Perihelion.Time  \n Min.   :0.08074     Min.   :  0.0069   Min.   :0.8038   Min.   :2450100  \n 1st Qu.:0.63038     1st Qu.: 95.6430   1st Qu.:1.2661   1st Qu.:2457815  \n Median :0.83288     Median :189.7729   Median :1.6182   Median :2457972  \n Mean   :0.81316     Mean   :184.0185   Mean   :1.9864   Mean   :2457726  \n 3rd Qu.:0.99718     3rd Qu.:271.9535   3rd Qu.:2.4497   3rd Qu.:2458108  \n Max.   :1.29983     Max.   :359.9931   Max.   :8.9839   Max.   :2458839  \n NA's   :22          NA's   :48         NA's   :38       NA's   :59       \n  Mean.Anomaly       Mean.Motion       Equinox       Hazardous    \n Min.   :  0.0032   Min.   :0.08628   J2000:4663   Min.   :0.000  \n 1st Qu.: 87.0069   1st Qu.:0.45147   NA's :  24   1st Qu.:0.000  \n Median :186.0219   Median :0.71137                Median :0.000  \n Mean   :181.2882   Mean   :0.73732                Mean   :0.176  \n 3rd Qu.:276.6418   3rd Qu.:0.98379                3rd Qu.:0.000  \n Max.   :359.9180   Max.   :2.03900                Max.   :1.000  \n NA's   :40         NA's   :48                     NA's   :4187   \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nnasa_imputed = missRanger::missRanger(data = nasa %&gt;% select(-Hazardous),\n                                      maxiter = 1, num.trees = 5L, verbose = 0)\nnasa_imputed$Hazardous = nasa$Hazardous\n\n\nSplit into training and test set:\n\n\ntrain = nasa_imputed[!is.na(nasa$Hazardous), ]\ntest = nasa_imputed[is.na(nasa$Hazardous), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(Hazardous~., data = train, classification = TRUE,\n            probability = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions[,2]\nhead(preds)\n\n[1] 0.6348055556 0.7525960317 0.0008444444 0.7733373016 0.1404333333\n[6] 0.1509190476\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "Appendix-Datasets.html#flower",
    "href": "Appendix-Datasets.html#flower",
    "title": "Appendix A — Datasets",
    "section": "A.6 Flower",
    "text": "A.6 Flower\nA collection of over 4000 flower images of 5 plant species. The data set is from kaggle but we downsampled the images from \\(320*240\\) to \\(80*80\\) pixels. You can a) download the data set here or b) get it via the EcoData package.\nNotes:\n\nCheck out convolutional neural network notebooks on kaggle (they are often written in Python but you can still copy the architectures), e.g. this one.\nLast year’s winners have used a transfer learning approach (they achieved around 70% accuracy), check out this notebook, see also the section about transfer learning @ref(transfer).\n\nResponse variable: “Plant species”\n\nLoad data set:\n\n\nlibrary(tensorflow)\nlibrary(keras3)\n\ntrain = EcoData::dataset_flower()$train/255\ntest = EcoData::dataset_flower()$test/255\nlabels = EcoData::dataset_flower()$labels\n\nLet’s visualize a flower:\n\ntrain[100,,,] %&gt;%\n  image_to_array() %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\n\n\n\n\n\n\nBuild and train model:\n\n\nmodel = keras_model_sequential()\nmodel %&gt;% \n  layer_conv_2d(filters = 4L, kernel_size = 2L,\n                input_shape = list(80L, 80L, 3L)) %&gt;% \n  layer_max_pooling_2d() %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 5L, activation = \"softmax\")\n\n### Model fitting ###\n\nmodel %&gt;% \n  compile(loss = loss_categorical_crossentropy, \n          optimizer = optimizer_adamax(learning_rate = 0.01))\n\nmodel %&gt;% \n  fit(x = train, y = keras::k_one_hot(labels, 5L))\n\n\nPredictions:\n\n\n# Prediction on training data:\npred = apply(model %&gt;% predict(train), 1, which.max)\nMetrics::accuracy(pred - 1L, labels)\ntable(pred)\n\n# Prediction for the submission server:\npred = model %&gt;% predict(test) %&gt;% apply(1, which.max) - 1L\ntable(pred)\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = pred), file = \"cnn.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "C1-TensorFlow.html",
    "href": "C1-TensorFlow.html",
    "title": "Appendix B — Introduction to TensorFlow and Keras",
    "section": "",
    "text": "B.1 Introduction to TensorFlow\nOne of the most commonly used frameworks for machine learning is TensorFlow. TensorFlow is an open source linear algebra library with focus on neural networks, published by Google in 2015. TensorFlow supports several interesting features, in particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization.\nThese advantages are nicely explained in the following video:\nTo sum up the most important points of the video:\nAll operations in TensorFlow are written in C++ and are highly optimized. But don’t worry, we don’t have to use C++ to use TensorFlow because there are several bindings for other languages. TensorFlow officially supports a Python API, but meanwhile there are several community carried APIs for other languages:\nIn this course we will use TensorFlow with the https://tensorflow.rstudio.com/ binding, that was developed and published 2017 by the RStudio team. First, they developed an R package (reticulate) for calling Python in R. Actually, we are using the Python TensorFlow module in R (more about this later).\nTensorFlow offers different levels of API. We could implement a neural network completely by ourselves or we could use Keras which is provided as a submodule by TensorFlow. Keras is a powerful module for building and training neural networks. It allows us building and training neural networks in a few lines of codes. Since the end of 2018, Keras and TensorFlow are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras for neural networks but also how we can use the TensorFlow’s automatic differenation for using complex objective functions.\nUseful links:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to TensorFlow and Keras</span>"
    ]
  },
  {
    "objectID": "C1-TensorFlow.html#introduction-to-tensorflow",
    "href": "C1-TensorFlow.html#introduction-to-tensorflow",
    "title": "Appendix B — Introduction to TensorFlow and Keras",
    "section": "",
    "text": "TensorFlow is a math library which is highly optimized for neural networks.\nIf a GPU is available, computations can be easily run on the GPU but even on a CPU TensorFlow is still very fast.\nThe “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for NVIDIA GPUs).\nThe interface (the part of TensorFlow we use) is written in Python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend.\n\n\n\nR\nGo\nRust\nSwift\nJavaScript\n\n\n\n\n\nTensorFlow documentation (This is for the Python API, but just replace the “.” with “$”.)\nRstudio TensorFlow website\n\n\nB.1.1 Data Containers\nTensorFlow has two data containers (structures):\n\nconstant (tf$constant): Creates a constant (immutable) value in the computation graph.\nvariable (tf$Variable): Creates a mutable value in the computation graph (used as parameter/weight in models).\n\nTo get started with TensorFlow, we have to load the library and check if the installation worked.\n\nlibrary(tensorflow)\nlibrary(keras3)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n\n[1] TRUE\n\nimmutable = tf$constant(5.0)\nmutable = tf$Variable(5.0)\n\nDon’t worry about weird messages (they will only appear once at the start of the session).\n\n\nB.1.2 Basic Operations\nWe now can define the variables and do some math with them:\n\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n\ntf.Tensor(5.0, shape=(), dtype=float32)\n\nprint(b)\n\ntf.Tensor(10.0, shape=(), dtype=float32)\n\nc = tf$add(a, b)\nprint(c)\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\n\nNormal R methods such as print() are provided by the R package “tensorflow”.\nThe TensorFlow library (created by the RStudio team) built R methods for all common operations:\n\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\n(a+b)\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\n\nTheir operators also automatically transform R numbers into constant tensors when attempting to add a tensor to an R number:\n\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nTensorFlow containers are objects, what means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object). For instance, there is a method to transform the tensor object back to an R object:\n\nclass(d)\n\n[1] \"tensorflow.tensor\"                               \n[2] \"tensorflow.python.framework.ops.EagerTensor\"     \n[3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n[4] \"tensorflow.python.framework.tensor.Tensor\"       \n[5] \"tensorflow.python.types.internal.NativeObject\"   \n[6] \"tensorflow.python.types.core.Symbol\"             \n[7] \"tensorflow.python.types.core.Value\"              \n[8] \"tensorflow.python.types.core.Tensor\"             \n[9] \"python.builtin.object\"                           \n\nclass(d$numpy())\n\n[1] \"numeric\"\n\nclass(as.matrix(d))\n\n[1] \"matrix\" \"array\" \n\n\n\n\nB.1.3 Data Types\nR uses dynamic typing, what means you can assign a number, character, function or whatever to a variable and the the type is automatically inferred. In other languages you have to state the type explicitly, e.g. in C:\n\nint a = 5;\nfloat a = 5.0;\nchar a = \"a\";\n\nWhile TensorFlow tries to infer the type dynamically, you must often state it explicitly. Common important types:\n\nfloat32 (floating point number with 32 bits, “single precision”)\nfloat64 (floating point number with 64 bits, “double precision”)\nint8 (integer with 8 bits)\n\nThe reason why TensorFlow is so explicit about types is that many GPUs (e.g. the NVIDIA GeForces) can handle only up to 32 bit numbers! (you do not need high precision in graphical modeling)\nBut let us see in practice what we have to do with these types and how to specifcy them:\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\n\nSo what went wrong here? We tried to divide a float32 by a float64 number, but we can only divide numbers of the same type!\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\n\nWe can also specify the type of the object by providing an object e.g. tf$float64.\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\n\nIn TensorFlow, arguments often require exact/explicit data types: TensorFlow often expects integers as arguments. In R however an integer is normally saved as float. Thus, we have to use an “L” after an integer to tell the R interpreter that it should be treated as an integer:\n\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()\n\nSkipping the “L” is one of the most common errors when using R-TensorFlow!\n\n\nB.1.4 Exercises\n\n\n\n\n\n\nQuestion: TensorFlow Operations\n\n\n\nTo run TensorFlow from R, note that you can access the different mathematical operations in TensorFlow via tf$…, e.g. there is a tf$math$… for all common math operations or the tf$linalg$… for different linear algebra operations. Tip: type tf$ and then hit the tab key to list all available options (sometimes you have to do this directly in the console).\nAn example: How to get the maximum value of a vector?\nAn example: How to get the maximum value of a vector?\n\nlibrary(tensorflow)\nlibrary(keras3)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\n\n[1] 100\n\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\ntf.Tensor(100, shape=(), dtype=int32)\n\nmax(y)  # Float!\n\n[1] 100\n\ntf$math$reduce_max(y) # Float!\n\ntf.Tensor(100.0, shape=(), dtype=float32)\n\n\nRewrite the following expressions (a to g) in TensorFlow:\n\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n\n[1] 1\n\n# b)\nmean(x)\n\n[1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n\n[1] 1\n\n# d) \nwhich.min(x)\n\n[1] 100\n\n# e) Tip: Use Google! \norder(x)\n\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n\n[1] TRUE\n\n\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\nRegistered S3 methods overwritten by 'keras':\n  method                               from  \n  as.data.frame.keras_training_history keras3\n  plot.keras_training_history          keras3\n  print.keras_training_history         keras3\n  r_to_py.R6ClassGenerator             keras3\n\n\n\nAttaching package: 'keras'\n\n\nThe following objects are masked from 'package:keras3':\n\n    %&lt;-active%, %py_class%, activation_elu, activation_exponential,\n    activation_gelu, activation_hard_sigmoid, activation_linear,\n    activation_relu, activation_selu, activation_sigmoid,\n    activation_softmax, activation_softplus, activation_softsign,\n    activation_tanh, adapt, application_densenet121,\n    application_densenet169, application_densenet201,\n    application_efficientnet_b0, application_efficientnet_b1,\n    application_efficientnet_b2, application_efficientnet_b3,\n    application_efficientnet_b4, application_efficientnet_b5,\n    application_efficientnet_b6, application_efficientnet_b7,\n    application_inception_resnet_v2, application_inception_v3,\n    application_mobilenet, application_mobilenet_v2,\n    application_mobilenet_v3_large, application_mobilenet_v3_small,\n    application_nasnetlarge, application_nasnetmobile,\n    application_resnet101, application_resnet101_v2,\n    application_resnet152, application_resnet152_v2,\n    application_resnet50, application_resnet50_v2, application_vgg16,\n    application_vgg19, application_xception, bidirectional,\n    callback_backup_and_restore, callback_csv_logger,\n    callback_early_stopping, callback_lambda,\n    callback_learning_rate_scheduler, callback_model_checkpoint,\n    callback_reduce_lr_on_plateau, callback_remote_monitor,\n    callback_tensorboard, clone_model, constraint_maxnorm,\n    constraint_minmaxnorm, constraint_nonneg, constraint_unitnorm,\n    count_params, custom_metric, dataset_boston_housing,\n    dataset_cifar10, dataset_cifar100, dataset_fashion_mnist,\n    dataset_imdb, dataset_imdb_word_index, dataset_mnist,\n    dataset_reuters, dataset_reuters_word_index, freeze_weights,\n    from_config, get_config, get_file, get_layer, get_vocabulary,\n    get_weights, image_array_save, image_dataset_from_directory,\n    image_load, image_to_array, imagenet_decode_predictions,\n    imagenet_preprocess_input, initializer_constant,\n    initializer_glorot_normal, initializer_glorot_uniform,\n    initializer_he_normal, initializer_he_uniform,\n    initializer_identity, initializer_lecun_normal,\n    initializer_lecun_uniform, initializer_ones,\n    initializer_orthogonal, initializer_random_normal,\n    initializer_random_uniform, initializer_truncated_normal,\n    initializer_variance_scaling, initializer_zeros, install_keras,\n    keras, keras_model, keras_model_sequential, Layer,\n    layer_activation, layer_activation_elu,\n    layer_activation_leaky_relu, layer_activation_parametric_relu,\n    layer_activation_relu, layer_activation_softmax,\n    layer_activity_regularization, layer_add, layer_additive_attention,\n    layer_alpha_dropout, layer_attention, layer_average,\n    layer_average_pooling_1d, layer_average_pooling_2d,\n    layer_average_pooling_3d, layer_batch_normalization,\n    layer_category_encoding, layer_center_crop, layer_concatenate,\n    layer_conv_1d, layer_conv_1d_transpose, layer_conv_2d,\n    layer_conv_2d_transpose, layer_conv_3d, layer_conv_3d_transpose,\n    layer_conv_lstm_1d, layer_conv_lstm_2d, layer_conv_lstm_3d,\n    layer_cropping_1d, layer_cropping_2d, layer_cropping_3d,\n    layer_dense, layer_depthwise_conv_1d, layer_depthwise_conv_2d,\n    layer_discretization, layer_dot, layer_dropout, layer_embedding,\n    layer_flatten, layer_gaussian_dropout, layer_gaussian_noise,\n    layer_global_average_pooling_1d, layer_global_average_pooling_2d,\n    layer_global_average_pooling_3d, layer_global_max_pooling_1d,\n    layer_global_max_pooling_2d, layer_global_max_pooling_3d,\n    layer_gru, layer_hashing, layer_input, layer_integer_lookup,\n    layer_lambda, layer_layer_normalization, layer_lstm, layer_masking,\n    layer_max_pooling_1d, layer_max_pooling_2d, layer_max_pooling_3d,\n    layer_maximum, layer_minimum, layer_multi_head_attention,\n    layer_multiply, layer_normalization, layer_permute,\n    layer_random_brightness, layer_random_contrast, layer_random_crop,\n    layer_random_flip, layer_random_rotation, layer_random_translation,\n    layer_random_zoom, layer_repeat_vector, layer_rescaling,\n    layer_reshape, layer_resizing, layer_rnn, layer_separable_conv_1d,\n    layer_separable_conv_2d, layer_simple_rnn,\n    layer_spatial_dropout_1d, layer_spatial_dropout_2d,\n    layer_spatial_dropout_3d, layer_string_lookup, layer_subtract,\n    layer_text_vectorization, layer_unit_normalization,\n    layer_upsampling_1d, layer_upsampling_2d, layer_upsampling_3d,\n    layer_zero_padding_1d, layer_zero_padding_2d,\n    layer_zero_padding_3d, learning_rate_schedule_cosine_decay,\n    learning_rate_schedule_cosine_decay_restarts,\n    learning_rate_schedule_exponential_decay,\n    learning_rate_schedule_inverse_time_decay,\n    learning_rate_schedule_piecewise_constant_decay,\n    learning_rate_schedule_polynomial_decay, loss_binary_crossentropy,\n    loss_categorical_crossentropy, loss_categorical_hinge,\n    loss_cosine_similarity, loss_hinge, loss_huber, loss_kl_divergence,\n    loss_mean_absolute_error, loss_mean_absolute_percentage_error,\n    loss_mean_squared_error, loss_mean_squared_logarithmic_error,\n    loss_poisson, loss_sparse_categorical_crossentropy,\n    loss_squared_hinge, mark_active, metric_auc,\n    metric_binary_accuracy, metric_binary_crossentropy,\n    metric_categorical_accuracy, metric_categorical_crossentropy,\n    metric_categorical_hinge, metric_cosine_similarity,\n    metric_false_negatives, metric_false_positives, metric_hinge,\n    metric_mean, metric_mean_absolute_error,\n    metric_mean_absolute_percentage_error, metric_mean_iou,\n    metric_mean_squared_error, metric_mean_squared_logarithmic_error,\n    metric_mean_wrapper, metric_poisson, metric_precision,\n    metric_precision_at_recall, metric_recall,\n    metric_recall_at_precision, metric_root_mean_squared_error,\n    metric_sensitivity_at_specificity,\n    metric_sparse_categorical_accuracy,\n    metric_sparse_categorical_crossentropy,\n    metric_sparse_top_k_categorical_accuracy,\n    metric_specificity_at_sensitivity, metric_squared_hinge,\n    metric_sum, metric_top_k_categorical_accuracy,\n    metric_true_negatives, metric_true_positives, new_callback_class,\n    new_layer_class, new_learning_rate_schedule_class, new_loss_class,\n    new_metric_class, new_model_class, normalize, optimizer_adadelta,\n    optimizer_adagrad, optimizer_adam, optimizer_adamax,\n    optimizer_ftrl, optimizer_nadam, optimizer_rmsprop, optimizer_sgd,\n    pad_sequences, pop_layer, predict_on_batch, regularizer_l1,\n    regularizer_l1_l2, regularizer_l2, regularizer_orthogonal,\n    set_vocabulary, set_weights, shape, test_on_batch,\n    text_dataset_from_directory, time_distributed,\n    timeseries_dataset_from_array, to_categorical, train_on_batch,\n    unfreeze_weights, use_backend, with_custom_object_scope, zip_lists\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n\ntf.Tensor(1, shape=(), dtype=int32)\n\ntf$math$reduce_min(y) # Float!\n\ntf.Tensor(1.0, shape=(), dtype=float32)\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n\n[1] 50.5\n\nmean(y)\n\n[1] 50.5\n\ntf$math$reduce_mean(x)  # Integer!\n\ntf.Tensor(50, shape=(), dtype=int32)\n\ntf$math$reduce_mean(y)  # Float!\n\ntf.Tensor(50.5, shape=(), dtype=float32)\n\n# c)    which.max(x)\ntf$argmax(x)\n\ntf.Tensor(0, shape=(), dtype=int64)\n\ntf$argmax(y)\n\ntf.Tensor(0, shape=(), dtype=int64)\n\n# d)    which.min(x)\ntf$argmin(x)\n\ntf.Tensor(99, shape=(), dtype=int64)\n\n# e)    order(x)\ntf$argsort(x)\n\ntf.Tensor(\n[99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n  3  2  1  0], shape=(100), dtype=int32)\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n\ntf.Tensor(\n[[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n  10.433675  10.103772   9.608109   8.582045 ]\n [11.311237  11.200746  11.076511  10.934624  10.769221  10.570932\n  10.323349   9.993557   9.498147   8.473241 ]\n [11.186988  11.076511  10.952296  10.810434  10.645068  10.446829\n  10.199324   9.869672   9.374583   8.351139 ]\n [11.045079  10.934624  10.810434  10.668607  10.503285  10.305112\n  10.05771    9.728241   9.233568   8.212026 ]\n [10.87965   10.769221  10.645068  10.503285  10.338026  10.139942\n   9.892679   9.563459   9.069353   8.0503845]\n [10.68132   10.570932  10.446829  10.305112  10.139942   9.941987\n   9.694924   9.366061   8.872767   7.857481 ]\n [10.433675  10.323349  10.199324  10.05771    9.892679   9.694924\n   9.448175   9.119868   8.62784    7.6182513]\n [10.103772   9.993557   9.869672   9.728241   9.563459   9.366061\n   9.119868   8.79255    8.302762   7.30317  ]\n [ 9.608109   9.498147   9.374583   9.233568   9.069353   8.872767\n   8.62784    8.302762   7.818028   6.8405466]\n [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n   7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n\ntf.Tensor(True, shape=(), dtype=bool)\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Runtime\n\n\n\nThis exercise compares the speed of R to TensorFlow. The first exercise is to rewrite the following function in TensorFlow:\n\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n\nHere, we provide a skeleton for a TensorFlow function:\n\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n\nWe can compare the speed using the Microbenchmark package:\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nTry different matrix sizes for the test matrix and compare the speed.\nTip: Have a look at the the tf.reduce_mean documentation and the “axis” argument.\n\nCompare the following with different matrix sizes:\n\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nAlso try the following:\n\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\n\n\n\nClick here to see the solution\n\n\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\n\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nWhy is R faster (the first time)?\n\n\nThe R functions we used (apply, mean, “-”) are also implemented in C.\n\n\nThe problem is not large enough and TensorFlow has an overhead.\n\n\n\n\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n\n\n\n\n\n\n\n\n\n\nQuestion: Linear Algebra\n\n\n\nGoogle to find out how to write the following expressions in TensorFlow:\n\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n\n# l)\neigen(A)\n\n# m)\ndet(A)\n\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras3)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n\n# j)    diag(A)\ntf$linalg$diag_part(A)\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n\n# l)    eigen(A)\ntf$linalg$eigh(A)\n\n# m)    det(A)\ntf$linalg$det(A)\n\n\n\n\n\n\n\n\n\n\nQuestion: Automatic differentation\n\n\n\nTensorFlow supports automatic differentiation (analytical and not numerical!). Let’s have a look at the function \\(f(x) = 5 x^2 + 3\\) with derivative \\(f'(x) = 10x\\). So for \\(f'(5)\\) we will get \\(10\\).\nLet’s do this in TensorFlow. Define the function:\n\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\n\nWe want to calculate the derivative for \\(x = 2.0\\):\n\nx = tf$constant(2.0)\n\nTo do automatic differentiation, we have to forward \\(x\\) through the function within the tf$GradientTape() environment. We have also have to tell TensorFlow which value to “watch”:\n\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n\nTo print the gradient:\n\n(tape$gradient(y, x))\n\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nWe can also calculate the second order derivative \\(f''(x) = 10\\):\n\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n\ntf.Tensor(10.0, shape=(), dtype=float32)\n\n\nWhat is happening here? Think about and discuss it.\nA more advanced example: Linear regression\nIn this case we first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows a normal distribution == error).\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\nIn R we would do the following to fit a linear regression model:\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60967 -0.14994 -0.01641  0.15907  0.72013 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01616    0.02638  -0.613    0.542    \nx1           1.95722    0.02190  89.368   &lt;2e-16 ***\nx2           3.50553    0.02409 145.530   &lt;2e-16 ***\nx3           2.16999    0.02254  96.269   &lt;2e-16 ***\nx4           1.92183    0.02312  83.122   &lt;2e-16 ***\nx5           2.09638    0.02273  92.246   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2585 on 94 degrees of freedom\nMultiple R-squared:  0.9983,    Adjusted R-squared:  0.9982 \nF-statistic: 1.117e+04 on 5 and 94 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights (\\(\\boldsymbol{w}\\)) of the regression model). We want our model to learn these weights.\nThe input (predictors, independent variables or features, \\(\\boldsymbol{X}\\)) and the observed (response, \\(\\boldsymbol{y}\\)) are constant and will not be learned/optimized.\n\nlibrary(tensorflow)\nlibrary(keras3)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ print(as.numeric(loss), message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n\n[1] 2.48493\n[1] 1.093104\n[1] 0.44713\n[1] 0.4174882\n[1] 0.3037115\n[1] 0.3142751\n[1] 0.3034402\n[1] 0.3028021\n[1] 0.3022693\n[1] 0.3016723\n\nprint(as.matrix(wTF), message = \"Resulting weights:\\n\")\n\n          [,1]\n[1,] 0.9044624\n[2,] 2.7220652\n[3,] 1.6548279\n[4,] 1.3135430\n[5,] 0.1999556\n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  0.882 2.718 1.652 1.321 0.189 \n\n\nDiscuss the code, go through the code line by line and try to understand it.\nAdditional exercise:\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras3)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ print(as.numeric(loss), message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, list(wTF, interceptTF))\n  optimizer$apply_gradients(purrr::transpose(list(grads, list(wTF, interceptTF))))\n}\n\n[1] 5.442895\n[1] 4.558433\n[1] 3.709029\n[1] 2.902339\n[1] 2.142875\n[1] 1.43612\n[1] 0.7944588\n[1] 0.3106539\n[1] 0.3024492\n[1] 0.2686584\n\nprint(as.matrix(wTF), message = \"Resulting weights:\\n\")\n\n          [,1]\n[1,] 2.0252784\n[2,] 4.0646193\n[3,] 0.3679218\n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  2.014 4.003 0.368 \n\nprint(as.numeric(interceptTF), message = \"Resulting intercept:\\n\")\n\n[1] 3.072313\n\ncat(\"Original intercept: \", intercept, \"\\n\")\n\nOriginal intercept:  3.112",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to TensorFlow and Keras</span>"
    ]
  },
  {
    "objectID": "C1-TensorFlow.html#introduction-to-pytorch",
    "href": "C1-TensorFlow.html#introduction-to-pytorch",
    "title": "Appendix B — Introduction to TensorFlow and Keras",
    "section": "B.2 Introduction to PyTorch",
    "text": "B.2 Introduction to PyTorch\nPyTorch is another famous library for deep learning. Like TensorFlow, Torch itself is written in C++ with an API for Python. In 2020, the RStudio team released R-Torch, and while R-TensorFlow calls the Python API in the background, the R-Torch API is built directly on the C++ Torch library!\nUseful links:\n\nPyTorch documentation (This is for the Python API, bust just replace the “.” with “$”.)\nR-Torch website\n\nTo get started with Torch, we have to load the library and check if the installation worked.\n\nlibrary(torch)\n\n\nAttaching package: 'torch'\n\n\nThe following object is masked from 'package:keras3':\n\n    as_iterator\n\n\n\nB.2.1 Data Containers\nUnlike TensorFlow, Torch doesn’t have two data containers for mutable and immutable variables. All variables are initialized via the torch_tensor function:\n\na = torch_tensor(1.)\n\nTo mark variables as mutable (and to track their operations for automatic differentiation) we have to set the argument ‘requires_grad’ to true in the torch_tensor function:\n\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)\n\n\n\nB.2.2 Basic Operations\nWe now can define the variables and do some math with them:\n\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n\nprint(b)\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n\nc = a$add(b)\nprint(c)\n\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n\n\nThe R-Torch package provides all common R methods (an advantage over TensorFlow).\n\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n\nprint(a/b)\n\ntorch_tensor\n 0.5000\n[ CPUFloatType{1} ]\n\nprint(a*b)\n\ntorch_tensor\n 50\n[ CPUFloatType{1} ]\n\n\nTheir operators also automatically transform R numbers into tensors when attempting to add a tensor to a R number:\n\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n\n\nAs for TensorFlow, we have to explicitly transform the tensors back to R:\n\nclass(d)\n\n[1] \"torch_tensor\" \"R7\"          \n\nclass(as.numeric(d))\n\n[1] \"numeric\"\n\n\n\n\nB.2.3 Data Types\nSimilar to TensorFlow:\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b \n\nBut here’s a difference! With TensorFlow we would get an error, but with R-Torch, m is automatically casted to a double (float64). However, this is still bad practice!\nDuring the course we will try to provide the corresponding PyTorch code snippets for all Keras/TensorFlow examples.\n\n\nB.2.4 Exercises\n\n\n\n\n\n\nQuestion: Torch Operations\n\n\n\nRewrite the following expressions (a to g) in torch:\n\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n\n[1] 1\n\n# b)\nmean(x)\n\n[1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n\n[1] 1\n\n# d) \nwhich.min(x)\n\n[1] 100\n\n# e) Tip: Use Google! \norder(x)\n\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n\n[1] TRUE\n\n\n\n\nClick here to see the solution\n\n\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\ntorch_min(y) # Float!\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n\n[1] 50.5\n\nmean(y)\n\n[1] 50.5\n\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n\ntorch_mean(y)  # Float!\n\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n\n# c)    which.max(x)\ntorch_argmax(x)\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\ntorch_argmax(y)\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\n# d)    which.min(x)\ntorch_argmin(x)\n\ntorch_tensor\n100\n[ CPULongType{} ]\n\n# e)    order(x)\ntorch_argsort(x)\n\ntorch_tensor\n 100\n  99\n  98\n  97\n  96\n  95\n  94\n  93\n  92\n  91\n  90\n  89\n  88\n  87\n  86\n  85\n  84\n  83\n  82\n  81\n  80\n  79\n  78\n  77\n  76\n  75\n  74\n  73\n  72\n  71\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{100} ]\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n\ntorch_tensor\n 4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n 4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n 4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n 4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n 4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n 3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n 3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n 3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n 2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n 2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n[ CPUFloatType{10,10} ]\n\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n\ntorch_tensor\n 1\n[ CPUBoolType{1} ]\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Runtime\n\n\n\n\nWhat is the meaning of “An effect is not significant”?\nIs an effect with three *** more significant / certain than an effect with one *?\n\n\n\nClick here to see the solution\n\nThis exercise compares the speed of R to torch The first exercise is to rewrite the following function in torch:\n\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n\nHere, we provide a skeleton for a TensorFlow function:\n\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n\nWe can compare the speed using the Microbenchmark package:\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nTry different matrix sizes for the test matrix and compare the speed.\nTip: Have a look at the the torch_mean documentation and the “dim” argument.\n\nCompare the following with different matrix sizes:\n\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nAlso try the following:\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n\n\n\nClick here to see the solution\n\n\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\n\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nWhy is R faster (the first time)?\n\n\nThe R functions we used (apply, mean, “-”) are also implemented in C.\n\n\nThe problem is not large enough and torch has an overhead.\n\n\n\n\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n\n\n:::\n\n\n\n\n\n\nQuestion: Linear Algebra\n\n\n\nGoogle to find out how to write the following tasks in torch:\n\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n\n# l)\neigen(A)\n\n# m)\ndet(A)\n\n\n\nClick here to see the solution\n\n\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n\ntorch_tensor\n 1.0000 -0.0000 -0.6667\n-1.0000  0.5000 -0.1667\n 0.0000  0.0000  0.3333\n[ CPUFloatType{3,3} ]\n\n# j)    diag(A)\ntorch_diag(A)\n\ntorch_tensor\n 1\n 2\n 3\n[ CPUFloatType{3} ]\n\n# k)    diag(diag(A))\ntorch_diag(A)$diag()\n\ntorch_tensor\n 1  0  0\n 0  2  0\n 0  0  3\n[ CPUFloatType{3,3} ]\n\n# l)    eigen(A)\nlinalg_eigh(A)\n\n[[1]]\ntorch_tensor\n-0.5616\n 3.0000\n 3.5616\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n-0.7882  0.0000  0.6154\n 0.6154  0.0000  0.7882\n 0.0000  1.0000  0.0000\n[ CPUFloatType{3,3} ]\n\n# m)    det(A)\nlinalg_det(A)\n\ntorch_tensor\n6\n[ CPUFloatType{} ]\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Automatic differentation\n\n\n\nTorch supports automatic differentiation (analytical and not numerical!). Let’s have a look at the function \\(f(x) = 5 x^2 + 3\\) with derivative \\(f'(x) = 10x\\). So for \\(f'(5)\\) we will get \\(10\\).\nLet’s do this in torch Define the function:\n\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\n\nWe want to calculate the derivative for \\(x = 2.0\\):\n\nx = torch_tensor(2.0, requires_grad = TRUE)\n\nTo do automatic differentiation, we have to forward \\(x\\) through the function and call the $backward() method of the result:\n\ny = f(x)\ny$backward(retain_graph=TRUE )\n\nTo print the gradient:\n\nx$grad\n\ntorch_tensor\n 20\n[ CPUFloatType{1} ]\n\n\nWe can also calculate the second order derivative \\(f''(x) = 10\\):\n\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ][ grad_fn = &lt;MulBackward0&gt; ]\n\n\nWhat is happening here? Think about and discuss it.\nA more advanced example: Linear regression\nIn this case we first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows a normal distribution == error).\n\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\nIn R we would do the following to fit a linear regression model:\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   &lt;2e-16 ***\nx2          2.741690   0.025328 108.249   &lt;2e-16 ***\nx3          1.179181   0.023644  49.872   &lt;2e-16 ***\nx4          0.591873   0.025154  23.530   &lt;2e-16 ***\nx5          2.302417   0.022575 101.991   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,    Adjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights (\\(\\boldsymbol{w}\\)) of the regression model). We want our model to learn these weights.\nThe input (predictors, independent variables or features, \\(\\boldsymbol{X}\\)) and the observed (response, \\(\\boldsymbol{y}\\)) are constant and will not be learned/optimized.\n\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n\n[1] \"Loss: 4.4065318107605\"\n[1] \"Loss: 2.37926030158997\"\n[1] \"Loss: 0.901207447052002\"\n[1] \"Loss: 0.403193771839142\"\n[1] \"Loss: 0.296265512704849\"\n[1] \"Loss: 0.268377929925919\"\n[1] \"Loss: 0.232994765043259\"\n[1] \"Loss: 0.219554618000984\"\n[1] \"Loss: 0.215328559279442\"\n[1] \"Loss: 0.213282018899918\"\n\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n\nInferred weights:  0.701 3.089 1.801 1.123 3.452 \n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  0.67 3.085 1.787 1.121 3.455 \n\n\nDiscuss the code, go through the code line by line and try to understand it.\nAdditional exercise:\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\nClick here to see the solution\n\n\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n\n[1] \"Loss: 3.51533484458923\"\n[1] \"Loss: 1.74870145320892\"\n[1] \"Loss: 0.41416934132576\"\n[1] \"Loss: 0.518697261810303\"\n[1] \"Loss: 0.293963432312012\"\n[1] \"Loss: 0.263338804244995\"\n[1] \"Loss: 0.258341491222382\"\n[1] \"Loss: 0.254723280668259\"\n[1] \"Loss: 0.252453774213791\"\n[1] \"Loss: 0.25116890668869\"\n\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n\nInferred weights:  3.118 -0.349 2.107 \n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  3.131 -0.353 2.11 \n\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n\nInferred intercept:  2.836 \n\ncat(\"Original intercept: \", intercept, \"\\n\")\n\nOriginal intercept:  2.832",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to TensorFlow and Keras</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline-mlr3.html",
    "href": "A4-MLpipeline-mlr3.html",
    "title": "Appendix C — Machine learning framework - mlr3",
    "section": "",
    "text": "C.1 mlr3\nAs we have seen today, many of the machine learning algorithms are distributed over several packages but the general machine learning pipeline is very similar for all models: feature engineering, feature selection, hyperparameter tuning and cross-validation.\nMachine learning frameworks such as mlr3 or tidymodels provide a general interface for the ML pipeline, in particular the training and the hyperparameter tuning with nested CV. They support most ML packages/algorithms.\nThe key features of mlr3 are:\nUseful links:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Machine learning framework - mlr3</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline-mlr3.html#sec-mlr",
    "href": "A4-MLpipeline-mlr3.html#sec-mlr",
    "title": "Appendix C — Machine learning framework - mlr3",
    "section": "",
    "text": "All common machine learning packages are integrated into mlr3, you can easily switch between different machine learning algorithms.\nA common ‘language’/workflow to specify machine learning pipelines.\nSupport for different cross-validation strategies.\nHyperparameter tuning for all supported machine learning algorithms.\nEnsemble models.\n\n\n\nmlr3-book (still in work)\nmlr3 website\nmlr3 cheatsheet\n\n\nC.1.1 mlr3 - The Basic Workflow\nThe mlr3 package actually consists of several packages for different tasks (e.g. mlr3tuning for hyperparameter tuning, mlr3pipelines for data preparation pipes). But let’s start with the basic workflow:\n\nlibrary(EcoData)\nlibrary(cito)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n\n'data.frame':   4687 obs. of  40 variables:\n $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n $ Orbital.Period              : num  419 1040 682 427 503 ...\n $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n\n\nLet’s drop time, name and ID variable and create a classification task:\n\ndata = nasa %&gt;% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\n\nCreate a generic pipeline of data transformation (imputation \\(\\rightarrow\\) scaling \\(\\rightarrow\\) encoding of categorical variables):\n\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n\n                   Hazardous           Absolute.Magnitude \n                        4187                            0 \n               Aphelion.Dist           Asc.Node.Longitude \n                           0                            0 \n                Eccentricity    Epoch.Date.Close.Approach \n                           0                            0 \n            Epoch.Osculation         Est.Dia.in.Feet.max. \n                           0                            0 \n        Est.Dia.in.Feet.min.           Est.Dia.in.KM.max. \n                           0                            0 \n          Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n                           0                            0 \n           Est.Dia.in.M.min.        Est.Dia.in.Miles.max. \n                           0                            0 \n       Est.Dia.in.Miles.min.                  Inclination \n                           0                            0 \n Jupiter.Tisserand.Invariant                 Mean.Anomaly \n                           0                            0 \n                 Mean.Motion               Miles.per.hour \n                           0                            0 \n  Minimum.Orbit.Intersection     Miss.Dist..Astronomical. \n                           0                            0 \n      Miss.Dist..kilometers.            Miss.Dist..lunar. \n                           0                            0 \n           Miss.Dist..miles.                     Orbit.ID \n                           0                            0 \n          Orbit.Uncertainity               Orbital.Period \n                           0                            0 \n              Perihelion.Arg          Perihelion.Distance \n                           0                            0 \n             Perihelion.Time  Relative.Velocity.km.per.hr \n                           0                            0 \nRelative.Velocity.km.per.sec              Semi.Major.Axis \n                           0                            0 \n               Equinox.J2000             Equinox..MISSING \n                           0                            0 \n         Orbiting.Body.Earth       Orbiting.Body..MISSING \n                           0                            0 \n\n\nWe can even visualize the preprocessing graph:\n\npreprocessing$plot()\n\n\n\n\n\n\n\n\nTo test our model (glmnet) with 10-fold cross-validated, we will do:\n\nSpecify the missing target rows as validation so that they will be ignored.\nSpecify the cross-validation, the learner (the machine learning model we want to use), and the measurement (AUC).\nRun (benchmark) our model.\n\n\nset.seed(123)\n\ntransformed_task$data()[1,]\n\n   Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity\n      &lt;fctr&gt;              &lt;num&gt;         &lt;num&gt;              &lt;num&gt;        &lt;num&gt;\n1:         0         -0.8132265    -0.3804201          -1.140837    -0.315606\n   Epoch.Date.Close.Approach Epoch.Osculation Est.Dia.in.Feet.max.\n                       &lt;num&gt;            &lt;num&gt;                &lt;num&gt;\n1:                 -4.792988        0.1402677            0.2714179\n   Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max.\n                  &lt;num&gt;              &lt;num&gt;              &lt;num&gt;             &lt;num&gt;\n1:            0.3134076          0.3007134          0.2565687         0.2710953\n   Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n               &lt;num&gt;                 &lt;num&gt;                 &lt;num&gt;       &lt;num&gt;\n1:         0.2916245             0.2620443              0.258651   0.5442288\n   Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n                         &lt;num&gt;        &lt;num&gt;       &lt;num&gt;          &lt;num&gt;\n1:                   0.3840868    -1.028761   0.3193953     -0.2541306\n   Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers.\n                        &lt;num&gt;                    &lt;num&gt;                  &lt;num&gt;\n1:                  -5.459119                -7.076926              0.2512296\n   Miss.Dist..lunar. Miss.Dist..miles.  Orbit.ID Orbit.Uncertainity\n               &lt;num&gt;             &lt;num&gt;     &lt;num&gt;              &lt;num&gt;\n1:         0.2398625         0.2381077 -9.651472          -1.007087\n   Orbital.Period Perihelion.Arg Perihelion.Distance Perihelion.Time\n            &lt;num&gt;          &lt;num&gt;               &lt;num&gt;           &lt;num&gt;\n1:     -0.3013135      -1.170536         -0.01831583       0.1052611\n   Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec Semi.Major.Axis\n                         &lt;num&gt;                        &lt;num&gt;           &lt;num&gt;\n1:                  -0.2816782                   -0.2841407      -0.2791037\n   Equinox.J2000 Equinox..MISSING Orbiting.Body.Earth Orbiting.Body..MISSING\n           &lt;num&gt;            &lt;num&gt;               &lt;num&gt;                  &lt;num&gt;\n1:             1                0                   1                      0\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n\n\nresult = mlr3::resample(transformed_task,\n                        EN, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nVery cool! Preprocessing + 10-fold cross-validation model evaluation in a few lines of code!\nLet’s create the final predictions:\n\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n\nYou could now submit the predictions here.\nBut we are still not happy with the results, let’s do some hyperparameter tuning!\n\n\nC.1.2 mlr3 - Hyperparameter Tuning\nWith mlr3, we can easily extend the above example to do hyperparameter tuning within nested cross-validation (the tuning has its own inner cross-validation).\nPrint the hyperparameter space of our glmnet learner:\n\nEN$param_set\n\n&lt;ParamSet&gt;\n\n\nWarning: Unknown argument 'on' has been passed.\n\n\nKey: &lt;id&gt;\n                      id    class lower upper nlevels\n                  &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n 1:                alpha ParamDbl     0     1     Inf\n 2:                  big ParamDbl  -Inf   Inf     Inf\n 3:               devmax ParamDbl     0     1     Inf\n 4:                dfmax ParamInt     0   Inf     Inf\n 5:                  eps ParamDbl     0     1     Inf\n 6:                epsnr ParamDbl     0     1     Inf\n 7:                exact ParamLgl    NA    NA       2\n 8:              exclude ParamInt     1   Inf     Inf\n 9:                 exmx ParamDbl  -Inf   Inf     Inf\n10:                 fdev ParamDbl     0     1     Inf\n11:                gamma ParamDbl  -Inf   Inf     Inf\n12:            intercept ParamLgl    NA    NA       2\n13:               lambda ParamUty    NA    NA     Inf\n14:     lambda.min.ratio ParamDbl     0     1     Inf\n15:         lower.limits ParamUty    NA    NA     Inf\n16:                maxit ParamInt     1   Inf     Inf\n17:                mnlam ParamInt     1   Inf     Inf\n18:                 mxit ParamInt     1   Inf     Inf\n19:               mxitnr ParamInt     1   Inf     Inf\n20:            newoffset ParamUty    NA    NA     Inf\n21:              nlambda ParamInt     1   Inf     Inf\n22:               offset ParamUty    NA    NA     Inf\n23:       penalty.factor ParamUty    NA    NA     Inf\n24:                 pmax ParamInt     0   Inf     Inf\n25:                 pmin ParamDbl     0     1     Inf\n26:                 prec ParamDbl  -Inf   Inf     Inf\n27:                relax ParamLgl    NA    NA       2\n28:                    s ParamDbl     0   Inf     Inf\n29:          standardize ParamLgl    NA    NA       2\n30: standardize.response ParamLgl    NA    NA       2\n31:               thresh ParamDbl     0   Inf     Inf\n32:             trace.it ParamInt     0     1       2\n33:        type.gaussian ParamFct    NA    NA       2\n34:        type.logistic ParamFct    NA    NA       2\n35:     type.multinomial ParamFct    NA    NA       2\n36:         upper.limits ParamUty    NA    NA     Inf\n                      id    class lower upper nlevels\n                                                                                      default\n                                                                                       &lt;list&gt;\n 1:                                                                                         1\n 2:                                                                                   9.9e+35\n 3:                                                                                     0.999\n 4: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n 5:                                                                                     1e-06\n 6:                                                                                     1e-08\n 7:                                                                                     FALSE\n 8: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n 9:                                                                                       250\n10:                                                                                     1e-05\n11:                                                                                         1\n12:                                                                                      TRUE\n13: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n14: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n15: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n16:                                                                                    100000\n17:                                                                                         5\n18:                                                                                       100\n19:                                                                                        25\n20: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n21:                                                                                       100\n22:                                                                                          \n23: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n24: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n25:                                                                                     1e-09\n26:                                                                                     1e-10\n27:                                                                                     FALSE\n28:                                                                                      0.01\n29:                                                                                      TRUE\n30:                                                                                     FALSE\n31:                                                                                     1e-07\n32:                                                                                         0\n33: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n34: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n35: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n36: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n                                                                                      default\n    parents  value\n     &lt;list&gt; &lt;list&gt;\n 1:               \n 2:               \n 3:               \n 4:               \n 5:               \n 6:               \n 7:               \n 8:               \n 9:               \n10:               \n11:   relax       \n12:               \n13:               \n14:               \n15:               \n16:               \n17:               \n18:               \n19:               \n20:               \n21:               \n22:               \n23:               \n24:               \n25:               \n26:               \n27:               \n28:               \n29:               \n30:               \n31:               \n32:               \n33:               \n34:               \n35:               \n36:               \n    parents  value\n\n\nDefine the hyperparameter space of the random forest:\n\nlibrary(paradox)\n\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\nprint(EN_pars)\n\n&lt;ParamSet&gt;\n       id    class lower upper nlevels\n   &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n1:  alpha ParamDbl     0   1.0     Inf\n2: lambda ParamDbl     0   0.5     Inf\n                                                                                     default\n                                                                                      &lt;list&gt;\n1: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n2: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n    value\n   &lt;list&gt;\n1:       \n2:       \n\n\nTo set up the tuning pipeline we need:\n\nInner cross-validation resampling object.\nTuning criterion (e.g. AUC).\nTuning method (e.g. random or block search).\nTuning terminator (When should we stop tuning? E.g. after \\(n\\) iterations).\n\n\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n\n&lt;AutoTuner:classif.glmnet.tuned&gt;\n* Model: list\n* Search Space:\n&lt;ParamSet&gt;\n       id    class lower upper nlevels\n   &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n1:  alpha ParamDbl     0   1.0     Inf\n2: lambda ParamDbl     0   0.5     Inf\n                                                                                     default\n                                                                                      &lt;list&gt;\n1: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n2: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n    value\n   &lt;list&gt;\n1:       \n2:       \n* Packages: mlr3, mlr3tuning, mlr3learners, glmnet\n* Predict Type: prob\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass, weights\n\n\nNow we can wrap it normally into the 10-fold cross-validated setup as done previously:\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nclassif.auc \n  0.6767554 \n\n\nLet’s create the final predictions:\n\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Machine learning framework - mlr3</span>"
    ]
  },
  {
    "objectID": "A4-MLpipeline-mlr3.html#exercises",
    "href": "A4-MLpipeline-mlr3.html#exercises",
    "title": "Appendix C — Machine learning framework - mlr3",
    "section": "C.2 Exercises",
    "text": "C.2 Exercises\n\nC.2.1 Tuning Regularization\n\n\n\n\n\n\nQuestion: Hyperparameter tuning - Titanic dataset\n\n\n\nTune architecture\n\nTune training parameters (learning rate, batch size) and regularization\n\nHints\ncito has a feature to automatically tune hyperparameters under Cross Validation!\n\npassing tune(...) to a hyperparameter will tell cito to tune this specific hyperparameter\nthe tuning = config_tuning(...) let you specify the cross-validation strategy and the number of hyperparameters that should be tested (steps = number of hyperparameter combinations that should be tried)\nafter tuning, cito will fit automatically a model with the best hyperparameters on the full data and will return this model\n\nMinimal example with the iris dataset:\n\nlibrary(cito)\ndf = iris\ndf[,1:4] = scale(df[,1:4])\n\nmodel_tuned = dnn(Species~., \n                  loss = \"softmax\",\n                  data = iris,\n                  lambda = tune(lower = 0.0, upper = 0.2), # you can pass the \"tune\" function to a hyerparameter\n                  tuning = config_tuning(CV = 3, steps = 20L)\n                  )\n\n# tuning results\nmodel_tuned$tuning\n\n\n# model_tuned is now already the best model!\n\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\n\nmodel = dnn(survived~., \n          hidden = c(10L, 10L), # change\n          activation = c(\"selu\", \"selu\"), # change\n          loss = \"binomial\", \n          lr = 0.05, #change\n          validation = 0.2,\n          lambda = 0.001, # change\n          alpha = 0.1, # change\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = data_obs, epochs = 40L, verbose = TRUE, plot= TRUE)\n\nLoss at epoch 1: training: 0.720, validation: 0.673, lr: 0.05000\n\n\n\n\n\n\n\n\n\nLoss at epoch 2: training: 0.658, validation: 0.811, lr: 0.05000\nLoss at epoch 3: training: 0.654, validation: 0.600, lr: 0.05000\nLoss at epoch 4: training: 0.647, validation: 0.824, lr: 0.05000\nLoss at epoch 5: training: 0.641, validation: 0.586, lr: 0.05000\nLoss at epoch 6: training: 0.611, validation: 0.563, lr: 0.05000\nLoss at epoch 7: training: 0.639, validation: 0.729, lr: 0.05000\nLoss at epoch 8: training: 0.618, validation: 0.565, lr: 0.05000\nLoss at epoch 9: training: 0.604, validation: 0.754, lr: 0.05000\nLoss at epoch 10: training: 0.602, validation: 0.566, lr: 0.05000\nLoss at epoch 11: training: 0.571, validation: 0.510, lr: 0.05000\nLoss at epoch 12: training: 0.593, validation: 0.529, lr: 0.05000\nLoss at epoch 13: training: 0.594, validation: 0.499, lr: 0.05000\nLoss at epoch 14: training: 0.564, validation: 0.560, lr: 0.05000\nLoss at epoch 15: training: 0.575, validation: 0.484, lr: 0.05000\nLoss at epoch 16: training: 0.559, validation: 0.503, lr: 0.05000\nLoss at epoch 17: training: 0.539, validation: 0.457, lr: 0.05000\nLoss at epoch 18: training: 0.537, validation: 0.553, lr: 0.05000\nLoss at epoch 19: training: 0.590, validation: 0.565, lr: 0.05000\nLoss at epoch 20: training: 0.592, validation: 0.549, lr: 0.05000\nLoss at epoch 21: training: 0.535, validation: 0.962, lr: 0.05000\nLoss at epoch 22: training: 0.515, validation: 0.461, lr: 0.05000\nLoss at epoch 23: training: 0.564, validation: 0.453, lr: 0.05000\nLoss at epoch 24: training: 0.514, validation: 0.416, lr: 0.05000\nLoss at epoch 25: training: 0.517, validation: 0.648, lr: 0.05000\nLoss at epoch 26: training: 0.483, validation: 0.573, lr: 0.05000\nLoss at epoch 27: training: 0.492, validation: 0.406, lr: 0.05000\nLoss at epoch 28: training: 0.501, validation: 0.458, lr: 0.05000\nLoss at epoch 29: training: 0.466, validation: 0.491, lr: 0.05000\nLoss at epoch 30: training: 0.594, validation: 0.423, lr: 0.05000\nLoss at epoch 31: training: 0.525, validation: 0.726, lr: 0.05000\nLoss at epoch 32: training: 0.482, validation: 0.397, lr: 0.05000\nLoss at epoch 33: training: 0.524, validation: 0.965, lr: 0.05000\nLoss at epoch 34: training: 0.525, validation: 0.885, lr: 0.05000\nLoss at epoch 35: training: 0.520, validation: 0.430, lr: 0.05000\nLoss at epoch 36: training: 0.546, validation: 0.577, lr: 0.05000\nLoss at epoch 37: training: 0.493, validation: 0.570, lr: 0.05000\nLoss at epoch 38: training: 0.465, validation: 1.023, lr: 0.05000\nLoss at epoch 39: training: 0.470, validation: 0.493, lr: 0.05000\nLoss at epoch 40: training: 0.482, validation: 0.392, lr: 0.05000\n\n# Predictions:\n\npredictions = predict(model, newdata = data_new, type = \"response\") # change prediction type to response so that cito predicts probabilities\n\nwrite.csv(data.frame(y = predictions[,1]), file = \"Max_titanic_dnn.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC.2.2 Bonus: mlr3\n\n\n\n\n\n\nTask: Use mlr3 for the titanic dataset\n\n\n\n\nUse mlr3 to tune glmnet for the titanic dataset using nested CV\nSubmit single predictions and multiple predictions\n\nIf you need help, take a look at the solution, go through it line by line and try to understand it.\n\n\n\n\nClick here to see the solution\n\nPrepare data\n\ndata = titanic_ml %&gt;% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n\n survived       age      boat     cabin  embarked      fare home.dest     parch \n      655       263         0         0         0         0         0         0 \n   pclass       sex     sibsp \n        0         0         0 \n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n\nHyperparameter tuning:\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\n\n\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = cv10, store_models = TRUE)\n\nEvaluation:\n\nmeasurement =  msr(\"classif.auc\")\nresult$aggregate(measurement)\n\nclassif.auc \n  0.9939211 \n\n\nPredictions:\nWe can extract a learner with optimized hyperparameters:\n\nmodel = result$learners[[1]]$learner$clone()\nmodel$param_set$values\n\n$alpha\n[1] 0.1832108\n\n$lambda\n[1] 0.1246408\n\n\nAnd we can fit it then on the full data set:\n\nmodel$train(transformed_task)\npredictions = model$predict(transformed_task, row_ids = transformed_task$row_roles$holdout)\npredictions = predictions$prob[,1]\nhead(predictions)\n\n[1] 0.8555042 0.1554276 0.3219091 0.7343347 0.8628420 0.8735773\n\n\nAnd submit to http://rhsbio7.uni-regensburg.de:8500\n\nwrite.csv(data.frame(y = predictions), file = \"glmnet.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Machine learning framework - mlr3</span>"
    ]
  }
]