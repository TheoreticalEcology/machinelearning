[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Deep Learning with R",
    "section": "",
    "text": "Preface\nMachine learning (ML) is the process of building a predictive model that makes predictions about new data based on observations (training data). The goal of this course is to enable you to build a robust ML model, one that generalizes well to new observations and doesn’t “overfit” your training data. To do this, you will need to master a number of skills, in particular\nIn recent years, a new field within ML called Deep Learning has emerged and attracted a lot of attention. The reason for this is that DL incorporates many different but very flexible architectures that allow to natively model different types of data, e.g. Convolutional Neural Networks for images or Recurrent Neural Networks for time series. However, exploiting the flexibility of DL requires a deeper, more fundamental understanding of the frameworks in which they are implemented. To this end, the course will also cover common DL frameworks such as TensorFlow (and PyTorch) and:"
  },
  {
    "objectID": "index.html#before-the-course",
    "href": "index.html#before-the-course",
    "title": "Machine Learning and Deep Learning with R",
    "section": "Before the course",
    "text": "Before the course\n\nPlease read the following two reviews about Machine Learning in General (Pichler and Hartig 2023) and Deep Learning (Borowiec et al. 2022)\nPlease install all dependencies before the course because it will take some time, see Chapter 1 for installation instructions\nThis course assumes advanced knowledge of the R programming language. If you want to refresh your knowledge about R, you can find a crashcourse in R in the book of the advanced statistic course: R-Crash-Course\n\nAuthors:\nMaximilian Pichler: @_Max_Pichler\nFlorian Hartig: @florianhartig\nContributors:\nJohannes Oberpriller, Matthias Meier\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\n\n\n\n\nBorowiec, Marek L, Rebecca B Dikow, Paul B Frandsen, Alexander McKeeken, Gabriele Valentini, and Alexander E White. 2022. “Deep Learning as a Tool for Ecology and Evolution.” Methods in Ecology and Evolution 13 (8): 1640–60.\n\n\nPichler, Maximilian, and Florian Hartig. 2023. “Machine Learning and Deep Learning—a Review for Ecologists.” Methods in Ecology and Evolution 14 (4): 994–1016."
  },
  {
    "objectID": "A1-GettingStarted.html#r-system",
    "href": "A1-GettingStarted.html#r-system",
    "title": "1  Getting Started",
    "section": "1.1 R System",
    "text": "1.1 R System\nMake sure you have a recent version of R (&gt;=4.0, ideally &gt;=4.2) and RStudio on your computers. For Mac users, if you have already a M1/M2 Mac, please install the R-ARM version (see here (not the x86_64 version))"
  },
  {
    "objectID": "A1-GettingStarted.html#tensorflow-and-keras",
    "href": "A1-GettingStarted.html#tensorflow-and-keras",
    "title": "1  Getting Started",
    "section": "1.2 TensorFlow and Keras",
    "text": "1.2 TensorFlow and Keras\nIf you want to run the code on your own computers, you need to install TensorFlow / Keras for R. For this, the following should work for most people:\n\nWindowsLinuxMacOS-M1/M2MacOS-Intel\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\nIf you have already installed anaconda/miniconda, please uninstall it first!\n\nDownload the ARM miniconda version from here and install it\nOpen the terminal (e.g. by pressing cmd+whitespace and search for ‘terminal’)\nRun in the terminal (not in R!):\n\nconda create -n \"r-reticulate\" python=3.10 \nconda install -c apple tensorflow-deps \npython -m pip install tensorflow-macos \npython -m pip install scipy\n\nOpen R, install the package ‘reticulate’ by running install.packages('reticulate') and run:\n\n\nreticulate::use_condaenv(\"r-reticulate\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf the installation has failed, run the following command:\npython -m pip install --upgrade numpy\n\n\n\n\n\ninstall.packages(\"keras\", dependencies = TRUE)\nkeras::install_keras()\n\n\n\n\nThis should work on most computers, in particular if all software is recent. Sometimes, however, things don’t work well, especially the python distribution often makes problems. If the installation does not work for you, we can look at it together. Also, we will provide some virtual machines in case your computers / laptops are too old or you don’t manage to install TensorFlow."
  },
  {
    "objectID": "A1-GettingStarted.html#torch-for-r",
    "href": "A1-GettingStarted.html#torch-for-r",
    "title": "1  Getting Started",
    "section": "1.3 Torch for R",
    "text": "1.3 Torch for R\nWe may also use Torch for R. This is an R frontend for the popular PyTorch framework. To install Torch, type in R:\n\ninstall.packages(\"torch\")\nlibrary(torch)\ntorch::install_torch()"
  },
  {
    "objectID": "A1-GettingStarted.html#ecodata",
    "href": "A1-GettingStarted.html#ecodata",
    "title": "1  Getting Started",
    "section": "1.4 EcoData",
    "text": "1.4 EcoData\nWe use data sets from the EcoData package. To install the package, run:\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = TRUE, build_vignettes = TRUE)\n\nThe default installation will install a number of packages that are useful for statistics. Especially in Linux, this may take some time to install. If you are in a hurry and only want the data, you can also run\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = FALSE, build_vignettes = FALSE)"
  },
  {
    "objectID": "A1-GettingStarted.html#further-used-libraries",
    "href": "A1-GettingStarted.html#further-used-libraries",
    "title": "1  Getting Started",
    "section": "1.5 Further Used Libraries",
    "text": "1.5 Further Used Libraries\nWe will make huge use of different libraries. So take a coffee or two (that will take a while…) and install the following libraries. Please do this in the given order unless you know what you’re doing, because there are some dependencies between the packages.\n\ninstall.packages(\"abind\")\ninstall.packages(\"animation\")\ninstall.packages(\"ape\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"Rgraphviz\", \"graph\", \"RBGL\"))\ninstall.packages(\"coro\")\ninstall.packages(\"dbscan\")\ninstall.packages(\"dendextend\")\ninstall.packages(\"devtools\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"e1071\")\ninstall.packages(\"factoextra\")\ninstall.packages(\"fields\")\ninstall.packages(\"forcats\")\ninstall.packages(\"glmnet\")\ninstall.packages(\"glmnetUtils\")\ninstall.packages(\"gym\")\ninstall.packages(\"kknn\")\ninstall.packages(\"knitr\")\ninstall.packages(\"iml\")\ninstall.packages(\"lavaan\")\ninstall.packages(\"lmtest\")\ninstall.packages(\"magick\")\ninstall.packages(\"mclust\")\ninstall.packages(\"Metrics\")\ninstall.packages(\"microbenchmark\")\ninstall.packages(\"missRanger\")\ninstall.packages(\"mlbench\")\ninstall.packages(\"mlr3\")\ninstall.packages(\"mlr3learners\")\ninstall.packages(\"mlr3measures\")\ninstall.packages(\"mlr3pipelines\")\ninstall.packages(\"mlr3tuning\")\ninstall.packages(\"paradox\")\ninstall.packages(\"partykit\")\ninstall.packages(\"pcalg\")\ninstall.packages(\"piecewiseSEM\")\ninstall.packages(\"purrr\")\ninstall.packages(\"randomForest\")\ninstall.packages(\"ranger\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"scales\")\ninstall.packages(\"semPlot\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tfprobability\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"torchvision\")\ninstall.packages(\"xgboost\")\ninstall.packages(\"tidymodels\")\n\ndevtools::install_github(\"andrie/deepviz\", dependencies = TRUE,\n                         upgrade = \"always\")\ndevtools::install_github('skinner927/reprtree')\ndevtools::install_version(\"lavaanPlot\", version = \"0.6.0\")\n\nreticulate::conda_install(\"r-reticulate\", packages = \"scipy\", pip = TRUE)\nreticulate::conda_install(\"r-reticulate\", packages = \"tensorflow_probability\", pip = TRUE)"
  },
  {
    "objectID": "A1-GettingStarted.html#linuxunix-systems-have-sometimes-to-fulfill-some-further-dependencies",
    "href": "A1-GettingStarted.html#linuxunix-systems-have-sometimes-to-fulfill-some-further-dependencies",
    "title": "1  Getting Started",
    "section": "1.6 Linux/UNIX systems have sometimes to fulfill some further dependencies",
    "text": "1.6 Linux/UNIX systems have sometimes to fulfill some further dependencies\nDebian based systems\nFor Debian based systems, we need:\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev"
  },
  {
    "objectID": "A1-GettingStarted.html#reminders-about-basic-operations-in-r",
    "href": "A1-GettingStarted.html#reminders-about-basic-operations-in-r",
    "title": "1  Getting Started",
    "section": "1.7 Reminders About Basic Operations in R",
    "text": "1.7 Reminders About Basic Operations in R\nBasic and advanced knowledge of R is required to successfully participate in this course. If you would like to refresh your knowledge of R, you can review the chapter ‘Reminder: R Basics’ from the advanced statistic course."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#overview",
    "href": "A2-MachineLearningTasks.html#overview",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nThere are three types of machine learning tasks:\n\nSupervised learning\nUnsupervised learning\nReinforcement learning\n\nIn supervised learning, you train algorithms using labeled data, what means that you already know the correct answer for a part of the data (the so called training data).\nUnsupervised learning in contrast is a technique, where one does not need to monitor the model or apply labels. Instead, you allow the model to work on its own to discover information.\nReinforcement learning is a technique that emulates a game-like situation. The algorithm finds a solution by trial and error and gets either rewards or penalties for every action. As in games, the goal is to maximize the rewards. We will talk more about this technique on the last day of the course.\nFor the moment, we will focus on the first two tasks, supervised and unsupervised learning. To do so, we will begin with a small example. But before you start with the code, here is a video to prepare you for what we will do in the class:\n\n\n\n2.1.1 Questions\n\nIn ML, predictors (or the explaining variables) are often called features: TRUEFALSE\nIn supervised learning the response (y) and the features (x) are known: TRUEFALSE\nIn unsupervised learning, only the features are known: TRUEFALSE\nIn reinforcement learning an agent (ML model) is trained by interacting with an environment: TRUEFALSE\nHave a look at the two textbooks on ML (Elements of statistical learning and introduction to statistical learning) in our further readings at the end of the GRIPS course - which of the following statements is true?\n\n Both books can be downloaded for free. Higher model complexity is always better for predicting."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#unsupervised-learning",
    "href": "A2-MachineLearningTasks.html#unsupervised-learning",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.2 Unsupervised Learning",
    "text": "2.2 Unsupervised Learning\nIn unsupervised learning, we want to identify patterns in data without having any examples (supervision) about what the correct patterns / classes are. As an example, consider the iris data set. Here, we have 150 observations of 4 floral traits:\n\niris = datasets::iris\ncolors = hcl.colors(3)\ntraits = as.matrix(iris[,1:4])\nspecies = iris$Species\nimage(y = 1:4, x = 1:length(species) , z = traits,\n      ylab = \"Floral trait\", xlab = \"Individual\")\nsegments(50.5, 0, 50.5, 5, col = \"black\", lwd = 2)\nsegments(100.5, 0, 100.5, 5, col = \"black\", lwd = 2)\n\n\n\n\nTrait distributions of iris dataset\n\n\n\n\nThe observations are from 3 species and indeed those species tend to have different traits, meaning that the observations form 3 clusters.\n\npairs(traits, pch = as.integer(species), col = colors[as.integer(species)])\n\n\n\n\nScatterplots for trait-trait combinations.\n\n\n\n\nHowever, imagine we don’t know what species are, what is basically the situation in which people in the antique have been. The people just noted that some plants have different flowers than others, and decided to give them different names. This kind of process is what unsupervised learning does.\n\n2.2.1 Hierarchical Clustering\nA cluster refers to a collection of data points aggregated together because of certain similarities.\nIn hierarchical clustering, a hierarchy (tree) between data points is built.\n\nAgglomerative: Start with each data point in their own cluster, merge them up hierarchically.\nDivisive: Start with all data points in one cluster, and split hierarchically.\n\nMerges / splits are done according to linkage criterion, which measures distance between (potential) clusters. Cut the tree at a certain height to get clusters.\nHere an example\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nd = dist(traits)\nhc = hclust(d, method = \"complete\")\n\nplot(hc, main=\"\")\nrect.hclust(hc, k = 3)  # Draw rectangles around the branches.\n\n\n\n\nResults of hierarchical clustering. Red rectangle is drawn around the corresponding clusters.\n\n\n\n\nSame plot, but with colors for true species identity\n\nlibrary(ape)\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes.\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\n\nCalculate confusion matrix. Note we are switching labels here so that it fits to the species.\n\ntmp = hcRes3\ntmp[hcRes3 == 2] = 3\ntmp[hcRes3 == 3] = 2\nhcRes3 = tmp\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n27\n1\n\n\n0\n23\n49\n\n\n\n\n\nNote that results might change if you choose a different agglomeration method, distance metric or scale of your variables. Compare, e.g. to this example:\n\nhc = hclust(d, method = \"ward.D2\")\n\nplot(as.phylo(hc),\n     tip.color = colors[as.integer(species)],\n     direction = \"downwards\")\n\n\n\n\nResults of hierarchical clustering. Colors correspond to the three species classes. Different agglomeration method\n\n\n\n\n\nhcRes3 = cutree(hc, k = 3)   #Cut a dendrogram tree into groups.\ntable(hcRes3, species)\n\n\n\n\nConfusion matrix for predicted and observed species classes.\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\nWhich method is best? firstsecond\n\nlibrary(dendextend)\n\n\nset.seed(123)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n             \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\nout = dendlist()   # Create a dendlist object from several dendrograms.\nfor(method in methods){\n  res = hclust(d, method = method)\n  out = dendlist(out, as.dendrogram(res))\n}\nnames(out) = methods\nprint(out)\n\n$ward.D\n'dendrogram' with 2 branches and 150 members total, at height 199.6205 \n\n$single\n'dendrogram' with 2 branches and 150 members total, at height 1.640122 \n\n$complete\n'dendrogram' with 2 branches and 150 members total, at height 7.085196 \n\n$average\n'dendrogram' with 2 branches and 150 members total, at height 4.062683 \n\n$mcquitty\n'dendrogram' with 2 branches and 150 members total, at height 4.497283 \n\n$median\n'dendrogram' with 2 branches and 150 members total, at height 2.82744 \n\n$centroid\n'dendrogram' with 2 branches and 150 members total, at height 2.994307 \n\n$ward.D2\n'dendrogram' with 2 branches and 150 members total, at height 32.44761 \n\nattr(,\"class\")\n[1] \"dendlist\"\n\nget_ordered_3_clusters = function(dend){\n  # order.dendrogram function returns the order (index)\n  # or the \"label\" attribute for the leaves.\n  # cutree: Cut the tree (dendrogram) into groups of data.\n  cutree(dend, k = 3)[order.dendrogram(dend)]\n}\ndend_3_clusters = lapply(out, get_ordered_3_clusters)\n\n# Calculate Fowlkes-Mallows Index (determine the similarity between clusterings)\ncompare_clusters_to_iris = function(clus){\n  FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)\n}\n\nclusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\ndotchart(sort(clusters_performance), xlim = c(0.3, 1),\n         xlab = \"Fowlkes-Mallows index\",\n         main = \"Performance of linkage methods\n         in detecting the 3 species \\n in our example\",\n         pch = 19)\n\n\n\n\nWe might conclude that ward.D2 works best here. However, as we will learn later, optimizing the method without a hold-out for testing implies that our model may be overfitting. We should check this using cross-validation.\n\n\n2.2.2 K-means Clustering\nAnother example for an unsupervised learning algorithm is k-means clustering, one of the simplest and most popular unsupervised machine learning algorithms.\nTo start with the algorithm, you first have to specify the number of clusters (for our example the number of species). Each cluster has a centroid, which is the assumed or real location representing the center of the cluster (for our example this would be how an average plant of a specific species would look like). The algorithm starts by randomly putting centroids somewhere. Afterwards each data point is assigned to the respective cluster that raises the overall in-cluster sum of squares (variance) related to the distance to the centroid least of all. After the algorithm has placed all data points into a cluster the centroids get updated. By iterating this procedure until the assignment doesn’t change any longer, the algorithm can find the (locally) optimal centroids and the data points belonging to this cluster. Note that results might differ according to the initial positions of the centroids. Thus several (locally) optimal solutions might be found.\nThe “k” in K-means refers to the number of clusters and the ‘means’ refers to averaging the data-points to find the centroids.\nA typical pipeline for using k-means clustering looks the same as for other algorithms. After having visualized the data, we fit a model, visualize the results and have a look at the performance by use of the confusion matrix. By setting a fixed seed, we can ensure that results are reproducible.\n\nset.seed(123)\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nkc = kmeans(traits, 3)\nprint(kc)\n\nK-means clustering with 3 clusters of sizes 50, 62, 38\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.006000    3.428000     1.462000    0.246000\n2     5.901613    2.748387     4.393548    1.433871\n3     6.850000    3.073684     5.742105    2.071053\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n[112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n[149] 3 2\n\nWithin cluster sum of squares by cluster:\n[1] 15.15100 39.82097 23.87947\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nVisualizing the results. Color codes true species identity, symbol shows cluster result.\n\nplot(iris[c(\"Sepal.Length\", \"Sepal.Width\")],\n     col =  colors[as.integer(species)], pch = kc$cluster)\npoints(kc$centers[, c(\"Sepal.Length\", \"Sepal.Width\")],\n       col = colors, pch = 1:3, cex = 3)\n\n\n\n\nWe see that there are are some discrepancies. Confusion matrix:\n\ntable(iris$Species, kc$cluster)\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 48  2\n  virginica   0 14 36\n\n\nIf you want to animate the clustering process, you could run\n\nlibrary(animation)\n\nsaveGIF(kmeans.ani(x = traits[,1:2], col = colors),\n        interval = 1, ani.width = 800, ani.height = 800)\n\nElbow technique to determine the probably best suited number of clusters:\n\nset.seed(123)\n\ngetSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n\n#Perform algorithm for different cluster sizes and retrieve variance.\niris.kmeans1to10 = sapply(1:10, getSumSq)\nplot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters K\",\n     ylab = \"Total within-clusters sum of squares\",\n     col = c(\"black\", \"red\", rep(\"black\", 8)))\n\n\n\n\nOften, one is interested in sparse models. Furthermore, higher k than necessary tends to overfitting. At the kink in the picture, the sum of squares dropped enough and k is still low enough. But keep in mind, this is only a rule of thumb and might be wrong in some special cases.\n\n\n2.2.3 Density-based Clustering\nDetermine the affinity of a data point according to the affinity of its k nearest neighbors. This is a very general description as there are many ways to do so.\n\n#Reminder: traits = as.matrix(iris[,1:4]).\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nset.seed(123)\n\nkNNdistplot(traits, k = 4)   # Calculate and plot k-nearest-neighbor distances.\nabline(h = 0.4, lty = 2)\n\n\n\ndc = dbscan(traits, eps = 0.4, minPts = 6)\nprint(dc)\n\nDBSCAN clustering for 150 objects.\nParameters: eps = 0.4, minPts = 6\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 4 cluster(s) and 32 noise points.\n\n 0  1  2  3  4 \n32 46 36 14 22 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints\n\n\n\nlibrary(factoextra)\n\n\nfviz_cluster(dc, traits, geom = \"point\", ggtheme = theme_light())\n\n\n\n\n\n\n2.2.4 Model-based Clustering\nThe last class of methods for unsupervised clustering are so-called model-based clustering methods.\n\nlibrary(mclust)\n\nPackage 'mclust' version 6.0.0\nType 'citation(\"mclust\")' for citing this R package in publications.\n\n\n\nmb = Mclust(traits)\n\nMclust automatically compares a number of candidate models (clusters, shape) according to BIC (The BIC is a criterion for classifying algorithms depending their prediction quality and their usage of parameters). We can look at the selected model via:\n\nmb$G # Two clusters.\n\n[1] 2\n\nmb$modelName # &gt; Ellipsoidal, equal shape.\n\n[1] \"VEV\"\n\n\nWe see that the algorithm prefers having 2 clusters. For better comparability to the other 2 methods, we will override this by setting:\n\nmb3 = Mclust(traits, 3)\n\nResult in terms of the predicted densities for 3 clusters\n\nplot(mb3, \"density\")\n\n\n\n\nPredicted clusters:\n\nplot(mb3, what=c(\"classification\"), add = T)\n\n\n\n\nConfusion matrix:\n\ntable(iris$Species, mb3$classification)\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n50\n0\n0\n\n\n0\n49\n15\n\n\n0\n1\n35\n\n\n\n\n\n\n\n2.2.5 Ordination\nOrdination is used in explorative analysis and compared to clustering, similar objects are ordered together. So there is a relationship between clustering and ordination. Here a PCA ordination on on the iris data set.\n\npcTraits = prcomp(traits, center = TRUE, scale. = TRUE)\nbiplot(pcTraits, xlim = c(-0.25, 0.25), ylim = c(-0.25, 0.25))\n\n\n\n\nYou can cluster the results of this ordination, ordinate before clustering, or superimpose one on the other.\n\n\n2.2.6 Exercise\n\n\n\n\n\n\nTask\n\n\n\nGo through the 4(5) algorithms above, and check if they are sensitive (i.e. if results change) if you scale the input features (= predictors), instead of using the raw data. Discuss in your group: Which is more appropriate for this analysis and/or in general: Scaling or not scaling?\n\n\nClick here to see the solution for hierarchical clustering\n\n\nlibrary(dendextend)\n\nmethods = c(\"ward.D\", \"single\", \"complete\", \"average\",\n            \"mcquitty\", \"median\", \"centroid\", \"ward.D2\")\n\ncluster_all_methods = function(distances){\n  out = dendlist()\n  for(method in methods){\n    res = hclust(distances, method = method)\n    out = dendlist(out, as.dendrogram(res))\n  }\n  names(out) = methods\n\n  return(out)\n}\n\nget_ordered_3_clusters = function(dend){\n  return(cutree(dend, k = 3)[order.dendrogram(dend)])\n}\n\ncompare_clusters_to_iris = function(clus){\n  return(FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE))\n}\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n  headline = \"Performance of linkage methods\\nin detecting the 3 species\\n\"\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = paste0(headline, \"Scaled\")\n  }else{ headline = paste0(headline, \"Not scaled\") }\n\n  distances = dist(traits)\n  out = cluster_all_methods(distances)\n  dend_3_clusters = lapply(out, get_ordered_3_clusters)\n  clusters_performance = sapply(dend_3_clusters, compare_clusters_to_iris)\n  dotchart(sort(clusters_performance), xlim = c(0.3,1),\n           xlab = \"Fowlkes-Mallows index\",\n           main = headline,\n           pch = 19)\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\nIt seems that scaling is harmful for hierarchical clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for K-means\n\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){\n    traits = scale(traits)  # Do scaling on copy of traits.\n    headline = \"K-means Clustering\\nScaled\\nSum of all tries: \"\n  }else{ headline = \"K-means Clustering\\nNot scaled\\nSum of all tries: \" }\n\n  getSumSq = function(k){ kmeans(traits, k, nstart = 25)$tot.withinss }\n  iris.kmeans1to10 = sapply(1:10, getSumSq)\n\n  headline = paste0(headline, round(sum(iris.kmeans1to10), 2))\n\n  plot(1:10, iris.kmeans1to10, type = \"b\", pch = 19, frame = FALSE,\n       main = headline,\n       xlab = \"Number of clusters K\",\n       ylab = \"Total within-clusters sum of squares\",\n       col = c(\"black\", \"red\", rep(\"black\", 8)) )\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\n\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\n\n\n\nIt seems that scaling is harmful for K-means clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for density-based clustering\n\n\nlibrary(dbscan)\n\ncorrect = as.factor(iris[,5])\n# Start at 1. Noise points will get 0 later.\nlevels(correct) = 1:length(levels(correct))\ncorrect\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\nLevels: 1 2 3\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  #####\n  # Play around with the parameters \"eps\" and \"minPts\" on your own!\n  #####\n  dc = dbscan(traits, eps = 0.41, minPts = 4)\n\n  labels = as.factor(dc$cluster)\n  noise = sum(dc$cluster == 0)\n  levels(labels) = c(\"noise\", 1:( length(levels(labels)) - 1))\n\n  tbl = table(correct, labels)\n  correct_classified = 0\n  for(i in 1:length(levels(correct))){\n    correct_classified = correct_classified + tbl[i, i + 1]\n  }\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", correct_classified, \" / \", length(iris[,5]))\n  cat(\"\\nSum of noise points: \", noise, \"\\n\")\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     3 47  0  0  0\n      2     5  0 38  3  4\n      3    17  0  0 33  0\n\nCorrect classified points:  118  /  150\nSum of noise points:  25 \n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n       labels\ncorrect noise  1  2  3  4\n      1     9 41  0  0  0\n      2    14  0 36  0  0\n      3    36  0  1  4  9\n\nCorrect classified points:  81  /  150\nSum of noise points:  59 \n\n\nIt seems that scaling is harmful for density based clustering. But this might be a deception. Be careful: If you have data on different units or magnitudes, scaling is definitely useful! Otherwise variables with higher values get higher influence.\n\n\n\nClick here to see the solution for model-based clustering\n\n\nlibrary(mclust)\n\ndo_clustering = function(traits, scale = FALSE){\n  set.seed(123)\n\n  if(scale){ traits = scale(traits) } # Do scaling on copy of traits.\n\n  mb3 = Mclust(traits, 3)\n\n  tbl = table(iris$Species, mb3$classification)\n\n  cat( if(scale){ \"Scaled\" }else{ \"Not scaled\" }, \"\\n\\n\" )\n  cat(\"Confusion matrix:\\n\")\n  print(tbl)\n  cat(\"\\nCorrect classified points: \", sum(diag(tbl)), \" / \", length(iris[,5]))\n}\n\ntraits = as.matrix(iris[,1:4])\n\n# Do clustering on unscaled data.\ndo_clustering(traits, FALSE)\n\nNot scaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n# Do clustering on scaled data.\ndo_clustering(traits, TRUE)\n\nScaled \n\nConfusion matrix:\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 45  5\n  virginica   0  0 50\n\nCorrect classified points:  145  /  150\n\n\nFor model based clustering, scaling does not matter.\n\n\n\nClick here to see the solution for ordination\n\n\ntraits = as.matrix(iris[,1:4])\n\nbiplot(prcomp(traits, center = TRUE, scale. = TRUE),\n       main = \"Use integrated scaling\")\n\n\n\nbiplot(prcomp(scale(traits), center = FALSE, scale. = FALSE),\n       main = \"Scale explicitly\")\n\n\n\nbiplot(prcomp(traits, center = FALSE, scale. = FALSE),\n       main = \"No scaling at all\")\n\n\n\n\nFor PCA ordination, scaling matters. Because we are interested in directions of maximal variance, all parameters should be scaled, or the one with the highest values might dominate all others."
  },
  {
    "objectID": "A2-MachineLearningTasks.html#supervised-learning",
    "href": "A2-MachineLearningTasks.html#supervised-learning",
    "title": "2  Typical Machine Learning Tasks",
    "section": "2.3 Supervised Learning",
    "text": "2.3 Supervised Learning\nThe two most prominent branches of supervised learning are regression and classification. Fundamentally, classification is about predicting a label and regression is about predicting a continuous variable. The following video explains that in more depth:\n\n\n\n2.3.1 Regression\nThe random forest (RF) algorithm is possibly the most widely used machine learning algorithm and can be used for regression and classification. We will talk more about the algorithm later.\nFor the moment, we want to go through a typical workflow for a supervised regression: First, we visualize the data. Next, we fit the model and lastly we visualize the results. We will again use the iris data set that we used before. The goal is now to predict Sepal.Length based on the information about the other variables (including species).\nFitting the model:\n\nlibrary(randomForest)\nset.seed(123)\n\nSepal.Length is a numerical variable:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nhist(iris$Sepal.Length)\n\n\n\n\nThe randomForest can be used similar to a linear regression model, we can specify the features using the formula syntax (~. means that all other variables should be used as features):\n\nm1 = randomForest(Sepal.Length ~ ., data = iris)   # ~.: Against all others.\nprint(m1)\n\n\nCall:\n randomForest(formula = Sepal.Length ~ ., data = iris) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1364625\n                    % Var explained: 79.97\n\n\nAs many other ML algorithms, the RF is not interpretable, so we don’t get coefficients that connect the variables to the response. But, at least we get the variable importance which is similar to an anova, telling us which variables were the most important ones:\n\nvarImpPlot(m1)\n\n\n\n\nAnd the finally, we can use the model to make predictions using the predict method:\n\nplot(predict(m1), iris$Sepal.Length, xlab = \"Predicted\", ylab = \"Observed\")\nabline(0, 1)\n\n\n\n\nTo understand the structure of a random forest in more detail, we can use a package from GitHub.\n\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\nHere, one of the regression trees is shown.\n\n\n2.3.2 Classification\nWith the random forest, we can also do classification. The steps are the same as for regression tasks, but we can additionally see how well it performed by looking at the confusion matrix. Each row of this matrix contains the instances in a predicted class and each column represents the instances in the actual class. Thus the diagonals are the correctly predicted classes and the off-diagonal elements are the falsely classified elements.\nSpecies is a factor with three levels:\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nFitting the model (syntax is the same as for the regression task):\n\nset.seed(123)\nlibrary(randomForest)\nm1 = randomForest(Species ~ ., data = iris)\nprint(m1)\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n\nvarImpPlot(m1)\n\n\n\n\nPredictions:\n\nhead(predict(m1))\n\n     1      2      3      4      5      6 \nsetosa setosa setosa setosa setosa setosa \nLevels: setosa versicolor virginica\n\n\nConfusion matrix:\n\ntable(predict(m1), as.integer(iris$Species))\n\n            \n              1  2  3\n  setosa     50  0  0\n  versicolor  0 47  4\n  virginica   0  3 46\n\n\nOur model made a few errors.\nVisualizing results ecologically:\n\nplot(iris$Petal.Width, iris$Petal.Length, col = iris$Species, main = \"Observed\")\n\n\n\nplot(iris$Petal.Width, iris$Petal.Length, col = predict(m1), main = \"Predicted\")\n\n\n\n\nVisualizing one of the fitted models:\n\nreprtree:::plot.getTree(m1, iris)\n\n\n\n\nConfusion matrix:\n\nknitr::kable(table(predict(m1), iris$Species))\n\n\n\n2.3.3 Exercise\nUsing a random forest on the iris dataset, which parameter would be more important (remember there is a function to check this) to predict Petal.Width?\n\n Species. Sepal.Width.\n\n\n\n\n\n\n\nTask: First deep neural network\n\n\n\nDeep neural networks are currently the state of the art in unsupervised learning. Their ability to model different types of data (e.g. graphs, images) is one of the reasons for their rise in recent years. However, their use beyond tabular data (tabular data == features have specific meanings) requires extensive (programming) knowledge of the underlying deep learning frameworks (e.g. TensorFlow or PyTorch), which we will teach you in two days. For tabular data, we can use packages like cito, which work similarly to regression functions like lm and allow us to train deep neural networks in one line of code.\nA demonstration with the iris dataset:\n\nlibrary(cito)\n\n# always scale your features when using DNNs\niris_scaled = iris\niris_scaled[,1:4] = scale(iris_scaled[,1:4])\n\n# the default architecture is 3 hidden layers, each with 10 hidden nodes (we will talk on Wednesday more about the architecture)\n# Similar to a lm/glm we have to specify the response/loss family, for multi-target (3 species) we use the softmax loss function\nmodel = dnn(Species~., lr = 0.1,data = iris_scaled, loss = \"softmax\")\n\n\n\n\nDNNs are not interpretable, i.e. no coefficients (slopes) that tell us how the features affect the response, however, similar to the RF, we can calculate a ‘variable importance’ which is similar to an anova:\n\nsummary(model)\n\nDeep Neural Network Model summary\nModel generated on basis of: \nFeature Importance:\n      variable importance\n1 Sepal.Length   1.010446\n2  Sepal.Width   1.106239\n3 Petal.Length   1.435946\n4  Petal.Width   1.475746\n\n\nPredictions\n\nhead(predict(model))\n\n        setosa   versicolor    virginica\n[1,] 0.9994506 0.0005492222 1.225359e-07\n[2,] 0.9984201 0.0015795394 4.002443e-07\n[3,] 0.9993551 0.0006447551 1.658843e-07\n[4,] 0.9990097 0.0009900426 2.514017e-07\n[5,] 0.9996005 0.0003993943 8.292547e-08\n[6,] 0.9993541 0.0006458615 1.188908e-07\n\n\nWe get three columns, one for each species, and they are probabilities.\n\nplot(iris$Sepal.Length, iris$Sepal.Width, col = apply(predict(model), 1, which.max))\n\n\n\n\nPerformance:\n\ntable(apply(predict(model), 1, which.max), as.integer(iris$Species))\n\n   \n     1  2  3\n  1 50  0  0\n  2  0 50  7\n  3  0  0 43\n\n\nTask:\n\npredict Sepal.Length instead of Species (classification -&gt; regression)\nUse the ‘mse’ loss function\nPlot predicted vs observed\n\n\n\nClick here to see the solution\n\nRegression:\nlosses such as “mse” (mean squared error) or the “msa” (mean absolute error) are used for regression tasks\n\nmodel = dnn(Sepal.Length~., lr = 0.1,data = iris_scaled, loss = \"mse\")\n\n\n\n\n\nsummary(model)\n\nDeep Neural Network Model summary\nModel generated on basis of: \nFeature Importance:\n      variable importance\n1  Sepal.Width  1.3924243\n2 Petal.Length  9.7724334\n3  Petal.Width  0.9492258\n4      Species  1.4818581\n\n\n\nplot(iris_scaled$Sepal.Length, predict(model))\n\n\n\n\nCalculate \\(R^2\\):\n\ncor(iris_scaled$Sepal.Length, predict(model))**2\n\n          [,1]\n[1,] 0.8769866"
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#understanding-the-bias-variance-trade-off",
    "href": "A3-BiasVarianceTradeOff.html#understanding-the-bias-variance-trade-off",
    "title": "3  Bias-variance trade-off",
    "section": "3.1 Understanding the bias-variance trade-off",
    "text": "3.1 Understanding the bias-variance trade-off\n\n\nWhich of the following statements about the bias-variance trade-off is correct? (see figure above)\n\n The goal of considering the bias-variance trade-off is to realize that increasing complexity typically leads to more flexibility (allowing you to reduce bias) but at the cost of uncertainty (variance) in the estimated parameters. The goal of considering the bias-variance trade-off is to get the bias of the model as small as possible."
  },
  {
    "objectID": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "href": "A3-BiasVarianceTradeOff.html#optimizing-the-bias-variance-trade-off",
    "title": "3  Bias-variance trade-off",
    "section": "3.2 Optimizing the bias-variance trade-off",
    "text": "3.2 Optimizing the bias-variance trade-off\nOptimizing the bias-variance trade-off means adjusting the complexity of the model which can be achieved by:\n\nFeature selection (more features increases the flexibility of the model)\nRegularization\n\n\n3.2.1 Feature selection\nAdding features increases the flexibility of the model and the goodness of fit:\n\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata(BostonHousing)\ndata = BostonHousing\n\nsummary(lm(medv~rm, data = data))\n\n\nCall:\nlm(formula = medv ~ rm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -34.671      2.650  -13.08   &lt;2e-16 ***\nrm             9.102      0.419   21.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,    Adjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~rm+dis, data = data))$r.squared\n\n[1] 0.4955246\n\nsummary(lm(medv~., data = data))$r.squared\n\n[1] 0.7406427\n\n# Main effects + all potential interactions:\nsummary(lm(medv~.^2, data = data))$r.squared\n\n[1] 0.9211876\n\n\nThe model with all features and their potential interactions has the highest \\(R^2\\), but it also has the highest uncertainty because there are on average only 5 observations for each parameter (92 parameters and 506 observations). So how do we decide which level of complexity is appropriate for our task? For the data we use to train the model, \\(R^2\\) will always get better with higher model complexity, so it is a poor decision criterion. We will show this in the Section 3.2.4 section. In short, the idea is that we need to split the data so that we have an evaluation (test) dataset that wasn’t used to train the model, which we can then use in turn to see if our model generalizes well to new data.\n\n\n3.2.2 Regularization\nRegularization means adding information or structure to a system in order to solve an ill-posed optimization problem or to prevent overfitting. There are many ways of regularizing a machine learning model. The most important distinction is between shrinkage estimators and estimators based on model averaging.\nShrikage estimators are based on the idea of adding a penalty to the loss function that penalizes deviations of the model parameters from a particular value (typically 0). In this way, estimates are “shrunk” to the specified default value. In practice, the most important penalties are the least absolute shrinkage and selection operator; also Lasso or LASSO, where the penalty is proportional to the sum of absolute deviations (\\(L1\\) penalty), and the Tikhonov regularization aka Ridge regression, where the penalty is proportional to the sum of squared distances from the reference (\\(L2\\) penalty). Thus, the loss function that we optimize is given by\n\\[\nloss = fit - \\lambda \\cdot d\n\\]\nwhere fit refers to the standard loss function, \\(\\lambda\\) is the strength of the regularization, and \\(d\\) is the chosen metric, e.g. \\(L1\\) or\\(L2\\):\n\\[\nloss_{L1} = fit - \\lambda \\cdot \\Vert weights \\Vert_1\n\\]\n\\[\nloss_{L2} = fit - \\lambda \\cdot \\Vert weights \\Vert_2\n\\]\n\\(\\lambda\\) and possibly d are typically optimized under cross-validation. \\(L1\\) and \\(L2\\) can be also combined what is then called elastic net (see Zou and Hastie (2005)).\nModel averaging refers to an entire set of techniques, including boosting, bagging and other averaging techniques. The general principle is that predictions are made by combining (= averaging) several models. This is based on on the insight that it is often more efficient having many simpler models and average them, than one “super model”. The reasons are complicated, and explained in more detail in Dormann et al. (2018).\nA particular important application of averaging is boosting, where the idea is that many weak learners are combined to a model average, resulting in a strong learner. Another related method is bootstrap aggregating, also called bagging. Idea here is to boostrap (use random sampling with replacement ) the data, and average the bootstrapped predictions.\nTo see how these techniques work in practice, let’s first focus on LASSO and Ridge regularization for weights in neural networks. We can imagine that the LASSO and Ridge act similar to a rubber band on the weights that pulls them to zero if the data does not strongly push them away from zero. This leads to important weights, which are supported by the data, being estimated as different from zero, whereas unimportant model structures are reduced (shrunken) to zero.\nLASSO \\(\\left(penalty \\propto \\sum_{}^{} \\mathrm{abs}(weights) \\right)\\) and Ridge \\(\\left(penalty \\propto \\sum_{}^{} weights^{2} \\right)\\) have slightly different properties. They are best understood if we express those as the effective prior preference they create on the parameters:\n\n\n\n\n\nAs you can see, the LASSO creates a very strong preference towards exactly zero, but falls off less strongly towards the tails. This means that parameters tend to be estimated either to exactly zero, or, if not, they are more free than the Ridge. For this reason, LASSO is often more interpreted as a model selection method.\nThe Ridge, on the other hand, has a certain area around zero where it is relatively indifferent about deviations from zero, thus rarely leading to exactly zero values. However, it will create a stronger shrinkage for values that deviate significantly from zero.\n\n3.2.2.1 Ridge - Example\nWe can use the glmnet package for Ridge, LASSO, and elastic-net regressions.\nWe want to predict the house prices of Boston (see help of the dataset):\n\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %&gt;% select(-medv, -chas) %&gt;% scale()\n\nhist(cor(X))\n\n\n\n\n\nm1 = glmnet(y = Y, x = X, alpha = 0)\n\nThe glmnet function automatically tries different values for lambda:\n\ncbind(coef(m1, s = 0.001), coef(m1, s = 100.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1          s1\n(Intercept) 22.53280632 22.53280632\ncrim        -0.79174957 -0.21113427\nzn           0.76313031  0.18846808\nindus       -0.17037817 -0.25120998\nnox         -1.32794787 -0.21314250\nrm           2.85780876  0.46463202\nage         -0.05389395 -0.18279762\ndis         -2.38716188  0.07906631\nrad          1.42772476 -0.17967948\ntax         -1.09026758 -0.24233282\nptratio     -1.93105019 -0.31587466\nb            0.86718037  0.18764060\nlstat       -3.43236617 -0.46055837\n\n\n\n\n3.2.2.2 LASSO - Example\nBy changing \\(alpha\\) to 1.0 we use a LASSO instead of a Ridge regression:\n\nm2 = glmnet(y = Y, x = X, alpha = 1.0)\ncbind(coef(m2, s = 0.001), coef(m2, s = 0.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1           s1\n(Intercept) 22.53280632 22.532806324\ncrim        -0.95543108 -0.135047323\nzn           1.06718108  .          \nindus        0.21519500  .          \nnox         -1.95945910 -0.000537715\nrm           2.71666891  2.998520195\nage          0.05184895  .          \ndis         -3.10566908 -0.244045205\nrad          2.73963771  .          \ntax         -2.20279273  .          \nptratio     -2.13052857 -1.644234575\nb            0.88420283  0.561686909\nlstat       -3.80177809 -3.682148016\n\n\n\n\n3.2.2.3 Elastic-net - Example\nBy setting \\(alpha\\) to a value between 0 and 1.0, we use a combination of LASSO and Rdige:\n\nm3 = glmnet(y = Y, x = X, alpha = 0.5)\ncbind(coef(m3, s = 0.001), coef(m3, s = 0.5))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s1         s1\n(Intercept) 22.53280632 22.5328063\ncrim        -0.95716118 -0.3488473\nzn           1.06836343  0.1995842\nindus        0.21825187  .        \nnox         -1.96211736 -0.7613698\nrm           2.71859592  3.0137090\nage          0.05299551  .        \ndis         -3.10330132 -1.3011740\nrad          2.73321635  .        \ntax         -2.19638611  .        \nptratio     -2.13041090 -1.8051547\nb            0.88458269  0.6897165\nlstat       -3.79836182 -3.6136853\n\n\n\n\n\n3.2.3 Hyperparameters\nGenerally, parameters such as \\(\\lambda\\) and \\(\\alpha\\) that, for example, control the complexity or other parameters that control their learning or the optimization are called hyperparameters. Comming back to our glmnet example:\nWe can plot the effect of \\(\\lambda\\) on the effect estimates:\n\nplot(m1)\n\n\n\n\nSo which lambda should we choose now? If we calculate the model fit for different lambdas (e.g. using the RMSE):\n\nlambdas = seq(0.001, 1.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = X, s = l)\n    RMSE = Metrics::rmse(Y, prediction)\n    return(RMSE)\n    })\nplot(lambdas, RMSEs)\n\n\n\n\nWe see that the lowest lambda achieved the highest RMSE - which is not surprising because the unconstrained model, the most complex model, has the highest fit, so no bias but probably high variance (with respect to the bias-variance tradeoff).\n\n3.2.3.1 Split data into training and testing\nWe want a model that generalizes well to new data, which we need to “simulate” here by splitting of a holdout before the training and using the holdout then for testing our model:\n\nset.seed(1)\nlibrary(mlbench)\nlibrary(dplyr)\ndata(BostonHousing)\ndata = BostonHousing\nY = data$medv\nX = data %&gt;% select(-medv, -chas) %&gt;% scale()\n\n# Split data\nindices = sample.int(nrow(X), 0.2*nrow(X))\ntrain_X = X[indices,]\ntest_X = X[-indices,]\ntrain_Y = Y[indices]\ntest_Y = Y[-indices]\n\n# Train model on train data\nm1 = glmnet(y = train_Y, x = train_X, alpha = 0.5)\n\n# Test model on test data\npred = predict(m1, newx = test_X, s = 0.01)\n\n# Calculate performance on test data\nMetrics::rmse(test_Y, pred)\n\n[1] 5.063774\n\n\nLet’s do it again for different values of lambdas:\n\nlambdas = seq(0.0000001, 0.5, length.out = 100)\nRMSEs = \n  sapply(lambdas, function(l) {\n    prediction = predict(m1, newx = test_X, s = l)\n    return(Metrics::rmse(test_Y, prediction))\n    })\nplot(lambdas, RMSEs, xlab = \"Lambda\", ylab = \"RMSE\", type = \"l\", las = 2)\nabline(v = lambdas[which.min(RMSEs)], col = \"red\", lwd = 1.5)\n\n\n\n\nHyperparameter tuning describes the process of finding the optimal set of hyperparameters for a certain task. They are usually data specific, so they have to tuned for each dataset.\nIf we do only one split it could happen that we only find a set of hyperparameters that are best suited for this specific split and thus we usally do several splits so that each observation is once an observation in the test dataset, cross-validation\n\n\n\n3.2.4 Cross-validation\nThe cv.glmnet function does per default a 5xCV (so 5 splits) and in each split different values for \\(\\lambda\\) are tested\n\nm1 = glmnet::cv.glmnet(x = X, y = Y, alpha = 0.5, nfolds = 5)\nm1\n\n\nCall:  glmnet::cv.glmnet(x = X, y = Y, nfolds = 5, alpha = 0.5) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure    SE Nonzero\nmin 0.0105    78   23.80 3.247      12\n1se 0.6905    33   26.88 4.014       8\n\nplot(m1)\n\n\n\nm1$lambda.min\n\n[1] 0.01049538\n\n\nSo low values of \\(\\lambda\\) seem to achieve the lowest error, thus the higehst predictive performance.\nThis is called hyperparameter tuning.\n\n\n\n\nDormann, Carsten F, Justin M Calabrese, Gurutzeta Guillera-Arroita, Eleni Matechou, Volker Bahn, Kamil Bartoń, Colin M Beale, et al. 2018. “Model Averaging in Ecology: A Review of Bayesian, Information-Theoretic, and Tactical Approaches for Predictive Inference.” Ecological Monographs 88 (4): 485–504.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20."
  },
  {
    "objectID": "A4-MLpipeline.html#data-preparation",
    "href": "A4-MLpipeline.html#data-preparation",
    "title": "4  Machine learning pipeline",
    "section": "4.1 Data preparation",
    "text": "4.1 Data preparation\nLoad necessary libraries:\n\nlibrary(tidyverse)\n\nLoad data set:\n\nlibrary(EcoData)\ndata(titanic_ml)\ndata = titanic_ml\n\nStandard summaries:\n\nstr(data)\n\n'data.frame':   1309 obs. of  14 variables:\n $ pclass   : int  2 1 3 3 3 3 3 1 3 1 ...\n $ survived : int  1 1 0 0 0 0 0 1 0 1 ...\n $ name     : chr  \"Sinkkonen, Miss. Anna\" \"Woolner, Mr. Hugh\" \"Sage, Mr. Douglas Bullen\" \"Palsson, Master. Paul Folke\" ...\n $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 1 1 1 ...\n $ age      : num  30 NA NA 6 30.5 38.5 20 53 NA 42 ...\n $ sibsp    : int  0 0 8 3 0 0 0 0 0 0 ...\n $ parch    : int  0 0 2 1 0 0 0 0 0 0 ...\n $ ticket   : Factor w/ 929 levels \"110152\",\"110413\",..: 221 123 779 542 589 873 472 823 588 834 ...\n $ fare     : num  13 35.5 69.55 21.07 8.05 ...\n $ cabin    : Factor w/ 187 levels \"\",\"A10\",\"A11\",..: 1 94 1 1 1 1 1 1 1 1 ...\n $ embarked : Factor w/ 4 levels \"\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 4 2 4 2 ...\n $ boat     : Factor w/ 28 levels \"\",\"1\",\"10\",\"11\",..: 3 28 1 1 1 1 1 19 1 15 ...\n $ body     : int  NA NA NA NA 50 32 NA NA NA NA ...\n $ home.dest: Factor w/ 370 levels \"\",\"?Havana, Cuba\",..: 121 213 1 1 1 1 322 350 1 1 ...\n\nsummary(data)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n\n\nThe name variable consists of 1309 unique factors (there are 1309 observations…) and could be now transformed. If you are interested in how to do that, take a look at the following box.\n\n\n\n\n\n\nFeature engineering of the name variable\n\n\n\n\n\n\nlength(unique(data$name))\n\n[1] 1307\n\n\nHowever, there is a title in each name. Let’s extract the titles:\n\nWe will extract all names and split each name after each comma “,”.\nWe will split the second split of the name after a point “.” and extract the titles.\n\n\nfirst_split = sapply(data$name,\n                     function(x) stringr::str_split(x, pattern = \",\")[[1]][2])\ntitles = sapply(first_split,\n                function(x) strsplit(x, \".\",fixed = TRUE)[[1]][1])\n\nWe get 18 unique titles:\n\ntable(titles)\n\ntitles\n         Capt           Col           Don          Dona            Dr \n            1             4             1             1             8 \n     Jonkheer          Lady         Major        Master          Miss \n            1             1             2            61           260 \n         Mlle           Mme            Mr           Mrs            Ms \n            2             1           757           197             2 \n          Rev           Sir  the Countess \n            8             1             1 \n\n\nA few titles have a very low occurrence rate:\n\ntitles = stringr::str_trim((titles))\ntitles %&gt;%\n fct_count()\n\n# A tibble: 18 × 2\n   f                n\n   &lt;fct&gt;        &lt;int&gt;\n 1 Capt             1\n 2 Col              4\n 3 Don              1\n 4 Dona             1\n 5 Dr               8\n 6 Jonkheer         1\n 7 Lady             1\n 8 Major            2\n 9 Master          61\n10 Miss           260\n11 Mlle             2\n12 Mme              1\n13 Mr             757\n14 Mrs            197\n15 Ms               2\n16 Rev              8\n17 Sir              1\n18 the Countess     1\n\n\nWe will combine titles with low occurrences into one title, which we can easily do with the forcats package.\n\ntitles2 =\n  forcats::fct_collapse(titles,\n                        officer = c(\"Capt\", \"Col\", \"Major\", \"Dr\", \"Rev\"),\n                        royal = c(\"Jonkheer\", \"Don\", \"Sir\",\n                                  \"the Countess\", \"Dona\", \"Lady\"),\n                        miss = c(\"Miss\", \"Mlle\"),\n                        mrs = c(\"Mrs\", \"Mme\", \"Ms\")\n                        )\n\nWe can count titles again to see the new number of titles:\n\ntitles2 %&gt;%  \n   fct_count()\n\n# A tibble: 6 × 2\n  f           n\n  &lt;fct&gt;   &lt;int&gt;\n1 officer    23\n2 royal       6\n3 Master     61\n4 miss      262\n5 mrs       200\n6 Mr        757\n\n\nAdd new title variable to data set:\n\ndata =\n  data %&gt;%\n    mutate(title = titles2)\n\n\n\n\n\n4.1.1 Imputation\nNAs are a common problem in ML. For example, the age variable has 20% NAs:\n\nsummary(data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1667 21.0000 28.0000 29.8811 39.0000 80.0000     263 \n\nsum(is.na(data$age)) / nrow(data)\n\n[1] 0.2009167\n\n\nEither we remove all observations with NAs, or we impute (fill) the missing values, e.g. with the median age. However, age itself might depend on other variables such as sex, class and title. We want to fill the NAs with the median age of these groups. In tidyverse we can easily “group” the data, i.e. we will nest the observations (here: group_by after sex, pclass and title). After grouping, all operations (such as our median(age….)) will be done within the specified groups.\n\ndata =\n  data %&gt;%\n    select(survived, sex, age, fare, pclass) %&gt;% \n    group_by(sex, pclass) %&gt;%\n    mutate(age2 = ifelse(is.na(age), median(age, na.rm = TRUE), age)) %&gt;%\n    mutate(fare2 = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare)) %&gt;%\n    ungroup()\n\n\n\n4.1.2 Preprocessing and Feature Selection\nLater (tomorrow), we want to use Keras in our example, but it cannot handle factors and requires the data to be scaled.\nNormally, one would do this for all predictors, but as we only show the pipeline here, we have sub-selected a bunch of predictors and do this only for them. We first scale the numeric predictors and change the factors with only two groups/levels into integers (this can be handled by Keras).\n\ndata_sub =\n  data %&gt;%\n    select(survived, sex, age2, fare2, pclass) %&gt;%\n    mutate(age2 = scales::rescale(age2, c(0, 1)),\n           fare2 = scales::rescale(fare2, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\n\n\n\n\n\n\n\nTransforming factors with more than two levels\n\n\n\n\n\nFactors with more than two levels should be one hot encoded (Make columns for every different factor level and write 1 in the respective column for every taken feature value and 0 else. For example: \\(\\{red, green, green, blue, red\\} \\rightarrow \\{(0,0,1), (0,1,0), (0,1,0), (1,0,0), (0,0,1)\\}\\)):\n\none_title = model.matrix(~0+as.factor(title), data = data)\ncolnames(one_title) = levels(data$title)\n\none_sex = model.matrix(~0+as.factor(sex), data = data)\ncolnames(one_sex) = levels(data$sex)\n\none_pclass = model.matrix(~0+as.factor(pclass), data = data)\ncolnames(one_pclass) = paste0(\"pclass\", 1:length(unique(data$pclass)))\n\nAnd we have to add the dummy encoded variables to the data set:\n\ndata = cbind(data.frame(survived= data$survived),\n                 one_title, one_sex, age = data$age2,\n                 fare = data$fare2, one_pclass)\nhead(data)"
  },
  {
    "objectID": "A4-MLpipeline.html#modelling",
    "href": "A4-MLpipeline.html#modelling",
    "title": "4  Machine learning pipeline",
    "section": "4.2 Modelling",
    "text": "4.2 Modelling\n\n4.2.1 Split data for final predictions\nTo tune our hyperparameters and evaluate our models, we need to split the data as we learned in the CV section. Before doing so, however, we must split off the new observations in the data set :\n\nsummary(data_sub$survived)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.0000  0.0000  0.3853  1.0000  1.0000     655 \n\n\n655 observations have NAs in our response variable, these are the observations for which we want to make predictions at the end of our pipeline (we have no information about their actual values!).\n\ndata_new = data_sub[is.na(data_sub$survived),]\ndata_obs = data_sub[!is.na(data_sub$survived),]\n\n\n\n4.2.2 Training and evaluation\nNow, we can do a simple 10xCV with the observed_data:\n\nlibrary(glmnet)\nlibrary(glmnetUtils)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 10\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = 1:cv,\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = 0.2)\n  results[i, 2] = Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                            alpha = 0.2,\n                                                            s = 0.01,\n                                                            type = \"response\"))\n\n}\n\n\nprint(results)\n\n   set       AUC\n1    1 0.8650794\n2    2 0.8723197\n3    3 0.7632114\n4    4 0.7541463\n5    5 0.8984221\n6    6 0.7200000\n7    7 0.7829268\n8    8 0.8701737\n9    9 0.8787879\n10  10 0.7867647\n\nprint(mean(results$AUC))\n\n[1] 0.8191832\n\n\n\n\n4.2.3 Hyperparameter optimization\nWe did a simple 10xCV to evaluate our model but we didn’t tune our hyperparameters (\\(\\lambda\\) and \\(\\alpha\\)). If we want to tune them, we need do another CV within each split of the model evaulation CV, which is called nested CV.\nWe used only one split (the split for the submission server doesn’t count) to evaluate the performance of the model before we made the final predictions. If we test many different hyperparameter combinations, how do we ensure that a certain hyperparameter is not only good for our training dataset but also good for the new data (our outer split on the submission server)? You may have guessed it already, we need to do another CV within the previous CV to check whether a certain hyperparameter solution generalizes to the whole data. To tune \\(\\lambda\\), we would need to split the data another time (called nested CV).\nWhy is it important to tune hyperparameters? Hyperparameters (configuration parameters of our ML algorithms that (mostly) control their complexity) are usually tuned (optimized) in an automatic / systematic way. A common procedure, called random search, is to sample random configuration combinations from the set of hyperparameters and test for each combination the prediction error.\nLet’s implement manually a nested CV to tune the \\(\\alpha\\). Let’s start with a 5CVx5CV and 20x different alpha values:\n\nlibrary(glmnet)\nlibrary(glmnetUtils)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(20,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  best_alpha = NULL\n  best_auc = NULL\n  \n  # inner split\n  for(j in 1:cv_inner) {\n    inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n    train_inner = train_outer[inner_split != j, ]\n    test_inner = train_outer[inner_split == j, ]\n    \n    tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = 0.01,\n                                                         type = \"response\")))\n      })\n    best_alpha[j] = hyper_alpha[which.max(tuning_results_inner)]\n    best_auc[j] = max(tuning_results_inner)\n  }\n  best_alpha = best_alpha[which.max(best_auc)]\n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = 0.01, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n\n  set     alpha       AUC\n1   1 0.1346666 0.8720588\n2   2 0.4577418 0.7554754\n3   3 0.9148060 0.8178054\n4   4 0.2861395 0.8238994\n5   5 0.9148060 0.8156749"
  },
  {
    "objectID": "A4-MLpipeline.html#predictions-and-submission",
    "href": "A4-MLpipeline.html#predictions-and-submission",
    "title": "4  Machine learning pipeline",
    "section": "4.3 Predictions and Submission",
    "text": "4.3 Predictions and Submission\nWhen we are satisfied with the performance of our model, we will create predictions for the new observations on the submission server. But first we will train now our model on the full observed dataset:\n\\(\\alpha = 0.915\\) has the highest AUC, let’s use it to train the model on the full dataset:\n\nmodel = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = 0.915)\n\nWe cannot assess the performance for the new observations because the true survival ratio is unknown, however, we can now submit our predictions to the submission server at http://rhsbio7.uni-regensburg.de:8500.\nFor the submission it is critical to change the predictions into a data.frame, select the second column (the probability to survive), and save it with the write.csv function:\n\ndata_new = data_sub[is.na(data_sub$survived),]\nwrite.csv(data.frame(y = predict(model, data_new, alpha = 0.915, s = 0.01, type = \"response\")[,1] ), file = \"Max_1.csv\")\n\nWe have now used the \\(\\alpha\\) value with the highest AUC here, but our tuning has shown that the best value of \\(\\alpha\\) depends on the partitioning, so it would probably be better to build ten models and combine their predictions (e.g., by averaging the predictions):\n\nprediction_ensemble = \n  sapply(results$alpha, function(alpha) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = alpha)\n    return(predict(model, data_new, alpha = alpha, s = 0.01, type = \"response\")[,1])\n  })\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_1.csv\")"
  },
  {
    "objectID": "A4-MLpipeline.html#exercises",
    "href": "A4-MLpipeline.html#exercises",
    "title": "4  Machine learning pipeline",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\n\n\n\n\n\nTask: Tuning \\(\\alpha\\) and \\(\\lambda\\)\n\n\n\n\nExtend the code from above and tune \\(\\alpha\\) and \\(\\lambda\\) (Nested-CV or via a simple CV)\nTrain the model with best set of hyperparameters and submit your predictions\nCompare the predictive performance from the single best model with the ensemble model\n\nSubmit both predictions (http://rhsbio7.uni-regensburg.de:8500/), which model has a higher AUC?\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\nlibrary(glmnet)\nlibrary(glmnetUtils)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\n\n# missRanger uses a random forest to impute NAs (RF is trained on the data to predict values for the NAs)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\nBonus:\n\nTry different features\nTry cito\nTry different datasets (see Appendix A)\n\nCode template for a simple CV (only \\(\\alpha\\) is tuned, add the tuning for \\(\\lambda\\):\n\nlibrary(glmnet)\nlibrary(glmnetUtils)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\nhyper_alpha = runif(20,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                         alpha = hyper_alpha[k],\n                                                         s = 0.01,\n                                                         type = \"response\")))\n      })\n  best_alpha = hyper_alpha[which.max(tuning_results)]\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n\n  set     alpha       AUC\n1   1 0.2861395 0.8745098\n2   2 0.8304476 0.7569686\n3   3 0.8304476 0.8206522\n4   4 0.1346666 0.8270440\n5   5 0.6569923 0.8144654\n\n\n\n\nClick here to see the solution for the single model\n\nNested CV:\n\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\ncv_inner = 5\nhyper_alpha = runif(30,0, 1)\nhyper_lambda = runif(30,0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  best_alpha = NULL\n  best_lambda = NULL\n  best_auc = NULL\n  # inner split\n  for(j in 1:cv_inner) {\n    inner_split = as.integer(cut(1:nrow(train_outer), breaks = cv_inner))\n    train_inner = train_outer[inner_split != j, ]\n    test_inner = train_outer[inner_split == j, ]\n    \n    tuning_results_inner = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_inner, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_inner$survived, predict(model, test_inner, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\")))\n      })\n    best_alpha[j] = hyper_alpha[which.max(tuning_results_inner)]\n    best_lambda[j] = hyper_lambda[which.max(tuning_results_inner)]\n    best_auc[j] = max(tuning_results_inner)\n  }\n  best_alpha = best_alpha[which.max(best_auc)]\n  best_lambda = best_lambda[which.max(best_auc)]\n  \n  model = glmnet(survived~., data = train_outer, alpha = best_alpha, family = \"binomial\")\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = best_lambda\n  results[i, 4] = Metrics::auc(test_outer$survived, predict(model, test_outer, s = best_lambda, alpha = best_alpha, type = \"response\"))\n}\n\nprint(results)\n\n  set     alpha      lambda       AUC\n1   1 0.6417455 0.003948339 0.8745098\n2   2 0.7365883 0.007334147 0.7557242\n3   3 0.7365883 0.007334147 0.8193582\n4   4 0.6417455 0.003948339 0.8253507\n5   5 0.6417455 0.003948339 0.8132559\n\n\nSimple CV:\n\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 5\nhyper_alpha = runif(20,0, 1)\nhyper_lambda = runif(20, 0, 1)\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nresults = data.frame(\n  set = rep(NA, cv),\n  alpha = rep(NA, cv),\n  lambda = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_alpha), function(k) {\n        model = glmnet(survived~.,data = train_outer, family = \"binomial\",alpha = hyper_alpha[k])\n        return(Metrics::auc(test_outer$survived, predict(model, test_outer, \n                                                         alpha = hyper_alpha[k],\n                                                         s = hyper_lambda[k],\n                                                         type = \"response\")))\n      })\n  best_alpha = hyper_alpha[which.max(tuning_results)]\n  best_lambda = hyper_lambda[which.max(tuning_results)]\n  results[i, 1] = i\n  results[i, 2] = best_alpha\n  results[i, 3] = best_lambda\n  results[i, 4] = max(tuning_results)\n}\n\nprint(results)\n\n  set     alpha      lambda       AUC\n1   1 0.4622928 0.003948339 0.8740196\n2   2 0.4622928 0.003948339 0.7567198\n3   3 0.1174874 0.207658973 0.8237578\n4   4 0.4622928 0.003948339 0.8255926\n5   5 0.9782264 0.007334147 0.8144654\n\n\nPredictions:\n\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n    model = glmnet(survived~.,data = data_obs, family = \"binomial\",alpha = results$alpha[i])\n    return(predict(model, data_new, alpha = results$alpha[i], s = results$lambda[i], type = \"response\")[,1])\n  })\n\n# Single predictions from the model with the highest AUC:\nwrite.csv(data.frame(y = prediction_ensemble[,which.max(results$AUC)]), file = \"Max_titanic_best_model.csv\")\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")"
  },
  {
    "objectID": "A4-MLpipeline.html#machine-learning-frameworks",
    "href": "A4-MLpipeline.html#machine-learning-frameworks",
    "title": "4  Machine learning pipeline",
    "section": "4.5 Machine learning frameworks",
    "text": "4.5 Machine learning frameworks\nAs we have seen today, many of the machine learning algorithms are distributed over several packages but the general machine learning pipeline is very similar for all models: feature engineering, feature selection, hyperparameter tuning and cross-validation.\nMachine learning frameworks such as mlr3 or tidymodels provide a general interface for the ML pipeline, in particular the training and the hyperparameter tuning with nested CV. They support most ML packages/algorithms.\n\n4.5.1 mlr3\nThe key features of mlr3 are:\n\nAll common machine learning packages are integrated into mlr3, you can easily switch between different machine learning algorithms.\nA common ‘language’/workflow to specify machine learning pipelines.\nSupport for different cross-validation strategies.\nHyperparameter tuning for all supported machine learning algorithms.\nEnsemble models.\n\nUseful links:\n\nmlr3-book (still in work)\nmlr3 website\nmlr3 cheatsheet\n\n\n4.5.1.1 mlr3 - The Basic Workflow\nThe mlr3 package actually consists of several packages for different tasks (e.g. mlr3tuning for hyperparameter tuning, mlr3pipelines for data preparation pipes). But let’s start with the basic workflow:\n\nlibrary(EcoData)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\nlibrary(mlr3measures)\ndata(nasa)\nstr(nasa)\n\n'data.frame':   4687 obs. of  40 variables:\n $ Neo.Reference.ID            : int  3449084 3702322 3406893 NA 2363305 3017307 2438430 3653917 3519490 2066391 ...\n $ Name                        : int  NA 3702322 3406893 3082923 2363305 3017307 2438430 3653917 3519490 NA ...\n $ Absolute.Magnitude          : num  18.7 22.1 24.8 21.6 21.4 18.2 20 21 20.9 16.5 ...\n $ Est.Dia.in.KM.min.          : num  0.4837 0.1011 0.0291 0.1272 0.1395 ...\n $ Est.Dia.in.KM.max.          : num  1.0815 0.226 0.0652 0.2845 0.3119 ...\n $ Est.Dia.in.M.min.           : num  483.7 NA 29.1 127.2 139.5 ...\n $ Est.Dia.in.M.max.           : num  1081.5 226 65.2 284.5 311.9 ...\n $ Est.Dia.in.Miles.min.       : num  0.3005 0.0628 NA 0.0791 0.0867 ...\n $ Est.Dia.in.Miles.max.       : num  0.672 0.1404 0.0405 0.1768 0.1938 ...\n $ Est.Dia.in.Feet.min.        : num  1586.9 331.5 95.6 417.4 457.7 ...\n $ Est.Dia.in.Feet.max.        : num  3548 741 214 933 1023 ...\n $ Close.Approach.Date         : Factor w/ 777 levels \"1995-01-01\",\"1995-01-08\",..: 511 712 472 239 273 145 428 694 87 732 ...\n $ Epoch.Date.Close.Approach   : num  NA 1.42e+12 1.21e+12 1.00e+12 1.03e+12 ...\n $ Relative.Velocity.km.per.sec: num  11.22 13.57 5.75 13.84 4.61 ...\n $ Relative.Velocity.km.per.hr : num  40404 48867 20718 49821 16583 ...\n $ Miles.per.hour              : num  25105 30364 12873 30957 10304 ...\n $ Miss.Dist..Astronomical.    : num  NA 0.0671 0.013 0.0583 0.0381 ...\n $ Miss.Dist..lunar.           : num  112.7 26.1 NA 22.7 14.8 ...\n $ Miss.Dist..kilometers.      : num  43348668 10030753 1949933 NA 5694558 ...\n $ Miss.Dist..miles.           : num  26935614 6232821 1211632 5418692 3538434 ...\n $ Orbiting.Body               : Factor w/ 1 level \"Earth\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Orbit.ID                    : int  NA 8 12 12 91 NA 24 NA NA 212 ...\n $ Orbit.Determination.Date    : Factor w/ 2680 levels \"2014-06-13 15:20:44\",..: 69 NA 1377 1774 2275 2554 1919 731 1178 2520 ...\n $ Orbit.Uncertainity          : int  0 8 6 0 0 0 1 1 1 0 ...\n $ Minimum.Orbit.Intersection  : num  NA 0.05594 0.00553 NA 0.0281 ...\n $ Jupiter.Tisserand.Invariant : num  5.58 3.61 4.44 5.5 NA ...\n $ Epoch.Osculation            : num  2457800 2457010 NA 2458000 2458000 ...\n $ Eccentricity                : num  0.276 0.57 0.344 0.255 0.22 ...\n $ Semi.Major.Axis             : num  1.1 NA 1.52 1.11 1.24 ...\n $ Inclination                 : num  20.06 4.39 5.44 23.9 3.5 ...\n $ Asc.Node.Longitude          : num  29.85 1.42 170.68 356.18 183.34 ...\n $ Orbital.Period              : num  419 1040 682 427 503 ...\n $ Perihelion.Distance         : num  0.794 0.864 0.994 0.828 0.965 ...\n $ Perihelion.Arg              : num  41.8 359.3 350 268.2 179.2 ...\n $ Aphelion.Dist               : num  1.4 3.15 2.04 1.39 1.51 ...\n $ Perihelion.Time             : num  2457736 2456941 2457937 NA 2458070 ...\n $ Mean.Anomaly                : num  55.1 NA NA 297.4 310.5 ...\n $ Mean.Motion                 : num  0.859 0.346 0.528 0.843 0.716 ...\n $ Equinox                     : Factor w/ 1 level \"J2000\": 1 1 NA 1 1 1 1 1 1 1 ...\n $ Hazardous                   : int  0 0 0 1 1 0 0 0 1 1 ...\n\n\nLet’s drop time, name and ID variable and create a classification task:\n\ndata = nasa %&gt;% select(-Orbit.Determination.Date,\n                       -Close.Approach.Date, -Name, -Neo.Reference.ID)\ndata$Hazardous = as.factor(data$Hazardous)\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"nasa\", backend = data,\n                       target = \"Hazardous\", positive = \"1\")\n\nCreate a generic pipeline of data transformation (imputation \\(\\rightarrow\\) scaling \\(\\rightarrow\\) encoding of categorical variables):\n\nset.seed(123)\n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$missings()\n\n                   Hazardous           Absolute.Magnitude \n                        4187                            0 \n               Aphelion.Dist           Asc.Node.Longitude \n                           0                            0 \n                Eccentricity    Epoch.Date.Close.Approach \n                           0                            0 \n            Epoch.Osculation         Est.Dia.in.Feet.max. \n                           0                            0 \n        Est.Dia.in.Feet.min.           Est.Dia.in.KM.max. \n                           0                            0 \n          Est.Dia.in.KM.min.            Est.Dia.in.M.max. \n                           0                            0 \n           Est.Dia.in.M.min.        Est.Dia.in.Miles.max. \n                           0                            0 \n       Est.Dia.in.Miles.min.                  Inclination \n                           0                            0 \n Jupiter.Tisserand.Invariant                 Mean.Anomaly \n                           0                            0 \n                 Mean.Motion               Miles.per.hour \n                           0                            0 \n  Minimum.Orbit.Intersection     Miss.Dist..Astronomical. \n                           0                            0 \n      Miss.Dist..kilometers.            Miss.Dist..lunar. \n                           0                            0 \n           Miss.Dist..miles.                     Orbit.ID \n                           0                            0 \n          Orbit.Uncertainity               Orbital.Period \n                           0                            0 \n              Perihelion.Arg          Perihelion.Distance \n                           0                            0 \n             Perihelion.Time  Relative.Velocity.km.per.hr \n                           0                            0 \nRelative.Velocity.km.per.sec              Semi.Major.Axis \n                           0                            0 \n               Equinox.J2000             Equinox..MISSING \n                           0                            0 \n         Orbiting.Body.Earth       Orbiting.Body..MISSING \n                           0                            0 \n\n\nWe can even visualize the preprocessing graph:\n\npreprocessing$plot()\n\n\n\n\nTo test our model (glmnet) with 10-fold cross-validated, we will do:\n\nSpecify the missing target rows as validation so that they will be ignored.\nSpecify the cross-validation, the learner (the machine learning model we want to use), and the measurement (AUC).\nRun (benchmark) our model.\n\n\nset.seed(123)\n\ntransformed_task$data()[1,]\n\n   Hazardous Absolute.Magnitude Aphelion.Dist Asc.Node.Longitude Eccentricity\n1:         0         -0.8132265    -0.3804201          -1.140837    -0.315606\n   Epoch.Date.Close.Approach Epoch.Osculation Est.Dia.in.Feet.max.\n1:                 -4.792988        0.1402677            0.2714179\n   Est.Dia.in.Feet.min. Est.Dia.in.KM.max. Est.Dia.in.KM.min. Est.Dia.in.M.max.\n1:            0.3134076          0.3007134          0.2565687         0.2710953\n   Est.Dia.in.M.min. Est.Dia.in.Miles.max. Est.Dia.in.Miles.min. Inclination\n1:         0.2916245             0.2620443              0.258651   0.5442288\n   Jupiter.Tisserand.Invariant Mean.Anomaly Mean.Motion Miles.per.hour\n1:                   0.3840868    -1.028761   0.3193953     -0.2541306\n   Minimum.Orbit.Intersection Miss.Dist..Astronomical. Miss.Dist..kilometers.\n1:                  -5.459119                -7.076926              0.2512296\n   Miss.Dist..lunar. Miss.Dist..miles.  Orbit.ID Orbit.Uncertainity\n1:         0.2398625         0.2381077 -9.651472          -1.007087\n   Orbital.Period Perihelion.Arg Perihelion.Distance Perihelion.Time\n1:     -0.3013135      -1.170536         -0.01831583       0.1052611\n   Relative.Velocity.km.per.hr Relative.Velocity.km.per.sec Semi.Major.Axis\n1:                  -0.2816782                   -0.2841407      -0.2791037\n   Equinox.J2000 Equinox..MISSING Orbiting.Body.Earth Orbiting.Body..MISSING\n1:             1                0                   1                      0\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$Hazardous)],\n                               \"holdout\")\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nmeasurement =  msr(\"classif.auc\")\n\n\nresult = mlr3::resample(transformed_task,\n                        EN, resampling = cv10, store_models = TRUE)\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nVery cool! Preprocessing + 10-fold cross-validation model evaluation in a few lines of code!\nLet’s create the final predictions:\n\npred = sapply(1:10, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)\n\nYou could now submit the predictions here.\nBut we are still not happy with the results, let’s do some hyperparameter tuning!\n\n\n4.5.1.2 mlr3 - Hyperparameter Tuning\nWith mlr3, we can easily extend the above example to do hyperparameter tuning within nested cross-validation (the tuning has its own inner cross-validation).\nPrint the hyperparameter space of our glmnet learner:\n\nEN$param_set\n\n&lt;ParamSet&gt;\n                      id    class lower upper nlevels        default parents\n 1:                alpha ParamDbl     0     1     Inf              1        \n 2:                  big ParamDbl  -Inf   Inf     Inf        9.9e+35        \n 3:               devmax ParamDbl     0     1     Inf          0.999        \n 4:                dfmax ParamInt     0   Inf     Inf &lt;NoDefault[3]&gt;        \n 5:                  eps ParamDbl     0     1     Inf          1e-06        \n 6:                epsnr ParamDbl     0     1     Inf          1e-08        \n 7:                exact ParamLgl    NA    NA       2          FALSE        \n 8:              exclude ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;        \n 9:                 exmx ParamDbl  -Inf   Inf     Inf            250        \n10:                 fdev ParamDbl     0     1     Inf          1e-05        \n11:                gamma ParamDbl  -Inf   Inf     Inf              1   relax\n12:            intercept ParamLgl    NA    NA       2           TRUE        \n13:               lambda ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n14:     lambda.min.ratio ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;        \n15:         lower.limits ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n16:                maxit ParamInt     1   Inf     Inf         100000        \n17:                mnlam ParamInt     1   Inf     Inf              5        \n18:                 mxit ParamInt     1   Inf     Inf            100        \n19:               mxitnr ParamInt     1   Inf     Inf             25        \n20:            newoffset ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n21:              nlambda ParamInt     1   Inf     Inf            100        \n22:               offset ParamUty    NA    NA     Inf                       \n23:       penalty.factor ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n24:                 pmax ParamInt     0   Inf     Inf &lt;NoDefault[3]&gt;        \n25:                 pmin ParamDbl     0     1     Inf          1e-09        \n26:                 prec ParamDbl  -Inf   Inf     Inf          1e-10        \n27:                relax ParamLgl    NA    NA       2          FALSE        \n28:                    s ParamDbl     0   Inf     Inf           0.01        \n29:          standardize ParamLgl    NA    NA       2           TRUE        \n30: standardize.response ParamLgl    NA    NA       2          FALSE        \n31:               thresh ParamDbl     0   Inf     Inf          1e-07        \n32:             trace.it ParamInt     0     1       2              0        \n33:        type.gaussian ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;        \n34:        type.logistic ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;        \n35:     type.multinomial ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;        \n36:         upper.limits ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;        \n                      id    class lower upper nlevels        default parents\n    value\n 1:      \n 2:      \n 3:      \n 4:      \n 5:      \n 6:      \n 7:      \n 8:      \n 9:      \n10:      \n11:      \n12:      \n13:      \n14:      \n15:      \n16:      \n17:      \n18:      \n19:      \n20:      \n21:      \n22:      \n23:      \n24:      \n25:      \n26:      \n27:      \n28:      \n29:      \n30:      \n31:      \n32:      \n33:      \n34:      \n35:      \n36:      \n    value\n\n\nDefine the hyperparameter space of the random forest:\n\nlibrary(paradox)\n\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\nprint(EN_pars)\n\n&lt;ParamSet&gt;\n       id    class lower upper nlevels        default value\n1:  alpha ParamDbl     0   1.0     Inf &lt;NoDefault[3]&gt;      \n2: lambda ParamDbl     0   0.5     Inf &lt;NoDefault[3]&gt;      \n\n\nTo set up the tuning pipeline we need:\n\nInner cross-validation resampling object.\nTuning criterion (e.g. AUC).\nTuning method (e.g. random or block search).\nTuning terminator (When should we stop tuning? E.g. after \\(n\\) iterations).\n\n\nset.seed(123)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\nprint(learner_tuner)\n\n&lt;AutoTuner:classif.glmnet.tuned&gt;\n* Model: list\n* Search Space:\n&lt;ParamSet&gt;\n       id    class lower upper nlevels        default value\n1:  alpha ParamDbl     0   1.0     Inf &lt;NoDefault[3]&gt;      \n2: lambda ParamDbl     0   0.5     Inf &lt;NoDefault[3]&gt;      \n* Packages: mlr3, mlr3tuning, mlr3learners, glmnet\n* Predict Type: prob\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass, weights\n\n\nNow we can wrap it normally into the 10-fold cross-validated setup as done previously:\n\n# Calculate the average AUC of the holdouts.\nresult$aggregate(measurement)\n\nclassif.auc \n  0.6767554 \n\n\nLet’s create the final predictions:\n\npred = sapply(1:3, function(i) result$learners[[i]]$predict(transformed_task,\nrow_ids = (1:nrow(data))[is.na(data$Hazardous)])$data$prob[, \"1\", drop = FALSE])\ndim(pred)\npredictions = apply(pred, 1, mean)"
  },
  {
    "objectID": "A4-MLpipeline.html#exercises-1",
    "href": "A4-MLpipeline.html#exercises-1",
    "title": "4  Machine learning pipeline",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n\n\n\n\n\n\nQuestion: Use mlr3 for the titanic dataset\n\n\n\n\nUse mlr3 to tune glmnet for the titanic dataset using nested CV\nSubmit single predictions and multiple predictions\n\nIf you need help, take a look at the solution, go through it line by line and try to understand it.\n\n\nClick here to see the solution\n\nPrepare data\n\ndata = titanic_ml %&gt;% select(-name, -ticket, -name, -body)\ndata$pclass = as.factor(data$pclass)\ndata$sex = as.factor(data$sex)\ndata$survived = as.factor(data$survived)\n\n# Change easy things manually:\ndata$embarked[data$embarked == \"\"] = \"S\"  # Fill in \"empty\" values.\ndata$embarked = droplevels(as.factor(data$embarked)) # Remove unused levels (\"\").\ndata$cabin = (data$cabin != \"\") * 1 # Dummy code the availability of a cabin.\ndata$fare[is.na(data$fare)] = mean(data$fare, na.rm = TRUE)\nlevels(data$home.dest)[levels(data$home.dest) == \"\"] = \"unknown\"\nlevels(data$boat)[levels(data$boat) == \"\"] = \"none\"\n\n# Create a classification task.\ntask = TaskClassif$new(id = \"titanic\", backend = data,\n                       target = \"survived\", positive = \"1\")\ntask$missings()\n\n survived       age      boat     cabin  embarked      fare home.dest     parch \n      655       263         0         0         0         0         0         0 \n   pclass       sex     sibsp \n        0         0         0 \n\n# Let's create the preprocessing graph.\npreprocessing = po(\"imputeoor\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% po(\"encode\") \n\n# Run the task.\ntransformed_task = preprocessing$train(task)[[1]]\n\ntransformed_task$set_row_roles((1:nrow(data))[is.na(data$survived)], \"holdout\")\n\nHyperparameter tuning:\n\ncv10 = mlr3::rsmp(\"cv\", folds = 10L)\n\ninner3 = mlr3::rsmp(\"cv\", folds = 3L)\nmeasurement =  msr(\"classif.auc\")\ntuner =  mlr3tuning::tnr(\"random_search\") \nterminator = mlr3tuning::trm(\"evals\", n_evals = 5L)\nEN = lrn(\"classif.glmnet\", predict_type = \"prob\")\nEN_pars = \n    paradox::ParamSet$new(\n      list(paradox::ParamDbl$new(\"alpha\", lower = 0, upper = 1L),\n           paradox::ParamDbl$new(\"lambda\", lower = 0, upper = 0.5 )) )\n\nlearner_tuner = AutoTuner$new(learner = EN, \n                              measure = measurement, \n                              tuner = tuner, \n                              terminator = terminator,\n                              search_space = EN_pars,\n                              resampling = inner3)\n\n\nresult = mlr3::resample(transformed_task, learner_tuner,\n                        resampling = cv10, store_models = TRUE)\n\nEvaluation:\n\nmeasurement =  msr(\"classif.auc\")\nresult$aggregate(measurement)\n\nclassif.auc \n  0.9924459 \n\n\nPredictions:\nWe can extract a learner with optimized hyperparameters:\n\nmodel = result$learners[[1]]$learner$clone()\nmodel$param_set$values\n\n$alpha\n[1] 0.2353052\n\n$lambda\n[1] 0.03832512\n\n\nAnd we can fit it then on the full data set:\n\nmodel$train(transformed_task)\npredictions = model$predict(transformed_task, row_ids = transformed_task$row_roles$holdout)\npredictions = predictions$prob[,1]\nhead(predictions)\n\n[1] 0.94428932 0.07481128 0.21730284 0.86504280 0.94653467 0.95414457\n\n\nAnd submit to http://rhsbio7.uni-regensburg.de:8500\n\nwrite.csv(data.frame(y = predictions), file = \"glmnet.csv\")"
  },
  {
    "objectID": "B1-Trees.html#classification-and-regression-trees",
    "href": "B1-Trees.html#classification-and-regression-trees",
    "title": "5  Tree-based Algorithms",
    "section": "5.1 Classification and Regression Trees",
    "text": "5.1 Classification and Regression Trees\nTree-based models in general use a series of if-then rules to generate predictions from one or more decision trees. In this lecture, we will explore regression and classification trees by the example of the airquality data set. There is one important hyperparameter for regression trees: “minsplit”.\n\nIt controls the depth of tree (see the help of rpart for a description).\nIt controls the complexity of the tree and can thus also be seen as a regularization parameter.\n\nWe first prepare and visualize the data and afterwards fit a decision tree.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\ndata = airquality[complete.cases(airquality),]\n\nFit and visualize one(!) regression tree:\n\nrt = rpart(Ozone~., data = data, control = rpart.control(minsplit = 10))\nrpart.plot(rt)\n\n\n\n\nVisualize the predictions:\n\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\nThe angular form of the prediction line is typical for regression trees and is a weakness of it."
  },
  {
    "objectID": "B1-Trees.html#random-forest",
    "href": "B1-Trees.html#random-forest",
    "title": "5  Tree-based Algorithms",
    "section": "5.2 Random Forest",
    "text": "5.2 Random Forest\nTo overcome this weakness, a random forest uses an ensemble of regression/classification trees. Thus, the random forest is in principle nothing else than a normal regression/classification tree, but it uses the idea of the “wisdom of the crowd” : By asking many people (regression/classification trees) one can make a more informed decision (prediction/classification). When you want to buy a new phone for example you also wouldn’t go directly into the shop, but search in the internet and ask your friends and family.\nThere are two randomization steps with the random forest that are responsible for their success:\n\nBootstrap samples for each tree (we will sample observations with replacement from the data set. For the phone this is like not everyone has experience about each phone).\nAt each split, we will sample a subset of predictors that is then considered as potential splitting criterion (for the phone this is like that not everyone has the same decision criteria). Annotation: While building a decision tree (random forests consist of many decision trees), one splits the data at some point according to their features. For example if you have females and males, big and small people in a crowd, you con split this crowd by gender and then by size or by size and then by gender to build a decision tree.\n\nApplying the random forest follows the same principle as for the methods before: We visualize the data (we have already done this so often for the airquality data set, thus we skip it here), fit the algorithm and then plot the outcomes.\nFit a random forest and visualize the predictions:\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\npred = predict(rf, data)\nplot(Ozone~Temp, data = data)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\nOne advantage of random forests is that we will get an importance of variables. At each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. Thus the variable importance shows us how important a variable is averaged over all trees.\n\nrf$importance\n\n        IncNodePurity\nSolar.R      17969.59\nWind         31978.36\nTemp         34176.71\nMonth        10753.73\nDay          15436.47\n\n\nThere are several important hyperparameters in a random forest that we can tune to get better results:\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\nmtry\nSubset of features randomly selected in each node (from which the algorithm can select the feature that will be used to split the data).\n\n\nminimum node size\nMinimal number of observations allowed in a node (before the branching is canceled)\n\n\nmax depth\nMaximum number of tree depth"
  },
  {
    "objectID": "B1-Trees.html#boosted-regression-trees",
    "href": "B1-Trees.html#boosted-regression-trees",
    "title": "5  Tree-based Algorithms",
    "section": "5.3 Boosted Regression Trees",
    "text": "5.3 Boosted Regression Trees\nRandom forests fit hundreds of trees independent of each other. Here, the idea of a boosted regression tree comes in. Maybe we could learn from the errors the previous weak learners made and thus enhance the performance of the algorithm.\nA boosted regression tree (BRT) starts with a simple regression tree (weak learner) and then sequentially fits additional trees to improve the results. There are two different strategies to do so:\n\nAdaBoost: Wrong classified observations (by the previous tree) will get a higher weight and therefore the next trees will focus on difficult/missclassified observations.\nGradient boosting (state of the art): Each sequential model will be fit on the residual errors of the previous model (strongly simplified, the actual algorithm is very complex).\n\nWe can fit a boosted regression tree using xgboost, but before we have to transform the data into a xgb.Dmatrix (which is a xgboost specific data type, the package sadly doesn’t support R matrices or data.frames).\n\nlibrary(xgboost)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\nbrt = xgboost(data_xg, nrounds = 16L)\n\n[1] train-rmse:39.724624 \n[2] train-rmse:30.225761 \n[3] train-rmse:23.134840 \n[4] train-rmse:17.899179 \n[5] train-rmse:14.097785 \n[6] train-rmse:11.375457 \n[7] train-rmse:9.391276 \n[8] train-rmse:7.889690 \n[9] train-rmse:6.646586 \n[10]    train-rmse:5.804859 \n[11]    train-rmse:5.128437 \n[12]    train-rmse:4.456416 \n[13]    train-rmse:4.069464 \n[14]    train-rmse:3.674615 \n[15]    train-rmse:3.424578 \n[16]    train-rmse:3.191301 \n\n\nThe parameter “nrounds” controls how many sequential trees we fit, in our example this was 16. When we predict on new data, we can limit the number of trees used to prevent overfitting (remember: each new tree tries to improve the predictions of the previous trees).\nLet us visualize the predictions for different numbers of trees:\n\noldpar = par(mfrow = c(2, 2))\nfor(i in 1:4){\n  pred = predict(brt, newdata = data_xg, ntreelimit = i)\n  plot(data$Temp, data$Ozone, main = i)\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\n[20:04:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[20:04:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[20:04:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[20:04:11] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n\n\npar(oldpar)\n\nThere are also other ways to control for complexity of the boosted regression tree algorithm:\n\nmax_depth: Maximum depth of each tree.\nshrinkage (each tree will get a weight and the weight will decrease with the number of trees).\n\nWhen having specified the final model, we can obtain the importance of the variables like for random forests:\n\nxgboost::xgb.importance(model = brt)\n\n   Feature        Gain     Cover  Frequency\n1:    Temp 0.570072012 0.2958229 0.24836601\n2:    Wind 0.348230653 0.3419576 0.24183007\n3: Solar.R 0.058795502 0.1571072 0.30718954\n4:     Day 0.019529985 0.1779925 0.16993464\n5:   Month 0.003371847 0.0271197 0.03267974\n\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n\n[1] 17.89918\n\ndata_xg = xgb.DMatrix(data = as.matrix(scale(data[,-1])), label = data$Ozone)\n\nOne important strength of xgboost is that we can directly do a cross-validation (which is independent of the boosted regression tree itself!) and specify its properties with the parameter “n-fold”:\n\nset.seed(123)\n\nbrt = xgboost(data_xg, nrounds = 5L)\n\n[1] train-rmse:39.724624 \n[2] train-rmse:30.225761 \n[3] train-rmse:23.134840 \n[4] train-rmse:17.899179 \n[5] train-rmse:14.097785 \n\nbrt_cv = xgboost::xgb.cv(data = data_xg, nfold = 3L,\n                         nrounds = 3L, nthreads = 4L)\n\n[1] train-rmse:39.895106+2.127355   test-rmse:40.685477+5.745327 \n[2] train-rmse:30.367660+1.728788   test-rmse:32.255812+5.572963 \n[3] train-rmse:23.446237+1.366757   test-rmse:27.282435+5.746244 \n\nprint(brt_cv)\n\n##### xgb.cv 3-folds\n iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std\n    1        39.89511       2.127355       40.68548      5.745327\n    2        30.36766       1.728788       32.25581      5.572963\n    3        23.44624       1.366757       27.28244      5.746244\n\n\nAnnotation: The original data set is randomly partitioned into \\(n\\) equal sized subsamples. Each time, the model is trained on \\(n - 1\\) subsets (training set) and tested on the left out set (test set) to judge the performance.\nIf we do three-folded cross-validation, we actually fit three different boosted regression tree models (xgboost models) on \\(\\approx 67\\%\\) of the data points. Afterwards, we judge the performance on the respective holdout. This now tells us how well the model performed.\nImportant hyperparameters:\n\n\n\n\n\n\n\nHyperparameter\nExplanation\n\n\n\n\neta\nlearning rate (weighting of the sequential trees)\n\n\nmax depth\nmaximal depth in the trees (small = low complexity, large = high complexity)\n\n\nsubsample\nsubsample ratio of the data (bootstrap ratio)\n\n\nlambda\nregularization strength of the individual trees\n\n\nmax tree\nmaximal number of trees in the ensemble"
  },
  {
    "objectID": "B1-Trees.html#exercises",
    "href": "B1-Trees.html#exercises",
    "title": "5  Tree-based Algorithms",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\n\n\n\n\n\nQuestion: Regression Trees\n\n\n\nWe will use the following code snippet to understand the effect of mincut and thus the predictive performance.\n\nlibrary(tree)\nset.seed(123)\n\ndata = airquality\nrt = tree(Ozone~., data = data,\n          control = tree.control(mincut = 1L, nobs = nrow(data)))\n\nplot(rt)\ntext(rt)\npred = predict(rt, data)\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\nsqrt(mean((data$Ozone - pred)^2)) # RMSE\n\nTry different mincut parameters and see what happens. (Compare the root mean squared error for different mincut parameters and explain what you see. Compare predictions for different mincut parameters and explain what happens.) What was wrong in the snippet above?\n\n\nClick here to see the solution\n\n\nlibrary(tree)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\ndoTask = function(mincut){\n  rt = tree(Ozone~., data = data,\n            control = tree.control(mincut = mincut, nobs = nrow(data)))\n\n  pred = predict(rt, data)\n  plot(data$Temp, data$Ozone,\n       main = paste0(\n         \"mincut: \", mincut,\n         \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n  )\n  lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n}\n\nfor(i in c(1, 2, 3, 5, 10, 15, 25, 50, 54, 55, 56, 57, 75, 100)){ doTask(i) }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproximately at mincut = 15, prediction is the best (mind overfitting). After mincut = 56, the prediction has no information at all and the RMSE stays constant.\nMind the complete cases of the airquality data set, that was the error.\n\n\n\n\n\n\n\n\n\nQuestion: Random forest\n\n\n\nWe will use the following code snippet to explore a random forest:\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n\n        IncNodePurity\nSolar.R      17969.59\nWind         31978.36\nTemp         34176.71\nMonth        10753.73\nDay          15436.47\n\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n\nRMSE:  9.507848 \n\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\nTry different values for the nodesize describe how the predictions depend on this parameter.\n\n\nClick here to see the solution\n\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\nfor(nodesize in c(1, 15, 50, 100)){\n  for(mtry in c(1, 3, 5)){\n    rf = randomForest(Ozone~., data = data, nodesize = nodesize)\n    \n    pred = predict(rf, data)\n    \n    plot(data$Temp, data$Ozone, main = paste0(\n        \"    nodesize: \", nodesize,\n        \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n    )\n    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodesize affects the complexity. In other words: The bigger the nodesize, the smaller the trees and the more bias/less variance.\n\n\n\n\n\n\n\n\n\nQuestion: Boosted regression trees\n\n\n\n\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\n\nmodel = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                         nrounds = 500L, verbose = 0L)\npred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n               ntreelimit = 10L)\n\nsaveGIF(\n  {\n    for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n      pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                     ntreelimit = i)\n      image(matrix(pred, 100, 100), main = paste0(\"Trees: \", i),\n            axes = FALSE, las = 2)\n      axis(1, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1))\n      axis(2, at = seq(0, 1, length.out = 10),\n           labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n    }\n  },\n  movie.name = \"boosting.gif\", autobrowse = FALSE\n)\n\n\nRun the above code and play with different values for max_depth and describe what you see!\nTip: have a look at the boosting.gif.\n\n\nClick here to see the solution\n\n\nlibrary(xgboost)\nlibrary(animation)\nset.seed(123)\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nx = expand.grid(x1, x2)\ny = apply(x, 1, function(t) exp(-t[1]^2 - t[2]^2))\n\nimage(matrix(y, 100, 100), main = \"Original image\", axes = FALSE, las = 2)\naxis(1, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1))\naxis(2, at = seq(0, 1, length.out = 10),\n     labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n\nfor(max_depth in c(3, 6, 10, 20)){\n  model = xgboost::xgboost(xgb.DMatrix(data = as.matrix(x), label = y),\n                           max_depth = max_depth,\n                           nrounds = 500, verbose = 0L)\n\n  saveGIF(\n    {\n      for(i in c(1, 2, 4, 8, 12, 20, 40, 80, 200)){\n        pred = predict(model, newdata = xgb.DMatrix(data = as.matrix(x)),\n                       ntreelimit = i)\n        image(matrix(pred, 100, 100),\n              main = paste0(\"eta: \", eta,\n                            \"    max_depth: \", max_depth,\n                            \"    Trees: \", i),\n              axes = FALSE, las = 2)\n        axis(1, at = seq(0, 1, length.out = 10),\n             labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0, 1, length.out = 10),\n             labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    },\n    movie.name = paste0(\"boosting_\", max_depth, \"_\", eta, \".gif\"),\n    autobrowse = FALSE\n  )\n}\n\nWe see that for high values of max_depth, the predictions “smooth out” faster. On the other hand, with a low max_depth (low complexity of the individual trees), more trees are required in the ensemble to achieve a smooth prediction surface.\n\n?xgboost::xgboost\n\nJust some examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning of random forest\n\n\n\nCombing back to the titanic dataset from the morning, we want to optimize min node size in our RF using a simple CV.\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:xgboost':\n\n    slice\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\nHints:\n\nadjust the ‘type’ argument in the predict(…) method (the default is to predict classes)\nwhen predicting probabilities, the randomForest will return a matrix, a column for each class, we are interested in the probability of surviving (so the second column)\n\nBonus:\n\ntune also mtry\nuse more features\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(randomForest)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_minnodesize = ...\n\nresults = data.frame(\n  set = rep(NA, cv),\n  minnodesize = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_minnodesize), function(k) {\n        model = randomForest(as.factor(survived)~., data = train_outer, nodesize = ... )\n        return(Metrics::auc(test_outer$survived, predict(model, newdata = test_outer, type = \"prob\")[,2]))\n      })\n  best_minnodesize = hyper_minnodesize[which.max(tuning_results)]\n  \n  results[i, 1] = i\n  results[i, 2] = best_minnodesize\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(randomForest)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_minnodesize = sample(300, 20)\n\nresults = data.frame(\n  set = rep(NA, cv),\n  minnodesize = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_minnodesize), function(k) {\n        model = randomForest(as.factor(survived)~., data = train_outer, nodesize = hyper_minnodesize[k] )\n        return(Metrics::auc(test_outer$survived, predict(model, newdata = test_outer, type = \"prob\")[,2]))\n      })\n  best_minnodesize = hyper_minnodesize[which.max(tuning_results)]\n  \n  results[i, 1] = i\n  results[i, 2] = best_minnodesize\n  results[i, 3] = max(tuning_results)\n}\n\nprint(results)\n\n  set minnodesize       AUC\n1   1          74 0.8116560\n2   2          20 0.8409981\n3   3          20 0.8513631\n\n\nMake predictions:\n\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n  model = randomForest(as.factor(survived)~., data = data_obs, nodesize = results$minnodesize[i] )\n    return(predict(model, data_obs, type = \"prob\")[,2])\n  })\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n\n\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning of boosted regression trees\n\n\n\nCombing back to the titanic dataset from the morning, we want to optimize max depth and the eta parameter in xgboost.\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(xgboost)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_depth = ...\nhyper_eta = ...\n\nresults = data.frame(\n  set = rep(NA, cv),\n  depth = rep(NA, cv),\n  eta = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_depth), function(k) {\n        \n        # Cast data to xgboost data types\n        data_xg = xgb.DMatrix(data = as.matrix(train_outer[,-1]), label = train_outer$survived)\n        model = xgboost(data_xg, nrounds = 16L, eta = hyper_eta[k], max_depth = hyper_depth[k])\n        predictions = predict(model, newdata = as.matrix(test_outer)[,-1])\n        \n        return(Metrics::auc(test_outer$survived, predictions)))\n      })\n  \n  results[i, 1] = i\n  results[i, 2] = hyper_depth[which.max(tuning_results)]\n  results[i, 3] = hyper_eta[which.max(tuning_results)]  \n  results[i, 4] = max(tuning_results)\n}\n\nprint(results)\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(xgboost)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_depth = sample(200, 20)\nhyper_eta = runif(20, 0, 1)\n\nresults = data.frame(\n  set = rep(NA, cv),\n  depth = rep(NA, cv),\n  eta = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_depth), function(k) {\n        \n        # Cast data to xgboost data types\n        data_xg = xgb.DMatrix(data = as.matrix(train_outer[,-1]), label = train_outer$survived)\n        model = xgboost(data_xg, nrounds = 16L, eta = hyper_eta[k], max_depth = hyper_depth[k])\n        predictions = predict(model, newdata = as.matrix(test_outer)[,-1])\n        \n        return(Metrics::auc(test_outer$survived, predictions))\n      })\n  \n  results[i, 1] = i\n  results[i, 2] = hyper_depth[which.max(tuning_results)]\n  results[i, 3] = hyper_eta[which.max(tuning_results)]  \n  results[i, 4] = max(tuning_results)\n}\n\n[1] train-rmse:0.175827 \n[2] train-rmse:0.109441 \n[3] train-rmse:0.092795 \n[4] train-rmse:0.088512 \n[5] train-rmse:0.087381 \n[6] train-rmse:0.087107 \n[7] train-rmse:0.087027 \n[8] train-rmse:0.087007 \n[9] train-rmse:0.087003 \n[10]    train-rmse:0.087002 \n[11]    train-rmse:0.087002 \n[12]    train-rmse:0.087002 \n[13]    train-rmse:0.087002 \n[14]    train-rmse:0.087002 \n[15]    train-rmse:0.087002 \n[16]    train-rmse:0.087002 \n[1] train-rmse:0.182387 \n[2] train-rmse:0.112793 \n[3] train-rmse:0.094016 \n[4] train-rmse:0.088861 \n[5] train-rmse:0.087508 \n[6] train-rmse:0.087140 \n[7] train-rmse:0.087039 \n[8] train-rmse:0.087011 \n[9] train-rmse:0.087004 \n[10]    train-rmse:0.087002 \n[11]    train-rmse:0.087002 \n[12]    train-rmse:0.087002 \n[13]    train-rmse:0.087002 \n[14]    train-rmse:0.087002 \n[15]    train-rmse:0.087002 \n[16]    train-rmse:0.087002 \n[1] train-rmse:0.467591 \n[2] train-rmse:0.437246 \n[3] train-rmse:0.409265 \n[4] train-rmse:0.383782 \n[5] train-rmse:0.360274 \n[6] train-rmse:0.337716 \n[7] train-rmse:0.316400 \n[8] train-rmse:0.296867 \n[9] train-rmse:0.278947 \n[10]    train-rmse:0.262474 \n[11]    train-rmse:0.246994 \n[12]    train-rmse:0.232921 \n[13]    train-rmse:0.220217 \n[14]    train-rmse:0.208335 \n[15]    train-rmse:0.197668 \n[16]    train-rmse:0.187673 \n[1] train-rmse:0.305459 \n[2] train-rmse:0.203594 \n[3] train-rmse:0.145842 \n[4] train-rmse:0.116749 \n[5] train-rmse:0.101977 \n[6] train-rmse:0.094731 \n[7] train-rmse:0.091107 \n[8] train-rmse:0.089156 \n[9] train-rmse:0.088162 \n[10]    train-rmse:0.087629 \n[11]    train-rmse:0.087345 \n[12]    train-rmse:0.087189 \n[13]    train-rmse:0.087103 \n[14]    train-rmse:0.087057 \n[15]    train-rmse:0.087032 \n[16]    train-rmse:0.087018 \n[1] train-rmse:0.350164 \n[2] train-rmse:0.252016 \n[3] train-rmse:0.186831 \n[4] train-rmse:0.147834 \n[5] train-rmse:0.123078 \n[6] train-rmse:0.108796 \n[7] train-rmse:0.100220 \n[8] train-rmse:0.095171 \n[9] train-rmse:0.092074 \n[10]    train-rmse:0.090201 \n[11]    train-rmse:0.088995 \n[12]    train-rmse:0.088264 \n[13]    train-rmse:0.087806 \n[14]    train-rmse:0.087516 \n[15]    train-rmse:0.087331 \n[16]    train-rmse:0.087211 \n[1] train-rmse:0.190103 \n[2] train-rmse:0.116106 \n[3] train-rmse:0.095590 \n[4] train-rmse:0.089478 \n[5] train-rmse:0.087726 \n[6] train-rmse:0.087214 \n[7] train-rmse:0.087064 \n[8] train-rmse:0.087020 \n[9] train-rmse:0.087006 \n[10]    train-rmse:0.087003 \n[11]    train-rmse:0.087003 \n[12]    train-rmse:0.087003 \n[13]    train-rmse:0.087003 \n[14]    train-rmse:0.087003 \n[15]    train-rmse:0.087003 \n[16]    train-rmse:0.087003 \n[1] train-rmse:0.329438 \n[2] train-rmse:0.229970 \n[3] train-rmse:0.165347 \n[4] train-rmse:0.130049 \n[5] train-rmse:0.110895 \n[6] train-rmse:0.100268 \n[7] train-rmse:0.094529 \n[8] train-rmse:0.091396 \n[9] train-rmse:0.089568 \n[10]    train-rmse:0.088477 \n[11]    train-rmse:0.087870 \n[12]    train-rmse:0.087513 \n[13]    train-rmse:0.087307 \n[14]    train-rmse:0.087184 \n[15]    train-rmse:0.087110 \n[16]    train-rmse:0.087066 \n[1] train-rmse:0.205885 \n[2] train-rmse:0.121338 \n[3] train-rmse:0.097982 \n[4] train-rmse:0.090629 \n[5] train-rmse:0.088183 \n[6] train-rmse:0.087395 \n[7] train-rmse:0.087133 \n[8] train-rmse:0.087045 \n[9] train-rmse:0.087016 \n[10]    train-rmse:0.087006 \n[11]    train-rmse:0.087003 \n[12]    train-rmse:0.087003 \n[13]    train-rmse:0.087003 \n[14]    train-rmse:0.087003 \n[15]    train-rmse:0.087003 \n[16]    train-rmse:0.087003 \n[1] train-rmse:0.232610 \n[2] train-rmse:0.136983 \n[3] train-rmse:0.105745 \n[4] train-rmse:0.094020 \n[5] train-rmse:0.089683 \n[6] train-rmse:0.088056 \n[7] train-rmse:0.087410 \n[8] train-rmse:0.087154 \n[9] train-rmse:0.087060 \n[10]    train-rmse:0.087024 \n[11]    train-rmse:0.087010 \n[12]    train-rmse:0.087005 \n[13]    train-rmse:0.087003 \n[14]    train-rmse:0.087003 \n[15]    train-rmse:0.087003 \n[16]    train-rmse:0.087003 \n[1] train-rmse:0.212223 \n[2] train-rmse:0.126366 \n[3] train-rmse:0.100020 \n[4] train-rmse:0.091146 \n[5] train-rmse:0.088439 \n[6] train-rmse:0.087504 \n[7] train-rmse:0.087178 \n[8] train-rmse:0.087062 \n[9] train-rmse:0.087022 \n[10]    train-rmse:0.087008 \n[11]    train-rmse:0.087004 \n[12]    train-rmse:0.087003 \n[13]    train-rmse:0.087003 \n[14]    train-rmse:0.087003 \n[15]    train-rmse:0.087003 \n[16]    train-rmse:0.087003 \n[1] train-rmse:0.350936 \n[2] train-rmse:0.253277 \n[3] train-rmse:0.189156 \n[4] train-rmse:0.148701 \n[5] train-rmse:0.123730 \n[6] train-rmse:0.109281 \n[7] train-rmse:0.100498 \n[8] train-rmse:0.095362 \n[9] train-rmse:0.092203 \n[10]    train-rmse:0.090257 \n[11]    train-rmse:0.089029 \n[12]    train-rmse:0.088286 \n[13]    train-rmse:0.087820 \n[14]    train-rmse:0.087527 \n[15]    train-rmse:0.087338 \n[16]    train-rmse:0.087215 \n[1] train-rmse:0.248468 \n[2] train-rmse:0.145743 \n[3] train-rmse:0.110049 \n[4] train-rmse:0.096179 \n[5] train-rmse:0.090720 \n[6] train-rmse:0.088565 \n[7] train-rmse:0.087669 \n[8] train-rmse:0.087290 \n[9] train-rmse:0.087122 \n[10]    train-rmse:0.087052 \n[11]    train-rmse:0.087023 \n[12]    train-rmse:0.087010 \n[13]    train-rmse:0.087005 \n[14]    train-rmse:0.087004 \n[15]    train-rmse:0.087004 \n[16]    train-rmse:0.087004 \n[1] train-rmse:0.498441 \n[2] train-rmse:0.496901 \n[3] train-rmse:0.495367 \n[4] train-rmse:0.493828 \n[5] train-rmse:0.492281 \n[6] train-rmse:0.490754 \n[7] train-rmse:0.489233 \n[8] train-rmse:0.487703 \n[9] train-rmse:0.486194 \n[10]    train-rmse:0.484701 \n[11]    train-rmse:0.483189 \n[12]    train-rmse:0.481697 \n[13]    train-rmse:0.480212 \n[14]    train-rmse:0.478716 \n[15]    train-rmse:0.477242 \n[16]    train-rmse:0.475773 \n[1] train-rmse:0.206652 \n[2] train-rmse:0.121658 \n[3] train-rmse:0.098195 \n[4] train-rmse:0.090556 \n[5] train-rmse:0.088205 \n[6] train-rmse:0.087406 \n[7] train-rmse:0.087138 \n[8] train-rmse:0.087045 \n[9] train-rmse:0.087015 \n[10]    train-rmse:0.087006 \n[11]    train-rmse:0.087003 \n[12]    train-rmse:0.087002 \n[13]    train-rmse:0.087002 \n[14]    train-rmse:0.087002 \n[15]    train-rmse:0.087002 \n[16]    train-rmse:0.087002 \n[1] train-rmse:0.497105 \n[2] train-rmse:0.494254 \n[3] train-rmse:0.491404 \n[4] train-rmse:0.488575 \n[5] train-rmse:0.485738 \n[6] train-rmse:0.482949 \n[7] train-rmse:0.480181 \n[8] train-rmse:0.477404 \n[9] train-rmse:0.474695 \n[10]    train-rmse:0.471927 \n[11]    train-rmse:0.469235 \n[12]    train-rmse:0.466506 \n[13]    train-rmse:0.463826 \n[14]    train-rmse:0.461135 \n[15]    train-rmse:0.458464 \n[16]    train-rmse:0.455845 \n[1] train-rmse:0.419005 \n[2] train-rmse:0.352627 \n[3] train-rmse:0.297627 \n[4] train-rmse:0.253814 \n[5] train-rmse:0.217581 \n[6] train-rmse:0.189113 \n[7] train-rmse:0.166227 \n[8] train-rmse:0.148174 \n[9] train-rmse:0.134004 \n[10]    train-rmse:0.123090 \n[11]    train-rmse:0.114698 \n[12]    train-rmse:0.108291 \n[13]    train-rmse:0.103426 \n[14]    train-rmse:0.099694 \n[15]    train-rmse:0.096845 \n[16]    train-rmse:0.094662 \n[1] train-rmse:0.189928 \n[2] train-rmse:0.116045 \n[3] train-rmse:0.095563 \n[4] train-rmse:0.089466 \n[5] train-rmse:0.087722 \n[6] train-rmse:0.087214 \n[7] train-rmse:0.087063 \n[8] train-rmse:0.087019 \n[9] train-rmse:0.087007 \n[10]    train-rmse:0.087003 \n[11]    train-rmse:0.087003 \n[12]    train-rmse:0.087003 \n[13]    train-rmse:0.087003 \n[14]    train-rmse:0.087003 \n[15]    train-rmse:0.087003 \n[16]    train-rmse:0.087003 \n[1] train-rmse:0.272092 \n[2] train-rmse:0.163696 \n[3] train-rmse:0.120192 \n[4] train-rmse:0.101641 \n[5] train-rmse:0.093493 \n[6] train-rmse:0.089993 \n[7] train-rmse:0.088384 \n[8] train-rmse:0.087659 \n[9] train-rmse:0.087310 \n[10]    train-rmse:0.087144 \n[11]    train-rmse:0.087068 \n[12]    train-rmse:0.087032 \n[13]    train-rmse:0.087016 \n[14]    train-rmse:0.087008 \n[15]    train-rmse:0.087004 \n[16]    train-rmse:0.087003 \n[1] train-rmse:0.354093 \n[2] train-rmse:0.257314 \n[3] train-rmse:0.192974 \n[4] train-rmse:0.151840 \n[5] train-rmse:0.126063 \n[6] train-rmse:0.110879 \n[7] train-rmse:0.101735 \n[8] train-rmse:0.096267 \n[9] train-rmse:0.092827 \n[10]    train-rmse:0.090699 \n[11]    train-rmse:0.089337 \n[12]    train-rmse:0.088477 \n[13]    train-rmse:0.087941 \n[14]    train-rmse:0.087602 \n[15]    train-rmse:0.087392 \n[16]    train-rmse:0.087254 \n[1] train-rmse:0.333494 \n[2] train-rmse:0.234593 \n[3] train-rmse:0.170822 \n[4] train-rmse:0.133994 \n[5] train-rmse:0.113482 \n[6] train-rmse:0.102127 \n[7] train-rmse:0.095877 \n[8] train-rmse:0.092217 \n[9] train-rmse:0.090080 \n[10]    train-rmse:0.088836 \n[11]    train-rmse:0.088098 \n[12]    train-rmse:0.087662 \n[13]    train-rmse:0.087401 \n[14]    train-rmse:0.087243 \n[15]    train-rmse:0.087148 \n[16]    train-rmse:0.087090 \n[1] train-rmse:0.188921 \n[2] train-rmse:0.126826 \n[3] train-rmse:0.111578 \n[4] train-rmse:0.107653 \n[5] train-rmse:0.106672 \n[6] train-rmse:0.106413 \n[7] train-rmse:0.106345 \n[8] train-rmse:0.106328 \n[9] train-rmse:0.106325 \n[10]    train-rmse:0.106325 \n[11]    train-rmse:0.106325 \n[12]    train-rmse:0.106325 \n[13]    train-rmse:0.106325 \n[14]    train-rmse:0.106325 \n[15]    train-rmse:0.106325 \n[16]    train-rmse:0.106325 \n[1] train-rmse:0.195457 \n[2] train-rmse:0.129858 \n[3] train-rmse:0.112563 \n[4] train-rmse:0.107986 \n[5] train-rmse:0.106780 \n[6] train-rmse:0.106446 \n[7] train-rmse:0.106357 \n[8] train-rmse:0.106331 \n[9] train-rmse:0.106327 \n[10]    train-rmse:0.106324 \n[11]    train-rmse:0.106324 \n[12]    train-rmse:0.106324 \n[13]    train-rmse:0.106324 \n[14]    train-rmse:0.106324 \n[15]    train-rmse:0.106324 \n[16]    train-rmse:0.106324 \n[1] train-rmse:0.468748 \n[2] train-rmse:0.439602 \n[3] train-rmse:0.412375 \n[4] train-rmse:0.387024 \n[5] train-rmse:0.363245 \n[6] train-rmse:0.341305 \n[7] train-rmse:0.321214 \n[8] train-rmse:0.302495 \n[9] train-rmse:0.285540 \n[10]    train-rmse:0.269621 \n[11]    train-rmse:0.254977 \n[12]    train-rmse:0.241989 \n[13]    train-rmse:0.229524 \n[14]    train-rmse:0.218100 \n[15]    train-rmse:0.207744 \n[16]    train-rmse:0.198353 \n[1] train-rmse:0.313094 \n[2] train-rmse:0.208954 \n[3] train-rmse:0.157890 \n[4] train-rmse:0.132285 \n[5] train-rmse:0.119543 \n[6] train-rmse:0.113201 \n[7] train-rmse:0.109925 \n[8] train-rmse:0.108229 \n[9] train-rmse:0.107343 \n[10]    train-rmse:0.106871 \n[11]    train-rmse:0.106623 \n[12]    train-rmse:0.106485 \n[13]    train-rmse:0.106412 \n[14]    train-rmse:0.106371 \n[15]    train-rmse:0.106348 \n[16]    train-rmse:0.106336 \n[1] train-rmse:0.355868 \n[2] train-rmse:0.259663 \n[3] train-rmse:0.200152 \n[4] train-rmse:0.162609 \n[5] train-rmse:0.139389 \n[6] train-rmse:0.125898 \n[7] train-rmse:0.118142 \n[8] train-rmse:0.113540 \n[9] train-rmse:0.110768 \n[10]    train-rmse:0.109049 \n[11]    train-rmse:0.108048 \n[12]    train-rmse:0.107412 \n[13]    train-rmse:0.107015 \n[14]    train-rmse:0.106758 \n[15]    train-rmse:0.106599 \n[16]    train-rmse:0.106498 \n[1] train-rmse:0.202989 \n[2] train-rmse:0.132188 \n[3] train-rmse:0.113915 \n[4] train-rmse:0.108559 \n[5] train-rmse:0.106982 \n[6] train-rmse:0.106521 \n[7] train-rmse:0.106381 \n[8] train-rmse:0.106339 \n[9] train-rmse:0.106327 \n[10]    train-rmse:0.106325 \n[11]    train-rmse:0.106325 \n[12]    train-rmse:0.106325 \n[13]    train-rmse:0.106325 \n[14]    train-rmse:0.106325 \n[15]    train-rmse:0.106325 \n[16]    train-rmse:0.106325 \n[1] train-rmse:0.336020 \n[2] train-rmse:0.239423 \n[3] train-rmse:0.178614 \n[4] train-rmse:0.145721 \n[5] train-rmse:0.128115 \n[6] train-rmse:0.118376 \n[7] train-rmse:0.113218 \n[8] train-rmse:0.110247 \n[9] train-rmse:0.108582 \n[10]    train-rmse:0.107646 \n[11]    train-rmse:0.107094 \n[12]    train-rmse:0.106781 \n[13]    train-rmse:0.106591 \n[14]    train-rmse:0.106481 \n[15]    train-rmse:0.106416 \n[16]    train-rmse:0.106377 \n[1] train-rmse:0.218171 \n[2] train-rmse:0.138561 \n[3] train-rmse:0.116578 \n[4] train-rmse:0.109665 \n[5] train-rmse:0.107391 \n[6] train-rmse:0.106672 \n[7] train-rmse:0.106438 \n[8] train-rmse:0.106361 \n[9] train-rmse:0.106335 \n[10]    train-rmse:0.106328 \n[11]    train-rmse:0.106326 \n[12]    train-rmse:0.106325 \n[13]    train-rmse:0.106325 \n[14]    train-rmse:0.106325 \n[15]    train-rmse:0.106325 \n[16]    train-rmse:0.106325 \n[1] train-rmse:0.243659 \n[2] train-rmse:0.151452 \n[3] train-rmse:0.122570 \n[4] train-rmse:0.112393 \n[5] train-rmse:0.108622 \n[6] train-rmse:0.107210 \n[7] train-rmse:0.106669 \n[8] train-rmse:0.106458 \n[9] train-rmse:0.106375 \n[10]    train-rmse:0.106343 \n[11]    train-rmse:0.106330 \n[12]    train-rmse:0.106327 \n[13]    train-rmse:0.106325 \n[14]    train-rmse:0.106324 \n[15]    train-rmse:0.106324 \n[16]    train-rmse:0.106324 \n[1] train-rmse:0.224229 \n[2] train-rmse:0.143881 \n[3] train-rmse:0.118744 \n[4] train-rmse:0.110552 \n[5] train-rmse:0.107774 \n[6] train-rmse:0.106820 \n[7] train-rmse:0.106494 \n[8] train-rmse:0.106382 \n[9] train-rmse:0.106343 \n[10]    train-rmse:0.106330 \n[11]    train-rmse:0.106326 \n[12]    train-rmse:0.106325 \n[13]    train-rmse:0.106325 \n[14]    train-rmse:0.106325 \n[15]    train-rmse:0.106325 \n[16]    train-rmse:0.106325 \n[1] train-rmse:0.356608 \n[2] train-rmse:0.260611 \n[3] train-rmse:0.201036 \n[4] train-rmse:0.163319 \n[5] train-rmse:0.139903 \n[6] train-rmse:0.126250 \n[7] train-rmse:0.118378 \n[8] train-rmse:0.113702 \n[9] train-rmse:0.110875 \n[10]    train-rmse:0.109121 \n[11]    train-rmse:0.108097 \n[12]    train-rmse:0.107444 \n[13]    train-rmse:0.107040 \n[14]    train-rmse:0.106780 \n[15]    train-rmse:0.106610 \n[16]    train-rmse:0.106506 \n[1] train-rmse:0.258755 \n[2] train-rmse:0.161024 \n[3] train-rmse:0.127393 \n[4] train-rmse:0.114806 \n[5] train-rmse:0.109861 \n[6] train-rmse:0.107794 \n[7] train-rmse:0.106926 \n[8] train-rmse:0.106577 \n[9] train-rmse:0.106431 \n[10]    train-rmse:0.106368 \n[11]    train-rmse:0.106342 \n[12]    train-rmse:0.106330 \n[13]    train-rmse:0.106326 \n[14]    train-rmse:0.106325 \n[15]    train-rmse:0.106324 \n[16]    train-rmse:0.106324 \n[1] train-rmse:0.498496 \n[2] train-rmse:0.497004 \n[3] train-rmse:0.495503 \n[4] train-rmse:0.494016 \n[5] train-rmse:0.492527 \n[6] train-rmse:0.491058 \n[7] train-rmse:0.489582 \n[8] train-rmse:0.488116 \n[9] train-rmse:0.486651 \n[10]    train-rmse:0.485198 \n[11]    train-rmse:0.483749 \n[12]    train-rmse:0.482301 \n[13]    train-rmse:0.480863 \n[14]    train-rmse:0.479433 \n[15]    train-rmse:0.478008 \n[16]    train-rmse:0.476554 \n[1] train-rmse:0.218904 \n[2] train-rmse:0.140297 \n[3] train-rmse:0.117255 \n[4] train-rmse:0.109975 \n[5] train-rmse:0.107500 \n[6] train-rmse:0.106715 \n[7] train-rmse:0.106452 \n[8] train-rmse:0.106366 \n[9] train-rmse:0.106337 \n[10]    train-rmse:0.106327 \n[11]    train-rmse:0.106325 \n[12]    train-rmse:0.106324 \n[13]    train-rmse:0.106324 \n[14]    train-rmse:0.106324 \n[15]    train-rmse:0.106324 \n[16]    train-rmse:0.106324 \n[1] train-rmse:0.497207 \n[2] train-rmse:0.494446 \n[3] train-rmse:0.491677 \n[4] train-rmse:0.488942 \n[5] train-rmse:0.486212 \n[6] train-rmse:0.483530 \n[7] train-rmse:0.480842 \n[8] train-rmse:0.478182 \n[9] train-rmse:0.475485 \n[10]    train-rmse:0.472820 \n[11]    train-rmse:0.470169 \n[12]    train-rmse:0.467528 \n[13]    train-rmse:0.464914 \n[14]    train-rmse:0.462323 \n[15]    train-rmse:0.459744 \n[16]    train-rmse:0.457167 \n[1] train-rmse:0.421967 \n[2] train-rmse:0.356180 \n[3] train-rmse:0.303317 \n[4] train-rmse:0.261215 \n[5] train-rmse:0.226402 \n[6] train-rmse:0.200018 \n[7] train-rmse:0.179061 \n[8] train-rmse:0.162063 \n[9] train-rmse:0.149095 \n[10]    train-rmse:0.138970 \n[11]    train-rmse:0.131664 \n[12]    train-rmse:0.125849 \n[13]    train-rmse:0.121325 \n[14]    train-rmse:0.117884 \n[15]    train-rmse:0.115363 \n[16]    train-rmse:0.113489 \n[1] train-rmse:0.202819 \n[2] train-rmse:0.132252 \n[3] train-rmse:0.113920 \n[4] train-rmse:0.108616 \n[5] train-rmse:0.106990 \n[6] train-rmse:0.106518 \n[7] train-rmse:0.106380 \n[8] train-rmse:0.106339 \n[9] train-rmse:0.106327 \n[10]    train-rmse:0.106325 \n[11]    train-rmse:0.106324 \n[12]    train-rmse:0.106324 \n[13]    train-rmse:0.106324 \n[14]    train-rmse:0.106324 \n[15]    train-rmse:0.106324 \n[16]    train-rmse:0.106324 \n[1] train-rmse:0.281255 \n[2] train-rmse:0.177293 \n[3] train-rmse:0.136263 \n[4] train-rmse:0.119489 \n[5] train-rmse:0.112259 \n[6] train-rmse:0.109022 \n[7] train-rmse:0.107610 \n[8] train-rmse:0.106922 \n[9] train-rmse:0.106606 \n[10]    train-rmse:0.106456 \n[11]    train-rmse:0.106385 \n[12]    train-rmse:0.106352 \n[13]    train-rmse:0.106336 \n[14]    train-rmse:0.106329 \n[15]    train-rmse:0.106326 \n[16]    train-rmse:0.106326 \n[1] train-rmse:0.359633 \n[2] train-rmse:0.264519 \n[3] train-rmse:0.204909 \n[4] train-rmse:0.165767 \n[5] train-rmse:0.142295 \n[6] train-rmse:0.128040 \n[7] train-rmse:0.119535 \n[8] train-rmse:0.114573 \n[9] train-rmse:0.111464 \n[10]    train-rmse:0.109553 \n[11]    train-rmse:0.108353 \n[12]    train-rmse:0.107599 \n[13]    train-rmse:0.107136 \n[14]    train-rmse:0.106842 \n[15]    train-rmse:0.106655 \n[16]    train-rmse:0.106536 \n[1] train-rmse:0.339902 \n[2] train-rmse:0.243940 \n[3] train-rmse:0.182473 \n[4] train-rmse:0.148246 \n[5] train-rmse:0.129837 \n[6] train-rmse:0.119676 \n[7] train-rmse:0.114036 \n[8] train-rmse:0.110838 \n[9] train-rmse:0.108953 \n[10]    train-rmse:0.107867 \n[11]    train-rmse:0.107235 \n[12]    train-rmse:0.106866 \n[13]    train-rmse:0.106645 \n[14]    train-rmse:0.106515 \n[15]    train-rmse:0.106438 \n[16]    train-rmse:0.106392 \n[1] train-rmse:0.172884 \n[2] train-rmse:0.114327 \n[3] train-rmse:0.097112 \n[4] train-rmse:0.092596 \n[5] train-rmse:0.091285 \n[6] train-rmse:0.090974 \n[7] train-rmse:0.090897 \n[8] train-rmse:0.090876 \n[9] train-rmse:0.090872 \n[10]    train-rmse:0.090871 \n[11]    train-rmse:0.090871 \n[12]    train-rmse:0.090871 \n[13]    train-rmse:0.090871 \n[14]    train-rmse:0.090871 \n[15]    train-rmse:0.090871 \n[16]    train-rmse:0.090871 \n[1] train-rmse:0.180143 \n[2] train-rmse:0.119365 \n[3] train-rmse:0.099512 \n[4] train-rmse:0.093257 \n[5] train-rmse:0.091529 \n[6] train-rmse:0.091051 \n[7] train-rmse:0.090931 \n[8] train-rmse:0.090886 \n[9] train-rmse:0.090874 \n[10]    train-rmse:0.090871 \n[11]    train-rmse:0.090871 \n[12]    train-rmse:0.090871 \n[13]    train-rmse:0.090871 \n[14]    train-rmse:0.090871 \n[15]    train-rmse:0.090871 \n[16]    train-rmse:0.090871 \n[1] train-rmse:0.467837 \n[2] train-rmse:0.438604 \n[3] train-rmse:0.411015 \n[4] train-rmse:0.385333 \n[5] train-rmse:0.362395 \n[6] train-rmse:0.340771 \n[7] train-rmse:0.320978 \n[8] train-rmse:0.302839 \n[9] train-rmse:0.285958 \n[10]    train-rmse:0.269559 \n[11]    train-rmse:0.254379 \n[12]    train-rmse:0.240419 \n[13]    train-rmse:0.227989 \n[14]    train-rmse:0.216288 \n[15]    train-rmse:0.205268 \n[16]    train-rmse:0.195270 \n[1] train-rmse:0.306273 \n[2] train-rmse:0.207907 \n[3] train-rmse:0.150508 \n[4] train-rmse:0.122639 \n[5] train-rmse:0.107649 \n[6] train-rmse:0.099862 \n[7] train-rmse:0.095745 \n[8] train-rmse:0.093491 \n[9] train-rmse:0.092295 \n[10]    train-rmse:0.091648 \n[11]    train-rmse:0.091298 \n[12]    train-rmse:0.091104 \n[13]    train-rmse:0.090998 \n[14]    train-rmse:0.090940 \n[15]    train-rmse:0.090908 \n[16]    train-rmse:0.090890 \n[1] train-rmse:0.350994 \n[2] train-rmse:0.256770 \n[3] train-rmse:0.192391 \n[4] train-rmse:0.152738 \n[5] train-rmse:0.128474 \n[6] train-rmse:0.113825 \n[7] train-rmse:0.105039 \n[8] train-rmse:0.099728 \n[9] train-rmse:0.096563 \n[10]    train-rmse:0.094503 \n[11]    train-rmse:0.093173 \n[12]    train-rmse:0.092334 \n[13]    train-rmse:0.091808 \n[14]    train-rmse:0.091472 \n[15]    train-rmse:0.091260 \n[16]    train-rmse:0.091121 \n[1] train-rmse:0.188462 \n[2] train-rmse:0.121134 \n[3] train-rmse:0.100574 \n[4] train-rmse:0.093793 \n[5] train-rmse:0.091724 \n[6] train-rmse:0.091118 \n[7] train-rmse:0.090942 \n[8] train-rmse:0.090891 \n[9] train-rmse:0.090876 \n[10]    train-rmse:0.090872 \n[11]    train-rmse:0.090872 \n[12]    train-rmse:0.090872 \n[13]    train-rmse:0.090872 \n[14]    train-rmse:0.090872 \n[15]    train-rmse:0.090872 \n[16]    train-rmse:0.090872 \n[1] train-rmse:0.330286 \n[2] train-rmse:0.233028 \n[3] train-rmse:0.171402 \n[4] train-rmse:0.136800 \n[5] train-rmse:0.116880 \n[6] train-rmse:0.105968 \n[7] train-rmse:0.099698 \n[8] train-rmse:0.096035 \n[9] train-rmse:0.093901 \n[10]    train-rmse:0.092672 \n[11]    train-rmse:0.091943 \n[12]    train-rmse:0.091513 \n[13]    train-rmse:0.091258 \n[14]    train-rmse:0.091101 \n[15]    train-rmse:0.091007 \n[16]    train-rmse:0.090952 \n[1] train-rmse:0.205089 \n[2] train-rmse:0.126427 \n[3] train-rmse:0.103381 \n[4] train-rmse:0.095116 \n[5] train-rmse:0.092337 \n[6] train-rmse:0.091355 \n[7] train-rmse:0.091028 \n[8] train-rmse:0.090923 \n[9] train-rmse:0.090887 \n[10]    train-rmse:0.090876 \n[11]    train-rmse:0.090873 \n[12]    train-rmse:0.090872 \n[13]    train-rmse:0.090872 \n[14]    train-rmse:0.090872 \n[15]    train-rmse:0.090872 \n[16]    train-rmse:0.090872 \n[1] train-rmse:0.232649 \n[2] train-rmse:0.140287 \n[3] train-rmse:0.109680 \n[4] train-rmse:0.098320 \n[5] train-rmse:0.093795 \n[6] train-rmse:0.092004 \n[7] train-rmse:0.091305 \n[8] train-rmse:0.091040 \n[9] train-rmse:0.090936 \n[10]    train-rmse:0.090895 \n[11]    train-rmse:0.090879 \n[12]    train-rmse:0.090875 \n[13]    train-rmse:0.090872 \n[14]    train-rmse:0.090872 \n[15]    train-rmse:0.090872 \n[16]    train-rmse:0.090872 \n[1] train-rmse:0.211676 \n[2] train-rmse:0.130184 \n[3] train-rmse:0.104742 \n[4] train-rmse:0.095784 \n[5] train-rmse:0.092631 \n[6] train-rmse:0.091497 \n[7] train-rmse:0.091087 \n[8] train-rmse:0.090944 \n[9] train-rmse:0.090896 \n[10]    train-rmse:0.090878 \n[11]    train-rmse:0.090874 \n[12]    train-rmse:0.090872 \n[13]    train-rmse:0.090872 \n[14]    train-rmse:0.090872 \n[15]    train-rmse:0.090872 \n[16]    train-rmse:0.090872 \n[1] train-rmse:0.351765 \n[2] train-rmse:0.257726 \n[3] train-rmse:0.193300 \n[4] train-rmse:0.153479 \n[5] train-rmse:0.129025 \n[6] train-rmse:0.114251 \n[7] train-rmse:0.105334 \n[8] train-rmse:0.099924 \n[9] train-rmse:0.096689 \n[10]    train-rmse:0.094583 \n[11]    train-rmse:0.093217 \n[12]    train-rmse:0.092362 \n[13]    train-rmse:0.091826 \n[14]    train-rmse:0.091482 \n[15]    train-rmse:0.091265 \n[16]    train-rmse:0.091124 \n[1] train-rmse:0.248807 \n[2] train-rmse:0.150750 \n[3] train-rmse:0.114629 \n[4] train-rmse:0.100943 \n[5] train-rmse:0.095022 \n[6] train-rmse:0.092671 \n[7] train-rmse:0.091637 \n[8] train-rmse:0.091212 \n[9] train-rmse:0.091015 \n[10]    train-rmse:0.090932 \n[11]    train-rmse:0.090895 \n[12]    train-rmse:0.090880 \n[13]    train-rmse:0.090874 \n[14]    train-rmse:0.090873 \n[15]    train-rmse:0.090873 \n[16]    train-rmse:0.090873 \n[1] train-rmse:0.498454 \n[2] train-rmse:0.496913 \n[3] train-rmse:0.495379 \n[4] train-rmse:0.493850 \n[5] train-rmse:0.492327 \n[6] train-rmse:0.490810 \n[7] train-rmse:0.489318 \n[8] train-rmse:0.487812 \n[9] train-rmse:0.486312 \n[10]    train-rmse:0.484818 \n[11]    train-rmse:0.483348 \n[12]    train-rmse:0.481865 \n[13]    train-rmse:0.480387 \n[14]    train-rmse:0.478934 \n[15]    train-rmse:0.477468 \n[16]    train-rmse:0.476007 \n[1] train-rmse:0.205887 \n[2] train-rmse:0.126584 \n[3] train-rmse:0.103145 \n[4] train-rmse:0.095044 \n[5] train-rmse:0.092488 \n[6] train-rmse:0.091395 \n[7] train-rmse:0.091042 \n[8] train-rmse:0.090927 \n[9] train-rmse:0.090888 \n[10]    train-rmse:0.090875 \n[11]    train-rmse:0.090872 \n[12]    train-rmse:0.090872 \n[13]    train-rmse:0.090872 \n[14]    train-rmse:0.090872 \n[15]    train-rmse:0.090872 \n[16]    train-rmse:0.090872 \n[1] train-rmse:0.497128 \n[2] train-rmse:0.494277 \n[3] train-rmse:0.491445 \n[4] train-rmse:0.488669 \n[5] train-rmse:0.485877 \n[6] train-rmse:0.483105 \n[7] train-rmse:0.480352 \n[8] train-rmse:0.477654 \n[9] train-rmse:0.474939 \n[10]    train-rmse:0.472245 \n[11]    train-rmse:0.469603 \n[12]    train-rmse:0.466945 \n[13]    train-rmse:0.464307 \n[14]    train-rmse:0.461687 \n[15]    train-rmse:0.459094 \n[16]    train-rmse:0.456486 \n[1] train-rmse:0.419566 \n[2] train-rmse:0.354595 \n[3] train-rmse:0.302976 \n[4] train-rmse:0.261853 \n[5] train-rmse:0.227510 \n[6] train-rmse:0.198239 \n[7] train-rmse:0.174989 \n[8] train-rmse:0.156728 \n[9] train-rmse:0.142304 \n[10]    train-rmse:0.131013 \n[11]    train-rmse:0.122163 \n[12]    train-rmse:0.115350 \n[13]    train-rmse:0.110198 \n[14]    train-rmse:0.106096 \n[15]    train-rmse:0.103045 \n[16]    train-rmse:0.100497 \n[1] train-rmse:0.188274 \n[2] train-rmse:0.121075 \n[3] train-rmse:0.100476 \n[4] train-rmse:0.093772 \n[5] train-rmse:0.091719 \n[6] train-rmse:0.091116 \n[7] train-rmse:0.090941 \n[8] train-rmse:0.090890 \n[9] train-rmse:0.090876 \n[10]    train-rmse:0.090872 \n[11]    train-rmse:0.090872 \n[12]    train-rmse:0.090872 \n[13]    train-rmse:0.090872 \n[14]    train-rmse:0.090872 \n[15]    train-rmse:0.090872 \n[16]    train-rmse:0.090872 \n[1] train-rmse:0.272716 \n[2] train-rmse:0.168770 \n[3] train-rmse:0.125837 \n[4] train-rmse:0.106769 \n[5] train-rmse:0.098221 \n[6] train-rmse:0.094337 \n[7] train-rmse:0.092501 \n[8] train-rmse:0.091659 \n[9] train-rmse:0.091242 \n[10]    train-rmse:0.091046 \n[11]    train-rmse:0.090953 \n[12]    train-rmse:0.090909 \n[13]    train-rmse:0.090888 \n[14]    train-rmse:0.090878 \n[15]    train-rmse:0.090875 \n[16]    train-rmse:0.090873 \n[1] train-rmse:0.354916 \n[2] train-rmse:0.261668 \n[3] train-rmse:0.197618 \n[4] train-rmse:0.157084 \n[5] train-rmse:0.131749 \n[6] train-rmse:0.116237 \n[7] train-rmse:0.106711 \n[8] train-rmse:0.100926 \n[9] train-rmse:0.097294 \n[10]    train-rmse:0.094981 \n[11]    train-rmse:0.093500 \n[12]    train-rmse:0.092565 \n[13]    train-rmse:0.091972 \n[14]    train-rmse:0.091582 \n[15]    train-rmse:0.091333 \n[16]    train-rmse:0.091172 \n[1] train-rmse:0.334342 \n[2] train-rmse:0.237612 \n[3] train-rmse:0.175374 \n[4] train-rmse:0.139616 \n[5] train-rmse:0.118774 \n[6] train-rmse:0.107187 \n[7] train-rmse:0.100534 \n[8] train-rmse:0.096630 \n[9] train-rmse:0.094311 \n[10]    train-rmse:0.092937 \n[11]    train-rmse:0.092117 \n[12]    train-rmse:0.091627 \n[13]    train-rmse:0.091330 \n[14]    train-rmse:0.091151 \n[15]    train-rmse:0.091040 \n[16]    train-rmse:0.090973 \n\nprint(results)\n\n  set depth         eta       AUC\n1   1   110 0.007334147 0.7782072\n2   2   153 0.082437558 0.8068937\n3   3    89 0.003948339 0.8102082\n\n\nMake predictions:\n\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n\n      data_xg = xgb.DMatrix(data = as.matrix(data_obs[,-1]), label = data_obs$survived)\n      model = xgboost(data_xg, nrounds = 16L, eta = results$eta[i], max_depth = results$depth[i])\n      predictions = predict(model, newdata = as.matrix(data_new)[,-1])\n    return(predictions)\n  })\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n\n\n\n\n\n\n\n\n\n\nBonus: Implement a BRT on your own!\n\n\n\nYou can easily implement a BRT or boosted linear model using the rpart package or the lm function.\n\n\nClick here to see the solution\n\nGo through the code line by line and try to understand it. Ask, if you have any questions you cannot solve.\nLet’s try it:\n\ndata = model.matrix(~. , data = airquality)\n\nmodel = get_boosting_model(x = data[,-2], y = data[,2], n_trees = 5L )\npred = predict(model, newdata = data[,-2])\nplot(data[,2], pred, xlab = \"observed\", ylab = \"predicted\")"
  },
  {
    "objectID": "B2-Distance.html#k-nearest-neighbor",
    "href": "B2-Distance.html#k-nearest-neighbor",
    "title": "6  Distance-based Algorithms",
    "section": "6.1 K-Nearest-Neighbor",
    "text": "6.1 K-Nearest-Neighbor\nK-nearest-neighbor (kNN) is a simple algorithm that stores all the available cases and classifies the new data based on a similarity measure. It is mostly used to classify a data point based on how its \\(k\\) nearest neighbors are classified.\nLet us first see an example:\n\nx = scale(iris[,1:4])\ny = iris[,5]\nplot(x[-100,1], x[-100, 3], col = y)\npoints(x[100,1], x[100, 3], col = \"blue\", pch = 18, cex = 1.3)\n\n\n\n\nWhich class would you decide for the blue point? What are the classes of the nearest points? Well, this procedure is used by the k-nearest-neighbors classifier and thus there is actually no “real” learning in a k-nearest-neighbors classification.\nFor applying a k-nearest-neighbors classification, we first have to scale the data set, because we deal with distances and want the same influence of all predictors. Imagine one variable has values from -10.000 to 10.000 and another from -1 to 1. Then the influence of the first variable on the distance to the other points is much stronger than the influence of the second variable. On the iris data set, we have to split the data into training and test set on our own. Then we will follow the usual pipeline.\n\ndata = iris\ndata[,1:4] = apply(data[,1:4],2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\nFit model and create predictions:\n\nlibrary(kknn)\nset.seed(123)\n\nknn = kknn(Species~., train = train, test = test)\nsummary(knn)\n\n\nCall:\nkknn(formula = Species ~ ., train = train, test = test)\n\nResponse: \"nominal\"\n          fit prob.setosa prob.versicolor prob.virginica\n1      setosa           1       0.0000000      0.0000000\n2      setosa           1       0.0000000      0.0000000\n3      setosa           1       0.0000000      0.0000000\n4      setosa           1       0.0000000      0.0000000\n5      setosa           1       0.0000000      0.0000000\n6      setosa           1       0.0000000      0.0000000\n7      setosa           1       0.0000000      0.0000000\n8      setosa           1       0.0000000      0.0000000\n9      setosa           1       0.0000000      0.0000000\n10     setosa           1       0.0000000      0.0000000\n11     setosa           1       0.0000000      0.0000000\n12     setosa           1       0.0000000      0.0000000\n13     setosa           1       0.0000000      0.0000000\n14     setosa           1       0.0000000      0.0000000\n15 versicolor           0       0.9843084      0.0156916\n16 versicolor           0       1.0000000      0.0000000\n17 versicolor           0       1.0000000      0.0000000\n18 versicolor           0       1.0000000      0.0000000\n19 versicolor           0       1.0000000      0.0000000\n20  virginica           0       0.4482986      0.5517014\n21 versicolor           0       1.0000000      0.0000000\n22 versicolor           0       1.0000000      0.0000000\n23 versicolor           0       1.0000000      0.0000000\n24 versicolor           0       1.0000000      0.0000000\n25  virginica           0       0.0000000      1.0000000\n26 versicolor           0       0.7783044      0.2216956\n27  virginica           0       0.1339415      0.8660585\n28  virginica           0       0.1008186      0.8991814\n29  virginica           0       0.0156916      0.9843084\n30  virginica           0       0.0000000      1.0000000\n31  virginica           0       0.0000000      1.0000000\n32  virginica           0       0.1257844      0.8742156\n33 versicolor           0       0.8742156      0.1257844\n34  virginica           0       0.0000000      1.0000000\n35  virginica           0       0.3569042      0.6430958\n36  virginica           0       0.4420312      0.5579688\n37  virginica           0       0.0000000      1.0000000\n38 versicolor           0       0.6931774      0.3068226\n39  virginica           0       0.0000000      1.0000000\n40  virginica           0       0.1885727      0.8114273\n41  virginica           0       0.0000000      1.0000000\n42  virginica           0       0.0000000      1.0000000\n43  virginica           0       0.4934627      0.5065373\n44  virginica           0       0.0000000      1.0000000\n45  virginica           0       0.2373872      0.7626128\n\ntable(test$Species, fitted(knn))\n\n            \n             setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      0          9         1\n  virginica       0          3        18"
  },
  {
    "objectID": "B2-Distance.html#support-vector-machines-svms",
    "href": "B2-Distance.html#support-vector-machines-svms",
    "title": "6  Distance-based Algorithms",
    "section": "6.2 Support Vector Machines (SVMs)",
    "text": "6.2 Support Vector Machines (SVMs)\nSupport vectors machines have a different approach. They try to divide the predictor space into sectors for each class. To do so, a support-vector machine fits the parameters of a hyperplane (a \\(n-1\\) dimensional subspace in a \\(n\\)-dimensional space) in the predictor space by optimizing the distance between the hyperplane and the nearest point from each class.\nFitting a support-vector machine:\n\nlibrary(e1071)\n\ndata = iris\ndata[,1:4] = apply(data[,1:4], 2, scale)\nindices = sample.int(nrow(data), 0.7*nrow(data))\ntrain = data[indices,]\ntest = data[-indices,]\n\nsm = svm(Species~., data = train, kernel = \"linear\")\npred = predict(sm, newdata = test)\n\n\noldpar = par(mfrow = c(1, 2))\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  pred, main = \"predicted\")\nplot(test$Sepal.Length, test$Petal.Length,\n     col =  test$Species, main = \"observed\")\n\n\n\npar(oldpar)\n\nmean(pred == test$Species) # Accuracy.\n\n[1] 0.9777778\n\n\nSupport-vector machines can only work on linearly separable problems. (A problem is called linearly separable if there exists at least one line in the plane with all of the points of one class on one side of the hyperplane and all the points of the others classes on the other side).\nIf this is not possible, we however, can use the so called kernel trick, which maps the predictor space into a (higher dimensional) space in which the problem is linear separable. After having identified the boundaries in the higher-dimensional space, we can project them back into the original dimensions.\n\nx1 = seq(-3, 3, length.out = 100)\nx2 = seq(-3, 3, length.out = 100)\nX = expand.grid(x1, x2)\ny = apply(X, 1, function(t) exp(-t[1]^2 - t[2]^2))\ny = ifelse(1/(1+exp(-y)) &lt; 0.62, 0, 1)\n\nimage(matrix(y, 100, 100))\nanimation::saveGIF(\n  {\n    for(i in c(\"truth\", \"linear\", \"radial\", \"sigmoid\")){\n      if(i == \"truth\"){\n        image(matrix(y, 100,100),\n        main = \"Ground truth\", axes = FALSE, las = 2)\n      }else{\n        sv = e1071::svm(x = x, y = factor(y), kernel = i)\n        image(matrix(as.numeric(as.character(predict(sv, x))), 100, 100),\n        main = paste0(\"Kernel: \", i), axes = FALSE, las = 2)\n        axis(1, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1))\n        axis(2, at = seq(0,1, length.out = 10),\n        labels = round(seq(-3, 3, length.out = 10), 1), las = 2)\n      }\n    }\n  },\n  movie.name = \"svm.gif\", autobrowse = FALSE, interval = 2\n)\n\n\n\n\n\n\nAs you have seen, this does not work with every kernel. Hence, the problem is to find the actual correct kernel, which is again an optimization procedure and can thus be approximated."
  },
  {
    "objectID": "B2-Distance.html#exercises",
    "href": "B2-Distance.html#exercises",
    "title": "6  Distance-based Algorithms",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\n\n\n\n\n\n\nQuestion: Hyperparameter tuning of kNN\n\n\n\nCombing back to the titanic dataset from the morning, we want to optimize the number of neighbors (k) and the kernel of the kNN:\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\nHints:\n\ncheck the help of the kNN function to get an idea about the hyperparameters\n\n\n\n\n\n\n\nCode template\n\n\n\n\n\n\nlibrary(kknn)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\nhyper_k = ... # must be integer vector\nhyper_kernel = ... # must be character vector\n\nresults = data.frame(\n  set = rep(NA, cv),\n  k = rep(NA, cv),\n  kernel = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_k), function(k) {\n        predictions = kknn(as.factor(survived)~., train = train_outer, test = test_outer, k = hyper_k[k], scale = FALSE, kernel = hyper_kernel[k])\n        return(Metrics::auc(test_outer$survived, predictions$prob[,2]))\n      })\n  \n  results[i, 1] = i\n  results[i, 2] = hyper_k[which.max(tuning_results)]\n  results[i, 3] = hyper_kernel[which.max(tuning_results)]  \n  results[i, 4] = max(tuning_results)\n}\n\nprint(results)\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(kknn)\nset.seed(42)\ndata_obs = data_sub[!is.na(data_sub$survived),] \ncv = 3\n\nouter_split = as.integer(cut(1:nrow(data_obs), breaks = cv))\n\n# sample minnodesize values (must be integers)\nhyper_k = sample(10, 10)\nhyper_kernel = sample(c(\"triangular\", \"inv\", \"gaussian\", \"rank\"), 10, replace = TRUE)\n\nresults = data.frame(\n  set = rep(NA, cv),\n  k = rep(NA, cv),\n  kernel = rep(NA, cv),\n  AUC = rep(NA, cv)\n)\n\nfor(i in 1:cv) {\n  train_outer = data_obs[outer_split != i, ]\n  test_outer = data_obs[outer_split == i, ]\n  \n  tuning_results = \n      sapply(1:length(hyper_k), function(k) {\n        predictions = kknn(as.factor(survived)~., train = train_outer, test = test_outer, k = hyper_k[k], scale = FALSE, kernel = hyper_kernel[k])\n        return(Metrics::auc(test_outer$survived, predictions$prob[,2]))\n      })\n  \n  results[i, 1] = i\n  results[i, 2] = hyper_k[which.max(tuning_results)]\n  results[i, 3] = hyper_kernel[which.max(tuning_results)]  \n  results[i, 4] = max(tuning_results)\n}\n\nprint(results)\n\n  set  k   kernel       AUC\n1   1 10 gaussian 0.8149876\n2   2  9     rank 0.8080321\n3   3  6      inv 0.8290219\n\n\nMake predictions:\n\nprediction_ensemble = \n  sapply(1:nrow(results), function(i) {\n    predictions = kknn(as.factor(survived)~., train = data_obs, test = data_new, k = results$k[i], scale = FALSE, kernel = results$kernel[i])\n    return(predictions$prob[,2])\n  })\n\n# Single predictions from the ensemble model:\nwrite.csv(data.frame(y = apply(prediction_ensemble, 1, mean)), file = \"Max_titanic_ensemble.csv\")\n\n\n\n\n\n\n\n\n\n\nQuestion: kNN and SVM\n\n\n\nFit a standard k-nearest-neighbor classifier and a support vector machine with a linear kernel (check help) on the Sonar dataset, and report what fitted better.\nPrepare dataset:\n\nlibrary(mlbench)\nset.seed(123)\n\ndata(Sonar)\ndata = Sonar\n#str(data)\n\n# Do not forget scaling! This may be done implicitly by most functions.\n# Here, it's done explicitly for teaching purposes.\ndata = cbind.data.frame(\n  scale(data[,-length(data)]),\n  \"class\" = data[,length(data)]\n)\n\nn = length(data[,1])\nindicesTrain = sample.int(n, (n+1) %/% 2) # Take (at least) 50 % of the data.\n\ntrain = data[indicesTrain,]\ntest = data[-indicesTrain,]\n\nTasks:\n\nFit a svm (from the e1071 package) on the train dataset and make predictions for the test dataset\nFit a kNN (from the kknn package) on the train dataset and make predictions for the test dataset\nCalculate confusion matrices to compare the performance\n\n\n\nClick here to see the solution\n\n\nlibrary(e1071)\nlibrary(kknn)\n\nknn = kknn(class~., train = train, test = test, scale = FALSE,\n           kernel = \"rectangular\")\npredKNN = predict(knn, newdata = test)\n\nsm = svm(class~., data = train, scale = FALSE, kernel = \"linear\")\npredSVM = predict(sm, newdata = test)\n\n\n\nK-nearest-neighbor, standard (rectangular) kernel:\n\n\n       labelsTest\npredKNN  M  R\n      M 46 29\n      R  8 21\n\n\nCorrectly classified:  67  /  104\n\n\n\n\nSupport-vector machine, linear kernel:\n\n\n       labelsTest\npredSVM  M  R\n      M 41 15\n      R 13 35\n\n\nCorrectly classified:  76  /  104\n\n\nK-nearest neighbor fitted (slightly) better."
  },
  {
    "objectID": "B3-NeuralNetworks.html#regularization",
    "href": "B3-NeuralNetworks.html#regularization",
    "title": "7  Artificial Neural Networks",
    "section": "7.1 Regularization",
    "text": "7.1 Regularization\nWe can use \\(\\lambda\\) and \\(\\alpha\\) to set L1 and L2 regularization on the weights in our NN:\n\nmodel = dnn(Ozone~., \n            hidden = c(10L, 10L), \n            activation = c(\"selu\", \"selu\"), \n            loss = \"mse\", \n            lr = 0.05,\n            lambda = 0.1,\n            alpha = 0.5,\n            lr_scheduler = config_lr_scheduler(\"step\", step_size = 30, gamma = 0.1),\n            # reduce learning all 30 epochs (new lr = 0.1* old lr)\n            data = data, epochs = 150L, verbose = FALSE)\n\n\n\nsummary(model)\n\nDeep Neural Network Model summary\nModel generated on basis of: \nFeature Importance:\n  variable importance\n1  Solar.R   1.170810\n2     Wind   1.854874\n3     Temp   2.703980\n4    Month   1.001610\n5      Day   1.014565\n\n\nBe careful that you don’t accidentally set all weights to 0 because of a too high regularization. We check the weights of the first layer:\n\nfields::image.plot(coef(model)[[1]][[1]]) # weights of the first layer"
  },
  {
    "objectID": "B3-NeuralNetworks.html#exercise",
    "href": "B3-NeuralNetworks.html#exercise",
    "title": "7  Artificial Neural Networks",
    "section": "7.2 Exercise",
    "text": "7.2 Exercise\n\n\n\n\n\n\nQuestion: Hyperparameter tuning - Titanic dataset\n\n\n\nTune architecture\n\nPlay around with the architecture and try to improve the AUC on the submission server\n\nBonus:\n\nTune the architecture! (depth and width of the NN via the hidden argument)\n\n\nlibrary(EcoData)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(missRanger)\ndata(titanic_ml)\ndata = titanic_ml\ndata = \n  data %&gt;% select(survived, sex, age, fare, pclass)\ndata[,-1] = missRanger(data[,-1], verbose = 0)\n\ndata_sub =\n  data %&gt;%\n    mutate(age = scales::rescale(age, c(0, 1)),\n           fare = scales::rescale(fare, c(0, 1))) %&gt;%\n    mutate(sex = as.integer(sex) - 1L,\n           pclass = as.integer(pclass - 1L))\ndata_new = data_sub[is.na(data_sub$survived),] # for which we want to make predictions at the end\ndata_obs = data_sub[!is.na(data_sub$survived),] # data with known response\n\n\nmodel = dnn(survived~., \n          hidden = c(10L, 10L), # change\n          activation = c(\"selu\", \"selu\"), # change\n          loss = \"binomial\", \n          lr = 0.05, #change\n          validation = 0.2,\n          lambda = 0.001, # change\n          alpha = 0.1, # change\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = data_obs, epochs = 40L, verbose = TRUE, plot= TRUE)\n\nLoss at epoch 1: training: 0.726, validation: 0.659, lr: 0.05000\n\n\n\n\n\nLoss at epoch 2: training: 0.666, validation: 0.660, lr: 0.05000\nLoss at epoch 3: training: 0.655, validation: 0.722, lr: 0.05000\nLoss at epoch 4: training: 0.634, validation: 0.635, lr: 0.05000\nLoss at epoch 5: training: 0.621, validation: 0.657, lr: 0.05000\nLoss at epoch 6: training: 0.608, validation: 0.574, lr: 0.05000\nLoss at epoch 7: training: 0.598, validation: 0.609, lr: 0.05000\nLoss at epoch 8: training: 0.575, validation: 0.639, lr: 0.05000\nLoss at epoch 9: training: 0.570, validation: 0.534, lr: 0.05000\nLoss at epoch 10: training: 0.551, validation: 0.526, lr: 0.05000\nLoss at epoch 11: training: 0.547, validation: 0.503, lr: 0.05000\nLoss at epoch 12: training: 0.537, validation: 0.487, lr: 0.05000\nLoss at epoch 13: training: 0.534, validation: 0.510, lr: 0.05000\nLoss at epoch 14: training: 0.529, validation: 0.539, lr: 0.05000\nLoss at epoch 15: training: 0.524, validation: 0.483, lr: 0.05000\nLoss at epoch 16: training: 0.521, validation: 0.528, lr: 0.05000\nLoss at epoch 17: training: 0.534, validation: 0.490, lr: 0.05000\nLoss at epoch 18: training: 0.519, validation: 0.520, lr: 0.05000\nLoss at epoch 19: training: 0.526, validation: 0.616, lr: 0.05000\nLoss at epoch 20: training: 0.529, validation: 0.492, lr: 0.05000\nLoss at epoch 21: training: 0.530, validation: 0.513, lr: 0.05000\nLoss at epoch 22: training: 0.523, validation: 0.485, lr: 0.05000\nLoss at epoch 23: training: 0.523, validation: 0.481, lr: 0.05000\nLoss at epoch 24: training: 0.520, validation: 0.490, lr: 0.05000\nLoss at epoch 25: training: 0.526, validation: 0.462, lr: 0.05000\nLoss at epoch 26: training: 0.516, validation: 0.571, lr: 0.05000\nLoss at epoch 27: training: 0.533, validation: 0.621, lr: 0.05000\nLoss at epoch 28: training: 0.515, validation: 0.523, lr: 0.05000\nLoss at epoch 29: training: 0.521, validation: 0.665, lr: 0.05000\nLoss at epoch 30: training: 0.517, validation: 0.531, lr: 0.05000\nLoss at epoch 31: training: 0.510, validation: 0.629, lr: 0.05000\nLoss at epoch 32: training: 0.515, validation: 0.570, lr: 0.05000\nLoss at epoch 33: training: 0.514, validation: 0.492, lr: 0.05000\nLoss at epoch 34: training: 0.511, validation: 0.584, lr: 0.05000\nLoss at epoch 35: training: 0.518, validation: 0.618, lr: 0.05000\nLoss at epoch 36: training: 0.519, validation: 0.569, lr: 0.04500\nLoss at epoch 37: training: 0.508, validation: 0.489, lr: 0.04500\nLoss at epoch 38: training: 0.512, validation: 0.576, lr: 0.04500\nLoss at epoch 39: training: 0.512, validation: 0.482, lr: 0.04500\nLoss at epoch 40: training: 0.511, validation: 0.626, lr: 0.04500\n\n# Predictions:\n\npredictions = predict(model, newdata = data_new)\n\nwrite.csv(data.frame(y = predictions[,1]), file = \"Max_titanic_ensemble.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Hyperparameter tuning - Plant-pollinator dataset\n\n\n\nsee Section A.2 for more information about the dataset.\nPrepare the data:\n\nlibrary(EcoData)\nlibrary(missRanger)\nlibrary(dplyr)\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\n\nplant_poll_imputed = plant_poll %&gt;% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction,\n                                           colour, \n                                           nectar,\n                                           feeding,\n                                           season)\n# Remove response variable interaction\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %&gt;%\n                                              select(-interaction), verbose = 0)\n\n# scale numeric variables\nplant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)] = scale(plant_poll_imputed[,sapply(plant_poll_imputed, is.numeric)])\n\n# Add response back to the dataset after the imputatiob\nplant_poll_imputed$interaction = plant_poll$interaction\nplant_poll_imputed$colour = as.factor(plant_poll_imputed$colour)\nplant_poll_imputed$nectar = as.factor(plant_poll_imputed$nectar)\nplant_poll_imputed$feeding = as.factor(plant_poll_imputed$feeding)\nplant_poll_imputed$season = as.factor(plant_poll_imputed$season)\n\n\ndata_new = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ] # for which we want to make predictions at the end\ndata_obs = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]# data with known response\ndim(data_obs)\n\n[1] 14690     9\n\n\nThe dataset is large! More than 10,000 observations. For now, let’s switch to a simple holdout strategy for validating our model (e.g. use 80% of the data to train the model and 20% of the data to validate your model.\nMoreover:\n\ntable(data_obs$interaction)\n\n\n    0     1 \n14095   595 \n\n\nThe data is strongly imbalanced, i.e. many 0s but only a few 1. There are different strategies how to deal with that, for example oversampling the 1s or undersampling the 0s.\nUndersampling the 0s:\n\ndata_obs = data_obs[c(sample(which(data_obs$interaction == 0), 2000), which(data_obs$interaction == 1)),]\ntable(data_obs$interaction)\n\n\n   0    1 \n2000  595 \n\ndata_obs$interaction = as.integer(data_obs$interaction)\n\n\n\nClick here to see the solution\n\nMinimal example:\n\nlibrary(cito)\nset.seed(42)\ntuning_steps = 2\nhyper_lambda = runif(tuning_steps,0.0001, 0.02)\nhyper_alpha = runif(tuning_steps,0, 1.0)\nhyper_hidden = sample.int(10, tuning_steps)\nhyper_nodes = sample(seq(5, 100), size = tuning_steps)\n\nouter_split = sample(nrow(data_obs), 0.2*nrow(data_obs))\n\nresults = data.frame(\n  set = 1,\n  lambda = rep(NA, 1),\n  alpha = rep(NA, 1),\n  hidden = rep(NA, 1),\n  nodes = rep(NA, 1),\n  AUC = rep(NA, 1)\n)\n\ntrain_outer = data_obs[-outer_split, ]\ntest_outer = data_obs[outer_split, ]\n\ntuning_results = \n    sapply(1:length(hyper_lambda), function(k) {\n      model = dnn(interaction~., \n          hidden = rep(hyper_nodes[k], hyper_hidden[k]), \n          activation = rep(\"selu\", hyper_hidden[k]), \n          loss = \"binomial\", \n          lr = 0.05,\n          lambda = hyper_lambda[k],\n          alpha = hyper_alpha[k],\n          batchsize = 100L, # increasing the batch size will reduce the runtime\n          lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n          data = train_outer, epochs = 50L, verbose = FALSE, plot= FALSE)\n      return(Metrics::auc(test_outer$interaction, predict(model, test_outer )[,1]))\n    })\nresults[1, 1] = 1\nresults[1, 2] =  hyper_lambda[which.max(tuning_results)]\nresults[1, 3] =  hyper_alpha[which.max(tuning_results)]\nresults[1, 4] =  hyper_hidden[which.max(tuning_results)]\nresults[1, 5] =  hyper_nodes[which.max(tuning_results)]  \nresults[1, 6] = max(tuning_results)\n\nprint(results)\n\n  set     lambda     alpha hidden nodes AUC\n1   1 0.01830464 0.2861395     10    22 0.5\n\n\nMake predictions:\n\nk = 1\nmodel = dnn(interaction~., \n    hidden = rep(results$nodes[k], results$hidden[k]), \n    activation = rep(\"selu\", hyper_hidden[k]), \n    loss = \"binomial\", \n    lr = 0.05,\n    lambda = results$lambda[k],\n    alpha = results$alpha[k],\n    batchsize = 100L, # increasing the batch size will reduce the runtime\n    lr_scheduler = config_lr_scheduler(\"reduce_on_plateau\", patience = 10, factor = 0.9),\n    data = train_outer, epochs = 50L, verbose = FALSE, plot= FALSE)\n\npredictions = predict(model, newdata = data_new)[,1]\n\nwrite.csv(data.frame(y = predictions), file = \"Max_plant_poll_ensemble.csv\")"
  },
  {
    "objectID": "C1-TensorFlow.html#introduction-to-tensorflow",
    "href": "C1-TensorFlow.html#introduction-to-tensorflow",
    "title": "8  Introduction to TensorFlow and Keras",
    "section": "8.1 Introduction to TensorFlow",
    "text": "8.1 Introduction to TensorFlow\nOne of the most commonly used frameworks for machine learning is TensorFlow. TensorFlow is an open source linear algebra library with focus on neural networks, published by Google in 2015. TensorFlow supports several interesting features, in particular automatic differentiation, several gradient optimizers and CPU and GPU parallelization.\nThese advantages are nicely explained in the following video:\n\n\nTo sum up the most important points of the video:\n\nTensorFlow is a math library which is highly optimized for neural networks.\nIf a GPU is available, computations can be easily run on the GPU but even on a CPU TensorFlow is still very fast.\nThe “backend” (i.e. all the functions and all computations) are written in C++ and CUDA (CUDA is a programming language for NVIDIA GPUs).\nThe interface (the part of TensorFlow we use) is written in Python and is also available in R, which means, we can write the code in R/Python but it will be executed by the (compiled) C++ backend.\n\nAll operations in TensorFlow are written in C++ and are highly optimized. But don’t worry, we don’t have to use C++ to use TensorFlow because there are several bindings for other languages. TensorFlow officially supports a Python API, but meanwhile there are several community carried APIs for other languages:\n\nR\nGo\nRust\nSwift\nJavaScript\n\nIn this course we will use TensorFlow with the https://tensorflow.rstudio.com/ binding, that was developed and published 2017 by the RStudio team. First, they developed an R package (reticulate) for calling Python in R. Actually, we are using the Python TensorFlow module in R (more about this later).\nTensorFlow offers different levels of API. We could implement a neural network completely by ourselves or we could use Keras which is provided as a submodule by TensorFlow. Keras is a powerful module for building and training neural networks. It allows us building and training neural networks in a few lines of codes. Since the end of 2018, Keras and TensorFlow are completly interoperable, allowing us to utilize the best of both. In this course, we will show how we can use Keras for neural networks but also how we can use the TensorFlow’s automatic differenation for using complex objective functions.\nUseful links:\n\nTensorFlow documentation (This is for the Python API, but just replace the “.” with “$”.)\nRstudio TensorFlow website\n\n\n8.1.1 Data Containers\nTensorFlow has two data containers (structures):\n\nconstant (tf$constant): Creates a constant (immutable) value in the computation graph.\nvariable (tf$Variable): Creates a mutable value in the computation graph (used as parameter/weight in models).\n\nTo get started with TensorFlow, we have to load the library and check if the installation worked.\n\nlibrary(tensorflow)\nlibrary(keras)\n\n# Don't worry about weird messages. TensorFlow supports additional optimizations.\nexists(\"tf\")\n\n[1] TRUE\n\nimmutable = tf$constant(5.0)\nmutable = tf$constant(5.0)\n\nDon’t worry about weird messages (they will only appear once at the start of the session).\n\n\n8.1.2 Basic Operations\nWe now can define the variables and do some math with them:\n\na = tf$constant(5)\nb = tf$constant(10)\nprint(a)\n\ntf.Tensor(5.0, shape=(), dtype=float32)\n\nprint(b)\n\ntf.Tensor(10.0, shape=(), dtype=float32)\n\nc = tf$add(a, b)\nprint(c)\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\ntf$print(c) # Prints to stderr. For stdout, use k_print_tensor(..., message).\nk_print_tensor(c) # Comes out of Keras!\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\n\nNormal R methods such as print() are provided by the R package “tensorflow”.\nThe TensorFlow library (created by the RStudio team) built R methods for all common operations:\n\n`+.tensorflow.tensor` = function(a, b){ return(tf$add(a,b)) }\n# Mind the backticks.\nk_print_tensor(a+b)\n\ntf.Tensor(15.0, shape=(), dtype=float32)\n\n\nTheir operators also automatically transform R numbers into constant tensors when attempting to add a tensor to an R number:\n\nd = c + 5  # 5 is automatically converted to a tensor.\nprint(d)\n\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nTensorFlow containers are objects, what means that they are not just simple variables of type numeric (class(5)), but they instead have so called methods. Methods are changing the state of a class (which for most of our purposes here is the values of the object). For instance, there is a method to transform the tensor object back to an R object:\n\nclass(d)\n\n[1] \"tensorflow.tensor\"                               \n[2] \"tensorflow.python.framework.ops.EagerTensor\"     \n[3] \"tensorflow.python.framework.ops._EagerTensorBase\"\n[4] \"tensorflow.python.framework.ops.Tensor\"          \n[5] \"tensorflow.python.types.internal.NativeObject\"   \n[6] \"tensorflow.python.types.core.Tensor\"             \n[7] \"python.builtin.object\"                           \n\nclass(d$numpy())\n\n[1] \"numeric\"\n\n\n\n\n8.1.3 Data Types\nR uses dynamic typing, what means you can assign a number, character, function or whatever to a variable and the the type is automatically inferred. In other languages you have to state the type explicitly, e.g. in C:\n\nint a = 5;\nfloat a = 5.0;\nchar a = \"a\";\n\nWhile TensorFlow tries to infer the type dynamically, you must often state it explicitly. Common important types:\n\nfloat32 (floating point number with 32 bits, “single precision”)\nfloat64 (floating point number with 64 bits, “double precision”)\nint8 (integer with 8 bits)\n\nThe reason why TensorFlow is so explicit about types is that many GPUs (e.g. the NVIDIA GeForces) can handle only up to 32 bit numbers! (you do not need high precision in graphical modeling)\nBut let us see in practice what we have to do with these types and how to specifcy them:\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float32\") \nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Doesn't work! We try to divide float32/float64.\n\nSo what went wrong here? We tried to divide a float32 by a float64 number, but we can only divide numbers of the same type!\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = \"float64\")\nb = tf$constant(2.0, dtype = \"float64\")\nc = m / b # Now it works.\n\nWe can also specify the type of the object by providing an object e.g. tf$float64.\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = tf$constant(r_matrix, dtype = tf$float64)\n\nIn TensorFlow, arguments often require exact/explicit data types: TensorFlow often expects integers as arguments. In R however an integer is normally saved as float. Thus, we have to use an “L” after an integer to tell the R interpreter that it should be treated as an integer:\n\nis.integer(5)\nis.integer(5L)\nmatrix(t(r_matrix), 5, 20, byrow = TRUE)\ntf$reshape(r_matrix, shape = c(5, 20))$numpy()\ntf$reshape(r_matrix, shape = c(5L, 20L))$numpy()\n\nSkipping the “L” is one of the most common errors when using R-TensorFlow!\n\n\n8.1.4 Exercises\n\n\n\n\n\n\nQuestion: TensorFlow Operations\n\n\n\nTo run TensorFlow from R, note that you can access the different mathematical operations in TensorFlow via tf$…, e.g. there is a tf$math$… for all common math operations or the tf$linalg$… for different linear algebra operations. Tip: type tf$ and then hit the tab key to list all available options (sometimes you have to do this directly in the console).\nAn example: How to get the maximum value of a vector?\nAn example: How to get the maximum value of a vector?\n\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\nmax(x)  # R solution. Integer!\ntf$math$reduce_max(x) # TensorFlow solution. Integer!\n\nmax(y)  # Float!\ntf$math$reduce_max(y) # Float!\n\nRewrite the following expressions (a to g) in TensorFlow:\n\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n\n[1] 1\n\n# b)\nmean(x)\n\n[1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n\n[1] 1\n\n# d) \nwhich.min(x)\n\n[1] 100\n\n# e) Tip: Use Google! \norder(x)\n\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n\n[1] TRUE\n\n\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntf$math$reduce_min(x) # Integer!\n\ntf.Tensor(1, shape=(), dtype=int32)\n\ntf$math$reduce_min(y) # Float!\n\ntf.Tensor(1.0, shape=(), dtype=float32)\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n\n[1] 50.5\n\nmean(y)\n\n[1] 50.5\n\ntf$math$reduce_mean(x)  # Integer!\n\ntf.Tensor(50, shape=(), dtype=int32)\n\ntf$math$reduce_mean(y)  # Float!\n\ntf.Tensor(50.5, shape=(), dtype=float32)\n\n# c)    which.max(x)\ntf$argmax(x)\n\ntf.Tensor(0, shape=(), dtype=int64)\n\ntf$argmax(y)\n\ntf.Tensor(0, shape=(), dtype=int64)\n\n# d)    which.min(x)\ntf$argmin(x)\n\ntf.Tensor(99, shape=(), dtype=int64)\n\n# e)    order(x)\ntf$argsort(x)\n\ntf.Tensor(\n[99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76\n 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52\n 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28\n 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4\n  3  2  1  0], shape=(100), dtype=int32)\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! TensorFlow just accepts floats in the following lines!\nmTF = tf$reshape(y, list(10L, 10L))\nm_2TF = tf$math$abs( tf$matmul(mTF, tf$transpose(mTF)) )\nm_2_logTF = tf$math$log(m_2TF)\nprint(m_2_logTF)\n\ntf.Tensor(\n[[11.4217415 11.311237  11.186988  11.045079  10.87965   10.68132\n  10.433675  10.103772   9.608109   8.582045 ]\n [11.311237  11.200746  11.076511  10.934624  10.769221  10.570932\n  10.323349   9.993557   9.498147   8.473241 ]\n [11.186988  11.076511  10.952296  10.810434  10.645068  10.446829\n  10.199324   9.869672   9.374583   8.351139 ]\n [11.045079  10.934624  10.810434  10.668607  10.503285  10.305112\n  10.05771    9.728241   9.233568   8.212026 ]\n [10.87965   10.769221  10.645068  10.503285  10.338026  10.139942\n   9.892679   9.563459   9.069353   8.0503845]\n [10.68132   10.570932  10.446829  10.305112  10.139942   9.941987\n   9.694924   9.366061   8.872767   7.857481 ]\n [10.433675  10.323349  10.199324  10.05771    9.892679   9.694924\n   9.448175   9.119868   8.62784    7.6182513]\n [10.103772   9.993557   9.869672   9.728241   9.563459   9.366061\n   9.119868   8.79255    8.302762   7.30317  ]\n [ 9.608109   9.498147   9.374583   9.233568   9.069353   8.872767\n   8.62784    8.302762   7.818028   6.8405466]\n [ 8.582045   8.473241   8.351139   8.212026   8.0503845  7.857481\n   7.6182513  7.30317    6.8405466  5.9532433]], shape=(10, 10), dtype=float32)\n\n# g)    # Custom mean function\nmean_TF = function(y){\n  result = tf$math$reduce_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_TF(y) == mean(y)\n\ntf.Tensor(True, shape=(), dtype=bool)\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Runtime\n\n\n\nThis exercise compares the speed of R to TensorFlow. The first exercise is to rewrite the following function in TensorFlow:\n\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n\nHere, we provide a skeleton for a TensorFlow function:\n\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n\nWe can compare the speed using the Microbenchmark package:\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nTry different matrix sizes for the test matrix and compare the speed.\nTip: Have a look at the the tf.reduce_mean documentation and the “axis” argument.\n\nCompare the following with different matrix sizes:\n\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nAlso try the following:\n\nmicrobenchmark::microbenchmark(\n   tf$matmul(testTF, tf$transpose(testTF)), # TensorFlow style.\n   test %*% t(test)  # R style.\n)\n\n\n\nClick here to see the solution\n\n\ndo_something_TF = function(x = matrix(0.0, 10L, 10L)){\n  x = tf$constant(x)  # Remember, this is a local copy!\n  mean_per_row = tf$reduce_mean(x, axis = 0L)\n  result = x - mean_per_row\n  return(result)\n}\n\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nWarning in microbenchmark::microbenchmark(do_something_R(test),\ndo_something_TF(test)): less accurate nanosecond times to avoid potential\ninteger overflows\n\n\nUnit: microseconds\n                  expr     min       lq     mean   median       uq      max\n  do_something_R(test) 414.715  440.012  466.703  454.157  467.031 1797.071\n do_something_TF(test) 962.352 1000.195 1159.644 1012.721 1031.417 7260.690\n neval cld\n   100  a \n   100   b\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_TF(test))\n\nUnit: milliseconds\n                  expr      min        lq     mean   median        uq      max\n  do_something_R(test) 8.804627 10.126508 13.82756 12.76506 15.390683 54.64804\n do_something_TF(test) 1.949509  2.543127  3.88259  3.19554  4.487758 17.61930\n neval cld\n   100  a \n   100   b\n\n\nWhy is R faster (the first time)?\n\n\nThe R functions we used (apply, mean, “-”) are also implemented in C.\n\n\nThe problem is not large enough and TensorFlow has an overhead.\n\n\n\n\ntest = matrix(0.0, 1000L, 500L)\ntestTF = tf$constant(test)\n\nmicrobenchmark::microbenchmark(\n  tf$matmul(testTF, tf$transpose(testTF)),  # TensorFlow style.\n  test %*% t(test) # R style.\n)\n\nUnit: milliseconds\n                                    expr        min       lq      mean\n tf$matmul(testTF, tf$transpose(testTF))   9.058048  10.1583  11.43048\n                        test %*% t(test) 264.590999 267.6229 279.17414\n    median        uq      max neval cld\n  11.05727  12.19709  20.5884   100  a \n 269.38302 272.82800 453.7901   100   b\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Linear Algebra\n\n\n\nGoogle to find out how to write the following expressions in TensorFlow:\n\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n\n[1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n# l)\neigen(A)\n\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n\n[1] 6\n\n\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\ntf$linalg$inv(A)\n\ntf.Tensor(\n[[ 1.          0.         -0.66666667]\n [-1.          0.5        -0.16666667]\n [ 0.          0.          0.33333333]], shape=(3, 3), dtype=float64)\n\n# j)    diag(A)\ntf$linalg$diag_part(A)\n\ntf.Tensor([1. 2. 3.], shape=(3), dtype=float64)\n\n# k)    diag(diag(A))\ntf$linalg$diag(tf$linalg$diag_part(A))\n\ntf.Tensor(\n[[1. 0. 0.]\n [0. 2. 0.]\n [0. 0. 3.]], shape=(3, 3), dtype=float64)\n\n# l)    eigen(A)\ntf$linalg$eigh(A)\n\n[[1]]\ntf.Tensor([-0.56155281  3.          3.56155281], shape=(3), dtype=float64)\n\n[[2]]\ntf.Tensor(\n[[-0.78820544  0.         -0.61541221]\n [ 0.61541221  0.         -0.78820544]\n [ 0.          1.         -0.        ]], shape=(3, 3), dtype=float64)\n\n# m)    det(A)\ntf$linalg$det(A)\n\ntf.Tensor(6.0, shape=(), dtype=float64)\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Automatic differentation\n\n\n\nTensorFlow supports automatic differentiation (analytical and not numerical!). Let’s have a look at the function \\(f(x) = 5 x^2 + 3\\) with derivative \\(f'(x) = 10x\\). So for \\(f'(5)\\) we will get \\(10\\).\nLet’s do this in TensorFlow. Define the function:\n\nf = function(x){ return(5.0 * tf$square(x) + 3.0) }\n\nWe want to calculate the derivative for \\(x = 2.0\\):\n\nx = tf$constant(2.0)\n\nTo do automatic differentiation, we have to forward \\(x\\) through the function within the tf$GradientTape() environment. We have also have to tell TensorFlow which value to “watch”:\n\nwith(tf$GradientTape() %as% tape,\n  {\n    tape$watch(x)\n    y = f(x)\n  }\n)\n\nTo print the gradient:\n\n(tape$gradient(y, x))\n\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nWe can also calculate the second order derivative \\(f''(x) = 10\\):\n\nwith(tf$GradientTape() %as% first,\n  {\n    first$watch(x)\n    with(tf$GradientTape() %as% second,\n      {\n        second$watch(x)\n        y = f(x)\n        g = first$gradient(y, x)\n      }\n    )\n  }\n)\n\n(second$gradient(g, x))\n\ntf.Tensor(10.0, shape=(), dtype=float32)\n\n\nWhat is happening here? Think about and discuss it.\nA more advanced example: Linear regression\nIn this case we first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows a normal distribution == error).\n\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\nIn R we would do the following to fit a linear regression model:\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   &lt;2e-16 ***\nx2          2.741690   0.025328 108.249   &lt;2e-16 ***\nx3          1.179181   0.023644  49.872   &lt;2e-16 ***\nx4          0.591873   0.025154  23.530   &lt;2e-16 ***\nx5          2.302417   0.022575 101.991   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,    Adjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights (\\(\\boldsymbol{w}\\)) of the regression model). We want our model to learn these weights.\nThe input (predictors, independent variables or features, \\(\\boldsymbol{X}\\)) and the observed (response, \\(\\boldsymbol{y}\\)) are constant and will not be learned/optimized.\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTF = tf$Variable(matrix(rnorm(5, 0, 0.01), 5, 1))\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.1)\n\nfor(i in 1:100){\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = tf$matmul(xTF, wTF)\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, wTF)\n  optimizer$apply_gradients(purrr::transpose(list(list(grads), list(wTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n\n&lt;tf.Variable 'Variable:0' shape=(5, 1) dtype=float64, numpy=\narray([[2.19290567],\n       [2.74534135],\n       [1.17146559],\n       [0.58811305],\n       [2.30174941]])&gt;\n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  2.217 2.719 1.165 0.593 2.303 \n\n\nDiscuss the code, go through the code line by line and try to understand it.\nAdditional exercise:\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTF = tf$Variable(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1))\ninterceptTF = tf$Variable(matrix(rnorm(1, 0, .5)), 1, 1) # Double, not float32.\n\nxTF = tf$constant(x)\nyTF = tf$constant(y)\n\noptimizer = tf$keras$optimizers$Adamax(learning_rate = 0.05)\n\nfor(i in 1:100){\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      pred = tf$add(interceptTF, tf$matmul(xTF, wTF))\n      loss = tf$sqrt(tf$reduce_mean(tf$square(yTF - pred)))\n    }\n  )\n\n  if(!i%%10){ k_print_tensor(loss, message = \"Loss: \") }  # Every 10 times.\n  grads = tape$gradient(loss, list(wTF, interceptTF))\n  optimizer$apply_gradients(purrr::transpose(list(grads, list(wTF, interceptTF))))\n}\n\nk_print_tensor(wTF, message = \"Resulting weights:\\n\")\n\n&lt;tf.Variable 'Variable:0' shape=(3, 1) dtype=float64, numpy=\narray([[2.48089253],\n       [2.47586968],\n       [1.00278615]])&gt;\n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  2.47 2.465 1.003 \n\nk_print_tensor(interceptTF, message = \"Resulting intercept:\\n\")\n\n&lt;tf.Variable 'Variable:0' shape=(1, 1) dtype=float64, numpy=array([[4.15394202]])&gt;\n\ncat(\"Original intercept: \", intercept, \"\\n\")\n\nOriginal intercept:  4.09"
  },
  {
    "objectID": "C1-TensorFlow.html#introduction-to-pytorch",
    "href": "C1-TensorFlow.html#introduction-to-pytorch",
    "title": "8  Introduction to TensorFlow and Keras",
    "section": "8.2 Introduction to PyTorch",
    "text": "8.2 Introduction to PyTorch\nPyTorch is another famous library for deep learning. Like TensorFlow, Torch itself is written in C++ with an API for Python. In 2020, the RStudio team released R-Torch, and while R-TensorFlow calls the Python API in the background, the R-Torch API is built directly on the C++ Torch library!\nUseful links:\n\nPyTorch documentation (This is for the Python API, bust just replace the “.” with “$”.)\nR-Torch website\n\nTo get started with Torch, we have to load the library and check if the installation worked.\n\nlibrary(torch)\n\n\n8.2.1 Data Containers\nUnlike TensorFlow, Torch doesn’t have two data containers for mutable and immutable variables. All variables are initialized via the torch_tensor function:\n\na = torch_tensor(1.)\n\nTo mark variables as mutable (and to track their operations for automatic differentiation) we have to set the argument ‘requires_grad’ to true in the torch_tensor function:\n\nmutable = torch_tensor(5, requires_grad = TRUE) # tf$Variable(...)\nimmutable = torch_tensor(5, requires_grad = FALSE) # tf$constant(...)\n\n\n\n8.2.2 Basic Operations\nWe now can define the variables and do some math with them:\n\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a)\n\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n\nprint(b)\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n\nc = a$add(b)\nprint(c)\n\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n\n\nThe R-Torch package provides all common R methods (an advantage over TensorFlow).\n\na = torch_tensor(5.)\nb = torch_tensor(10.)\nprint(a+b)\n\ntorch_tensor\n 15\n[ CPUFloatType{1} ]\n\nprint(a/b)\n\ntorch_tensor\n 0.5000\n[ CPUFloatType{1} ]\n\nprint(a*b)\n\ntorch_tensor\n 50\n[ CPUFloatType{1} ]\n\n\nTheir operators also automatically transform R numbers into tensors when attempting to add a tensor to a R number:\n\nd = a + 5  # 5 is automatically converted to a tensor.\nprint(d)\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ]\n\n\nAs for TensorFlow, we have to explicitly transform the tensors back to R:\n\nclass(d)\n\n[1] \"torch_tensor\" \"R7\"          \n\nclass(as.numeric(d))\n\n[1] \"numeric\"\n\n\n\n\n8.2.3 Data Types\nSimilar to TensorFlow:\n\nr_matrix = matrix(runif(10*10), 10, 10)\nm = torch_tensor(r_matrix, dtype = torch_float32()) \nb = torch_tensor(2.0, dtype = torch_float64())\nc = m / b \n\nBut here’s a difference! With TensorFlow we would get an error, but with R-Torch, m is automatically casted to a double (float64). However, this is still bad practice!\nDuring the course we will try to provide the corresponding PyTorch code snippets for all Keras/TensorFlow examples.\n\n\n8.2.4 Exercises\n\n\n\n\n\n\nQuestion: Torch Operations\n\n\n\nRewrite the following expressions (a to g) in torch:\n\nx = 100:1\ny = as.double(100:1)\n\n# a)\nmin(x)\n\n[1] 1\n\n# b)\nmean(x)\n\n[1] 50.5\n\n# c) Tip: Use Google!\nwhich.max(x)\n\n[1] 1\n\n# d) \nwhich.min(x)\n\n[1] 100\n\n# e) Tip: Use Google! \norder(x)\n\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1\n\n# f) Tip: See tf$reshape.\nm = matrix(y, 10, 10) # Mind: We use y here! (Float)\nm_2 = abs(m %*% t(m))  # m %*% m is the normal matrix multiplication.\nm_2_log = log(m_2)\nprint(m_2_log)\n\n          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n [1,] 10.55841 10.54402 10.52943 10.51461 10.49957 10.48431 10.46880 10.45305\n [2,] 10.54402 10.52969 10.51515 10.50040 10.48542 10.47022 10.45478 10.43910\n [3,] 10.52943 10.51515 10.50067 10.48598 10.47107 10.45593 10.44057 10.42496\n [4,] 10.51461 10.50040 10.48598 10.47135 10.45651 10.44144 10.42614 10.41061\n [5,] 10.49957 10.48542 10.47107 10.45651 10.44173 10.42674 10.41151 10.39605\n [6,] 10.48431 10.47022 10.45593 10.44144 10.42674 10.41181 10.39666 10.38127\n [7,] 10.46880 10.45478 10.44057 10.42614 10.41151 10.39666 10.38158 10.36628\n [8,] 10.45305 10.43910 10.42496 10.41061 10.39605 10.38127 10.36628 10.35105\n [9,] 10.43705 10.42317 10.40910 10.39482 10.38034 10.36565 10.35073 10.33559\n[10,] 10.42079 10.40699 10.39299 10.37879 10.36439 10.34977 10.33495 10.31989\n          [,9]    [,10]\n [1,] 10.43705 10.42079\n [2,] 10.42317 10.40699\n [3,] 10.40910 10.39299\n [4,] 10.39482 10.37879\n [5,] 10.38034 10.36439\n [6,] 10.36565 10.34977\n [7,] 10.35073 10.33495\n [8,] 10.33559 10.31989\n [9,] 10.32022 10.30461\n[10,] 10.30461 10.28909\n\n# g) Custom mean function i.e. rewrite the function using TensorFlow. \nmean_R = function(y){\n  result = sum(y) / length(y)\n  return(result)\n}\n\nmean_R(y) == mean(y)    # Test for equality.\n\n[1] TRUE\n\n\n\n\nClick here to see the solution\n\n\nlibrary(torch)\n\n\nx = 100:1\ny = as.double(100:1)\n\n# a)    min(x)\ntorch_min(x) # Integer!\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\ntorch_min(y) # Float!\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n# b)    mean(x)\n# Check out the difference here:\nmean(x)\n\n[1] 50.5\n\nmean(y)\n\n[1] 50.5\n\ntorch_mean(torch_tensor(x, dtype = torch_float32()))  # Integer! Why?\n\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n\ntorch_mean(y)  # Float!\n\ntorch_tensor\n50.5\n[ CPUFloatType{} ]\n\n# c)    which.max(x)\ntorch_argmax(x)\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\ntorch_argmax(y)\n\ntorch_tensor\n1\n[ CPULongType{} ]\n\n# d)    which.min(x)\ntorch_argmin(x)\n\ntorch_tensor\n100\n[ CPULongType{} ]\n\n# e)    order(x)\ntorch_argsort(x)\n\ntorch_tensor\n 100\n  99\n  98\n  97\n  96\n  95\n  94\n  93\n  92\n  91\n  90\n  89\n  88\n  87\n  86\n  85\n  84\n  83\n  82\n  81\n  80\n  79\n  78\n  77\n  76\n  75\n  74\n  73\n  72\n  71\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{100} ]\n\n# f)\n# m = matrix(y, 10, 10)\n# m_2 = abs(m %*% m)\n# m_2_log = log(m_2)\n\n# Mind: We use y here! \nmTorch = torch_reshape(y, c(10, 10))\nmTorch2 = torch_abs(torch_matmul(mTorch, torch_t(mTorch))) # hard to read!\n\n# Better:\nmTorch2 = mTorch$matmul( mTorch$t() )$abs()\nmTorch2_log = mTorch$log()\n\nprint(mTorch2_log)\n\ntorch_tensor\n 4.6052  4.5951  4.5850  4.5747  4.5643  4.5539  4.5433  4.5326  4.5218  4.5109\n 4.4998  4.4886  4.4773  4.4659  4.4543  4.4427  4.4308  4.4188  4.4067  4.3944\n 4.3820  4.3694  4.3567  4.3438  4.3307  4.3175  4.3041  4.2905  4.2767  4.2627\n 4.2485  4.2341  4.2195  4.2047  4.1897  4.1744  4.1589  4.1431  4.1271  4.1109\n 4.0943  4.0775  4.0604  4.0431  4.0254  4.0073  3.9890  3.9703  3.9512  3.9318\n 3.9120  3.8918  3.8712  3.8501  3.8286  3.8067  3.7842  3.7612  3.7377  3.7136\n 3.6889  3.6636  3.6376  3.6109  3.5835  3.5553  3.5264  3.4965  3.4657  3.4340\n 3.4012  3.3673  3.3322  3.2958  3.2581  3.2189  3.1781  3.1355  3.0910  3.0445\n 2.9957  2.9444  2.8904  2.8332  2.7726  2.7081  2.6391  2.5649  2.4849  2.3979\n 2.3026  2.1972  2.0794  1.9459  1.7918  1.6094  1.3863  1.0986  0.6931  0.0000\n[ CPUFloatType{10,10} ]\n\n# g)    # Custom mean function\nmean_Torch = function(y){\n  result = torch_sum(y)\n  return( result / length(y) )  # If y is an R object.\n}\nmean_Torch(y) == mean(y)\n\ntorch_tensor\n 1\n[ CPUBoolType{1} ]\n\n\n\n\n\n::: {.callout-caution icon=“false”} #### Question: Runtime\n\nWhat is the meaning of “An effect is not significant”?\nIs an effect with three *** more significant / certain than an effect with one *?\n\n\n\nClick here to see the solution\n\nThis exercise compares the speed of R to torch The first exercise is to rewrite the following function in torch:\n\ndo_something_R = function(x = matrix(0.0, 10L, 10L)){\n  mean_per_row = apply(x, 1, mean)\n  result = x - mean_per_row\n  return(result)\n}\n\nHere, we provide a skeleton for a TensorFlow function:\n\ndo_something_torch= function(x = matrix(0.0, 10L, 10L)){\n   ...\n}\n\nWe can compare the speed using the Microbenchmark package:\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nTry different matrix sizes for the test matrix and compare the speed.\nTip: Have a look at the the torch_mean documentation and the “dim” argument.\n\nCompare the following with different matrix sizes:\n\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nAlso try the following:\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n\n\n\nClick here to see the solution\n\n\ndo_something_torch = function(x = matrix(0.0, 10L, 10L)){\n  x = torch_tensor(x)  # Remember, this is a local copy!\n  mean_per_row = torch_mean(x, dim = 1)\n  result = x - mean_per_row\n  return(result)\n}\n\n\ntest = matrix(0.0, 100L, 100L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nUnit: microseconds\n                     expr     min       lq     mean  median       uq      max\n     do_something_R(test) 433.370 447.4945 471.1732 456.289 462.9515 1771.733\n do_something_torch(test) 100.245 111.3970 162.8106 118.039 129.2320 2562.828\n neval cld\n   100  a \n   100   b\n\ntest = matrix(0.0, 1000L, 500L)\nmicrobenchmark::microbenchmark(do_something_R(test), do_something_torch(test))\n\nUnit: milliseconds\n                     expr      min       lq      mean   median        uq\n     do_something_R(test) 8.315333 8.935458 10.997386 9.384715 13.612328\n do_something_torch(test) 1.134921 1.548734  1.813882 1.689138  1.863942\n      max neval cld\n 19.61374   100  a \n  4.60553   100   b\n\n\nWhy is R faster (the first time)?\n\n\nThe R functions we used (apply, mean, “-”) are also implemented in C.\n\n\nThe problem is not large enough and torch has an overhead.\n\n\n\n\ntest = matrix(0.0, 1000L, 500L)\ntestTorch = torch_tensor(test)\n\nmicrobenchmark::microbenchmark(\n   torch_matmul(testTorch, testTorch$t()), # Torch style.\n   test %*% t(test)  # R style.\n)\n\nUnit: milliseconds\n                                   expr        min         lq       mean\n torch_matmul(testTorch, testTorch$t())   1.301586   1.663636   2.038418\n                       test %*% t(test) 264.344220 266.891550 273.989479\n     median         uq        max neval cld\n   1.830588   2.179704   6.187023   100  a \n 267.547529 270.850244 445.837977   100   b\n\n\n\n:::\n\n\n\n\n\n\nQuestion: Linear Algebra\n\n\n\nGoogle to find out how to write the following tasks in torch:\n\nA = matrix(c(1, 2, 0, 0, 2, 0, 2, 5, 3), 3, 3)\n\n# i)\nsolve(A)  # Solve equation AX = B. If just A  is given, invert it.\n\n     [,1] [,2]       [,3]\n[1,]    1  0.0 -0.6666667\n[2,]   -1  0.5 -0.1666667\n[3,]    0  0.0  0.3333333\n\n# j)\ndiag(A) # Diagonal of A, if no matrix is given, construct diagonal matrix.\n\n[1] 1 2 3\n\n# k)\ndiag(diag(A)) # Diagonal matrix with entries diag(A).\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n# l)\neigen(A)\n\neigen() decomposition\n$values\n[1] 3 2 1\n\n$vectors\n          [,1] [,2]       [,3]\n[1,] 0.1400280    0  0.4472136\n[2,] 0.9801961    1 -0.8944272\n[3,] 0.1400280    0  0.0000000\n\n# m)\ndet(A)\n\n[1] 6\n\n\n\n\nClick here to see the solution\n\n\nlibrary(torch)\n\nA = matrix(c(1., 2., 0., 0., 2., 0., 2., 5., 3.), 3, 3)\n# Do not use the \"L\" form here!\n\n# i)    solve(A)\nlinalg_inv(A)\n\ntorch_tensor\n 1.0000 -0.0000 -0.6667\n-1.0000  0.5000 -0.1667\n 0.0000  0.0000  0.3333\n[ CPUFloatType{3,3} ]\n\n# j)    diag(A)\ntorch_diag(A)\n\ntorch_tensor\n 1\n 2\n 3\n[ CPUFloatType{3} ]\n\n# k)    diag(diag(A))\ntorch_diag(A)$diag()\n\ntorch_tensor\n 1  0  0\n 0  2  0\n 0  0  3\n[ CPUFloatType{3,3} ]\n\n# l)    eigen(A)\nlinalg_eigh(A)\n\n[[1]]\ntorch_tensor\n-0.5616\n 3.0000\n 3.5616\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n-0.7882  0.0000  0.6154\n 0.6154  0.0000  0.7882\n 0.0000  1.0000  0.0000\n[ CPUFloatType{3,3} ]\n\n# m)    det(A)\nlinalg_det(A)\n\ntorch_tensor\n6\n[ CPUFloatType{} ]\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Automatic differentation\n\n\n\nTorch supports automatic differentiation (analytical and not numerical!). Let’s have a look at the function \\(f(x) = 5 x^2 + 3\\) with derivative \\(f'(x) = 10x\\). So for \\(f'(5)\\) we will get \\(10\\).\nLet’s do this in torch Define the function:\n\nf = function(x){ return(5.0 * torch_pow(x, 2.) + 3.0) }\n\nWe want to calculate the derivative for \\(x = 2.0\\):\n\nx = torch_tensor(2.0, requires_grad = TRUE)\n\nTo do automatic differentiation, we have to forward \\(x\\) through the function and call the $backward() method of the result:\n\ny = f(x)\ny$backward(retain_graph=TRUE )\n\nTo print the gradient:\n\nx$grad\n\ntorch_tensor\n 20\n[ CPUFloatType{1} ]\n\n\nWe can also calculate the second order derivative \\(f''(x) = 10\\):\n\nx = torch_tensor(2.0, requires_grad = TRUE)\ny = f(x)\ngrad = torch::autograd_grad(y, x, retain_graph = TRUE, create_graph = TRUE)[[1]] # first\n(torch::autograd_grad(grad, x, retain_graph = TRUE, create_graph = TRUE)[[1]]) # second\n\ntorch_tensor\n 10\n[ CPUFloatType{1} ][ grad_fn = &lt;MulBackward0&gt; ]\n\n\nWhat is happening here? Think about and discuss it.\nA more advanced example: Linear regression\nIn this case we first simulate data following \\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{w} + \\boldsymbol{\\epsilon}\\) (\\(\\boldsymbol{\\epsilon}\\) follows a normal distribution == error).\n\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\nIn R we would do the following to fit a linear regression model:\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67893 -0.16399  0.00968  0.15058  0.51099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.004865   0.027447   0.177     0.86    \nx1          2.191511   0.023243  94.287   &lt;2e-16 ***\nx2          2.741690   0.025328 108.249   &lt;2e-16 ***\nx3          1.179181   0.023644  49.872   &lt;2e-16 ***\nx4          0.591873   0.025154  23.530   &lt;2e-16 ***\nx5          2.302417   0.022575 101.991   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2645 on 94 degrees of freedom\nMultiple R-squared:  0.9974,    Adjusted R-squared:  0.9972 \nF-statistic:  7171 on 5 and 94 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s build our own model in TensorFlow. Here, we use now the variable data container type (remember they are mutable and we need this type for the weights (\\(\\boldsymbol{w}\\)) of the regression model). We want our model to learn these weights.\nThe input (predictors, independent variables or features, \\(\\boldsymbol{X}\\)) and the observed (response, \\(\\boldsymbol{y}\\)) are constant and will not be learned/optimized.\n\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nx = matrix(round(runif(500, -2, 2), 3), 100, 5)\nw = round(rnorm(5, 2, 1), 3)\ny = x %*% w + round(rnorm(100, 0, 0.25), 4)\n\n# Weights we want to learn.\n# We know the real weights but in reality we wouldn't know them.\n# So use guessed ones.\nwTorch = torch_tensor(matrix(rnorm(5, 0, 0.01), 5, 1), requires_grad = TRUE)\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n\n[1] \"Loss: 4.4065318107605\"\n[1] \"Loss: 2.37926030158997\"\n[1] \"Loss: 0.901207685470581\"\n[1] \"Loss: 0.403193712234497\"\n[1] \"Loss: 0.296265542507172\"\n[1] \"Loss: 0.268377959728241\"\n[1] \"Loss: 0.232994809746742\"\n[1] \"Loss: 0.219554677605629\"\n[1] \"Loss: 0.215328559279442\"\n[1] \"Loss: 0.213282078504562\"\n\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n\nInferred weights:  0.701 3.089 1.801 1.123 3.452 \n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  0.67 3.085 1.787 1.121 3.455 \n\n\nDiscuss the code, go through the code line by line and try to understand it.\nAdditional exercise:\nPlay around with the simulation, increase/decrease the number of weights, add an intercept (you also need an additional variable in model).\n\n\nClick here to see the solution\n\n\nlibrary(torch)\ntorch::torch_manual_seed(42L)\n\nnumberOfWeights = 3\nnumberOfFeatures = 10000\n\nx = matrix(round(runif(numberOfFeatures * numberOfWeights, -2, 2), 3),\n           numberOfFeatures, numberOfWeights)\nw = round(rnorm(numberOfWeights, 2, 1), 3)\nintercept = round(rnorm(1, 3, .5), 3)\ny = intercept + x %*% w + round(rnorm(numberOfFeatures, 0, 0.25), 4)\n\n# Guessed weights and intercept.\nwTorch = torch_tensor(matrix(rnorm(numberOfWeights, 0, 0.01), numberOfWeights, 1), requires_grad = TRUE)\ninterceptTorch = torch_tensor(matrix(rnorm(1, 0, .5), 1, 1), requires_grad = TRUE) # Double, not float32.\n\nxTorch = torch_tensor(x)\nyTorch = torch_tensor(y)\n\n# We need an optimizer which updates the weights (wTF).\noptimizer = optim_adam(params = list(wTorch, interceptTorch), lr = 0.1)\n\nfor(i in 1:100){\n  pred = xTorch$matmul(wTorch)$add(interceptTorch)\n  loss = (yTorch - pred)$pow(2.0)$mean()$sqrt()\n\n  if(!i%%10){ print(paste0(\"Loss: \", as.numeric(loss)))}  # Every 10 times.\n  loss$backward()\n  optimizer$step() # do optimization step\n  optimizer$zero_grad() # reset gradients\n}\n\n[1] \"Loss: 3.51533484458923\"\n[1] \"Loss: 1.74870145320892\"\n[1] \"Loss: 0.41416934132576\"\n[1] \"Loss: 0.518697261810303\"\n[1] \"Loss: 0.293963462114334\"\n[1] \"Loss: 0.263338804244995\"\n[1] \"Loss: 0.258341491222382\"\n[1] \"Loss: 0.254723280668259\"\n[1] \"Loss: 0.252453774213791\"\n[1] \"Loss: 0.25116890668869\"\n\ncat(\"Inferred weights: \", round(as.numeric(wTorch), 3), \"\\n\")\n\nInferred weights:  3.118 -0.349 2.107 \n\ncat(\"Original weights: \", w, \"\\n\")\n\nOriginal weights:  3.131 -0.353 2.11 \n\ncat(\"Inferred intercept: \", round(as.numeric(interceptTorch), 3), \"\\n\")\n\nInferred intercept:  2.836 \n\ncat(\"Original intercept: \", intercept, \"\\n\")\n\nOriginal intercept:  2.832"
  },
  {
    "objectID": "C2-DeepNeuralNetworks.html#example-workflow-in-keras-torch",
    "href": "C2-DeepNeuralNetworks.html#example-workflow-in-keras-torch",
    "title": "9  Deep Neural Networks (DNN)",
    "section": "9.1 Example workflow in Keras / Torch",
    "text": "9.1 Example workflow in Keras / Torch\nWe build a small classifier to predict the three species of the iris data set. Load the necessary packages and data sets:\n\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(torch)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nFor neural networks, it is beneficial to scale the predictors (scaling = centering and standardization, see ?scale). We also split our data into predictors (X) and response (Y = the three species).\n\nX = scale(iris[,1:4])\nY = iris[,5]\n\nAdditionally, Keras/TensorFlow cannot handle factors and we have to create contrasts (one-hot encoding). To do so, we have to specify the number of categories. This can be tricky for a beginner, because in other programming languages like Python and C++, arrays start at zero. Thus, when we would specify 3 as number of classes for our three species, we would have the classes 0,1,2,3. Keep this in mind.\n\nY = to_categorical(as.integer(Y) - 1L, 3)\nhead(Y) # 3 columns, one for each level of the response.\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    1    0    0\n[3,]    1    0    0\n[4,]    1    0    0\n[5,]    1    0    0\n[6,]    1    0    0\n\n\nAfter having prepared the data, we start now with the typical workflow in keras.\n1. Initialize a sequential model in Keras:\n\nKerasTorch\n\n\n\nmodel = keras_model_sequential()\n\n\n\nTorch users can skip this step.\n\n\n\nA sequential Keras model is a higher order type of model within Keras and consists of one input and one output model.\n2. Add hidden layers to the model (we will learn more about hidden layers during the next days).\nWhen specifying the hidden layers, we also have to specify the shape and a so called activation function. You can think of the activation function as decision for what is forwarded to the next neuron (but we will learn more about it later). If you want to know this topic in even more depth, consider watching the videos presented in section @ref(basicMath).\nThe shape of the input is the number of predictors (here 4) and the shape of the output is the number of classes (here 3).\n\nKerasTorch\n\n\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(4L)) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 3L, activation = \"softmax\") \n\n\n\nThe Torch syntax is very similar, we will give a list of layers to the “nn_sequential” function. Here, we have to specify the softmax activation function as an extra layer:\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(4L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 20L),\n    nn_linear(20L, 3L),\n    nn_softmax(2)\n  )\n\n\n\n\n\nsoftmax scales a potential multidimensional vector to the interval \\((0, 1]\\) for each component. The sum of all components equals 1. This might be very useful for example for handling probabilities. Ensure ther the labels start at 0! Otherwise the softmax function does not work well!\n\n3. Compile the model with a loss function (here: cross entropy) and an optimizer (here: Adamax).\nWe will learn about other options later, so for now, do not worry about the “learning_rate” (“lr” in Torch or earlier in TensorFlow) argument, cross entropy or the optimizer.\n\nKerasTorch\n\n\n\nmodel %&gt;%\n  compile(loss = loss_categorical_crossentropy,\n          keras::optimizer_adamax(learning_rate = 0.001))\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_3 (Dense)                    (None, 20)                      100         \n dense_2 (Dense)                    (None, 20)                      420         \n dense_1 (Dense)                    (None, 20)                      420         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 1,003\nTrainable params: 1,003\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\nSpecify optimizer and the parameters which will be trained (in our case the parameters of the network):\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.001)\n\n\n\n\n4. Fit model in 30 iterations (epochs)\n\nKerasTorch\n\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel_history =\n  model %&gt;%\n    fit(x = X, y = apply(Y, 2, as.integer), epochs = 30L,\n        batch_size = 20L, shuffle = TRUE)\n\n\n\nIn Torch, we jump directly to the training loop, however, here we have to write our own training loop:\n\nGet a batch of data.\nPredict on batch.\nCcalculate loss between predictions and true labels.\nBackpropagate error.\nUpdate weights.\nGo to step 1 and repeat.\n\n\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\n# Calculate number of training steps.\nepochs = 30\nbatch_size = 20\nsteps = round(nrow(X)/batch_size * epochs)\n\nX_torch = torch_tensor(X)\nY_torch = torch_tensor(apply(Y, 1, which.max)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch.\n  indices = sample.int(nrow(X), batch_size)\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_torch[indices, ])\n  loss = nnf_cross_entropy(pred, Y_torch[indices])\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n\n\n\n\n5. Plot training history:\n\nKerasTorch\n\n\n\nplot(model_history)\n\n\n\n\n\n\n\nplot(log_losses, xlab = \"steps\", ylab = \"loss\", las = 1)\n\n\n\n\n\n\n\n6. Create predictions:\n\nKerasTorch\n\n\n\npredictions = predict(model, X) # Probabilities for each class.\n\nGet probabilities:\n\nhead(predictions) # Quasi-probabilities for each species.\n\n          [,1]        [,2]         [,3]\n[1,] 0.9915600 0.006817889 0.0016221496\n[2,] 0.9584184 0.037489697 0.0040918575\n[3,] 0.9910416 0.007848956 0.0011094128\n[4,] 0.9813542 0.016901711 0.0017440914\n[5,] 0.9949830 0.004031503 0.0009855649\n[6,] 0.9905725 0.006884387 0.0025430375\n\n\nFor each plant, we want to know for which species we got the highest probability:\n\npreds = apply(predictions, 1, which.max) \nprint(preds)\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 2 1 1 1 1 1 1 1 1 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2\n [75] 2 2 2 3 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\n\n\nmodel_torch$eval()\npreds_torch = model_torch(torch_tensor(X))\npreds_torch = apply(preds_torch, 1, which.max) \nprint(preds_torch)\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\n\n\n7. Calculate Accuracy (how often we have been correct):\n\nKerasTorch\n\n\n\nmean(preds == as.integer(iris$Species))\n\n[1] 0.9066667\n\n\n\n\n\nmean(preds_torch == as.integer(iris$Species))\n\n[1] 0.9666667\n\n\n\n\n\n8. Plot predictions, to see if we have done a good job:\n\noldpar = par(mfrow = c(1, 2))\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species,\n     main = \"Observed\")\nplot(iris$Sepal.Length, iris$Petal.Length, col = preds,\n     main = \"Predicted\")\n\n\n\npar(oldpar)   # Reset par.\n\nSo you see, building a neural network is very easy with Keras or Torch and you can already do it on your own."
  },
  {
    "objectID": "C2-DeepNeuralNetworks.html#exercise",
    "href": "C2-DeepNeuralNetworks.html#exercise",
    "title": "9  Deep Neural Networks (DNN)",
    "section": "9.2 Exercise",
    "text": "9.2 Exercise\n\n\n\n\n\n\nTask: Regression with keras\n\n\n\nWe now build a regression for the airquality data set with Keras/Torch. We want to predict the variable “Ozone” (continuous).\nTasks:\n\nFill in the missing steps\n\n\nKerasTorch\n\n\nBefore we start, load and prepare the data set:\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = airquality\n\nsummary(data)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\n\nThere are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don’t know how to remove NAs from a data.frame, use Google (e.g. with the query: “remove-rows-with-all-or-some-nas-missing-values-in-data-frame”).\n\n\n\nSolution\n\n\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n\n\n\n\nSplit the data in features (\\(\\boldsymbol{X}\\)) and response (\\(\\boldsymbol{y}\\), Ozone) and scale the \\(\\boldsymbol{X}\\) matrix.\n\n\n\nSolution\n\n\nx = scale(data[,2:6])\ny = data[,1]\n\n\n\nCreate a sequential Keras model.\n\n\n\nSolution\n\n\nlibrary(tensorflow)\nlibrary(keras)\nmodel = keras_model_sequential()\n\n\n\nAdd hidden layers (input and output layer are already specified, you have to add hidden layers between them):\n\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %&gt;%\n   ....\n  layer_dense(units = 1L, activation = \"linear\")\n\n\nWhy do we use 5L as input shape?\nWhy only one output node and “linear” activation layer?\n\n\n\nSolution\n\n\nmodel %&gt;%\n  layer_dense(units = 20L, activation = \"relu\", input_shape = list(5L)) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 20L) %&gt;%\n  layer_dense(units = 1L, activation = \"linear\")\n\n\n\nCompile the model\n\n\nmodel %&gt;%\n  compile(loss = loss_mean_squared_error, optimizer_adamax(learning_rate = 0.05))\n\nWhat is the “mean_squared_error” loss?\n\nFit model:\n\n\n\nSolution\n\n\nmodel_history =\n  model %&gt;%\n  fit(x = x, y = as.numeric(y), epochs = 100L,\n      batch_size = 20L, shuffle = TRUE)\n\n\n\nPlot training history.\n\n\n\nSolution\n\n\nplot(model_history)\n\n\n\n\n\n\nCreate predictions.\n\n\n\nSolution\n\n\npred_keras = predict(model, x)\n\n\n\nCompare your Keras model with a linear model:\n\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_keras = mean(sqrt((y - pred_keras)^2))\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_keras)\n\n[1] 9.067499\n\n\n\n\nBefore we start, load and prepare the data set:\n\nlibrary(torch)\n\ndata = airquality\nsummary(data)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\nplot(data)\n\n\n\n\n\nThere are NAs in the data, which we have to remove because Keras cannot handle NAs. If you don’t know how to remove NAs from a data.frame, use Google (e.g. with the query: “remove-rows-with-all-or-some-nas-missing-values-in-data-frame”).\n\n\n\nSolution\n\n\ndata = data[complete.cases(data),]  # Remove NAs.\nsummary(data)\n\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n\n\n\n\nSplit the data in features (\\(\\boldsymbol{X}\\)) and response (\\(\\boldsymbol{y}\\), Ozone) and scale the \\(\\boldsymbol{X}\\) matrix.\n\n\n\nSolution\n\n\nx = scale(data[,2:6])\ny = data[,1]\n\n\n\nPass a list of layer objects to a sequential network class of torch (input and output layer are already specified, you have to add hidden layers between them):\n\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    ...\n    nn_linear(20L, 1L),\n  )\n\n\nWhy do we use 5L as input shape?\nWhy only one output node and no activation layer?\n\n\n\nSolution\n\n\nlibrary(torch)\n\nmodel_torch = \n  nn_sequential(\n    nn_linear(5L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 20L),\n    nn_relu(),\n    nn_linear(20L, 1L),\n  )\n\n\n\nCreate optimizer\n\nWe have to pass the network’s parameters to the optimizer (how is this different to keras?)\n\noptimizer_torch = optim_adam(params = model_torch$parameters, lr = 0.05)\n\n\nFit model\n\nIn torch we write the trainings loop on our own. Complete the trainings loop:\n\n# Calculate number of training steps.\nepochs = ...\nbatch_size = 32\nsteps = ...\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, ...) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = ...\n  Y_batch = ...\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = ...\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n\n\n\nSolution\n\n\n# Calculate number of training steps.\nepochs = 100\nbatch_size = 32\nsteps = round(nrow(x)/batch_size*epochs)\n\nX_torch = torch_tensor(x)\nY_torch = torch_tensor(y, dtype = torch_float32())$view(list(-1, 1)) \n\n# Set model into training status.\nmodel_torch$train()\n\nlog_losses = NULL\n\n# Training loop.\nfor(i in 1:steps){\n  # Get batch indices.\n  indices = sample.int(nrow(x), batch_size)\n  X_batch = X_torch[indices,]\n  Y_batch = Y_torch[indices,]\n  \n  # Reset backpropagation.\n  optimizer_torch$zero_grad()\n  \n  # Predict and calculate loss.\n  pred = model_torch(X_batch)\n  loss = nnf_mse_loss(pred, Y_batch)\n  \n  # Backpropagation and weight update.\n  loss$backward()\n  optimizer_torch$step()\n  \n  log_losses[i] = as.numeric(loss)\n}\n\n\nTips:\n\nNumber of training $ steps = Number of rows / batchsize * Epochs $\nSearch torch::nnf_… for the correct loss function (mse…)\nMake sure that X_torch and Y_torch have the same data type! (you can set the dtype via torch_tensor(…, dtype = …)) _ Check the dimension of Y_torch, we need a matrix!\n\n\nPlot training history.\n\n\n\nSolution\n\n\nplot(y = log_losses, x = 1:steps, xlab = \"Epoch\", ylab = \"MSE\")\n\n\n\n\n\n\nCreate predictions.\n\n\n\nSolution\n\n\npred_torch = model_torch(X_torch)\npred_torch = as.numeric(pred_torch) # cast torch to R object \n\n\n\nCompare your Torch model with a linear model:\n\n\nfit = lm(Ozone ~ ., data = data)\npred_lm = predict(fit, data)\nrmse_lm = mean(sqrt((y - pred_lm)^2))\nrmse_torch = mean(sqrt((y - pred_torch)^2))\nprint(rmse_lm)\n\n[1] 14.78897\n\nprint(rmse_torch)\n\n[1] 6.872959\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask: Titanic dataset\n\n\n\nBuild a Keras DNN for the titanic dataset\n\n\n\n\n\n\n\n\nBonus Task: More details on the inner working of Keras\n\n\n\nThe next task differs for Torch and Keras users. Keras users will learn more about the inner working of training while Torch users will learn how to simplify and generalize the training loop.\nGo through the code and try to understand it.\n\nKerasTorch\n\n\nSimilar to Torch, here we will write the training loop ourselves in the following. The training loop consists of several steps:\n\nSample batches of X and Y data\nOpen the gradientTape to create a computational graph (autodiff)\nMake predictions and calculate loss\nUpdate parameters based on the gradients at the loss (go back to 1. and repeat)\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = data[,1]\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(5L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 1L, activation = NULL) # No activation == \"linear\".\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient\n# in each optimization step, we use a random subset of the data.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices]))\n}\nget_batch() # Try out this function.\n\n$bX\n        Solar.R        Wind        Temp      Month          Day\n137 -1.76410028  0.26993754 -0.71278225  1.2106304 -0.223487138\n153  0.41905906  0.43858520 -1.02757865  1.2106304  1.614073775\n44  -0.40373969 -0.54519280  0.44147123 -0.8254298 -0.338334695\n93  -1.11683193 -0.85438017  0.33653910  0.5319436 -1.716505380\n131  0.38614711  0.10128988  0.02174269  1.2106304 -0.912572481\n24  -1.01809608  0.57912491 -1.76210359 -1.5041165  0.924988433\n67   1.41738821  0.26993754  0.54640337 -0.1467431 -1.142267595\n105  0.96759156  0.43858520  0.44147123  0.5319436 -0.338334695\n94  -1.76410028  1.08506788  0.33653910  0.5319436 -1.601657823\n64   0.56167751 -0.20789749  0.33653910 -0.1467431 -1.486810266\n21  -1.93963068 -0.06735777 -1.97196786 -1.5041165  0.580445762\n4    1.40641756  0.43858520 -1.65717146 -1.5041165 -1.371962709\n126 -0.01976694 -2.00680582  1.59572471  1.2106304 -1.486810266\n69   0.90176766 -1.02302783  1.49079258 -0.1467431 -0.912572481\n47   0.06799826  1.39425525 -0.08318944 -0.8254298  0.006207976\n76  -1.50080468  1.22560759  0.23160696 -0.1467431 -0.108639581\n38  -0.63412333 -0.06735777  0.44147123 -0.8254298 -1.027420038\n108 -1.24847973  0.10128988 -0.08318944  0.5319436  0.006207976\n118  0.33129386 -0.54519280  0.86119977  0.5319436  1.154683547\n106 -0.30500384 -0.06735777  0.23160696  0.5319436 -0.223487138\n74  -0.10753214  1.39425525  0.33653910 -0.1467431 -0.338334695\n40   1.16506326  1.08506788  1.28092831 -0.8254298 -0.797724924\n104  0.07896891  0.43858520  0.86119977  0.5319436 -0.453182252\n41   1.51612406  0.43858520  0.96613190 -0.8254298 -0.682877366\n148 -1.80798288  1.87209028 -1.55223932  1.2106304  1.039835990\n145 -1.87380678 -0.20789749 -0.71278225  1.2106304  0.695293319\n2   -0.73285918 -0.54519280 -0.60785011 -1.5041165 -1.601657823\n112  0.05702761  0.10128988  0.02174269  0.5319436  0.465598204\n3   -0.39276904  0.74777257 -0.39798584 -1.5041165 -1.486810266\n128 -0.98518413 -0.71384046  0.96613190  1.2106304 -1.257115152\n22   1.48321211  1.87209028 -0.50291798 -1.5041165  0.695293319\n122  0.57264816 -1.02302783  1.91052111  0.5319436  1.614073775\n\n$bY\n [1]  9 20 23 39 23 32 40 28  9 32  1 18 73 97 21  7 29 22 73 65 27 71 44 39 14\n[26] 23 36 44 12 47 11 84\n\nsteps = floor(nrow(x)/32) * epochs  # We need nrow(x)/32 steps for each epoch.\nfor(i in 1:steps){\n  # Get data.\n  batch = get_batch()\n\n  # Transform it into tensors.\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(matrix(batch$bY, ncol = 1L))\n  \n  # Automatic differentiation:\n  # Record computations with respect to our model variables.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # We record the operation for our model weights.\n      loss = tf$reduce_mean(tf$keras$losses$mean_squared_error(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for our model$weights at the loss / backpropagation.\n  gradients = tape$gradient(loss, model$weights) \n\n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights))) \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n\nLoss:  406.3992 \nLoss:  527.0806 \nLoss:  252.544 \nLoss:  416.4157 \nLoss:  243.0384 \nLoss:  248.1294 \nLoss:  239.0951 \nLoss:  411.0923 \nLoss:  243.4587 \nLoss:  304.1434 \nLoss:  391.0959 \nLoss:  157.4986 \nLoss:  300.0546 \nLoss:  244.0995 \nLoss:  209.3407 \nLoss:  243.3395 \nLoss:  178.5952 \nLoss:  339.505 \nLoss:  175.7782 \nLoss:  160.5273 \n\n\n\n\nKeras and Torch use dataloaders to generate the data batches. Dataloaders are objects that return batches of data infinetly. Keras create the dataloader object automatically in the fit function, in Torch we have to write them ourselves:\n\nDefine a dataset object. This object informs the dataloader function about the inputs, outputs, length (nrow), and how to sample from it.\nCreate an instance of the dataset object by calling it and passing the actual data to it\nPass the initiated dataset to the dataloader function\n\n\nlibrary(torch)\n\ndata = airquality\ndata = data[complete.cases(data),]  # Remove NAs.\nx = scale(data[,2:6])\ny = matrix(data[,1], ncol = 1L)\n\n\ntorch_dataset = torch::dataset(\n    name = \"airquality\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(as.matrix(Y), dtype = torch_float32())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index,]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\nOur dataloader is again an object which has to be initiated. The initiated object returns a list of two elements, batch x and batch y. The initated object stops returning batches when the dataset was completly transversed (no worries, we don’t have to all of this ourselves).\nOur training loop has changed:\n\nmodel_torch = nn_sequential(\n  nn_linear(5L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 1L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss = c()\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_mse_loss(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n\nLoss at epoch 10: 349.544273\nLoss at epoch 20: 289.687613\nLoss at epoch 30: 269.088741\nLoss at epoch 40: 260.377857\nLoss at epoch 50: 213.073002\n\n\n\nplot(train_losses, type = \"o\", pch = 15,\n        col = \"darkblue\", lty = 1, xlab = \"Epoch\",\n        ylab = \"Loss\", las = 1)\n\n\n\n\n\n\n\nNow change the code from above for the iris data set. Tip: In tf\\(keras\\)losses$… you can find various loss functions.\n\nKerasTorch\n\n\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = keras::to_categorical(as.integer(Y)-1L, 3)\n\nlayers = tf$keras$layers\nmodel = tf$keras$models$Sequential(\n  c(\n    layers$InputLayer(input_shape = list(4L)),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 20L, activation = tf$nn$relu),\n    layers$Dense(units = 3L, activation = tf$nn$softmax)\n  )\n)\n\nepochs = 200L\noptimizer = tf$keras$optimizers$Adamax(0.01)\n\n# Stochastic gradient optimization is more efficient.\nget_batch = function(batch_size = 32L){\n  indices = sample.int(nrow(x), size = batch_size)\n  return(list(bX = x[indices,], bY = y[indices,]))\n}\n\nsteps = floor(nrow(x)/32) * epochs # We need nrow(x)/32 steps for each epoch.\n\nfor(i in 1:steps){\n  batch = get_batch()\n  bX = tf$constant(batch$bX)\n  bY = tf$constant(batch$bY)\n  \n  # Automatic differentiation.\n  with(tf$GradientTape() %as% tape,\n    {\n      pred = model(bX) # we record the operation for our model weights\n      loss = tf$reduce_mean(tf$keras$losses$categorical_crossentropy(bY, pred))\n    }\n  )\n  \n  # Calculate the gradients for the loss at our model$weights / backpropagation.\n  gradients = tape$gradient(loss, model$weights)\n  \n  # Update our model weights with the learning rate specified above.\n  optimizer$apply_gradients(purrr::transpose(list(gradients, model$weights)))\n  \n  if(! i%%30){\n    cat(\"Loss: \", loss$numpy(), \"\\n\") # Print loss every 30 steps (not epochs!).\n  }\n}\n\nLoss:  0.002633849 \nLoss:  0.0005500487 \nLoss:  0.001006462 \nLoss:  0.0001315936 \nLoss:  0.0004843124 \nLoss:  0.0004023896 \nLoss:  0.0004356128 \nLoss:  0.000235351 \nLoss:  4.823796e-05 \nLoss:  0.0001512702 \nLoss:  0.0002624761 \nLoss:  0.0001274793 \nLoss:  7.111725e-05 \nLoss:  0.0001509234 \nLoss:  0.0002024032 \nLoss:  0.0001532886 \nLoss:  9.489701e-05 \nLoss:  0.0001040314 \nLoss:  7.334561e-05 \nLoss:  2.743953e-05 \nLoss:  9.655961e-05 \nLoss:  2.361947e-05 \nLoss:  6.918395e-05 \nLoss:  1.603245e-05 \nLoss:  1.772152e-05 \nLoss:  2.512357e-05 \n\n\n\n\n\n\n\nClick here to see the solution\n\n\nlibrary(torch)\n\nx = scale(iris[,1:4])\ny = iris[,5]\ny = as.integer(iris$Species)\n\n\ntorch_dataset = torch::dataset(\n    name = \"iris\",\n    initialize = function(X,Y) {\n      self$X = torch::torch_tensor(as.matrix(X), dtype = torch_float32())\n      self$Y = torch::torch_tensor(Y, dtype = torch_long())\n    },\n    .getitem = function(index) {\n      x = self$X[index,]\n      y = self$Y[index]\n      list(x, y)\n    },\n    .length = function() {\n      self$Y$size()[[1]]\n    }\n  )\ndataset = torch_dataset(x,y)\ndataloader = torch::dataloader(dataset, batch_size = 30L, shuffle = TRUE)\n\n\nmodel_torch = nn_sequential(\n  nn_linear(4L, 50L),\n  nn_relu(),\n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 50L),\n  nn_relu(), \n  nn_linear(50L, 3L)\n)\nepochs = 50L\nopt = optim_adam(model_torch$parameters, 0.01)\ntrain_losses = c()\nfor(epoch in 1:epochs){\n  train_loss\n  coro::loop(\n    for(batch in dataloader) { \n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]])\n      loss$backward()\n      opt$step()\n      train_loss = c(train_loss, loss$item())\n    }\n  )\n  train_losses = c(train_losses, mean(train_loss))\n  if(!epoch%%10) cat(sprintf(\"Loss at epoch %d: %3f\\n\", epoch, mean(train_loss)))\n}\n\nLoss at epoch 10: 15.999315\nLoss at epoch 20: 8.324385\nLoss at epoch 30: 5.632413\nLoss at epoch 40: 4.256267\nLoss at epoch 50: 3.425689\n\n\n\n\n\n\nRemarks:\n\nMind the different input and output layer numbers.\nThe loss function increases randomly, because different subsets of the data were drawn. This is a downside of stochastic gradient descent.\nA positive thing about stochastic gradient descent is, that local valleys or hills may be left and global ones can be found instead."
  },
  {
    "objectID": "C2-DeepNeuralNetworks.html#basicMath",
    "href": "C2-DeepNeuralNetworks.html#basicMath",
    "title": "9  Deep Neural Networks (DNN)",
    "section": "9.3 Underlying mathematical concepts - optional",
    "text": "9.3 Underlying mathematical concepts - optional\nIf are not yet familiar with the underlying concepts of neural networks and want to know more about that, it is suggested to read / view the following videos / sites. Consider the Links and videos with descriptions in parentheses as optional bonus.\nThis might be useful to understand the further concepts in more depth.\n\n(https://en.wikipedia.org/wiki/Newton%27s_method#Description (Especially the animated graphic is interesting).)\nhttps://en.wikipedia.org/wiki/Gradient_descent#Description\nNeural networks (Backpropagation, etc.).\nActivation functions in detail (requires the above as prerequisite).\n\nVideos about the topic:\n\nGradient descent explained\n\n\n\n\n(Stochastic gradient descent explained)\n\n\n\n\n(Entropy explained)\n\n\n\n\nShort explanation of entropy, cross entropy and Kullback–Leibler divergence\n\n\n\n\nDeep Learning (chapter 1)\n\n\n\n\nHow neural networks learn - Deep Learning (chapter 2)\n\n\n\n\nBackpropagation - Deep Learning (chapter 3)\n\n\n\n\nAnother video about backpropagation (extends the previous one) - Deep Learning (chapter 4)\n\n\n\n\n9.3.1 Caveats of neural network optimization\nDepending on activation functions, it might occur that the network won’t get updated, even with high learning rates (called vanishing gradient, especially for “sigmoid” functions). Furthermore, updates might overshoot (called exploding gradients) or activation functions will result in many zeros (especially for “relu”, dying relu).\nIn general, the first layers of a network tend to learn (much) more slowly than subsequent ones."
  },
  {
    "objectID": "C3-ConvolutionalNeuralNetworks.html#example-mnist",
    "href": "C3-ConvolutionalNeuralNetworks.html#example-mnist",
    "title": "10  Convolutional Neural Networks (CNN)",
    "section": "10.1 Example MNIST",
    "text": "10.1 Example MNIST\nWe will show the use of convolutional neural networks with the MNIST data set. This data set is maybe one of the most famous image data sets. It consists of 60,000 handwritten digits from 0-9.\nTo do so, we define a few helper functions:\n\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nrotate = function(x){ t(apply(x, 2, rev)) }\n\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE,\n     main = paste0(\"Label: \", as.character(title)))\n}\n\nThe MNIST data set is so famous that there is an automatic download function in Keras:\n\ndata = dataset_mnist()\ntrain = data$train\ntest = data$test\n\nLet’s visualize a few digits:\n\noldpar = par(mfrow = c(1, 3))\n.n = sapply(1:3, function(x) imgPlot(train$x[x,,], train$y[x]))\n\n\n\npar(oldpar)\n\nSimilar to the normal machine learning workflow, we have to scale the pixels (from 0-255) to the range of \\([0, 1]\\) and one hot encode the response. For scaling the pixels, we will use arrays instead of matrices. Arrays are called tensors in mathematics and a 2D array/tensor is typically called a matrix.\n\ntrain_x = array(train$x/255, c(dim(train$x), 1))\ntest_x = array(test$x/255, c(dim(test$x), 1))\ntrain_y = to_categorical(train$y, 10)\ntest_y = to_categorical(test$y, 10)\n\nThe last dimension denotes the number of channels in the image. In our case we have only one channel because the images are black and white.\nMost times, we would have at least 3 color channels, for example RGB (red, green, blue) or HSV (hue, saturation, value), sometimes with several additional dimensions like transparency.\nTo build our convolutional model, we have to specify a kernel. In our case, we will use 16 convolutional kernels (filters) of size \\(2\\times2\\). These are 2D kernels because our images are 2D. For movies for example, one would use 3D kernels (the third dimension would correspond to time and not to the color channels).\n\nKerasTorch\n\n\n\nmodel = keras_model_sequential()\nmodel %&gt;%\n layer_conv_2d(input_shape = c(28L, 28L, 1L), filters = 16L,\n               kernel_size = c(2L, 2L), activation = \"relu\") %&gt;%\n layer_max_pooling_2d() %&gt;%\n layer_conv_2d(filters = 16L, kernel_size = c(3L, 3L), activation = \"relu\") %&gt;%\n layer_max_pooling_2d() %&gt;%\n layer_flatten() %&gt;%\n layer_dense(100L, activation = \"relu\") %&gt;%\n layer_dense(10L, activation = \"softmax\")\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 27, 27, 16)              80          \n max_pooling2d_1 (MaxPooling2D)     (None, 13, 13, 16)              0           \n conv2d (Conv2D)                    (None, 11, 11, 16)              2320        \n max_pooling2d (MaxPooling2D)       (None, 5, 5, 16)                0           \n flatten (Flatten)                  (None, 400)                     0           \n dense_1 (Dense)                    (None, 100)                     40100       \n dense (Dense)                      (None, 10)                      1010        \n================================================================================\nTotal params: 43,510\nTrainable params: 43,510\nNon-trainable params: 0\n________________________________________________________________________________\n\n\n\n\nPrepare/download data:\n\nlibrary(torch)\nlibrary(torchvision)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntrain_ds = mnist_dataset(\n  \".\",\n  download = TRUE,\n  train = TRUE,\n  transform = transform_to_tensor\n)\n\ntest_ds = mnist_dataset(\n  \".\",\n  download = TRUE,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n\nBuild dataloader:\n\ntrain_dl = dataloader(train_ds, batch_size = 32, shuffle = TRUE)\ntest_dl = dataloader(test_ds, batch_size = 32)\nfirst_batch = train_dl$.iter()\ndf = first_batch$.next()\n\ndf$x$size()\n\n[1] 32  1 28 28\n\n\nBuild convolutional neural network: We have here to calculate the shapes of our layers on our own:\nWe start with our input of shape (batch_size, 1, 28, 28)\n\nsample = df$x\nsample$size()\n\n[1] 32  1 28 28\n\n\nFirst convolutional layer has shape (input channel = 1, number of feature maps = 16, kernel size = 2)\n\nconv1 = nn_conv2d(1, 16L, 2L, stride = 1L)\n(sample %&gt;% conv1)$size()\n\n[1] 32 16 27 27\n\n\nOutput: batch_size = 32, number of feature maps = 16, dimensions of each feature map = \\((27 , 27)\\) Wit a kernel size of two and stride = 1 we will lose one pixel in each dimension… Questions:\n\nWhat happens if we increase the stride?\nWhat happens if we increase the kernel size?\n\nPooling layer summarizes each feature map\n\n(sample %&gt;% conv1 %&gt;% nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()\n\n[1] 32 16 13 13\n\n\nkernel_size = 2L and stride = 2L halfs the pixel dimensions of our image.\nFully connected layer\nNow we have to flatten our final output of the convolutional neural network model to use a normal fully connected layer, but to do so we have to calculate the number of inputs for the fully connected layer:\n\ndims = (sample %&gt;% conv1 %&gt;%\n          nnf_max_pool2d(kernel_size = 2L, stride = 2L))$size()\n# Without the batch size of course.\nfinal = prod(dims[-1]) \nprint(final)\n\n[1] 2704\n\nfc = nn_linear(final, 10L)\n(sample %&gt;% conv1 %&gt;% nnf_max_pool2d(kernel_size = 2L, stride = 2L)\n  %&gt;% torch_flatten(start_dim = 2L) %&gt;% fc)$size()\n\n[1] 32 10\n\n\nBuild the network:\n\nnet = nn_module(\n  \"mnist\",\n  initialize = function(){\n    self$conv1 = nn_conv2d(1, 16L, 2L)\n    self$conv2 = nn_conv2d(16L, 16L, 3L)\n    self$fc1 = nn_linear(400L, 100L)\n    self$fc2 = nn_linear(100L, 10L)\n  },\n  forward = function(x){\n    x %&gt;%\n      self$conv1() %&gt;%\n      nnf_relu() %&gt;%\n      nnf_max_pool2d(2) %&gt;%\n      self$conv2() %&gt;%\n      nnf_relu() %&gt;%\n      nnf_max_pool2d(2) %&gt;%\n      torch_flatten(start_dim = 2) %&gt;%\n      self$fc1() %&gt;%\n      nnf_relu() %&gt;%\n      self$fc2()\n  }\n)\n\n\n\n\nWe additionally used a pooling layer for downsizing the resulting feature maps. Without further specification, a \\(2\\times2\\) pooling layer is taken automatically. Pooling layers take the input feature map and divide it into (in our case) parts of \\(2\\times2\\) size. Then the respective pooling operation is executed. For every input map/layer, you get one (downsized) output map/layer.\nAs we are using the max pooling layer (there are sever other methods like the mean pooling), only the maximum value of these 4 parts is taken and forwarded further. Example input:\n1   2   |   5   8   |   3   6\n6   5   |   2   4   |   8   1\n------------------------------\n9   4   |   3   7   |   2   5\n0   3   |   2   7   |   4   9\nWe use max pooling for every field:\nmax(1, 2, 6, 5)   |   max(5, 8, 2, 4)   |   max(3, 6, 8, 1)\n-----------------------------------------------------------\nmax(9, 4, 0, 3)   |   max(3, 7, 2, 7)   |   max(2, 5, 4, 9)\nSo the resulting pooled information is:\n6   |   8   |   8\n------------------\n9   |   7   |   9\nIn this example, a \\(4\\times6\\) layer was transformed to a \\(2\\times3\\) layer and thus downsized. This is similar to the biological process called lateral inhibition where active neurons inhibit the activity of neighboring neurons. It’s a loss of information but often very useful for aggregating information and prevent overfitting.\nAfter another convolution and pooling layer, we flatten the output. This means that the following dense layer treats the previous layer as a full layer (so the dense layer is connected to all the weights from the last feature maps). You can think of this as transforming a matrix (2D) into a simple 1D vector. The full vector is then used. After flattening the layer, we can simply use our typical output layer.\n\nKerasTorch\n\n\nThe rest is as usual:\nFirst we compile the model:\n\nmodel %&gt;%\n  keras::compile(\n      optimizer = keras::optimizer_adamax(0.01),\n      loss = loss_categorical_crossentropy\n  )\nsummary(model)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 27, 27, 16)              80          \n max_pooling2d_1 (MaxPooling2D)     (None, 13, 13, 16)              0           \n conv2d (Conv2D)                    (None, 11, 11, 16)              2320        \n max_pooling2d (MaxPooling2D)       (None, 5, 5, 16)                0           \n flatten (Flatten)                  (None, 400)                     0           \n dense_1 (Dense)                    (None, 100)                     40100       \n dense (Dense)                      (None, 10)                      1010        \n================================================================================\nTotal params: 43,510\nTrainable params: 43,510\nNon-trainable params: 0\n________________________________________________________________________________\n\n\nThen, we train the model:\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nepochs = 5L\nbatch_size = 32L\nmodel %&gt;%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = epochs,\n    batch_size = batch_size,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\n\n\nTrain model:\n\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\nmodel_torch = net()\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nfor(e in 1:3){\n  losses = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]], reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      losses = c(losses, loss$item())\n    }\n  )\n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", e, mean(losses)))\n}\n\nEvaluation:\n\nmodel_torch$eval()\n\ntest_losses = c()\ntotal = 0\ncorrect = 0\n\ncoro::loop(\n  for(batch in test_dl){\n    output = model_torch(batch[[1]])\n    labels = batch[[2]]\n    loss = nnf_cross_entropy(output, labels)\n    test_losses = c(test_losses, loss$item())\n    predicted = torch_max(output$data(), dim = 2)[[2]]\n    total = total + labels$size(1)\n    correct = correct + (predicted == labels)$sum()$item()\n  }\n)\n\nmean(test_losses)\ntest_accuracy =  correct/total\ntest_accuracy"
  },
  {
    "objectID": "C3-ConvolutionalNeuralNetworks.html#example-cifar",
    "href": "C3-ConvolutionalNeuralNetworks.html#example-cifar",
    "title": "10  Convolutional Neural Networks (CNN)",
    "section": "10.2 Example CIFAR",
    "text": "10.2 Example CIFAR\nCIFAR10 is another famous image classification dataset. It consists of ten classes with colored images (see https://www.cs.toronto.edu/~kriz/cifar.html).\n\nlibrary(keras)\ndata = keras::dataset_cifar10()\ntrain = data$train\ntest = data$test\nimage = train$x[1,,,]\nimage %&gt;% \n image_to_array() %&gt;%\n `/`(., 255) %&gt;%\n as.raster() %&gt;%\n plot()\n## normalize pixel to 0-1\ntrain_x = array(train$x/255, c(dim(train$x)))\ntest_x = array(test$x/255, c(dim(test$x)))\ntrain_y = to_categorical(train$y, 10)\ntest_y = to_categorical(test$y, 10)\nmodel = keras_model_sequential()\nmodel %&gt;% \n layer_conv_2d(input_shape = c(32L, 32L,3L),filters = 16L, kernel_size = c(2L,2L), activation = \"relu\") %&gt;% \n layer_max_pooling_2d() %&gt;% \n layer_dropout(0.3) %&gt;% \n layer_conv_2d(filters = 16L, kernel_size = c(3L,3L), activation = \"relu\") %&gt;% \n layer_max_pooling_2d() %&gt;% \n layer_flatten() %&gt;% \n layer_dense(10, activation = \"softmax\")\nsummary(model)\nmodel %&gt;% \n compile(\n optimizer = optimizer_adamax(),\n loss = loss_categorical_crossentropy\n )\nearly = callback_early_stopping(patience = 5L)\nepochs = 1L\nbatch_size =20L\nmodel %&gt;% fit(\n x = train_x, \n y = train_y,\n epochs = epochs,\n batch_size = batch_size,\n shuffle = TRUE,\n validation_split = 0.2,\n callbacks = c(early)\n)"
  },
  {
    "objectID": "C3-ConvolutionalNeuralNetworks.html#exercise",
    "href": "C3-ConvolutionalNeuralNetworks.html#exercise",
    "title": "10  Convolutional Neural Networks (CNN)",
    "section": "10.3 Exercise",
    "text": "10.3 Exercise\n\n\n\n\n\n\nTask: CNN for flower dataset\n\n\n\nThe next exercise is based on the flower dataset in the Ecodata package.\nFollow the steps above and build your own convolutional neural network.\nFinally, submit your predictions to the submission server. If you have extra time, take a look at kaggle and find the flower dataset challenge for specific architectures tailored for this dataset.\nTasks:\n\nIf you are unsure how do it, take a look at the solution and try to make the model more complex (e.g. add convolutional layers, regularization, etc.)\nTake a look at this notebook from kaggle , try to copy their architecture (it is the same dataset but upsized (i.e. more pixels, the only difference is the input dimension))\n\nPrepare data:\n\nlibrary(tensorflow)\nlibrary(keras)\n\ntrain = EcoData::dataset_flower()$train/255\ntest = EcoData::dataset_flower()$test/255\nlabels = EcoData::dataset_flower()$labels\n\nPlot flower:\n\ntrain[100,,,] %&gt;%\n  image_to_array() %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\n\nTip: Take a look at the dataset chapter.\n\n\nClick here to see the solution for a minimal example\n\nBuild model:\n\nmodel = keras_model_sequential()\nmodel %&gt;% \n  layer_conv_2d(filters = 4L, kernel_size = 2L,\n                input_shape = list(80L, 80L, 3L)) %&gt;% \n  layer_max_pooling_2d() %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 5L, activation = \"softmax\")\n\n### Model fitting ###\n\nmodel %&gt;% \n  compile(loss = loss_categorical_crossentropy, \n          optimizer = optimizer_adamax(learning_rate = 0.01))\n\nmodel %&gt;% \n  fit(x = train, y = keras::k_one_hot(labels, 5L))\n\nPredictions:\n\n# Prediction on training data:\npred = apply(model %&gt;% predict(train), 1, which.max)\nMetrics::accuracy(pred - 1L, labels)\n\n[1] 0.9490572\n\ntable(pred)\n\npred\n  1   2   3   4   5 \n530 811 529 509 644 \n\n# Prediction for the submission server:\npred = model %&gt;% predict(test) %&gt;% apply(1, which.max) - 1L\ntable(pred)\n\npred\n  0   1   2   3   4 \n184 425 237 218 236 \n\n\nSubmission:\n\nwrite.csv(data.frame(y = pred), file = \"cnn.csv\")"
  },
  {
    "objectID": "C3-ConvolutionalNeuralNetworks.html#advanced-training-techniques",
    "href": "C3-ConvolutionalNeuralNetworks.html#advanced-training-techniques",
    "title": "10  Convolutional Neural Networks (CNN)",
    "section": "10.4 Advanced Training Techniques",
    "text": "10.4 Advanced Training Techniques\n\n10.4.1 Data Augmentation\nHaving to train a convolutional neural network using very little data is a common problem. Data augmentation helps to artificially increase the number of images.\nThe idea is that a convolutional neural network learns specific structures such as edges from images. Rotating, adding noise, and zooming in and out will preserve the overall key structure we are interested in, but the model will see new images and has to search once again for the key structures.\nLuckily, it is very easy to use data augmentation in Keras.\nTo show this, we will use our flower data set. We have to define a generator object (a specific object which infinitely draws samples from our data set). In the generator we can turn on the data augmentation.\n\nKerasTorch\n\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = EcoData::dataset_flower()\ntrain = data$train/255\nlabels = data$labels\n\nmodel = keras_model_sequential()\nmodel %&gt;%\n  layer_conv_2d(filter = 16L, kernel_size = c(5L, 5L),\n                input_shape = c(80L, 80L, 3L), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d() %&gt;%\n  layer_conv_2d(filter = 32L, kernel_size = c(3L, 3L),\n                activation = \"relu\") %&gt;%\n  layer_max_pooling_2d() %&gt;%\n  layer_conv_2d(filter = 64L, kernel_size = c(3L, 3L),\n                strides = c(2L, 2L), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d() %&gt;%\n  layer_flatten() %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(units = 5L, activation = \"softmax\")\n\n  \n# Data augmentation.\naug = image_data_generator(rotation_range = 90, \n                           zoom_range = c(0.3), \n                           horizontal_flip = TRUE, \n                           vertical_flip = TRUE)\n\n# Data preparation / splitting.\nindices = sample.int(nrow(train), 0.1 * nrow(train))\ngenerator = flow_images_from_data(train[-indices,,,],\n                                  k_one_hot(labels[-indices], num_classes = 5L),\n                                  generator = aug,\n                                  batch_size = 25L,\n                                  shuffle = TRUE)\n\ntest = train[indices,,,]\n\n## Training loop with early stopping:\n\n# As we use an iterator (the generator), validation loss is not applicable.\n# An available metric is the normal loss.\nearly = keras::callback_early_stopping(patience = 2L, monitor = \"loss\")\n\nmodel %&gt;%\n    keras::compile(loss = loss_categorical_crossentropy,\n                   optimizer = keras::optimizer_adamax(learning_rate = 0.01))\n\nmodel %&gt;%\n    fit(generator, epochs = 20L, batch_size = 25L,\n        shuffle = TRUE, callbacks = c(early))\n\n# Predictions on the training set:\npred = predict(model, data$train[-indices,,,]) %&gt;% apply(1, which.max) - 1\nMetrics::accuracy(pred, labels[-indices])\ntable(pred)\n\n# Predictions on the holdout / test set:\npred = predict(model, test) %&gt;% apply(1, which.max) - 1\nMetrics::accuracy(pred, labels[indices])\ntable(pred)\n\n# If you want to predict on the holdout for submission, use:\npred = predict(model, EcoData::dataset_flower()$test/255) %&gt;%\n  apply(1, which.max) - 1\ntable(pred)\n\nUsing data augmentation we can artificially increase the number of images.\n\n\nIn Torch, we have to change the transform function (but only for the train dataloader):\n\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntrain_transforms = function(img){\n  img %&gt;%\n    transform_to_tensor() %&gt;%\n    transform_random_horizontal_flip(p = 0.3) %&gt;%\n    transform_random_resized_crop(size = c(28L, 28L)) %&gt;%\n    transform_random_vertical_flip(0.3)\n}\n\ntrain_ds = mnist_dataset(\".\", download = TRUE, train = TRUE,\n                         transform = train_transforms)\ntest_ds = mnist_dataset(\".\", download = TRUE, train = FALSE,\n                        transform = transform_to_tensor)\n\ntrain_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)\ntest_dl = dataloader(test_ds, batch_size = 100L)\n\nmodel_torch = net()\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nfor(e in 1:1){\n  losses = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]], reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      losses = c(losses, loss$item())\n    }\n  )\n  \n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", e, mean(losses)))\n}\n\nmodel_torch$eval()\n\ntest_losses = c()\ntotal = 0\ncorrect = 0\n\ncoro::loop(\n  for(batch in test_dl){\n    output = model_torch(batch[[1]])\n    labels = batch[[2]]\n    loss = nnf_cross_entropy(output, labels)\n    test_losses = c(test_losses, loss$item())\n    predicted = torch_max(output$data(), dim = 2)[[2]]\n    total = total + labels$size(1)\n    correct = correct + (predicted == labels)$sum()$item()\n  }\n)\n\ntest_accuracy =  correct/total\nprint(test_accuracy)\n\n\n\n\n\n\n10.4.2 Transfer Learning\nAnother approach to reduce the necessary number of images or to speed up convergence of the models is the use of transfer learning.\nThe main idea of transfer learning is that all the convolutional layers have mainly one task - learning to identify highly correlated neighboring features. This knowledge is then used for new tasks. The convolutional layers learn structures such as edges in images and only the top layer, the dense layer is the actual classifier of the convolutional neural network for a specific task. Thus, one could think that we could only train the top layer as classifier. To do so, it will be confronted by sets of different edges/structures and has to decide the label based on these.\nAgain, this sounds very complicated but it is again quite easy with Keras and Torch.\n\nKerasTorch\n\n\nWe will do this now with the CIFAR10 data set, so we have to prepare the data:\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = keras::dataset_cifar10()\ntrain = data$train\ntest = data$test\n\nrm(data)\n\nimage = train$x[5,,,]\nimage %&gt;%\n  image_to_array() %&gt;%\n  `/`(., 255) %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\ntrain_x = array(train$x/255, c(dim(train$x)))\ntest_x = array(test$x/255, c(dim(test$x)))\ntrain_y = to_categorical(train$y, 10)\ntest_y = to_categorical(test$y, 10)\n\nrm(train, test)\n\nKeras provides download functions for all famous architectures/convolutional neural network models which are already trained on the imagenet data set (another famous data set). These trained networks come already without their top layer, so we have to set include_top to false and change the input shape.\n\ndensenet = application_densenet201(include_top = FALSE,\n                                   input_shape  = c(32L, 32L, 3L))\n\nNow, we will not use a sequential model but just a “keras_model” where we can specify the inputs and outputs. Thereby, the output is our own top layer, but the inputs are the densenet inputs, as these are already pre-trained.\n\nmodel = keras::keras_model(\n  inputs = densenet$input,\n  outputs = layer_flatten(\n    layer_dense(densenet$output, units = 10L, activation = \"softmax\")\n  )\n)\n\n# Notice that this snippet just creates one (!) new layer.\n# The densenet's inputs are connected with the model's inputs.\n# The densenet's outputs are connected with our own layer (with 10 nodes).\n# This layer is also the output layer of the model.\n\nIn the next step we want to freeze all layers except for our own last layer. Freezing means that these are not trained: We do not want to train the complete model, we only want to train the last layer. You can check the number of trainable weights via summary(model).\n\nmodel %&gt;% freeze_weights(to = length(model$layers) - 1)\nsummary(model)\n\nModel: \"model\"\n________________________________________________________________________________\n Layer (type)         Output Shape   Param #  Connected to           Trainable  \n================================================================================\n input_1 (InputLayer)  [(None, 32, 3  0       []                     N          \n                      2, 3)]                                                    \n zero_padding2d (Zero  (None, 38, 38  0       ['input_1[0][0]']      N          \n Padding2D)           , 3)                                                      \n conv1/conv (Conv2D)  (None, 16, 16  9408     ['zero_padding2d[0][0  N          \n                      , 64)                   ]']                               \n conv1/bn (BatchNorma  (None, 16, 16  256     ['conv1/conv[0][0]']   N          \n lization)            , 64)                                                     \n conv1/relu (Activati  (None, 16, 16  0       ['conv1/bn[0][0]']     N          \n on)                  , 64)                                                     \n zero_padding2d_1 (Ze  (None, 18, 18  0       ['conv1/relu[0][0]']   N          \n roPadding2D)         , 64)                                                     \n pool1 (MaxPooling2D)  (None, 8, 8,   0       ['zero_padding2d_1[0]  N          \n                      64)                     [0]']                             \n conv2_block1_0_bn (B  (None, 8, 8,   256     ['pool1[0][0]']        N          \n atchNormalization)   64)                                                       \n conv2_block1_0_relu   (None, 8, 8,   0       ['conv2_block1_0_bn[0  N          \n (Activation)         64)                     ][0]']                            \n conv2_block1_1_conv   (None, 8, 8,   8192    ['conv2_block1_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv2_block1_1_bn (B  (None, 8, 8,   512     ['conv2_block1_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv2_block1_1_relu   (None, 8, 8,   0       ['conv2_block1_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv2_block1_2_conv   (None, 8, 8,   36864   ['conv2_block1_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv2_block1_concat   (None, 8, 8,   0       ['pool1[0][0]',        N          \n (Concatenate)        96)                      'conv2_block1_2_conv             \n                                              [0][0]']                          \n conv2_block2_0_bn (B  (None, 8, 8,   384     ['conv2_block1_concat  N          \n atchNormalization)   96)                     [0][0]']                          \n conv2_block2_0_relu   (None, 8, 8,   0       ['conv2_block2_0_bn[0  N          \n (Activation)         96)                     ][0]']                            \n conv2_block2_1_conv   (None, 8, 8,   12288   ['conv2_block2_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv2_block2_1_bn (B  (None, 8, 8,   512     ['conv2_block2_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv2_block2_1_relu   (None, 8, 8,   0       ['conv2_block2_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv2_block2_2_conv   (None, 8, 8,   36864   ['conv2_block2_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv2_block2_concat   (None, 8, 8,   0       ['conv2_block1_concat  N          \n (Concatenate)        128)                    [0][0]',                          \n                                               'conv2_block2_2_conv             \n                                              [0][0]']                          \n conv2_block3_0_bn (B  (None, 8, 8,   512     ['conv2_block2_concat  N          \n atchNormalization)   128)                    [0][0]']                          \n conv2_block3_0_relu   (None, 8, 8,   0       ['conv2_block3_0_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv2_block3_1_conv   (None, 8, 8,   16384   ['conv2_block3_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv2_block3_1_bn (B  (None, 8, 8,   512     ['conv2_block3_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv2_block3_1_relu   (None, 8, 8,   0       ['conv2_block3_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv2_block3_2_conv   (None, 8, 8,   36864   ['conv2_block3_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv2_block3_concat   (None, 8, 8,   0       ['conv2_block2_concat  N          \n (Concatenate)        160)                    [0][0]',                          \n                                               'conv2_block3_2_conv             \n                                              [0][0]']                          \n conv2_block4_0_bn (B  (None, 8, 8,   640     ['conv2_block3_concat  N          \n atchNormalization)   160)                    [0][0]']                          \n conv2_block4_0_relu   (None, 8, 8,   0       ['conv2_block4_0_bn[0  N          \n (Activation)         160)                    ][0]']                            \n conv2_block4_1_conv   (None, 8, 8,   20480   ['conv2_block4_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv2_block4_1_bn (B  (None, 8, 8,   512     ['conv2_block4_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv2_block4_1_relu   (None, 8, 8,   0       ['conv2_block4_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv2_block4_2_conv   (None, 8, 8,   36864   ['conv2_block4_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv2_block4_concat   (None, 8, 8,   0       ['conv2_block3_concat  N          \n (Concatenate)        192)                    [0][0]',                          \n                                               'conv2_block4_2_conv             \n                                              [0][0]']                          \n conv2_block5_0_bn (B  (None, 8, 8,   768     ['conv2_block4_concat  N          \n atchNormalization)   192)                    [0][0]']                          \n conv2_block5_0_relu   (None, 8, 8,   0       ['conv2_block5_0_bn[0  N          \n (Activation)         192)                    ][0]']                            \n conv2_block5_1_conv   (None, 8, 8,   24576   ['conv2_block5_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv2_block5_1_bn (B  (None, 8, 8,   512     ['conv2_block5_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv2_block5_1_relu   (None, 8, 8,   0       ['conv2_block5_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv2_block5_2_conv   (None, 8, 8,   36864   ['conv2_block5_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv2_block5_concat   (None, 8, 8,   0       ['conv2_block4_concat  N          \n (Concatenate)        224)                    [0][0]',                          \n                                               'conv2_block5_2_conv             \n                                              [0][0]']                          \n conv2_block6_0_bn (B  (None, 8, 8,   896     ['conv2_block5_concat  N          \n atchNormalization)   224)                    [0][0]']                          \n conv2_block6_0_relu   (None, 8, 8,   0       ['conv2_block6_0_bn[0  N          \n (Activation)         224)                    ][0]']                            \n conv2_block6_1_conv   (None, 8, 8,   28672   ['conv2_block6_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv2_block6_1_bn (B  (None, 8, 8,   512     ['conv2_block6_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv2_block6_1_relu   (None, 8, 8,   0       ['conv2_block6_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv2_block6_2_conv   (None, 8, 8,   36864   ['conv2_block6_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv2_block6_concat   (None, 8, 8,   0       ['conv2_block5_concat  N          \n (Concatenate)        256)                    [0][0]',                          \n                                               'conv2_block6_2_conv             \n                                              [0][0]']                          \n pool2_bn (BatchNorma  (None, 8, 8,   1024    ['conv2_block6_concat  N          \n lization)            256)                    [0][0]']                          \n pool2_relu (Activati  (None, 8, 8,   0       ['pool2_bn[0][0]']     N          \n on)                  256)                                                      \n pool2_conv (Conv2D)  (None, 8, 8,   32768    ['pool2_relu[0][0]']   N          \n                      128)                                                      \n pool2_pool (AverageP  (None, 4, 4,   0       ['pool2_conv[0][0]']   N          \n ooling2D)            128)                                                      \n conv3_block1_0_bn (B  (None, 4, 4,   512     ['pool2_pool[0][0]']   N          \n atchNormalization)   128)                                                      \n conv3_block1_0_relu   (None, 4, 4,   0       ['conv3_block1_0_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block1_1_conv   (None, 4, 4,   16384   ['conv3_block1_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block1_1_bn (B  (None, 4, 4,   512     ['conv3_block1_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block1_1_relu   (None, 4, 4,   0       ['conv3_block1_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block1_2_conv   (None, 4, 4,   36864   ['conv3_block1_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block1_concat   (None, 4, 4,   0       ['pool2_pool[0][0]',   N          \n (Concatenate)        160)                     'conv3_block1_2_conv             \n                                              [0][0]']                          \n conv3_block2_0_bn (B  (None, 4, 4,   640     ['conv3_block1_concat  N          \n atchNormalization)   160)                    [0][0]']                          \n conv3_block2_0_relu   (None, 4, 4,   0       ['conv3_block2_0_bn[0  N          \n (Activation)         160)                    ][0]']                            \n conv3_block2_1_conv   (None, 4, 4,   20480   ['conv3_block2_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block2_1_bn (B  (None, 4, 4,   512     ['conv3_block2_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block2_1_relu   (None, 4, 4,   0       ['conv3_block2_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block2_2_conv   (None, 4, 4,   36864   ['conv3_block2_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block2_concat   (None, 4, 4,   0       ['conv3_block1_concat  N          \n (Concatenate)        192)                    [0][0]',                          \n                                               'conv3_block2_2_conv             \n                                              [0][0]']                          \n conv3_block3_0_bn (B  (None, 4, 4,   768     ['conv3_block2_concat  N          \n atchNormalization)   192)                    [0][0]']                          \n conv3_block3_0_relu   (None, 4, 4,   0       ['conv3_block3_0_bn[0  N          \n (Activation)         192)                    ][0]']                            \n conv3_block3_1_conv   (None, 4, 4,   24576   ['conv3_block3_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block3_1_bn (B  (None, 4, 4,   512     ['conv3_block3_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block3_1_relu   (None, 4, 4,   0       ['conv3_block3_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block3_2_conv   (None, 4, 4,   36864   ['conv3_block3_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block3_concat   (None, 4, 4,   0       ['conv3_block2_concat  N          \n (Concatenate)        224)                    [0][0]',                          \n                                               'conv3_block3_2_conv             \n                                              [0][0]']                          \n conv3_block4_0_bn (B  (None, 4, 4,   896     ['conv3_block3_concat  N          \n atchNormalization)   224)                    [0][0]']                          \n conv3_block4_0_relu   (None, 4, 4,   0       ['conv3_block4_0_bn[0  N          \n (Activation)         224)                    ][0]']                            \n conv3_block4_1_conv   (None, 4, 4,   28672   ['conv3_block4_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block4_1_bn (B  (None, 4, 4,   512     ['conv3_block4_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block4_1_relu   (None, 4, 4,   0       ['conv3_block4_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block4_2_conv   (None, 4, 4,   36864   ['conv3_block4_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block4_concat   (None, 4, 4,   0       ['conv3_block3_concat  N          \n (Concatenate)        256)                    [0][0]',                          \n                                               'conv3_block4_2_conv             \n                                              [0][0]']                          \n conv3_block5_0_bn (B  (None, 4, 4,   1024    ['conv3_block4_concat  N          \n atchNormalization)   256)                    [0][0]']                          \n conv3_block5_0_relu   (None, 4, 4,   0       ['conv3_block5_0_bn[0  N          \n (Activation)         256)                    ][0]']                            \n conv3_block5_1_conv   (None, 4, 4,   32768   ['conv3_block5_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block5_1_bn (B  (None, 4, 4,   512     ['conv3_block5_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block5_1_relu   (None, 4, 4,   0       ['conv3_block5_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block5_2_conv   (None, 4, 4,   36864   ['conv3_block5_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block5_concat   (None, 4, 4,   0       ['conv3_block4_concat  N          \n (Concatenate)        288)                    [0][0]',                          \n                                               'conv3_block5_2_conv             \n                                              [0][0]']                          \n conv3_block6_0_bn (B  (None, 4, 4,   1152    ['conv3_block5_concat  N          \n atchNormalization)   288)                    [0][0]']                          \n conv3_block6_0_relu   (None, 4, 4,   0       ['conv3_block6_0_bn[0  N          \n (Activation)         288)                    ][0]']                            \n conv3_block6_1_conv   (None, 4, 4,   36864   ['conv3_block6_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block6_1_bn (B  (None, 4, 4,   512     ['conv3_block6_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block6_1_relu   (None, 4, 4,   0       ['conv3_block6_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block6_2_conv   (None, 4, 4,   36864   ['conv3_block6_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block6_concat   (None, 4, 4,   0       ['conv3_block5_concat  N          \n (Concatenate)        320)                    [0][0]',                          \n                                               'conv3_block6_2_conv             \n                                              [0][0]']                          \n conv3_block7_0_bn (B  (None, 4, 4,   1280    ['conv3_block6_concat  N          \n atchNormalization)   320)                    [0][0]']                          \n conv3_block7_0_relu   (None, 4, 4,   0       ['conv3_block7_0_bn[0  N          \n (Activation)         320)                    ][0]']                            \n conv3_block7_1_conv   (None, 4, 4,   40960   ['conv3_block7_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block7_1_bn (B  (None, 4, 4,   512     ['conv3_block7_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block7_1_relu   (None, 4, 4,   0       ['conv3_block7_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block7_2_conv   (None, 4, 4,   36864   ['conv3_block7_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block7_concat   (None, 4, 4,   0       ['conv3_block6_concat  N          \n (Concatenate)        352)                    [0][0]',                          \n                                               'conv3_block7_2_conv             \n                                              [0][0]']                          \n conv3_block8_0_bn (B  (None, 4, 4,   1408    ['conv3_block7_concat  N          \n atchNormalization)   352)                    [0][0]']                          \n conv3_block8_0_relu   (None, 4, 4,   0       ['conv3_block8_0_bn[0  N          \n (Activation)         352)                    ][0]']                            \n conv3_block8_1_conv   (None, 4, 4,   45056   ['conv3_block8_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block8_1_bn (B  (None, 4, 4,   512     ['conv3_block8_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block8_1_relu   (None, 4, 4,   0       ['conv3_block8_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block8_2_conv   (None, 4, 4,   36864   ['conv3_block8_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block8_concat   (None, 4, 4,   0       ['conv3_block7_concat  N          \n (Concatenate)        384)                    [0][0]',                          \n                                               'conv3_block8_2_conv             \n                                              [0][0]']                          \n conv3_block9_0_bn (B  (None, 4, 4,   1536    ['conv3_block8_concat  N          \n atchNormalization)   384)                    [0][0]']                          \n conv3_block9_0_relu   (None, 4, 4,   0       ['conv3_block9_0_bn[0  N          \n (Activation)         384)                    ][0]']                            \n conv3_block9_1_conv   (None, 4, 4,   49152   ['conv3_block9_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv3_block9_1_bn (B  (None, 4, 4,   512     ['conv3_block9_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv3_block9_1_relu   (None, 4, 4,   0       ['conv3_block9_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv3_block9_2_conv   (None, 4, 4,   36864   ['conv3_block9_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv3_block9_concat   (None, 4, 4,   0       ['conv3_block8_concat  N          \n (Concatenate)        416)                    [0][0]',                          \n                                               'conv3_block9_2_conv             \n                                              [0][0]']                          \n conv3_block10_0_bn (  (None, 4, 4,   1664    ['conv3_block9_concat  N          \n BatchNormalization)  416)                    [0][0]']                          \n conv3_block10_0_relu  (None, 4, 4,   0       ['conv3_block10_0_bn[  N          \n  (Activation)        416)                    0][0]']                           \n conv3_block10_1_conv  (None, 4, 4,   53248   ['conv3_block10_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv3_block10_1_bn (  (None, 4, 4,   512     ['conv3_block10_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv3_block10_1_relu  (None, 4, 4,   0       ['conv3_block10_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv3_block10_2_conv  (None, 4, 4,   36864   ['conv3_block10_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv3_block10_concat  (None, 4, 4,   0       ['conv3_block9_concat  N          \n  (Concatenate)       448)                    [0][0]',                          \n                                               'conv3_block10_2_con             \n                                              v[0][0]']                         \n conv3_block11_0_bn (  (None, 4, 4,   1792    ['conv3_block10_conca  N          \n BatchNormalization)  448)                    t[0][0]']                         \n conv3_block11_0_relu  (None, 4, 4,   0       ['conv3_block11_0_bn[  N          \n  (Activation)        448)                    0][0]']                           \n conv3_block11_1_conv  (None, 4, 4,   57344   ['conv3_block11_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv3_block11_1_bn (  (None, 4, 4,   512     ['conv3_block11_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv3_block11_1_relu  (None, 4, 4,   0       ['conv3_block11_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv3_block11_2_conv  (None, 4, 4,   36864   ['conv3_block11_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv3_block11_concat  (None, 4, 4,   0       ['conv3_block10_conca  N          \n  (Concatenate)       480)                    t[0][0]',                         \n                                               'conv3_block11_2_con             \n                                              v[0][0]']                         \n conv3_block12_0_bn (  (None, 4, 4,   1920    ['conv3_block11_conca  N          \n BatchNormalization)  480)                    t[0][0]']                         \n conv3_block12_0_relu  (None, 4, 4,   0       ['conv3_block12_0_bn[  N          \n  (Activation)        480)                    0][0]']                           \n conv3_block12_1_conv  (None, 4, 4,   61440   ['conv3_block12_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv3_block12_1_bn (  (None, 4, 4,   512     ['conv3_block12_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv3_block12_1_relu  (None, 4, 4,   0       ['conv3_block12_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv3_block12_2_conv  (None, 4, 4,   36864   ['conv3_block12_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv3_block12_concat  (None, 4, 4,   0       ['conv3_block11_conca  N          \n  (Concatenate)       512)                    t[0][0]',                         \n                                               'conv3_block12_2_con             \n                                              v[0][0]']                         \n pool3_bn (BatchNorma  (None, 4, 4,   2048    ['conv3_block12_conca  N          \n lization)            512)                    t[0][0]']                         \n pool3_relu (Activati  (None, 4, 4,   0       ['pool3_bn[0][0]']     N          \n on)                  512)                                                      \n pool3_conv (Conv2D)  (None, 4, 4,   131072   ['pool3_relu[0][0]']   N          \n                      256)                                                      \n pool3_pool (AverageP  (None, 2, 2,   0       ['pool3_conv[0][0]']   N          \n ooling2D)            256)                                                      \n conv4_block1_0_bn (B  (None, 2, 2,   1024    ['pool3_pool[0][0]']   N          \n atchNormalization)   256)                                                      \n conv4_block1_0_relu   (None, 2, 2,   0       ['conv4_block1_0_bn[0  N          \n (Activation)         256)                    ][0]']                            \n conv4_block1_1_conv   (None, 2, 2,   32768   ['conv4_block1_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block1_1_bn (B  (None, 2, 2,   512     ['conv4_block1_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block1_1_relu   (None, 2, 2,   0       ['conv4_block1_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block1_2_conv   (None, 2, 2,   36864   ['conv4_block1_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block1_concat   (None, 2, 2,   0       ['pool3_pool[0][0]',   N          \n (Concatenate)        288)                     'conv4_block1_2_conv             \n                                              [0][0]']                          \n conv4_block2_0_bn (B  (None, 2, 2,   1152    ['conv4_block1_concat  N          \n atchNormalization)   288)                    [0][0]']                          \n conv4_block2_0_relu   (None, 2, 2,   0       ['conv4_block2_0_bn[0  N          \n (Activation)         288)                    ][0]']                            \n conv4_block2_1_conv   (None, 2, 2,   36864   ['conv4_block2_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block2_1_bn (B  (None, 2, 2,   512     ['conv4_block2_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block2_1_relu   (None, 2, 2,   0       ['conv4_block2_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block2_2_conv   (None, 2, 2,   36864   ['conv4_block2_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block2_concat   (None, 2, 2,   0       ['conv4_block1_concat  N          \n (Concatenate)        320)                    [0][0]',                          \n                                               'conv4_block2_2_conv             \n                                              [0][0]']                          \n conv4_block3_0_bn (B  (None, 2, 2,   1280    ['conv4_block2_concat  N          \n atchNormalization)   320)                    [0][0]']                          \n conv4_block3_0_relu   (None, 2, 2,   0       ['conv4_block3_0_bn[0  N          \n (Activation)         320)                    ][0]']                            \n conv4_block3_1_conv   (None, 2, 2,   40960   ['conv4_block3_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block3_1_bn (B  (None, 2, 2,   512     ['conv4_block3_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block3_1_relu   (None, 2, 2,   0       ['conv4_block3_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block3_2_conv   (None, 2, 2,   36864   ['conv4_block3_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block3_concat   (None, 2, 2,   0       ['conv4_block2_concat  N          \n (Concatenate)        352)                    [0][0]',                          \n                                               'conv4_block3_2_conv             \n                                              [0][0]']                          \n conv4_block4_0_bn (B  (None, 2, 2,   1408    ['conv4_block3_concat  N          \n atchNormalization)   352)                    [0][0]']                          \n conv4_block4_0_relu   (None, 2, 2,   0       ['conv4_block4_0_bn[0  N          \n (Activation)         352)                    ][0]']                            \n conv4_block4_1_conv   (None, 2, 2,   45056   ['conv4_block4_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block4_1_bn (B  (None, 2, 2,   512     ['conv4_block4_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block4_1_relu   (None, 2, 2,   0       ['conv4_block4_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block4_2_conv   (None, 2, 2,   36864   ['conv4_block4_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block4_concat   (None, 2, 2,   0       ['conv4_block3_concat  N          \n (Concatenate)        384)                    [0][0]',                          \n                                               'conv4_block4_2_conv             \n                                              [0][0]']                          \n conv4_block5_0_bn (B  (None, 2, 2,   1536    ['conv4_block4_concat  N          \n atchNormalization)   384)                    [0][0]']                          \n conv4_block5_0_relu   (None, 2, 2,   0       ['conv4_block5_0_bn[0  N          \n (Activation)         384)                    ][0]']                            \n conv4_block5_1_conv   (None, 2, 2,   49152   ['conv4_block5_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block5_1_bn (B  (None, 2, 2,   512     ['conv4_block5_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block5_1_relu   (None, 2, 2,   0       ['conv4_block5_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block5_2_conv   (None, 2, 2,   36864   ['conv4_block5_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block5_concat   (None, 2, 2,   0       ['conv4_block4_concat  N          \n (Concatenate)        416)                    [0][0]',                          \n                                               'conv4_block5_2_conv             \n                                              [0][0]']                          \n conv4_block6_0_bn (B  (None, 2, 2,   1664    ['conv4_block5_concat  N          \n atchNormalization)   416)                    [0][0]']                          \n conv4_block6_0_relu   (None, 2, 2,   0       ['conv4_block6_0_bn[0  N          \n (Activation)         416)                    ][0]']                            \n conv4_block6_1_conv   (None, 2, 2,   53248   ['conv4_block6_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block6_1_bn (B  (None, 2, 2,   512     ['conv4_block6_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block6_1_relu   (None, 2, 2,   0       ['conv4_block6_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block6_2_conv   (None, 2, 2,   36864   ['conv4_block6_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block6_concat   (None, 2, 2,   0       ['conv4_block5_concat  N          \n (Concatenate)        448)                    [0][0]',                          \n                                               'conv4_block6_2_conv             \n                                              [0][0]']                          \n conv4_block7_0_bn (B  (None, 2, 2,   1792    ['conv4_block6_concat  N          \n atchNormalization)   448)                    [0][0]']                          \n conv4_block7_0_relu   (None, 2, 2,   0       ['conv4_block7_0_bn[0  N          \n (Activation)         448)                    ][0]']                            \n conv4_block7_1_conv   (None, 2, 2,   57344   ['conv4_block7_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block7_1_bn (B  (None, 2, 2,   512     ['conv4_block7_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block7_1_relu   (None, 2, 2,   0       ['conv4_block7_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block7_2_conv   (None, 2, 2,   36864   ['conv4_block7_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block7_concat   (None, 2, 2,   0       ['conv4_block6_concat  N          \n (Concatenate)        480)                    [0][0]',                          \n                                               'conv4_block7_2_conv             \n                                              [0][0]']                          \n conv4_block8_0_bn (B  (None, 2, 2,   1920    ['conv4_block7_concat  N          \n atchNormalization)   480)                    [0][0]']                          \n conv4_block8_0_relu   (None, 2, 2,   0       ['conv4_block8_0_bn[0  N          \n (Activation)         480)                    ][0]']                            \n conv4_block8_1_conv   (None, 2, 2,   61440   ['conv4_block8_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block8_1_bn (B  (None, 2, 2,   512     ['conv4_block8_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block8_1_relu   (None, 2, 2,   0       ['conv4_block8_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block8_2_conv   (None, 2, 2,   36864   ['conv4_block8_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block8_concat   (None, 2, 2,   0       ['conv4_block7_concat  N          \n (Concatenate)        512)                    [0][0]',                          \n                                               'conv4_block8_2_conv             \n                                              [0][0]']                          \n conv4_block9_0_bn (B  (None, 2, 2,   2048    ['conv4_block8_concat  N          \n atchNormalization)   512)                    [0][0]']                          \n conv4_block9_0_relu   (None, 2, 2,   0       ['conv4_block9_0_bn[0  N          \n (Activation)         512)                    ][0]']                            \n conv4_block9_1_conv   (None, 2, 2,   65536   ['conv4_block9_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv4_block9_1_bn (B  (None, 2, 2,   512     ['conv4_block9_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv4_block9_1_relu   (None, 2, 2,   0       ['conv4_block9_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv4_block9_2_conv   (None, 2, 2,   36864   ['conv4_block9_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv4_block9_concat   (None, 2, 2,   0       ['conv4_block8_concat  N          \n (Concatenate)        544)                    [0][0]',                          \n                                               'conv4_block9_2_conv             \n                                              [0][0]']                          \n conv4_block10_0_bn (  (None, 2, 2,   2176    ['conv4_block9_concat  N          \n BatchNormalization)  544)                    [0][0]']                          \n conv4_block10_0_relu  (None, 2, 2,   0       ['conv4_block10_0_bn[  N          \n  (Activation)        544)                    0][0]']                           \n conv4_block10_1_conv  (None, 2, 2,   69632   ['conv4_block10_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block10_1_bn (  (None, 2, 2,   512     ['conv4_block10_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block10_1_relu  (None, 2, 2,   0       ['conv4_block10_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block10_2_conv  (None, 2, 2,   36864   ['conv4_block10_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block10_concat  (None, 2, 2,   0       ['conv4_block9_concat  N          \n  (Concatenate)       576)                    [0][0]',                          \n                                               'conv4_block10_2_con             \n                                              v[0][0]']                         \n conv4_block11_0_bn (  (None, 2, 2,   2304    ['conv4_block10_conca  N          \n BatchNormalization)  576)                    t[0][0]']                         \n conv4_block11_0_relu  (None, 2, 2,   0       ['conv4_block11_0_bn[  N          \n  (Activation)        576)                    0][0]']                           \n conv4_block11_1_conv  (None, 2, 2,   73728   ['conv4_block11_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block11_1_bn (  (None, 2, 2,   512     ['conv4_block11_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block11_1_relu  (None, 2, 2,   0       ['conv4_block11_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block11_2_conv  (None, 2, 2,   36864   ['conv4_block11_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block11_concat  (None, 2, 2,   0       ['conv4_block10_conca  N          \n  (Concatenate)       608)                    t[0][0]',                         \n                                               'conv4_block11_2_con             \n                                              v[0][0]']                         \n conv4_block12_0_bn (  (None, 2, 2,   2432    ['conv4_block11_conca  N          \n BatchNormalization)  608)                    t[0][0]']                         \n conv4_block12_0_relu  (None, 2, 2,   0       ['conv4_block12_0_bn[  N          \n  (Activation)        608)                    0][0]']                           \n conv4_block12_1_conv  (None, 2, 2,   77824   ['conv4_block12_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block12_1_bn (  (None, 2, 2,   512     ['conv4_block12_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block12_1_relu  (None, 2, 2,   0       ['conv4_block12_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block12_2_conv  (None, 2, 2,   36864   ['conv4_block12_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block12_concat  (None, 2, 2,   0       ['conv4_block11_conca  N          \n  (Concatenate)       640)                    t[0][0]',                         \n                                               'conv4_block12_2_con             \n                                              v[0][0]']                         \n conv4_block13_0_bn (  (None, 2, 2,   2560    ['conv4_block12_conca  N          \n BatchNormalization)  640)                    t[0][0]']                         \n conv4_block13_0_relu  (None, 2, 2,   0       ['conv4_block13_0_bn[  N          \n  (Activation)        640)                    0][0]']                           \n conv4_block13_1_conv  (None, 2, 2,   81920   ['conv4_block13_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block13_1_bn (  (None, 2, 2,   512     ['conv4_block13_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block13_1_relu  (None, 2, 2,   0       ['conv4_block13_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block13_2_conv  (None, 2, 2,   36864   ['conv4_block13_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block13_concat  (None, 2, 2,   0       ['conv4_block12_conca  N          \n  (Concatenate)       672)                    t[0][0]',                         \n                                               'conv4_block13_2_con             \n                                              v[0][0]']                         \n conv4_block14_0_bn (  (None, 2, 2,   2688    ['conv4_block13_conca  N          \n BatchNormalization)  672)                    t[0][0]']                         \n conv4_block14_0_relu  (None, 2, 2,   0       ['conv4_block14_0_bn[  N          \n  (Activation)        672)                    0][0]']                           \n conv4_block14_1_conv  (None, 2, 2,   86016   ['conv4_block14_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block14_1_bn (  (None, 2, 2,   512     ['conv4_block14_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block14_1_relu  (None, 2, 2,   0       ['conv4_block14_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block14_2_conv  (None, 2, 2,   36864   ['conv4_block14_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block14_concat  (None, 2, 2,   0       ['conv4_block13_conca  N          \n  (Concatenate)       704)                    t[0][0]',                         \n                                               'conv4_block14_2_con             \n                                              v[0][0]']                         \n conv4_block15_0_bn (  (None, 2, 2,   2816    ['conv4_block14_conca  N          \n BatchNormalization)  704)                    t[0][0]']                         \n conv4_block15_0_relu  (None, 2, 2,   0       ['conv4_block15_0_bn[  N          \n  (Activation)        704)                    0][0]']                           \n conv4_block15_1_conv  (None, 2, 2,   90112   ['conv4_block15_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block15_1_bn (  (None, 2, 2,   512     ['conv4_block15_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block15_1_relu  (None, 2, 2,   0       ['conv4_block15_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block15_2_conv  (None, 2, 2,   36864   ['conv4_block15_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block15_concat  (None, 2, 2,   0       ['conv4_block14_conca  N          \n  (Concatenate)       736)                    t[0][0]',                         \n                                               'conv4_block15_2_con             \n                                              v[0][0]']                         \n conv4_block16_0_bn (  (None, 2, 2,   2944    ['conv4_block15_conca  N          \n BatchNormalization)  736)                    t[0][0]']                         \n conv4_block16_0_relu  (None, 2, 2,   0       ['conv4_block16_0_bn[  N          \n  (Activation)        736)                    0][0]']                           \n conv4_block16_1_conv  (None, 2, 2,   94208   ['conv4_block16_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block16_1_bn (  (None, 2, 2,   512     ['conv4_block16_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block16_1_relu  (None, 2, 2,   0       ['conv4_block16_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block16_2_conv  (None, 2, 2,   36864   ['conv4_block16_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block16_concat  (None, 2, 2,   0       ['conv4_block15_conca  N          \n  (Concatenate)       768)                    t[0][0]',                         \n                                               'conv4_block16_2_con             \n                                              v[0][0]']                         \n conv4_block17_0_bn (  (None, 2, 2,   3072    ['conv4_block16_conca  N          \n BatchNormalization)  768)                    t[0][0]']                         \n conv4_block17_0_relu  (None, 2, 2,   0       ['conv4_block17_0_bn[  N          \n  (Activation)        768)                    0][0]']                           \n conv4_block17_1_conv  (None, 2, 2,   98304   ['conv4_block17_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block17_1_bn (  (None, 2, 2,   512     ['conv4_block17_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block17_1_relu  (None, 2, 2,   0       ['conv4_block17_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block17_2_conv  (None, 2, 2,   36864   ['conv4_block17_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block17_concat  (None, 2, 2,   0       ['conv4_block16_conca  N          \n  (Concatenate)       800)                    t[0][0]',                         \n                                               'conv4_block17_2_con             \n                                              v[0][0]']                         \n conv4_block18_0_bn (  (None, 2, 2,   3200    ['conv4_block17_conca  N          \n BatchNormalization)  800)                    t[0][0]']                         \n conv4_block18_0_relu  (None, 2, 2,   0       ['conv4_block18_0_bn[  N          \n  (Activation)        800)                    0][0]']                           \n conv4_block18_1_conv  (None, 2, 2,   102400  ['conv4_block18_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block18_1_bn (  (None, 2, 2,   512     ['conv4_block18_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block18_1_relu  (None, 2, 2,   0       ['conv4_block18_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block18_2_conv  (None, 2, 2,   36864   ['conv4_block18_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block18_concat  (None, 2, 2,   0       ['conv4_block17_conca  N          \n  (Concatenate)       832)                    t[0][0]',                         \n                                               'conv4_block18_2_con             \n                                              v[0][0]']                         \n conv4_block19_0_bn (  (None, 2, 2,   3328    ['conv4_block18_conca  N          \n BatchNormalization)  832)                    t[0][0]']                         \n conv4_block19_0_relu  (None, 2, 2,   0       ['conv4_block19_0_bn[  N          \n  (Activation)        832)                    0][0]']                           \n conv4_block19_1_conv  (None, 2, 2,   106496  ['conv4_block19_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block19_1_bn (  (None, 2, 2,   512     ['conv4_block19_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block19_1_relu  (None, 2, 2,   0       ['conv4_block19_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block19_2_conv  (None, 2, 2,   36864   ['conv4_block19_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block19_concat  (None, 2, 2,   0       ['conv4_block18_conca  N          \n  (Concatenate)       864)                    t[0][0]',                         \n                                               'conv4_block19_2_con             \n                                              v[0][0]']                         \n conv4_block20_0_bn (  (None, 2, 2,   3456    ['conv4_block19_conca  N          \n BatchNormalization)  864)                    t[0][0]']                         \n conv4_block20_0_relu  (None, 2, 2,   0       ['conv4_block20_0_bn[  N          \n  (Activation)        864)                    0][0]']                           \n conv4_block20_1_conv  (None, 2, 2,   110592  ['conv4_block20_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block20_1_bn (  (None, 2, 2,   512     ['conv4_block20_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block20_1_relu  (None, 2, 2,   0       ['conv4_block20_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block20_2_conv  (None, 2, 2,   36864   ['conv4_block20_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block20_concat  (None, 2, 2,   0       ['conv4_block19_conca  N          \n  (Concatenate)       896)                    t[0][0]',                         \n                                               'conv4_block20_2_con             \n                                              v[0][0]']                         \n conv4_block21_0_bn (  (None, 2, 2,   3584    ['conv4_block20_conca  N          \n BatchNormalization)  896)                    t[0][0]']                         \n conv4_block21_0_relu  (None, 2, 2,   0       ['conv4_block21_0_bn[  N          \n  (Activation)        896)                    0][0]']                           \n conv4_block21_1_conv  (None, 2, 2,   114688  ['conv4_block21_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block21_1_bn (  (None, 2, 2,   512     ['conv4_block21_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block21_1_relu  (None, 2, 2,   0       ['conv4_block21_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block21_2_conv  (None, 2, 2,   36864   ['conv4_block21_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block21_concat  (None, 2, 2,   0       ['conv4_block20_conca  N          \n  (Concatenate)       928)                    t[0][0]',                         \n                                               'conv4_block21_2_con             \n                                              v[0][0]']                         \n conv4_block22_0_bn (  (None, 2, 2,   3712    ['conv4_block21_conca  N          \n BatchNormalization)  928)                    t[0][0]']                         \n conv4_block22_0_relu  (None, 2, 2,   0       ['conv4_block22_0_bn[  N          \n  (Activation)        928)                    0][0]']                           \n conv4_block22_1_conv  (None, 2, 2,   118784  ['conv4_block22_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block22_1_bn (  (None, 2, 2,   512     ['conv4_block22_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block22_1_relu  (None, 2, 2,   0       ['conv4_block22_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block22_2_conv  (None, 2, 2,   36864   ['conv4_block22_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block22_concat  (None, 2, 2,   0       ['conv4_block21_conca  N          \n  (Concatenate)       960)                    t[0][0]',                         \n                                               'conv4_block22_2_con             \n                                              v[0][0]']                         \n conv4_block23_0_bn (  (None, 2, 2,   3840    ['conv4_block22_conca  N          \n BatchNormalization)  960)                    t[0][0]']                         \n conv4_block23_0_relu  (None, 2, 2,   0       ['conv4_block23_0_bn[  N          \n  (Activation)        960)                    0][0]']                           \n conv4_block23_1_conv  (None, 2, 2,   122880  ['conv4_block23_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block23_1_bn (  (None, 2, 2,   512     ['conv4_block23_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block23_1_relu  (None, 2, 2,   0       ['conv4_block23_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block23_2_conv  (None, 2, 2,   36864   ['conv4_block23_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block23_concat  (None, 2, 2,   0       ['conv4_block22_conca  N          \n  (Concatenate)       992)                    t[0][0]',                         \n                                               'conv4_block23_2_con             \n                                              v[0][0]']                         \n conv4_block24_0_bn (  (None, 2, 2,   3968    ['conv4_block23_conca  N          \n BatchNormalization)  992)                    t[0][0]']                         \n conv4_block24_0_relu  (None, 2, 2,   0       ['conv4_block24_0_bn[  N          \n  (Activation)        992)                    0][0]']                           \n conv4_block24_1_conv  (None, 2, 2,   126976  ['conv4_block24_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block24_1_bn (  (None, 2, 2,   512     ['conv4_block24_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block24_1_relu  (None, 2, 2,   0       ['conv4_block24_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block24_2_conv  (None, 2, 2,   36864   ['conv4_block24_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block24_concat  (None, 2, 2,   0       ['conv4_block23_conca  N          \n  (Concatenate)       1024)                   t[0][0]',                         \n                                               'conv4_block24_2_con             \n                                              v[0][0]']                         \n conv4_block25_0_bn (  (None, 2, 2,   4096    ['conv4_block24_conca  N          \n BatchNormalization)  1024)                   t[0][0]']                         \n conv4_block25_0_relu  (None, 2, 2,   0       ['conv4_block25_0_bn[  N          \n  (Activation)        1024)                   0][0]']                           \n conv4_block25_1_conv  (None, 2, 2,   131072  ['conv4_block25_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block25_1_bn (  (None, 2, 2,   512     ['conv4_block25_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block25_1_relu  (None, 2, 2,   0       ['conv4_block25_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block25_2_conv  (None, 2, 2,   36864   ['conv4_block25_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block25_concat  (None, 2, 2,   0       ['conv4_block24_conca  N          \n  (Concatenate)       1056)                   t[0][0]',                         \n                                               'conv4_block25_2_con             \n                                              v[0][0]']                         \n conv4_block26_0_bn (  (None, 2, 2,   4224    ['conv4_block25_conca  N          \n BatchNormalization)  1056)                   t[0][0]']                         \n conv4_block26_0_relu  (None, 2, 2,   0       ['conv4_block26_0_bn[  N          \n  (Activation)        1056)                   0][0]']                           \n conv4_block26_1_conv  (None, 2, 2,   135168  ['conv4_block26_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block26_1_bn (  (None, 2, 2,   512     ['conv4_block26_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block26_1_relu  (None, 2, 2,   0       ['conv4_block26_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block26_2_conv  (None, 2, 2,   36864   ['conv4_block26_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block26_concat  (None, 2, 2,   0       ['conv4_block25_conca  N          \n  (Concatenate)       1088)                   t[0][0]',                         \n                                               'conv4_block26_2_con             \n                                              v[0][0]']                         \n conv4_block27_0_bn (  (None, 2, 2,   4352    ['conv4_block26_conca  N          \n BatchNormalization)  1088)                   t[0][0]']                         \n conv4_block27_0_relu  (None, 2, 2,   0       ['conv4_block27_0_bn[  N          \n  (Activation)        1088)                   0][0]']                           \n conv4_block27_1_conv  (None, 2, 2,   139264  ['conv4_block27_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block27_1_bn (  (None, 2, 2,   512     ['conv4_block27_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block27_1_relu  (None, 2, 2,   0       ['conv4_block27_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block27_2_conv  (None, 2, 2,   36864   ['conv4_block27_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block27_concat  (None, 2, 2,   0       ['conv4_block26_conca  N          \n  (Concatenate)       1120)                   t[0][0]',                         \n                                               'conv4_block27_2_con             \n                                              v[0][0]']                         \n conv4_block28_0_bn (  (None, 2, 2,   4480    ['conv4_block27_conca  N          \n BatchNormalization)  1120)                   t[0][0]']                         \n conv4_block28_0_relu  (None, 2, 2,   0       ['conv4_block28_0_bn[  N          \n  (Activation)        1120)                   0][0]']                           \n conv4_block28_1_conv  (None, 2, 2,   143360  ['conv4_block28_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block28_1_bn (  (None, 2, 2,   512     ['conv4_block28_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block28_1_relu  (None, 2, 2,   0       ['conv4_block28_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block28_2_conv  (None, 2, 2,   36864   ['conv4_block28_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block28_concat  (None, 2, 2,   0       ['conv4_block27_conca  N          \n  (Concatenate)       1152)                   t[0][0]',                         \n                                               'conv4_block28_2_con             \n                                              v[0][0]']                         \n conv4_block29_0_bn (  (None, 2, 2,   4608    ['conv4_block28_conca  N          \n BatchNormalization)  1152)                   t[0][0]']                         \n conv4_block29_0_relu  (None, 2, 2,   0       ['conv4_block29_0_bn[  N          \n  (Activation)        1152)                   0][0]']                           \n conv4_block29_1_conv  (None, 2, 2,   147456  ['conv4_block29_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block29_1_bn (  (None, 2, 2,   512     ['conv4_block29_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block29_1_relu  (None, 2, 2,   0       ['conv4_block29_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block29_2_conv  (None, 2, 2,   36864   ['conv4_block29_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block29_concat  (None, 2, 2,   0       ['conv4_block28_conca  N          \n  (Concatenate)       1184)                   t[0][0]',                         \n                                               'conv4_block29_2_con             \n                                              v[0][0]']                         \n conv4_block30_0_bn (  (None, 2, 2,   4736    ['conv4_block29_conca  N          \n BatchNormalization)  1184)                   t[0][0]']                         \n conv4_block30_0_relu  (None, 2, 2,   0       ['conv4_block30_0_bn[  N          \n  (Activation)        1184)                   0][0]']                           \n conv4_block30_1_conv  (None, 2, 2,   151552  ['conv4_block30_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block30_1_bn (  (None, 2, 2,   512     ['conv4_block30_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block30_1_relu  (None, 2, 2,   0       ['conv4_block30_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block30_2_conv  (None, 2, 2,   36864   ['conv4_block30_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block30_concat  (None, 2, 2,   0       ['conv4_block29_conca  N          \n  (Concatenate)       1216)                   t[0][0]',                         \n                                               'conv4_block30_2_con             \n                                              v[0][0]']                         \n conv4_block31_0_bn (  (None, 2, 2,   4864    ['conv4_block30_conca  N          \n BatchNormalization)  1216)                   t[0][0]']                         \n conv4_block31_0_relu  (None, 2, 2,   0       ['conv4_block31_0_bn[  N          \n  (Activation)        1216)                   0][0]']                           \n conv4_block31_1_conv  (None, 2, 2,   155648  ['conv4_block31_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block31_1_bn (  (None, 2, 2,   512     ['conv4_block31_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block31_1_relu  (None, 2, 2,   0       ['conv4_block31_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block31_2_conv  (None, 2, 2,   36864   ['conv4_block31_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block31_concat  (None, 2, 2,   0       ['conv4_block30_conca  N          \n  (Concatenate)       1248)                   t[0][0]',                         \n                                               'conv4_block31_2_con             \n                                              v[0][0]']                         \n conv4_block32_0_bn (  (None, 2, 2,   4992    ['conv4_block31_conca  N          \n BatchNormalization)  1248)                   t[0][0]']                         \n conv4_block32_0_relu  (None, 2, 2,   0       ['conv4_block32_0_bn[  N          \n  (Activation)        1248)                   0][0]']                           \n conv4_block32_1_conv  (None, 2, 2,   159744  ['conv4_block32_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block32_1_bn (  (None, 2, 2,   512     ['conv4_block32_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block32_1_relu  (None, 2, 2,   0       ['conv4_block32_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block32_2_conv  (None, 2, 2,   36864   ['conv4_block32_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block32_concat  (None, 2, 2,   0       ['conv4_block31_conca  N          \n  (Concatenate)       1280)                   t[0][0]',                         \n                                               'conv4_block32_2_con             \n                                              v[0][0]']                         \n conv4_block33_0_bn (  (None, 2, 2,   5120    ['conv4_block32_conca  N          \n BatchNormalization)  1280)                   t[0][0]']                         \n conv4_block33_0_relu  (None, 2, 2,   0       ['conv4_block33_0_bn[  N          \n  (Activation)        1280)                   0][0]']                           \n conv4_block33_1_conv  (None, 2, 2,   163840  ['conv4_block33_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block33_1_bn (  (None, 2, 2,   512     ['conv4_block33_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block33_1_relu  (None, 2, 2,   0       ['conv4_block33_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block33_2_conv  (None, 2, 2,   36864   ['conv4_block33_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block33_concat  (None, 2, 2,   0       ['conv4_block32_conca  N          \n  (Concatenate)       1312)                   t[0][0]',                         \n                                               'conv4_block33_2_con             \n                                              v[0][0]']                         \n conv4_block34_0_bn (  (None, 2, 2,   5248    ['conv4_block33_conca  N          \n BatchNormalization)  1312)                   t[0][0]']                         \n conv4_block34_0_relu  (None, 2, 2,   0       ['conv4_block34_0_bn[  N          \n  (Activation)        1312)                   0][0]']                           \n conv4_block34_1_conv  (None, 2, 2,   167936  ['conv4_block34_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block34_1_bn (  (None, 2, 2,   512     ['conv4_block34_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block34_1_relu  (None, 2, 2,   0       ['conv4_block34_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block34_2_conv  (None, 2, 2,   36864   ['conv4_block34_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block34_concat  (None, 2, 2,   0       ['conv4_block33_conca  N          \n  (Concatenate)       1344)                   t[0][0]',                         \n                                               'conv4_block34_2_con             \n                                              v[0][0]']                         \n conv4_block35_0_bn (  (None, 2, 2,   5376    ['conv4_block34_conca  N          \n BatchNormalization)  1344)                   t[0][0]']                         \n conv4_block35_0_relu  (None, 2, 2,   0       ['conv4_block35_0_bn[  N          \n  (Activation)        1344)                   0][0]']                           \n conv4_block35_1_conv  (None, 2, 2,   172032  ['conv4_block35_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block35_1_bn (  (None, 2, 2,   512     ['conv4_block35_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block35_1_relu  (None, 2, 2,   0       ['conv4_block35_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block35_2_conv  (None, 2, 2,   36864   ['conv4_block35_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block35_concat  (None, 2, 2,   0       ['conv4_block34_conca  N          \n  (Concatenate)       1376)                   t[0][0]',                         \n                                               'conv4_block35_2_con             \n                                              v[0][0]']                         \n conv4_block36_0_bn (  (None, 2, 2,   5504    ['conv4_block35_conca  N          \n BatchNormalization)  1376)                   t[0][0]']                         \n conv4_block36_0_relu  (None, 2, 2,   0       ['conv4_block36_0_bn[  N          \n  (Activation)        1376)                   0][0]']                           \n conv4_block36_1_conv  (None, 2, 2,   176128  ['conv4_block36_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block36_1_bn (  (None, 2, 2,   512     ['conv4_block36_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block36_1_relu  (None, 2, 2,   0       ['conv4_block36_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block36_2_conv  (None, 2, 2,   36864   ['conv4_block36_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block36_concat  (None, 2, 2,   0       ['conv4_block35_conca  N          \n  (Concatenate)       1408)                   t[0][0]',                         \n                                               'conv4_block36_2_con             \n                                              v[0][0]']                         \n conv4_block37_0_bn (  (None, 2, 2,   5632    ['conv4_block36_conca  N          \n BatchNormalization)  1408)                   t[0][0]']                         \n conv4_block37_0_relu  (None, 2, 2,   0       ['conv4_block37_0_bn[  N          \n  (Activation)        1408)                   0][0]']                           \n conv4_block37_1_conv  (None, 2, 2,   180224  ['conv4_block37_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block37_1_bn (  (None, 2, 2,   512     ['conv4_block37_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block37_1_relu  (None, 2, 2,   0       ['conv4_block37_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block37_2_conv  (None, 2, 2,   36864   ['conv4_block37_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block37_concat  (None, 2, 2,   0       ['conv4_block36_conca  N          \n  (Concatenate)       1440)                   t[0][0]',                         \n                                               'conv4_block37_2_con             \n                                              v[0][0]']                         \n conv4_block38_0_bn (  (None, 2, 2,   5760    ['conv4_block37_conca  N          \n BatchNormalization)  1440)                   t[0][0]']                         \n conv4_block38_0_relu  (None, 2, 2,   0       ['conv4_block38_0_bn[  N          \n  (Activation)        1440)                   0][0]']                           \n conv4_block38_1_conv  (None, 2, 2,   184320  ['conv4_block38_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block38_1_bn (  (None, 2, 2,   512     ['conv4_block38_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block38_1_relu  (None, 2, 2,   0       ['conv4_block38_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block38_2_conv  (None, 2, 2,   36864   ['conv4_block38_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block38_concat  (None, 2, 2,   0       ['conv4_block37_conca  N          \n  (Concatenate)       1472)                   t[0][0]',                         \n                                               'conv4_block38_2_con             \n                                              v[0][0]']                         \n conv4_block39_0_bn (  (None, 2, 2,   5888    ['conv4_block38_conca  N          \n BatchNormalization)  1472)                   t[0][0]']                         \n conv4_block39_0_relu  (None, 2, 2,   0       ['conv4_block39_0_bn[  N          \n  (Activation)        1472)                   0][0]']                           \n conv4_block39_1_conv  (None, 2, 2,   188416  ['conv4_block39_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block39_1_bn (  (None, 2, 2,   512     ['conv4_block39_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block39_1_relu  (None, 2, 2,   0       ['conv4_block39_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block39_2_conv  (None, 2, 2,   36864   ['conv4_block39_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block39_concat  (None, 2, 2,   0       ['conv4_block38_conca  N          \n  (Concatenate)       1504)                   t[0][0]',                         \n                                               'conv4_block39_2_con             \n                                              v[0][0]']                         \n conv4_block40_0_bn (  (None, 2, 2,   6016    ['conv4_block39_conca  N          \n BatchNormalization)  1504)                   t[0][0]']                         \n conv4_block40_0_relu  (None, 2, 2,   0       ['conv4_block40_0_bn[  N          \n  (Activation)        1504)                   0][0]']                           \n conv4_block40_1_conv  (None, 2, 2,   192512  ['conv4_block40_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block40_1_bn (  (None, 2, 2,   512     ['conv4_block40_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block40_1_relu  (None, 2, 2,   0       ['conv4_block40_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block40_2_conv  (None, 2, 2,   36864   ['conv4_block40_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block40_concat  (None, 2, 2,   0       ['conv4_block39_conca  N          \n  (Concatenate)       1536)                   t[0][0]',                         \n                                               'conv4_block40_2_con             \n                                              v[0][0]']                         \n conv4_block41_0_bn (  (None, 2, 2,   6144    ['conv4_block40_conca  N          \n BatchNormalization)  1536)                   t[0][0]']                         \n conv4_block41_0_relu  (None, 2, 2,   0       ['conv4_block41_0_bn[  N          \n  (Activation)        1536)                   0][0]']                           \n conv4_block41_1_conv  (None, 2, 2,   196608  ['conv4_block41_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block41_1_bn (  (None, 2, 2,   512     ['conv4_block41_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block41_1_relu  (None, 2, 2,   0       ['conv4_block41_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block41_2_conv  (None, 2, 2,   36864   ['conv4_block41_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block41_concat  (None, 2, 2,   0       ['conv4_block40_conca  N          \n  (Concatenate)       1568)                   t[0][0]',                         \n                                               'conv4_block41_2_con             \n                                              v[0][0]']                         \n conv4_block42_0_bn (  (None, 2, 2,   6272    ['conv4_block41_conca  N          \n BatchNormalization)  1568)                   t[0][0]']                         \n conv4_block42_0_relu  (None, 2, 2,   0       ['conv4_block42_0_bn[  N          \n  (Activation)        1568)                   0][0]']                           \n conv4_block42_1_conv  (None, 2, 2,   200704  ['conv4_block42_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block42_1_bn (  (None, 2, 2,   512     ['conv4_block42_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block42_1_relu  (None, 2, 2,   0       ['conv4_block42_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block42_2_conv  (None, 2, 2,   36864   ['conv4_block42_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block42_concat  (None, 2, 2,   0       ['conv4_block41_conca  N          \n  (Concatenate)       1600)                   t[0][0]',                         \n                                               'conv4_block42_2_con             \n                                              v[0][0]']                         \n conv4_block43_0_bn (  (None, 2, 2,   6400    ['conv4_block42_conca  N          \n BatchNormalization)  1600)                   t[0][0]']                         \n conv4_block43_0_relu  (None, 2, 2,   0       ['conv4_block43_0_bn[  N          \n  (Activation)        1600)                   0][0]']                           \n conv4_block43_1_conv  (None, 2, 2,   204800  ['conv4_block43_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block43_1_bn (  (None, 2, 2,   512     ['conv4_block43_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block43_1_relu  (None, 2, 2,   0       ['conv4_block43_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block43_2_conv  (None, 2, 2,   36864   ['conv4_block43_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block43_concat  (None, 2, 2,   0       ['conv4_block42_conca  N          \n  (Concatenate)       1632)                   t[0][0]',                         \n                                               'conv4_block43_2_con             \n                                              v[0][0]']                         \n conv4_block44_0_bn (  (None, 2, 2,   6528    ['conv4_block43_conca  N          \n BatchNormalization)  1632)                   t[0][0]']                         \n conv4_block44_0_relu  (None, 2, 2,   0       ['conv4_block44_0_bn[  N          \n  (Activation)        1632)                   0][0]']                           \n conv4_block44_1_conv  (None, 2, 2,   208896  ['conv4_block44_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block44_1_bn (  (None, 2, 2,   512     ['conv4_block44_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block44_1_relu  (None, 2, 2,   0       ['conv4_block44_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block44_2_conv  (None, 2, 2,   36864   ['conv4_block44_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block44_concat  (None, 2, 2,   0       ['conv4_block43_conca  N          \n  (Concatenate)       1664)                   t[0][0]',                         \n                                               'conv4_block44_2_con             \n                                              v[0][0]']                         \n conv4_block45_0_bn (  (None, 2, 2,   6656    ['conv4_block44_conca  N          \n BatchNormalization)  1664)                   t[0][0]']                         \n conv4_block45_0_relu  (None, 2, 2,   0       ['conv4_block45_0_bn[  N          \n  (Activation)        1664)                   0][0]']                           \n conv4_block45_1_conv  (None, 2, 2,   212992  ['conv4_block45_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block45_1_bn (  (None, 2, 2,   512     ['conv4_block45_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block45_1_relu  (None, 2, 2,   0       ['conv4_block45_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block45_2_conv  (None, 2, 2,   36864   ['conv4_block45_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block45_concat  (None, 2, 2,   0       ['conv4_block44_conca  N          \n  (Concatenate)       1696)                   t[0][0]',                         \n                                               'conv4_block45_2_con             \n                                              v[0][0]']                         \n conv4_block46_0_bn (  (None, 2, 2,   6784    ['conv4_block45_conca  N          \n BatchNormalization)  1696)                   t[0][0]']                         \n conv4_block46_0_relu  (None, 2, 2,   0       ['conv4_block46_0_bn[  N          \n  (Activation)        1696)                   0][0]']                           \n conv4_block46_1_conv  (None, 2, 2,   217088  ['conv4_block46_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block46_1_bn (  (None, 2, 2,   512     ['conv4_block46_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block46_1_relu  (None, 2, 2,   0       ['conv4_block46_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block46_2_conv  (None, 2, 2,   36864   ['conv4_block46_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block46_concat  (None, 2, 2,   0       ['conv4_block45_conca  N          \n  (Concatenate)       1728)                   t[0][0]',                         \n                                               'conv4_block46_2_con             \n                                              v[0][0]']                         \n conv4_block47_0_bn (  (None, 2, 2,   6912    ['conv4_block46_conca  N          \n BatchNormalization)  1728)                   t[0][0]']                         \n conv4_block47_0_relu  (None, 2, 2,   0       ['conv4_block47_0_bn[  N          \n  (Activation)        1728)                   0][0]']                           \n conv4_block47_1_conv  (None, 2, 2,   221184  ['conv4_block47_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block47_1_bn (  (None, 2, 2,   512     ['conv4_block47_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block47_1_relu  (None, 2, 2,   0       ['conv4_block47_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block47_2_conv  (None, 2, 2,   36864   ['conv4_block47_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block47_concat  (None, 2, 2,   0       ['conv4_block46_conca  N          \n  (Concatenate)       1760)                   t[0][0]',                         \n                                               'conv4_block47_2_con             \n                                              v[0][0]']                         \n conv4_block48_0_bn (  (None, 2, 2,   7040    ['conv4_block47_conca  N          \n BatchNormalization)  1760)                   t[0][0]']                         \n conv4_block48_0_relu  (None, 2, 2,   0       ['conv4_block48_0_bn[  N          \n  (Activation)        1760)                   0][0]']                           \n conv4_block48_1_conv  (None, 2, 2,   225280  ['conv4_block48_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv4_block48_1_bn (  (None, 2, 2,   512     ['conv4_block48_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv4_block48_1_relu  (None, 2, 2,   0       ['conv4_block48_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv4_block48_2_conv  (None, 2, 2,   36864   ['conv4_block48_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv4_block48_concat  (None, 2, 2,   0       ['conv4_block47_conca  N          \n  (Concatenate)       1792)                   t[0][0]',                         \n                                               'conv4_block48_2_con             \n                                              v[0][0]']                         \n pool4_bn (BatchNorma  (None, 2, 2,   7168    ['conv4_block48_conca  N          \n lization)            1792)                   t[0][0]']                         \n pool4_relu (Activati  (None, 2, 2,   0       ['pool4_bn[0][0]']     N          \n on)                  1792)                                                     \n pool4_conv (Conv2D)  (None, 2, 2,   1605632  ['pool4_relu[0][0]']   N          \n                      896)                                                      \n pool4_pool (AverageP  (None, 1, 1,   0       ['pool4_conv[0][0]']   N          \n ooling2D)            896)                                                      \n conv5_block1_0_bn (B  (None, 1, 1,   3584    ['pool4_pool[0][0]']   N          \n atchNormalization)   896)                                                      \n conv5_block1_0_relu   (None, 1, 1,   0       ['conv5_block1_0_bn[0  N          \n (Activation)         896)                    ][0]']                            \n conv5_block1_1_conv   (None, 1, 1,   114688  ['conv5_block1_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block1_1_bn (B  (None, 1, 1,   512     ['conv5_block1_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block1_1_relu   (None, 1, 1,   0       ['conv5_block1_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block1_2_conv   (None, 1, 1,   36864   ['conv5_block1_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block1_concat   (None, 1, 1,   0       ['pool4_pool[0][0]',   N          \n (Concatenate)        928)                     'conv5_block1_2_conv             \n                                              [0][0]']                          \n conv5_block2_0_bn (B  (None, 1, 1,   3712    ['conv5_block1_concat  N          \n atchNormalization)   928)                    [0][0]']                          \n conv5_block2_0_relu   (None, 1, 1,   0       ['conv5_block2_0_bn[0  N          \n (Activation)         928)                    ][0]']                            \n conv5_block2_1_conv   (None, 1, 1,   118784  ['conv5_block2_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block2_1_bn (B  (None, 1, 1,   512     ['conv5_block2_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block2_1_relu   (None, 1, 1,   0       ['conv5_block2_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block2_2_conv   (None, 1, 1,   36864   ['conv5_block2_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block2_concat   (None, 1, 1,   0       ['conv5_block1_concat  N          \n (Concatenate)        960)                    [0][0]',                          \n                                               'conv5_block2_2_conv             \n                                              [0][0]']                          \n conv5_block3_0_bn (B  (None, 1, 1,   3840    ['conv5_block2_concat  N          \n atchNormalization)   960)                    [0][0]']                          \n conv5_block3_0_relu   (None, 1, 1,   0       ['conv5_block3_0_bn[0  N          \n (Activation)         960)                    ][0]']                            \n conv5_block3_1_conv   (None, 1, 1,   122880  ['conv5_block3_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block3_1_bn (B  (None, 1, 1,   512     ['conv5_block3_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block3_1_relu   (None, 1, 1,   0       ['conv5_block3_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block3_2_conv   (None, 1, 1,   36864   ['conv5_block3_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block3_concat   (None, 1, 1,   0       ['conv5_block2_concat  N          \n (Concatenate)        992)                    [0][0]',                          \n                                               'conv5_block3_2_conv             \n                                              [0][0]']                          \n conv5_block4_0_bn (B  (None, 1, 1,   3968    ['conv5_block3_concat  N          \n atchNormalization)   992)                    [0][0]']                          \n conv5_block4_0_relu   (None, 1, 1,   0       ['conv5_block4_0_bn[0  N          \n (Activation)         992)                    ][0]']                            \n conv5_block4_1_conv   (None, 1, 1,   126976  ['conv5_block4_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block4_1_bn (B  (None, 1, 1,   512     ['conv5_block4_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block4_1_relu   (None, 1, 1,   0       ['conv5_block4_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block4_2_conv   (None, 1, 1,   36864   ['conv5_block4_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block4_concat   (None, 1, 1,   0       ['conv5_block3_concat  N          \n (Concatenate)        1024)                   [0][0]',                          \n                                               'conv5_block4_2_conv             \n                                              [0][0]']                          \n conv5_block5_0_bn (B  (None, 1, 1,   4096    ['conv5_block4_concat  N          \n atchNormalization)   1024)                   [0][0]']                          \n conv5_block5_0_relu   (None, 1, 1,   0       ['conv5_block5_0_bn[0  N          \n (Activation)         1024)                   ][0]']                            \n conv5_block5_1_conv   (None, 1, 1,   131072  ['conv5_block5_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block5_1_bn (B  (None, 1, 1,   512     ['conv5_block5_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block5_1_relu   (None, 1, 1,   0       ['conv5_block5_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block5_2_conv   (None, 1, 1,   36864   ['conv5_block5_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block5_concat   (None, 1, 1,   0       ['conv5_block4_concat  N          \n (Concatenate)        1056)                   [0][0]',                          \n                                               'conv5_block5_2_conv             \n                                              [0][0]']                          \n conv5_block6_0_bn (B  (None, 1, 1,   4224    ['conv5_block5_concat  N          \n atchNormalization)   1056)                   [0][0]']                          \n conv5_block6_0_relu   (None, 1, 1,   0       ['conv5_block6_0_bn[0  N          \n (Activation)         1056)                   ][0]']                            \n conv5_block6_1_conv   (None, 1, 1,   135168  ['conv5_block6_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block6_1_bn (B  (None, 1, 1,   512     ['conv5_block6_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block6_1_relu   (None, 1, 1,   0       ['conv5_block6_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block6_2_conv   (None, 1, 1,   36864   ['conv5_block6_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block6_concat   (None, 1, 1,   0       ['conv5_block5_concat  N          \n (Concatenate)        1088)                   [0][0]',                          \n                                               'conv5_block6_2_conv             \n                                              [0][0]']                          \n conv5_block7_0_bn (B  (None, 1, 1,   4352    ['conv5_block6_concat  N          \n atchNormalization)   1088)                   [0][0]']                          \n conv5_block7_0_relu   (None, 1, 1,   0       ['conv5_block7_0_bn[0  N          \n (Activation)         1088)                   ][0]']                            \n conv5_block7_1_conv   (None, 1, 1,   139264  ['conv5_block7_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block7_1_bn (B  (None, 1, 1,   512     ['conv5_block7_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block7_1_relu   (None, 1, 1,   0       ['conv5_block7_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block7_2_conv   (None, 1, 1,   36864   ['conv5_block7_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block7_concat   (None, 1, 1,   0       ['conv5_block6_concat  N          \n (Concatenate)        1120)                   [0][0]',                          \n                                               'conv5_block7_2_conv             \n                                              [0][0]']                          \n conv5_block8_0_bn (B  (None, 1, 1,   4480    ['conv5_block7_concat  N          \n atchNormalization)   1120)                   [0][0]']                          \n conv5_block8_0_relu   (None, 1, 1,   0       ['conv5_block8_0_bn[0  N          \n (Activation)         1120)                   ][0]']                            \n conv5_block8_1_conv   (None, 1, 1,   143360  ['conv5_block8_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block8_1_bn (B  (None, 1, 1,   512     ['conv5_block8_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block8_1_relu   (None, 1, 1,   0       ['conv5_block8_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block8_2_conv   (None, 1, 1,   36864   ['conv5_block8_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block8_concat   (None, 1, 1,   0       ['conv5_block7_concat  N          \n (Concatenate)        1152)                   [0][0]',                          \n                                               'conv5_block8_2_conv             \n                                              [0][0]']                          \n conv5_block9_0_bn (B  (None, 1, 1,   4608    ['conv5_block8_concat  N          \n atchNormalization)   1152)                   [0][0]']                          \n conv5_block9_0_relu   (None, 1, 1,   0       ['conv5_block9_0_bn[0  N          \n (Activation)         1152)                   ][0]']                            \n conv5_block9_1_conv   (None, 1, 1,   147456  ['conv5_block9_0_relu  N          \n (Conv2D)             128)                    [0][0]']                          \n conv5_block9_1_bn (B  (None, 1, 1,   512     ['conv5_block9_1_conv  N          \n atchNormalization)   128)                    [0][0]']                          \n conv5_block9_1_relu   (None, 1, 1,   0       ['conv5_block9_1_bn[0  N          \n (Activation)         128)                    ][0]']                            \n conv5_block9_2_conv   (None, 1, 1,   36864   ['conv5_block9_1_relu  N          \n (Conv2D)             32)                     [0][0]']                          \n conv5_block9_concat   (None, 1, 1,   0       ['conv5_block8_concat  N          \n (Concatenate)        1184)                   [0][0]',                          \n                                               'conv5_block9_2_conv             \n                                              [0][0]']                          \n conv5_block10_0_bn (  (None, 1, 1,   4736    ['conv5_block9_concat  N          \n BatchNormalization)  1184)                   [0][0]']                          \n conv5_block10_0_relu  (None, 1, 1,   0       ['conv5_block10_0_bn[  N          \n  (Activation)        1184)                   0][0]']                           \n conv5_block10_1_conv  (None, 1, 1,   151552  ['conv5_block10_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block10_1_bn (  (None, 1, 1,   512     ['conv5_block10_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block10_1_relu  (None, 1, 1,   0       ['conv5_block10_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block10_2_conv  (None, 1, 1,   36864   ['conv5_block10_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block10_concat  (None, 1, 1,   0       ['conv5_block9_concat  N          \n  (Concatenate)       1216)                   [0][0]',                          \n                                               'conv5_block10_2_con             \n                                              v[0][0]']                         \n conv5_block11_0_bn (  (None, 1, 1,   4864    ['conv5_block10_conca  N          \n BatchNormalization)  1216)                   t[0][0]']                         \n conv5_block11_0_relu  (None, 1, 1,   0       ['conv5_block11_0_bn[  N          \n  (Activation)        1216)                   0][0]']                           \n conv5_block11_1_conv  (None, 1, 1,   155648  ['conv5_block11_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block11_1_bn (  (None, 1, 1,   512     ['conv5_block11_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block11_1_relu  (None, 1, 1,   0       ['conv5_block11_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block11_2_conv  (None, 1, 1,   36864   ['conv5_block11_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block11_concat  (None, 1, 1,   0       ['conv5_block10_conca  N          \n  (Concatenate)       1248)                   t[0][0]',                         \n                                               'conv5_block11_2_con             \n                                              v[0][0]']                         \n conv5_block12_0_bn (  (None, 1, 1,   4992    ['conv5_block11_conca  N          \n BatchNormalization)  1248)                   t[0][0]']                         \n conv5_block12_0_relu  (None, 1, 1,   0       ['conv5_block12_0_bn[  N          \n  (Activation)        1248)                   0][0]']                           \n conv5_block12_1_conv  (None, 1, 1,   159744  ['conv5_block12_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block12_1_bn (  (None, 1, 1,   512     ['conv5_block12_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block12_1_relu  (None, 1, 1,   0       ['conv5_block12_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block12_2_conv  (None, 1, 1,   36864   ['conv5_block12_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block12_concat  (None, 1, 1,   0       ['conv5_block11_conca  N          \n  (Concatenate)       1280)                   t[0][0]',                         \n                                               'conv5_block12_2_con             \n                                              v[0][0]']                         \n conv5_block13_0_bn (  (None, 1, 1,   5120    ['conv5_block12_conca  N          \n BatchNormalization)  1280)                   t[0][0]']                         \n conv5_block13_0_relu  (None, 1, 1,   0       ['conv5_block13_0_bn[  N          \n  (Activation)        1280)                   0][0]']                           \n conv5_block13_1_conv  (None, 1, 1,   163840  ['conv5_block13_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block13_1_bn (  (None, 1, 1,   512     ['conv5_block13_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block13_1_relu  (None, 1, 1,   0       ['conv5_block13_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block13_2_conv  (None, 1, 1,   36864   ['conv5_block13_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block13_concat  (None, 1, 1,   0       ['conv5_block12_conca  N          \n  (Concatenate)       1312)                   t[0][0]',                         \n                                               'conv5_block13_2_con             \n                                              v[0][0]']                         \n conv5_block14_0_bn (  (None, 1, 1,   5248    ['conv5_block13_conca  N          \n BatchNormalization)  1312)                   t[0][0]']                         \n conv5_block14_0_relu  (None, 1, 1,   0       ['conv5_block14_0_bn[  N          \n  (Activation)        1312)                   0][0]']                           \n conv5_block14_1_conv  (None, 1, 1,   167936  ['conv5_block14_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block14_1_bn (  (None, 1, 1,   512     ['conv5_block14_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block14_1_relu  (None, 1, 1,   0       ['conv5_block14_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block14_2_conv  (None, 1, 1,   36864   ['conv5_block14_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block14_concat  (None, 1, 1,   0       ['conv5_block13_conca  N          \n  (Concatenate)       1344)                   t[0][0]',                         \n                                               'conv5_block14_2_con             \n                                              v[0][0]']                         \n conv5_block15_0_bn (  (None, 1, 1,   5376    ['conv5_block14_conca  N          \n BatchNormalization)  1344)                   t[0][0]']                         \n conv5_block15_0_relu  (None, 1, 1,   0       ['conv5_block15_0_bn[  N          \n  (Activation)        1344)                   0][0]']                           \n conv5_block15_1_conv  (None, 1, 1,   172032  ['conv5_block15_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block15_1_bn (  (None, 1, 1,   512     ['conv5_block15_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block15_1_relu  (None, 1, 1,   0       ['conv5_block15_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block15_2_conv  (None, 1, 1,   36864   ['conv5_block15_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block15_concat  (None, 1, 1,   0       ['conv5_block14_conca  N          \n  (Concatenate)       1376)                   t[0][0]',                         \n                                               'conv5_block15_2_con             \n                                              v[0][0]']                         \n conv5_block16_0_bn (  (None, 1, 1,   5504    ['conv5_block15_conca  N          \n BatchNormalization)  1376)                   t[0][0]']                         \n conv5_block16_0_relu  (None, 1, 1,   0       ['conv5_block16_0_bn[  N          \n  (Activation)        1376)                   0][0]']                           \n conv5_block16_1_conv  (None, 1, 1,   176128  ['conv5_block16_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block16_1_bn (  (None, 1, 1,   512     ['conv5_block16_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block16_1_relu  (None, 1, 1,   0       ['conv5_block16_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block16_2_conv  (None, 1, 1,   36864   ['conv5_block16_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block16_concat  (None, 1, 1,   0       ['conv5_block15_conca  N          \n  (Concatenate)       1408)                   t[0][0]',                         \n                                               'conv5_block16_2_con             \n                                              v[0][0]']                         \n conv5_block17_0_bn (  (None, 1, 1,   5632    ['conv5_block16_conca  N          \n BatchNormalization)  1408)                   t[0][0]']                         \n conv5_block17_0_relu  (None, 1, 1,   0       ['conv5_block17_0_bn[  N          \n  (Activation)        1408)                   0][0]']                           \n conv5_block17_1_conv  (None, 1, 1,   180224  ['conv5_block17_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block17_1_bn (  (None, 1, 1,   512     ['conv5_block17_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block17_1_relu  (None, 1, 1,   0       ['conv5_block17_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block17_2_conv  (None, 1, 1,   36864   ['conv5_block17_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block17_concat  (None, 1, 1,   0       ['conv5_block16_conca  N          \n  (Concatenate)       1440)                   t[0][0]',                         \n                                               'conv5_block17_2_con             \n                                              v[0][0]']                         \n conv5_block18_0_bn (  (None, 1, 1,   5760    ['conv5_block17_conca  N          \n BatchNormalization)  1440)                   t[0][0]']                         \n conv5_block18_0_relu  (None, 1, 1,   0       ['conv5_block18_0_bn[  N          \n  (Activation)        1440)                   0][0]']                           \n conv5_block18_1_conv  (None, 1, 1,   184320  ['conv5_block18_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block18_1_bn (  (None, 1, 1,   512     ['conv5_block18_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block18_1_relu  (None, 1, 1,   0       ['conv5_block18_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block18_2_conv  (None, 1, 1,   36864   ['conv5_block18_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block18_concat  (None, 1, 1,   0       ['conv5_block17_conca  N          \n  (Concatenate)       1472)                   t[0][0]',                         \n                                               'conv5_block18_2_con             \n                                              v[0][0]']                         \n conv5_block19_0_bn (  (None, 1, 1,   5888    ['conv5_block18_conca  N          \n BatchNormalization)  1472)                   t[0][0]']                         \n conv5_block19_0_relu  (None, 1, 1,   0       ['conv5_block19_0_bn[  N          \n  (Activation)        1472)                   0][0]']                           \n conv5_block19_1_conv  (None, 1, 1,   188416  ['conv5_block19_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block19_1_bn (  (None, 1, 1,   512     ['conv5_block19_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block19_1_relu  (None, 1, 1,   0       ['conv5_block19_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block19_2_conv  (None, 1, 1,   36864   ['conv5_block19_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block19_concat  (None, 1, 1,   0       ['conv5_block18_conca  N          \n  (Concatenate)       1504)                   t[0][0]',                         \n                                               'conv5_block19_2_con             \n                                              v[0][0]']                         \n conv5_block20_0_bn (  (None, 1, 1,   6016    ['conv5_block19_conca  N          \n BatchNormalization)  1504)                   t[0][0]']                         \n conv5_block20_0_relu  (None, 1, 1,   0       ['conv5_block20_0_bn[  N          \n  (Activation)        1504)                   0][0]']                           \n conv5_block20_1_conv  (None, 1, 1,   192512  ['conv5_block20_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block20_1_bn (  (None, 1, 1,   512     ['conv5_block20_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block20_1_relu  (None, 1, 1,   0       ['conv5_block20_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block20_2_conv  (None, 1, 1,   36864   ['conv5_block20_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block20_concat  (None, 1, 1,   0       ['conv5_block19_conca  N          \n  (Concatenate)       1536)                   t[0][0]',                         \n                                               'conv5_block20_2_con             \n                                              v[0][0]']                         \n conv5_block21_0_bn (  (None, 1, 1,   6144    ['conv5_block20_conca  N          \n BatchNormalization)  1536)                   t[0][0]']                         \n conv5_block21_0_relu  (None, 1, 1,   0       ['conv5_block21_0_bn[  N          \n  (Activation)        1536)                   0][0]']                           \n conv5_block21_1_conv  (None, 1, 1,   196608  ['conv5_block21_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block21_1_bn (  (None, 1, 1,   512     ['conv5_block21_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block21_1_relu  (None, 1, 1,   0       ['conv5_block21_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block21_2_conv  (None, 1, 1,   36864   ['conv5_block21_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block21_concat  (None, 1, 1,   0       ['conv5_block20_conca  N          \n  (Concatenate)       1568)                   t[0][0]',                         \n                                               'conv5_block21_2_con             \n                                              v[0][0]']                         \n conv5_block22_0_bn (  (None, 1, 1,   6272    ['conv5_block21_conca  N          \n BatchNormalization)  1568)                   t[0][0]']                         \n conv5_block22_0_relu  (None, 1, 1,   0       ['conv5_block22_0_bn[  N          \n  (Activation)        1568)                   0][0]']                           \n conv5_block22_1_conv  (None, 1, 1,   200704  ['conv5_block22_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block22_1_bn (  (None, 1, 1,   512     ['conv5_block22_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block22_1_relu  (None, 1, 1,   0       ['conv5_block22_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block22_2_conv  (None, 1, 1,   36864   ['conv5_block22_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block22_concat  (None, 1, 1,   0       ['conv5_block21_conca  N          \n  (Concatenate)       1600)                   t[0][0]',                         \n                                               'conv5_block22_2_con             \n                                              v[0][0]']                         \n conv5_block23_0_bn (  (None, 1, 1,   6400    ['conv5_block22_conca  N          \n BatchNormalization)  1600)                   t[0][0]']                         \n conv5_block23_0_relu  (None, 1, 1,   0       ['conv5_block23_0_bn[  N          \n  (Activation)        1600)                   0][0]']                           \n conv5_block23_1_conv  (None, 1, 1,   204800  ['conv5_block23_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block23_1_bn (  (None, 1, 1,   512     ['conv5_block23_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block23_1_relu  (None, 1, 1,   0       ['conv5_block23_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block23_2_conv  (None, 1, 1,   36864   ['conv5_block23_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block23_concat  (None, 1, 1,   0       ['conv5_block22_conca  N          \n  (Concatenate)       1632)                   t[0][0]',                         \n                                               'conv5_block23_2_con             \n                                              v[0][0]']                         \n conv5_block24_0_bn (  (None, 1, 1,   6528    ['conv5_block23_conca  N          \n BatchNormalization)  1632)                   t[0][0]']                         \n conv5_block24_0_relu  (None, 1, 1,   0       ['conv5_block24_0_bn[  N          \n  (Activation)        1632)                   0][0]']                           \n conv5_block24_1_conv  (None, 1, 1,   208896  ['conv5_block24_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block24_1_bn (  (None, 1, 1,   512     ['conv5_block24_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block24_1_relu  (None, 1, 1,   0       ['conv5_block24_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block24_2_conv  (None, 1, 1,   36864   ['conv5_block24_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block24_concat  (None, 1, 1,   0       ['conv5_block23_conca  N          \n  (Concatenate)       1664)                   t[0][0]',                         \n                                               'conv5_block24_2_con             \n                                              v[0][0]']                         \n conv5_block25_0_bn (  (None, 1, 1,   6656    ['conv5_block24_conca  N          \n BatchNormalization)  1664)                   t[0][0]']                         \n conv5_block25_0_relu  (None, 1, 1,   0       ['conv5_block25_0_bn[  N          \n  (Activation)        1664)                   0][0]']                           \n conv5_block25_1_conv  (None, 1, 1,   212992  ['conv5_block25_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block25_1_bn (  (None, 1, 1,   512     ['conv5_block25_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block25_1_relu  (None, 1, 1,   0       ['conv5_block25_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block25_2_conv  (None, 1, 1,   36864   ['conv5_block25_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block25_concat  (None, 1, 1,   0       ['conv5_block24_conca  N          \n  (Concatenate)       1696)                   t[0][0]',                         \n                                               'conv5_block25_2_con             \n                                              v[0][0]']                         \n conv5_block26_0_bn (  (None, 1, 1,   6784    ['conv5_block25_conca  N          \n BatchNormalization)  1696)                   t[0][0]']                         \n conv5_block26_0_relu  (None, 1, 1,   0       ['conv5_block26_0_bn[  N          \n  (Activation)        1696)                   0][0]']                           \n conv5_block26_1_conv  (None, 1, 1,   217088  ['conv5_block26_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block26_1_bn (  (None, 1, 1,   512     ['conv5_block26_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block26_1_relu  (None, 1, 1,   0       ['conv5_block26_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block26_2_conv  (None, 1, 1,   36864   ['conv5_block26_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block26_concat  (None, 1, 1,   0       ['conv5_block25_conca  N          \n  (Concatenate)       1728)                   t[0][0]',                         \n                                               'conv5_block26_2_con             \n                                              v[0][0]']                         \n conv5_block27_0_bn (  (None, 1, 1,   6912    ['conv5_block26_conca  N          \n BatchNormalization)  1728)                   t[0][0]']                         \n conv5_block27_0_relu  (None, 1, 1,   0       ['conv5_block27_0_bn[  N          \n  (Activation)        1728)                   0][0]']                           \n conv5_block27_1_conv  (None, 1, 1,   221184  ['conv5_block27_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block27_1_bn (  (None, 1, 1,   512     ['conv5_block27_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block27_1_relu  (None, 1, 1,   0       ['conv5_block27_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block27_2_conv  (None, 1, 1,   36864   ['conv5_block27_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block27_concat  (None, 1, 1,   0       ['conv5_block26_conca  N          \n  (Concatenate)       1760)                   t[0][0]',                         \n                                               'conv5_block27_2_con             \n                                              v[0][0]']                         \n conv5_block28_0_bn (  (None, 1, 1,   7040    ['conv5_block27_conca  N          \n BatchNormalization)  1760)                   t[0][0]']                         \n conv5_block28_0_relu  (None, 1, 1,   0       ['conv5_block28_0_bn[  N          \n  (Activation)        1760)                   0][0]']                           \n conv5_block28_1_conv  (None, 1, 1,   225280  ['conv5_block28_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block28_1_bn (  (None, 1, 1,   512     ['conv5_block28_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block28_1_relu  (None, 1, 1,   0       ['conv5_block28_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block28_2_conv  (None, 1, 1,   36864   ['conv5_block28_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block28_concat  (None, 1, 1,   0       ['conv5_block27_conca  N          \n  (Concatenate)       1792)                   t[0][0]',                         \n                                               'conv5_block28_2_con             \n                                              v[0][0]']                         \n conv5_block29_0_bn (  (None, 1, 1,   7168    ['conv5_block28_conca  N          \n BatchNormalization)  1792)                   t[0][0]']                         \n conv5_block29_0_relu  (None, 1, 1,   0       ['conv5_block29_0_bn[  N          \n  (Activation)        1792)                   0][0]']                           \n conv5_block29_1_conv  (None, 1, 1,   229376  ['conv5_block29_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block29_1_bn (  (None, 1, 1,   512     ['conv5_block29_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block29_1_relu  (None, 1, 1,   0       ['conv5_block29_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block29_2_conv  (None, 1, 1,   36864   ['conv5_block29_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block29_concat  (None, 1, 1,   0       ['conv5_block28_conca  N          \n  (Concatenate)       1824)                   t[0][0]',                         \n                                               'conv5_block29_2_con             \n                                              v[0][0]']                         \n conv5_block30_0_bn (  (None, 1, 1,   7296    ['conv5_block29_conca  N          \n BatchNormalization)  1824)                   t[0][0]']                         \n conv5_block30_0_relu  (None, 1, 1,   0       ['conv5_block30_0_bn[  N          \n  (Activation)        1824)                   0][0]']                           \n conv5_block30_1_conv  (None, 1, 1,   233472  ['conv5_block30_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block30_1_bn (  (None, 1, 1,   512     ['conv5_block30_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block30_1_relu  (None, 1, 1,   0       ['conv5_block30_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block30_2_conv  (None, 1, 1,   36864   ['conv5_block30_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block30_concat  (None, 1, 1,   0       ['conv5_block29_conca  N          \n  (Concatenate)       1856)                   t[0][0]',                         \n                                               'conv5_block30_2_con             \n                                              v[0][0]']                         \n conv5_block31_0_bn (  (None, 1, 1,   7424    ['conv5_block30_conca  N          \n BatchNormalization)  1856)                   t[0][0]']                         \n conv5_block31_0_relu  (None, 1, 1,   0       ['conv5_block31_0_bn[  N          \n  (Activation)        1856)                   0][0]']                           \n conv5_block31_1_conv  (None, 1, 1,   237568  ['conv5_block31_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block31_1_bn (  (None, 1, 1,   512     ['conv5_block31_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block31_1_relu  (None, 1, 1,   0       ['conv5_block31_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block31_2_conv  (None, 1, 1,   36864   ['conv5_block31_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block31_concat  (None, 1, 1,   0       ['conv5_block30_conca  N          \n  (Concatenate)       1888)                   t[0][0]',                         \n                                               'conv5_block31_2_con             \n                                              v[0][0]']                         \n conv5_block32_0_bn (  (None, 1, 1,   7552    ['conv5_block31_conca  N          \n BatchNormalization)  1888)                   t[0][0]']                         \n conv5_block32_0_relu  (None, 1, 1,   0       ['conv5_block32_0_bn[  N          \n  (Activation)        1888)                   0][0]']                           \n conv5_block32_1_conv  (None, 1, 1,   241664  ['conv5_block32_0_rel  N          \n  (Conv2D)            128)                    u[0][0]']                         \n conv5_block32_1_bn (  (None, 1, 1,   512     ['conv5_block32_1_con  N          \n BatchNormalization)  128)                    v[0][0]']                         \n conv5_block32_1_relu  (None, 1, 1,   0       ['conv5_block32_1_bn[  N          \n  (Activation)        128)                    0][0]']                           \n conv5_block32_2_conv  (None, 1, 1,   36864   ['conv5_block32_1_rel  N          \n  (Conv2D)            32)                     u[0][0]']                         \n conv5_block32_concat  (None, 1, 1,   0       ['conv5_block31_conca  N          \n  (Concatenate)       1920)                   t[0][0]',                         \n                                               'conv5_block32_2_con             \n                                              v[0][0]']                         \n bn (BatchNormalizati  (None, 1, 1,   7680    ['conv5_block32_conca  N          \n on)                  1920)                   t[0][0]']                         \n relu (Activation)    (None, 1, 1,   0        ['bn[0][0]']           N          \n                      1920)                                                     \n dense_3 (Dense)      (None, 1, 1,   19210    ['relu[0][0]']         N          \n                      10)                                                       \n flatten_2 (Flatten)  (None, 10)     0        ['dense_3[0][0]']      Y          \n================================================================================\nTotal params: 18,341,194\nTrainable params: 0\nNon-trainable params: 18,341,194\n________________________________________________________________________________\n\n\nAnd then the usual training:\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nmodel %&gt;%\n  keras::compile(loss = loss_categorical_crossentropy, \n                 optimizer = optimizer_adamax())\n\nmodel %&gt;%\n  fit(\n    x = train_x, \n    y = train_y,\n    epochs = 1L,\n    batch_size = 32L,\n    shuffle = TRUE,\n    validation_split = 0.2\n  )\n\nWe have seen, that transfer learning can easily be done using Keras.\n\n\n\nlibrary(torchvision)\nlibrary(torch)\ntorch_manual_seed(321L)\nset.seed(123)\n\ntrain_ds = cifar10_dataset(\".\", download = TRUE, train = TRUE,\n                           transform = transform_to_tensor)\ntest_ds = cifar10_dataset(\".\", download = TRUE, train = FALSE,\n                          transform = transform_to_tensor)\n\ntrain_dl = dataloader(train_ds, batch_size = 100L, shuffle = TRUE)\ntest_dl = dataloader(test_ds, batch_size = 100L)\n\nmodel_torch = model_resnet18(pretrained = TRUE)\n\n# We will set all model parameters to constant values:\nmodel_torch$parameters %&gt;%\n  purrr::walk(function(param) param$requires_grad_(FALSE))\n\n# Let's replace the last layer (last layer is named 'fc') with our own layer:\ninFeat = model_torch$fc$in_features\nmodel_torch$fc = nn_linear(inFeat, out_features = 10L)\n\nopt = optim_adam(params = model_torch$parameters, lr = 0.01)\n\nfor(e in 1:1){\n  losses = c()\n  coro::loop(\n    for(batch in train_dl){\n      opt$zero_grad()\n      pred = model_torch(batch[[1]])\n      loss = nnf_cross_entropy(pred, batch[[2]], reduction = \"mean\")\n      loss$backward()\n      opt$step()\n      losses = c(losses, loss$item())\n    }\n  )\n  \n  cat(sprintf(\"Loss at epoch %d: %3f\\n\", e, mean(losses)))\n}\n\nmodel_torch$eval()\n\ntest_losses = c()\ntotal = 0\ncorrect = 0\n\ncoro::loop(\n  for(batch in test_dl){\n    output = model_torch(batch[[1]])\n    labels = batch[[2]]\n    loss = nnf_cross_entropy(output, labels)\n    test_losses = c(test_losses, loss$item())\n    predicted = torch_max(output$data(), dim = 2)[[2]]\n    total = total + labels$size(1)\n    correct = correct + (predicted == labels)$sum()$item()\n  }\n)\n\ntest_accuracy =  correct/total\nprint(test_accuracy)\n\n\n\n\n\n\n10.4.3 Example: Flower dataset\nLet’s do that with our flower data set:\n\nlibrary(keras)\nlibrary(tensorflow)\n\ndata = EcoData::dataset_flower()\n\ntrain = data$train/127.5 - 1 \ntest = data$test/127.5 - 1\nlabels = data$labels\n\n\n# Transfer learning\n\n# weights were trained to imagenet\npretrained_model = keras::application_efficientnet_b1(include_top = FALSE,\n                                                      input_shape = c(80L, 80L, 3L))\n# pretrained_model\n\nkeras::freeze_weights(pretrained_model)\npretrained_model\n\n# Build model\n\ndnn = pretrained_model$output %&gt;% \n  layer_flatten() %&gt;% \n  layer_dropout(0.2) %&gt;% \n  layer_dense(units = 5L, activation = \"softmax\")\ndnn\n\nmodel = keras_model(inputs = pretrained_model$input,\n                    outputs = dnn\n                    )\nmodel %&gt;%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 optimizer = keras::optimizer_rmsprop(learning_rate = 0.0005))\n\n\nmodel %&gt;% \n  fit(x = train, y = k_one_hot(labels, 5L), validation_split = 0.2, epochs = 5L)\n\n\n\n# Data augmentation\n# Transfer learning\n\n# weights were trained to imagenet\npretrained_model = keras::application_efficientnet_b1(include_top = FALSE,\n                                                      input_shape = c(80L, 80L, 3L))\n# pretrained_model\n\nkeras::freeze_weights(pretrained_model)\npretrained_model\n\n# Build model\n\ndnn = pretrained_model$output %&gt;% \n  layer_flatten() %&gt;% \n  layer_dropout(0.2) %&gt;% \n  layer_dense(units = 5L, activation = \"softmax\")\ndnn\n\nmodel = keras_model(inputs = pretrained_model$input,\n                    outputs = dnn\n)\n\n### Set up augmentation\naug = image_data_generator(rotation_range = 180, zoom_range = 0.4,\n                           width_shift_range = 0.2, height_shift_range = 0.2,\n                           vertical_flip = TRUE, horizontal_flip = TRUE)\n\n\n### Set up the data\nindices = sample.int(nrow(train), 0.1 * nrow(train)) # for validation\ngenerator = flow_images_from_data(x = train[-indices,,,],\n                                  y = k_one_hot(labels[-indices], 5L),\n                                  generator = aug\n                                  )\ngenerator\n\n\nmodel %&gt;%\n  keras::compile(loss = loss_categorical_crossentropy,\n                 optimizer = keras::optimizer_rmsprop(learning_rate = 0.0005))\n\nsteps_per_epoch = nrow(train[-indices,,,]) /45\nsteps_per_epoch = floor(steps_per_epoch)\n\nmodel %&gt;% \n  fit(generator, epochs = 5L, batch_size = 45L, steps_per_epoch = steps_per_epoch, \n      validation_data = list(train[indices,,,], k_one_hot(labels[indices], 5L))\n      )\n\npred = predict(model, test)\npred = apply(pred, 1, which.max) - 1\npred"
  },
  {
    "objectID": "C4-RecurrentNeuralNetworks.html#case-study-predicting-drought",
    "href": "C4-RecurrentNeuralNetworks.html#case-study-predicting-drought",
    "title": "11  Recurrent Neural Networks (RNN)",
    "section": "11.1 Case Study: Predicting drought",
    "text": "11.1 Case Study: Predicting drought\nWe will use a subset of the data explained in this github repository\n\nutils::download.file(\"https://www.dropbox.com/s/radyscnl5zcf57b/weather_soil.RDS?raw=1\", destfile = \"weather_soil.RDS\")\ndata = readRDS(\"weather_soil.RDS\")\nX = data$train # Features of the last 180 days\ndim(X)\n\n[1] 999 180  21\n\n# 999 batches of 180 days with 21 features each\nY = data$target\ndim(Y)\n\n[1] 999   6\n\n# 999 batches of 6 week drought predictions\n\n# let's visualize drought over 24 months:\n# -&gt; We have to take 16 batches (16*6 = 96 weaks ( = 24 months) )\nplot(as.vector(Y[1:16,]), type = \"l\", xlab = \"week\", ylab = \"Drought\")\n\n\n\n\n\nlibrary(keras)\n\nholdout = 700:999\nX_train = X[-holdout,,]\nX_test = X[holdout,,]\n\nY_train = Y[-holdout,]\nY_test = Y[holdout,]\n\nmodel = keras_model_sequential()\nmodel %&gt;% \n  layer_rnn(cell = layer_lstm_cell(units = 60L),input_shape = dim(X)[2:3]) %&gt;% \n  layer_dense(units = 6L)\n\nmodel %&gt;% compile(loss = loss_mean_squared_error, optimizer = optimizer_adamax(learning_rate = 0.01))\n  \nmodel %&gt;% fit(x = X_train, y = Y_train, epochs = 30L)\n\npreds = \n  model %&gt;% predict(X_test)\n\n\nmatplot(cbind(as.vector(preds[1:48,]),  \n              as.vector(Y_test[1:48,])), \n        col = c(\"darkblue\", \"darkred\"),\n        type = \"o\", \n        pch = c(15, 16),\n        xlab = \"week\", ylab = \"Drought\")\nlegend(\"topright\", bty = \"n\", \n       col = c(\"darkblue\", \"darkred\"),\n      pch = c(15, 16), \n      legend = c(\"Prediction\", \"True Values\"))\n\n\n\n\nThe following code snippet shows you many (technical) things you need for building more complex network structures, even with LSTM cells (the following example doesn’t have any functionality, it is just an example for how to process two different inputs in different ways within one network):\n\nKerasTorch\n\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ntf$keras$backend$clear_session()  # Resets especially layer counter.\n\ninputDimension1 = 50L\ninputDimension2 = 10L\n\ninput1 = layer_input(shape = inputDimension1)\ninput2 = layer_input(shape = inputDimension2)\n\nmodelInput2 = input2 %&gt;%\n  layer_dropout(rate = 0.5) %&gt;%\n  layer_dense(units = inputDimension2, activation = \"gelu\")\n\nmodelMemory = input1 %&gt;%\n  layer_embedding(input_dim = inputDimension1, output_dim = 64L) %&gt;%\n  layer_lstm(units = 64L) %&gt;%\n  layer_dropout(rate = 0.5) %&gt;%\n  layer_dense(units = 2L, activation = \"sigmoid\")\n\nmodelDeep = input1 %&gt;%\n  layer_dropout(rate = 0.5) %&gt;%\n  layer_dense(units = 64L, activation = \"relu\") %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  layer_dense(units = 64L, activation = \"relu\") %&gt;%\n  layer_dense(units = 64L, activation = \"relu\") %&gt;%\n  layer_dense(units = 5L, activation = \"sigmoid\")\n\nmodelMain = layer_concatenate(c(modelMemory, modelDeep, modelInput2)) %&gt;%\n  layer_dropout(rate = 0.25) %&gt;%\n  layer_dense(units = 64L, activation = \"relu\") %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  layer_dense(units = 64L, activation = \"relu\") %&gt;%\n  layer_dense(units = 2L, activation = \"sigmoid\")\n\nmodel = keras_model(\n  inputs = c(input1, input2),\n  outputs = c(modelMain)  # Use the whole modelMain (resp. its output) as output.\n)\n\nsummary(model)\n\nModel: \"model\"\n________________________________________________________________________________\n Layer (type)             Output Shape      Param #  Connected to               \n================================================================================\n input_1 (InputLayer)     [(None, 50)]      0        []                         \n dropout_3 (Dropout)      (None, 50)        0        ['input_1[0][0]']          \n dense_5 (Dense)          (None, 64)        3264     ['dropout_3[0][0]']        \n embedding (Embedding)    (None, 50, 64)    3200     ['input_1[0][0]']          \n dropout_2 (Dropout)      (None, 64)        0        ['dense_5[0][0]']          \n lstm (LSTM)              (None, 64)        33024    ['embedding[0][0]']        \n dense_4 (Dense)          (None, 64)        4160     ['dropout_2[0][0]']        \n input_2 (InputLayer)     [(None, 10)]      0        []                         \n dropout_1 (Dropout)      (None, 64)        0        ['lstm[0][0]']             \n dense_3 (Dense)          (None, 64)        4160     ['dense_4[0][0]']          \n dropout (Dropout)        (None, 10)        0        ['input_2[0][0]']          \n dense_1 (Dense)          (None, 2)         130      ['dropout_1[0][0]']        \n dense_2 (Dense)          (None, 5)         325      ['dense_3[0][0]']          \n dense (Dense)            (None, 10)        110      ['dropout[0][0]']          \n concatenate (Concatenate  (None, 17)       0        ['dense_1[0][0]',          \n )                                                    'dense_2[0][0]',          \n                                                      'dense[0][0]']            \n dropout_5 (Dropout)      (None, 17)        0        ['concatenate[0][0]']      \n dense_8 (Dense)          (None, 64)        1152     ['dropout_5[0][0]']        \n dropout_4 (Dropout)      (None, 64)        0        ['dense_8[0][0]']          \n dense_7 (Dense)          (None, 64)        4160     ['dropout_4[0][0]']        \n dense_6 (Dense)          (None, 2)         130      ['dense_7[0][0]']          \n================================================================================\nTotal params: 53,815\nTrainable params: 53,815\nNon-trainable params: 0\n________________________________________________________________________________\n\n# model %&gt;% plot_model()\n\n\n\n\nlibrary(torch)\n\nmodel_torch = nn_module(\n  initialize = function(type, inputDimension1 = 50L, inputDimension2 = 10L) {\n    self$dim1 = inputDimension1\n    self$dim2 = inputDimension2\n    self$modelInput2 = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(in_features = self$dim2, out_features = self$dim2),\n      nn_selu()\n    )\n    self$modelMemory = nn_sequential(\n      nn_embedding(self$dim1, 64),\n      nn_lstm(64, 64)\n    )\n    self$modelMemoryOutput = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(64L, 2L),\n      nn_sigmoid()\n    )\n    \n    self$modelDeep = nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(self$dim1, 64L),\n      nn_relu(),\n      nn_dropout(0.3),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_linear(64, 5),\n      nn_sigmoid()\n    )\n    \n    self$modelMain = nn_sequential(\n      nn_linear(7+self$dim2, 64),\n      nn_relu(),\n      nn_dropout(0.5),\n      nn_linear(64, 64),\n      nn_relu(),\n      nn_dropout(),\n      nn_linear(64, 2),\n      nn_sigmoid()\n    )\n  },\n  \n  forward = function(x) {\n    input1 = x[[1]]\n    input2 = x[[2]]\n    out2 = self$modelInput2(input2)\n    out1 = self$modelMemoryOutput( self$modelMemory(input1)$view(list(dim(input1)[1], -1)) )\n    out3 = self$modelDeep(input1)\n    out = self$modelMain(torch_cat(list(out1, out2, out3), 2))\n    return(out)\n  }\n  \n)\n\n(model_torch())\n\nAn `nn_module` containing 54,071 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• modelInput2: &lt;nn_sequential&gt; #110 parameters\n• modelMemory: &lt;nn_sequential&gt; #36,480 parameters\n• modelMemoryOutput: &lt;nn_sequential&gt; #130 parameters\n• modelDeep: &lt;nn_sequential&gt; #11,909 parameters\n• modelMain: &lt;nn_sequential&gt; #5,442 parameters"
  },
  {
    "objectID": "C5-GNN.html",
    "href": "C5-GNN.html",
    "title": "12  Graph Neural Networks (GNNs)",
    "section": "",
    "text": "Graph neural networks (GNN) is a young representative of the deep neural network family but is receiving more and more attention in the last years because of their ability to process non-Euclidean data such as graphs.\nCurrently there is no R package for GNNs available. However, we can use the ‘reticulate’ package to use the python packages ‘torch’ (python version) and ‘torch_geometric’.\nThe following example was mostly adapted from the ‘Node Classification with Graph Neural Networks’ example from the torch_geometric documentation.\nThe dataset is also provided by the ‘torch_geometric’ package and consists of molecules presented as graphs and the task is to predict whether HIV virus replication is inhibited by the molecule or not (classification, binary classification).\n\nlibrary(reticulate)\n# Load python packages torch and torch_geometric via the reticulate R package\ntorch = import(\"torch\") \ntorch_geometric = import(\"torch_geometric\")\n# helper functions from the torch_geometric modules\nGCNConv = torch_geometric$nn$GCNConv\nglobal_mean_pool = torch_geometric$nn$global_mean_pool\n# Download the MUTAG TUDataset\ndataset = torch_geometric$datasets$TUDataset(root='data/TUDataset', \n                                             name='MUTAG')\ndataloader = torch_geometric$loader$DataLoader(dataset, \n                                               batch_size=64L,\n                                               shuffle=TRUE)\n# Create the model with a python class\n# There are two classes in the response variable\nGCN = PyClass(\n  \"GCN\", \n   inherit = torch$nn$Module, \n   defs = list(\n       `__init__` = function(self, hidden_channels) {\n         super()$`__init__`()\n         torch$manual_seed(42L)\n         self$conv = GCNConv(dataset$num_node_features, hidden_channels)\n         self$linear = torch$nn$Linear(hidden_channels, dataset$num_classes)\n         NULL\n       },\n       forward = function(self, x, edge_index, batch) {\n         x = self$conv(x, edge_index)\n         x = x$relu()\n         x = global_mean_pool(x, batch)\n         \n         x = torch$nn$functional$dropout(x, p = 0.5, training=self$training)\n         x = self$linear(x)\n         return(x)\n       }\n   ))\n\nTraining loop:\n\n# create model object\nmodel = GCN(hidden_channels = 64L)\n# get optimizer and loss function\noptimizer = torch$optim$Adamax(model$parameters(), lr = 0.01)\nloss_func = torch$nn$CrossEntropyLoss()\n# set model into training mode (because of the dropout layer)\nmodel$train()\n# train model\nfor(e in 1:50) {\n  iterator = reticulate::as_iterator(dataloader)\n  coro::loop(for (b in iterator) { \n     pred = model(b$x, b$edge_index, b$batch)\n     loss = loss_func(pred, b$y)\n     loss$backward()\n     optimizer$step()\n     optimizer$zero_grad()\n  })\n  if(e %% 10 ==0) cat(paste0(\"Epoch: \",e,\" Loss: \", round(loss$item()[1], 4), \"\\n\"))\n}\n## Epoch: 10 Loss: 0.6151\n## Epoch: 20 Loss: 0.6163\n## Epoch: 30 Loss: 0.5745\n## Epoch: 40 Loss: 0.5362\n## Epoch: 50 Loss: 0.5829\n\nMake predictions:\n\npreds = list()\ntest = torch_geometric$loader$DataLoader(dataset, batch_size=64L,shuffle=FALSE)\niterator = reticulate::as_iterator(test)\nmodel$eval()\ncounter = 1\ncoro::loop(for (b in iterator) {\n  preds[[counter]] = model(b$x, b$edge_index, b$batch)\n  counter &lt;&lt;- counter + 1\n  })\nhead(torch$concat(preds)$sigmoid()$data$cpu()$numpy(), n = 3)\n##          [,1]      [,2]\n## [1,] 0.3076028 0.6427078\n## [2,] 0.4121239 0.5515330\n## [3,] 0.4119514 0.5516798"
  },
  {
    "objectID": "D1-causality.html#causalInference",
    "href": "D1-causality.html#causalInference",
    "title": "13  Causal Inference and Machine Learning",
    "section": "13.1 Causal Inference on Static Data",
    "text": "13.1 Causal Inference on Static Data\nMethods for causal inference depend on whether we have dynamic or static data. The latter is the more common case. With static data, the problem is confounding. If you have several correlated predictors, you can get spurious correlations between a given predictor and the response, although there is no causal effect in general.\nMultiple regression and few other methods are able to correct for other predictors and thus isolate the causal effect. The same is not necessarily true for machine learning algorithms and xAI methods. This is not a bug, but a feature - for making good predictions, it is often no problem, but rather an advantage to also use non-causal predictors.\nHere an example for the indicators of variable importance in the random forest algorithm. The purpose of this script is to show that random forest variable importance will split importance values for collinear variables evenly, even if collinearity is low enough so that variables are separable and would be correctly separated by an lm / ANOVA.\nWe first simulate a data set with 2 predictors that are strongly correlated, but only one of them has an effect on the response.\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nset.seed(123)\n\n# Simulation parameters.\nn = 1000\ncol = 0.7\n\n# Create collinear predictors.\nx1 = runif(n)\nx2 = col * x1 + (1-col) * runif(n)\n\n# Response is only influenced by x1.\ny = x1 + rnorm(n)\n\nlm / anova correctly identify \\(x1\\) as causal variable.\n\nsummary(lm(y ~ x1 + x2))\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0709 -0.6939  0.0102  0.6976  3.3373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.02837    0.08705   0.326 0.744536    \nx1           1.07383    0.27819   3.860 0.000121 ***\nx2          -0.04547    0.37370  -0.122 0.903186    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 997 degrees of freedom\nMultiple R-squared:  0.08104,   Adjusted R-squared:  0.0792 \nF-statistic: 43.96 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nFit random forest and show variable importance:\n\nset.seed(123)\n\nfit = randomForest(y ~ x1 + x2, importance = TRUE)\nvarImpPlot(fit)\n\n\n\n\nVariable importance is now split nearly evenly.\nTask: understand why this is - remember:\n\nHow the random forest works - variables are randomly hidden from the regression tree when the trees for the forest are built.\nRemember that as \\(x1 \\propto x2\\), we can use \\(x2\\) as a replacement for \\(x1\\).\nRemember that the variable importance measures the average contributions of the different variables in the trees of the forest."
  },
  {
    "objectID": "D1-causality.html#structural-equation-models",
    "href": "D1-causality.html#structural-equation-models",
    "title": "13  Causal Inference and Machine Learning",
    "section": "13.2 Structural Equation Models",
    "text": "13.2 Structural Equation Models\nIf causal relationships get more complicated, it will not be possible to adjust correctly with a simple lm. In this case, in statistics, we will usually use structural equation models (SEMs). Structural equation models are designed to estimate entire causal diagrams. There are two main SEM packages in R: For anything that is non-normal, you will currently have to estimate the directed acyclic graph (that depicts causal relations) piece-wise with CRAN package piecewiseSEM. Example for a vegetation data set:\n\nlibrary(piecewiseSEM)\n\nmod = psem(\n lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover,\n    data = keeley),\n lm(firesev ~ elev + age + cover, data = keeley),\n lm(cover ~ age + elev + hetero + abiotic, data = keeley)\n)\nsummary(mod)\nplot(mod)\n\nFor linear structural equation models, we can estimate the entire directed acyclic graph at once. This also allows having unobserved variables in the directed acyclic graph. One of the most popular packages for this is lavaan.\n\nlibrary(lavaan)\n\nmod = \"\n rich ~ distance + elev + abiotic + age + hetero + firesev + cover\n firesev ~ elev + age + cover\n cover ~ age + elev + abiotic\n\"\nfit = sem(mod, data = keeley)\nsummary(fit)\n\nlavaan 0.6.15 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                            90\n\nModel Test User Model:\n                                                      \n  Test statistic                                10.437\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.064\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n  rich ~                                               \n    distance           0.616    0.177    3.485    0.000\n    elev              -0.009    0.006   -1.644    0.100\n    abiotic            0.488    0.156    3.134    0.002\n    age                0.024    0.105    0.229    0.819\n    hetero            44.414    9.831    4.517    0.000\n    firesev           -1.018    0.759   -1.341    0.180\n    cover             12.400    3.841    3.228    0.001\n  firesev ~                                            \n    elev              -0.001    0.001   -0.951    0.342\n    age                0.047    0.013    3.757    0.000\n    cover             -1.521    0.509   -2.991    0.003\n  cover ~                                              \n    age               -0.009    0.002   -3.875    0.000\n    elev               0.000    0.000    2.520    0.012\n    abiotic           -0.000    0.004   -0.115    0.909\n\nVariances:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)\n   .rich              97.844   14.586    6.708    0.000\n   .firesev            1.887    0.281    6.708    0.000\n   .cover              0.081    0.012    6.708    0.000\n\n\nThe default plot options are not so nice as before.\n\nlibrary(lavaanPlot)\n\nlavaanPlot(model = fit)\n\n\n\n\n\nAnother plotting option is using semPlot.\n\nlibrary(semPlot)\n\nsemPaths(fit)"
  },
  {
    "objectID": "D1-causality.html#automatic-causal-discovery",
    "href": "D1-causality.html#automatic-causal-discovery",
    "title": "13  Causal Inference and Machine Learning",
    "section": "13.3 Automatic Causal Discovery",
    "text": "13.3 Automatic Causal Discovery\nBut how to get the causal graph? In statistics, it is common to “guess” it and afterwards do residual checks, in the same way as we guess the structure of a regression. For more complicated problems, however, this is unsatisfying. Some groups therefore work on so-called causal discovery algorithms, i.e. algorithms that automatically generate causal graphs from data. One of the most classic algorithms of this sort is the PC algorithm. Here an example using the pcalg package:\n\nlibrary(pcalg)\n\nLoading the data:\n\ndata(\"gmG\", package = \"pcalg\") # Loads data sets gmG and gmG8.\nsuffStat = list(C = cor(gmG8$x), n = nrow(gmG8$x))\nvarNames = gmG8$g@nodes\n\nFirst, the skeleton algorithm creates a basic graph without connections (a skeleton of the graph).\n\nskel.gmG8 = skeleton(suffStat, indepTest = gaussCItest,\nlabels = varNames, alpha = 0.01)\nRgraphviz::plot(skel.gmG8@graph)\n\n\n\n\nWhat is missing here is the direction of the errors. The PC algorithm now makes tests for conditional independence, which allows fixing a part (but typically not all) of the directions of the causal arrows.\n\npc.gmG8 = pc(suffStat, indepTest = gaussCItest,\nlabels = varNames, alpha = 0.01)\nRgraphviz::plot(pc.gmG8@graph )"
  },
  {
    "objectID": "D1-causality.html#causal-inference-on-dynamic-data",
    "href": "D1-causality.html#causal-inference-on-dynamic-data",
    "title": "13  Causal Inference and Machine Learning",
    "section": "13.4 Causal Inference on Dynamic Data",
    "text": "13.4 Causal Inference on Dynamic Data\nWhen working with dynamic data, we can use an additional piece of information - the cause usually precedes the effect, which means that we can test for a time-lag between cause and effect to determine the direction of causality. This way of testing for causality is known as Granger causality, or Granger methods. Here an example:\n\nlibrary(lmtest)\n\n## What came first: the chicken or the egg?\ndata(ChickEgg)\ngrangertest(egg ~ chicken, order = 3, data = ChickEgg)\n\nGranger causality test\n\nModel 1: egg ~ Lags(egg, 1:3) + Lags(chicken, 1:3)\nModel 2: egg ~ Lags(egg, 1:3)\n  Res.Df Df      F Pr(&gt;F)\n1     44                 \n2     47 -3 0.5916 0.6238\n\ngrangertest(chicken ~ egg, order = 3, data = ChickEgg)\n\nGranger causality test\n\nModel 1: chicken ~ Lags(chicken, 1:3) + Lags(egg, 1:3)\nModel 2: chicken ~ Lags(chicken, 1:3)\n  Res.Df Df     F   Pr(&gt;F)   \n1     44                     \n2     47 -3 5.405 0.002966 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "D1-causality.html#outlook-for-machine-learning",
    "href": "D1-causality.html#outlook-for-machine-learning",
    "title": "13  Causal Inference and Machine Learning",
    "section": "13.5 Outlook for Machine Learning",
    "text": "13.5 Outlook for Machine Learning\nAs we have seen, there are already a few methods / algorithms for discovering causality from large data sets, but the systematic transfer of these concepts to machine learning, in particular deep learning, is still at its infancy. At the moment, this field is actively researched and changes extremely fast, so we recommend using Google to see what is currently going on. Particular in business and industry, there is a large interest in learning about causal effect from large data sets. In our opinion, a great topic for young scientists to specialize on."
  },
  {
    "objectID": "D2-explainableAI.html#a-practical-example",
    "href": "D2-explainableAI.html#a-practical-example",
    "title": "14  Explainable AI",
    "section": "14.1 A Practical Example",
    "text": "14.1 A Practical Example\nIn this lecture we will work with an African Elephant occurrence dataset.\nWe will fit a random forest and use the iml package for xAI, see https://christophm.github.io/interpretable-ml-book/.\n\nlibrary(iml)\nlibrary(ranger) # different random Forest package!\nlibrary(EcoData)\nset.seed(123)\n\n\ndata = EcoData::elephant$occurenceData\nhead(data)\n\n      Presence       bio1       bio2       bio3       bio4        bio5\n3364         0 -0.4981747 -0.2738045  0.5368968 -0.5409999 -0.36843571\n6268         0  0.6085908 -0.5568352  1.0340686 -1.2492050 -0.11835651\n10285        0 -0.7973005  1.4648130 -1.0540532  2.0759423  0.07614953\n2247         0  0.6385034  1.3435141 -0.1591439 -0.5107148  1.10425291\n9821         0  0.6684160 -0.6781341  0.6363311 -0.9906170  0.15950927\n1351         0  0.9675418 -0.6781341 -0.3580126 -0.3748202  0.77081398\n            bio6       bio7       bio8       bio9       bio10       bio11\n3364   0.2947850 -0.5260099 -1.2253960  0.2494100 -0.64527314 -0.06267842\n6268   0.8221087 -0.8938475  0.4233787  0.7746249  0.09168503  0.94419518\n10285 -1.5860029  1.6284678  0.2768209 -1.5153122 -0.03648161 -1.44165748\n2247  -0.1622288  0.8577603  0.4600181  0.5855475  0.54026827  0.68153250\n9821   0.9099960 -0.8062671  0.3867393  0.8586593  0.31597665  0.94419518\n1351   0.8748411 -0.3858812  0.3134604  1.0477367  0.98885151  0.94419518\n           bio12      bio13       bio14        bio15      bio16      bio17\n3364   0.6285371  0.6807958 -0.29703736 -0.008455252  0.7124535 -0.2949994\n6268   1.1121516  0.5918442  0.01619202 -0.884507980  0.5607328  0.3506918\n10285 -1.2351482 -1.3396742 -0.50585695  0.201797403 -1.3499999 -0.5616980\n2247   0.5951165  0.8714061 -0.55806185  0.236839512  1.1012378 -0.5616980\n9821   1.1003561  0.5537222  0.59044589 -1.024676416  0.6413344  0.7437213\n1351   0.7287986  1.1255533 -0.50585695  0.236839512  1.2956300 -0.4494038\n            bio18       bio19\n3364  -1.06812752  1.96201807\n6268   1.22589281 -0.36205814\n10285 -0.42763181 -0.62895735\n2247  -0.20541902 -0.58378979\n9821   0.06254347 -0.05409751\n1351  -0.90473576  2.47939193\n\n?EcoData::elephant\n\nMeaning of the bioclim variables:\n\n\n\n\n\n\n\nBioclim variable\nMeaning\n\n\n\n\nbio1\nAnnual Mean Temperature\n\n\nbio2\nMean Diurnal Range (Mean of monthly (max temp - min temp))\n\n\nbio3\nIsothermality (BIO2/BIO7) (×100)\n\n\nbio4\nTemperature Seasonality (standard deviation ×100)\n\n\nbio5\nMax Temperature of Warmest Month\n\n\nbio6\nMin Temperature of Coldest Month\n\n\nbio7\nTemperature Annual Range (BIO5-BIO6)\n\n\nbio8\nMean Temperature of Wettest Quarter\n\n\nbio9\nMean Temperature of Driest Quarter\n\n\nbio10\nMean Temperature of Warmest Quarter\n\n\nbio11\nMean Temperature of Coldest Quarter\n\n\nbio12\nAnnual Precipitation\n\n\nbio13\nPrecipitation of Wettest Month\n\n\nbio14\nPrecipitation of Driest Month\n\n\nbio15\nPrecipitation Seasonality (Coefficient of Variation)\n\n\nbio16\nPrecipitation of Wettest Quarter\n\n\nbio17\nPrecipitation of Driest Quarter\n\n\nbio18\nPrecipitation of Warmest Quarter\n\n\nbio19\nPrecipitation of Coldest Quarter\n\n\n\n\nrf = ranger(as.factor(Presence) ~ ., data = data, probability = TRUE)\n\nxAI packages are written generic, i.e. they can handle almost all machine learning models. When we want to use them, we first have to create a predictor object, that holds the model and the data. The iml package uses R6 classes, that means new objects can be created by calling Predictor$new(). (Do not worry if you do not know what R6 classes are, just use the command.)\nWe often have to warp our predict function inside a so called wrapper function so that the output of the predict function fits to iml (iml expects that the predict function returns a vector of predictions:\n\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions[,2]\n\npredictor = Predictor$new(rf, data = data[,-1], y = data[,1], predict.function = predict_wrapper)\npredictor$task = \"classif\" # set task to classification\n# \"Predictor\" is an object generator."
  },
  {
    "objectID": "D2-explainableAI.html#feature-importance",
    "href": "D2-explainableAI.html#feature-importance",
    "title": "14  Explainable AI",
    "section": "14.2 Feature Importance",
    "text": "14.2 Feature Importance\nFeature importance should not be mistaken with the random forest variable importance though they are related. It tells us how important the individual variables are for predictions, can be calculated for all machine learning models and is based on a permutation approach (have a look at the book):\n\nimp = FeatureImp$new(predictor, loss = \"ce\")\nplot(imp)\n\n\n\n\nbio9 (Precipitation of the wettest Quarter) is the most important variable."
  },
  {
    "objectID": "D2-explainableAI.html#partial-dependencies",
    "href": "D2-explainableAI.html#partial-dependencies",
    "title": "14  Explainable AI",
    "section": "14.3 Partial Dependencies",
    "text": "14.3 Partial Dependencies\nPartial dependencies are similar to allEffects plots for normal regressions. The idea is to visualize “marginal effects” of predictors (with the “feature” argument we specify the variable we want to visualize):\n\neff = FeatureEffect$new(predictor, feature = \"bio9\", method = \"pdp\",\n                        grid.size = 30)\nplot(eff)\n\n\n\n\nOne disadvantage of partial dependencies is that they are sensitive to correlated predictors. Accumulated local effects can be used for accounting for correlation of predictors."
  },
  {
    "objectID": "D2-explainableAI.html#accumulated-local-effects",
    "href": "D2-explainableAI.html#accumulated-local-effects",
    "title": "14  Explainable AI",
    "section": "14.4 Accumulated Local Effects",
    "text": "14.4 Accumulated Local Effects\nAccumulated local effects (ALE) are basically partial dependencies plots but try to correct for correlations between predictors.\n\nale = FeatureEffect$new(predictor, feature = \"bio9\", method = \"ale\")\nale$plot()\n\n\n\n\nIf there is no collinearity, you shouldn’t see much difference between partial dependencies and ALE plots."
  },
  {
    "objectID": "D2-explainableAI.html#friedmans-h-statistic",
    "href": "D2-explainableAI.html#friedmans-h-statistic",
    "title": "14  Explainable AI",
    "section": "14.5 Friedman’s H-statistic",
    "text": "14.5 Friedman’s H-statistic\nThe H-statistic can be used to find interactions between predictors. However, again, keep in mind that the H-statistic is sensible to correlation between predictors:\n\ninteract = Interaction$new(predictor, \"bio9\",grid.size = 5L)\nplot(interact)"
  },
  {
    "objectID": "D2-explainableAI.html#global-explainer---simplifying-the-machine-learning-model",
    "href": "D2-explainableAI.html#global-explainer---simplifying-the-machine-learning-model",
    "title": "14  Explainable AI",
    "section": "14.6 Global Explainer - Simplifying the Machine Learning Model",
    "text": "14.6 Global Explainer - Simplifying the Machine Learning Model\nAnother idea is simplifying the machine learning model with another simpler model such as a decision tree. We create predictions with the machine learning model for a lot of different input values and then we fit a decision tree on these predictions. We can then interpret the easier model.\n\nlibrary(partykit)\n\ntree = TreeSurrogate$new(predictor, maxdepth = 2)\nplot(tree$tree)"
  },
  {
    "objectID": "D2-explainableAI.html#local-explainer---lime-explaining-single-instances-observations",
    "href": "D2-explainableAI.html#local-explainer---lime-explaining-single-instances-observations",
    "title": "14  Explainable AI",
    "section": "14.7 Local Explainer - LIME Explaining Single Instances (observations)",
    "text": "14.7 Local Explainer - LIME Explaining Single Instances (observations)\nThe global approach is to simplify the entire machine learning-black-box model via a simpler model, which is then interpretable.\nHowever, sometimes we are only interested in understanding how single predictions are generated. The LIME (Local interpretable model-agnostic explanations) approach explores the feature space around one observation and based on this locally fits a simpler model (e.g. a linear model):\n\nlime.explain = LocalModel$new(predictor, x.interest = data[1,-1])\nlime.explain$results\n\n             beta x.recoded       effect        x.original feature\nbio9  -0.03972318 0.2494100 -0.009907356 0.249409955204759    bio9\nbio16 -0.12035200 0.7124535 -0.085745198 0.712453479144842   bio16\n                feature.value\nbio9   bio9=0.249409955204759\nbio16 bio16=0.712453479144842\n\nplot(lime.explain)"
  },
  {
    "objectID": "D2-explainableAI.html#local-explainer---shapley",
    "href": "D2-explainableAI.html#local-explainer---shapley",
    "title": "14  Explainable AI",
    "section": "14.8 Local Explainer - Shapley",
    "text": "14.8 Local Explainer - Shapley\nThe Shapley method computes the so called Shapley value, feature contributions for single predictions, and is based on an approach from cooperative game theory. The idea is that each feature value of the instance is a “player” in a game, where the prediction is the reward. The Shapley value tells us how to fairly distribute the reward among the features.\n\nshapley = Shapley$new(predictor, x.interest = data[1,-1])\nshapley$plot()"
  },
  {
    "objectID": "D2-explainableAI.html#exercises",
    "href": "D2-explainableAI.html#exercises",
    "title": "14  Explainable AI",
    "section": "14.9 Exercises",
    "text": "14.9 Exercises\n\n\n\n\n\n\nQuestion\n\n\n\nUse one of the non-image based data sets (preferably Wine, which is also described in the data sets section Appendix A but wasn’t used yet, but you can also use Nasa or Titanic) and fit a random forest or a BRT using xgboost. Explore / interpret the fitted model using iml (see also the book: https://christophm.github.io/interpretable-ml-book/).\nTip:\nIf you use iml, you need to provide a proper prediction function wrapper:\n\n# random Forest (ranger), regression:\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions\n\n# random Forest (ranger), classification:\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions[,2]\n\n# xgboost:\npredict_wrapper = function(model, newdata) predict(model, as.matrix(newdata))\n\n\n\nClick here to see the solution for RF\n\n\nlibrary(ranger)\nlibrary(\"iml\")\nset.seed(1234)\n\ndata = as.data.frame(EcoData::wine)\nsubmission = data[which(is.na(data$quality)), -which(colnames(data) == \"quality\")]\ndata = data[complete.cases(data), ] # Removes sumbmission data as well.\n\n# Remark: Features don't need to be scaled for regression trees.\n\nrf = ranger(quality ~ ., data = data, importance = \"impurity\")\npred = round(predict(rf, data)$predictions)\ntable(pred, data$quality)\n\n    \npred   3   4   5   6   7   8\n   4   2   6   0   0   0   0\n   5   0   4 133   1   0   0\n   6   0   0   3 112   9   0\n   7   0   0   0   0  26   3\n\n(accuracy = mean(pred == data$quality)) # Fits pretty well (on the training data...)\n\n[1] 0.9264214\n\n# For submission:\n#write.csv(round(predict(rf, submission)), file = \"wine_RF.csv\")\n\n# Standard depiction of importance:\nrf$importance\n\n[1] \"impurity\"\n\n# Setup wrapper\npredict_wrapper = function(model, newdata) predict(model, data=newdata)$predictions\n\n\n# IML:\npredictor = Predictor$new(\n    rf, data = data[,which(names(data) != \"quality\")], y = data$quality,\n    predict.function = predict_wrapper\n    )\n\n# Mind: This is stochastical!\nimportance = FeatureImp$new(predictor, loss = \"mae\")\n\nplot(importance)\n\n\n\n# Comparison between standard importance and IML importance:\nimportanceRf = names(rf$variable.importance)[order(rf$variable.importance, decreasing = TRUE)]\nimportanceIML = importance$results[1]\ncomparison = cbind(importanceIML, importanceRf)\ncolnames(comparison) = c(\"IML\", \"RF\")\nas.matrix(comparison)\n\n      IML                    RF                    \n [1,] \"alcohol\"              \"alcohol\"             \n [2,] \"sulphates\"            \"volatile.acidity\"    \n [3,] \"volatile.acidity\"     \"sulphates\"           \n [4,] \"citric.acid\"          \"density\"             \n [5,] \"total.sulfur.dioxide\" \"total.sulfur.dioxide\"\n [6,] \"density\"              \"citric.acid\"         \n [7,] \"fixed.acidity\"        \"fixed.acidity\"       \n [8,] \"pH\"                   \"chlorides\"           \n [9,] \"free.sulfur.dioxide\"  \"pH\"                  \n[10,] \"chlorides\"            \"residual.sugar\"      \n[11,] \"residual.sugar\"       \"free.sulfur.dioxide\" \n\n\nMind that feature importance, and the random forest’s variable importance are related but not equal! Variable importance is a measure for determining importance while creating the forest (i.e. for fitting). Feature importance is a measure for how important a variable is for prediction.\nMaybe you want to see other explanation methods as well. Surely you can use the other techniques of this section on your own.\n\n\n\nClick here to see the solution for xgboost\n\n\nlibrary(xgboost)\nlibrary(\"iml\")\nset.seed(1234)\n\ndata = as.data.frame(EcoData::wine)\nsubmission = data[which(is.na(data$quality)), -which(colnames(data) == \"quality\")]\ndata = data[complete.cases(data), ] # Removes sumbmission data as well.\n\n\ndata_xg = xgb.DMatrix(\n  data = as.matrix(data[,which(names(data) != \"quality\")]),\n  label = data$quality\n)\nbrt = xgboost(data_xg, nrounds = 24)\n\n[1] train-rmse:3.656523 \n[2] train-rmse:2.609494 \n[3] train-rmse:1.884807 \n[4] train-rmse:1.384918 \n[5] train-rmse:1.037362 \n[6] train-rmse:0.800110 \n[7] train-rmse:0.629324 \n[8] train-rmse:0.508917 \n[9] train-rmse:0.426155 \n[10]    train-rmse:0.369580 \n[11]    train-rmse:0.313017 \n[12]    train-rmse:0.274227 \n[13]    train-rmse:0.236959 \n[14]    train-rmse:0.207364 \n[15]    train-rmse:0.195811 \n[16]    train-rmse:0.182500 \n[17]    train-rmse:0.173310 \n[18]    train-rmse:0.154747 \n[19]    train-rmse:0.144045 \n[20]    train-rmse:0.139083 \n[21]    train-rmse:0.129605 \n[22]    train-rmse:0.118541 \n[23]    train-rmse:0.110689 \n[24]    train-rmse:0.097798 \n\npred = round(predict(brt, newdata = data_xg)) # On\n\ntable(pred, data$quality)\n\n    \npred   3   4   5   6   7   8\n   3   2   0   0   0   0   0\n   4   0  10   0   0   0   0\n   5   0   0 136   0   0   0\n   6   0   0   0 113   1   0\n   7   0   0   0   0  34   0\n   8   0   0   0   0   0   3\n\n(accuracy = mean(pred == data$quality)) # Fits pretty well (on the training data...)\n\n[1] 0.9966555\n\n# For submission:\n#write.csv(round(predict(rf, submission)), file = \"wine_RF.csv\")\n\n# Standard depiction of importance:\nxgboost::xgb.importance(model = brt)\n\n                 Feature       Gain      Cover  Frequency\n 1:              alcohol 0.28363854 0.13667721 0.07073509\n 2:            sulphates 0.11331809 0.07015405 0.06657420\n 3:        fixed.acidity 0.09844424 0.11359510 0.19278779\n 4:     volatile.acidity 0.09582787 0.07098397 0.12760055\n 5: total.sulfur.dioxide 0.09207959 0.09147259 0.07212205\n 6:              density 0.07374571 0.14910006 0.08321775\n 7:            chlorides 0.06025507 0.07972405 0.08876560\n 8:       residual.sugar 0.05307941 0.07202137 0.08044383\n 9:  free.sulfur.dioxide 0.04602735 0.04743503 0.06518724\n10:                   pH 0.04477571 0.12562892 0.07489598\n11:          citric.acid 0.03880842 0.04320764 0.07766990\n\n# Setup wrapper\npredict_wrapper = function(model, newdata) predict(model, as.matrix(newdata))\n\n\n# IML:\npredictor = Predictor$new(\n    brt, data = data[,which(names(data) != \"quality\")], y = data$quality,\n    predict.function = predict_wrapper\n    )\n\n# Mind: This is stochastical!\nimportance = FeatureImp$new(predictor, loss = \"mae\")\n\nplot(importance)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAs we show in Section 13 of this chapter, a random forest will partition the importance of variables across collinear predictors, while a linear regression model (lm()) can identify which predictor is causally affecting the response (at least in theory, if all confounders are controlled). What about a boosted regression tree or an artificial neural network? Take the random forest example and add a boosted regression tree (easier, you can use e.g. https://rdrr.io/cran/xgboost/man/xgb.importance.html) or an artificial neural network and see if they are better than the random forest at identifying causal predictors.\n\n\nClick here to see the solution\n\n\nlibrary(xgboost)\nset.seed(1234)\n\ndata = as.data.frame(EcoData::wine)\nsubmission = data[which(is.na(data$quality)), -which(colnames(data) == \"quality\")]\ndata = data[complete.cases(data), ] # Removes sumbmission data as well.\n\ndata_xg = xgb.DMatrix(\n  data = as.matrix(data[,which(names(data) != \"quality\")]),\n  label = data$quality\n)\nbrt = xgboost(data_xg, nrounds = 24)\n\n[1] train-rmse:3.656523 \n[2] train-rmse:2.609494 \n[3] train-rmse:1.884807 \n[4] train-rmse:1.384918 \n[5] train-rmse:1.037362 \n[6] train-rmse:0.800110 \n[7] train-rmse:0.629324 \n[8] train-rmse:0.508917 \n[9] train-rmse:0.426155 \n[10]    train-rmse:0.369580 \n[11]    train-rmse:0.313017 \n[12]    train-rmse:0.274227 \n[13]    train-rmse:0.236959 \n[14]    train-rmse:0.207364 \n[15]    train-rmse:0.195811 \n[16]    train-rmse:0.182500 \n[17]    train-rmse:0.173310 \n[18]    train-rmse:0.154747 \n[19]    train-rmse:0.144045 \n[20]    train-rmse:0.139083 \n[21]    train-rmse:0.129605 \n[22]    train-rmse:0.118541 \n[23]    train-rmse:0.110689 \n[24]    train-rmse:0.097798 \n\npred = round(predict(brt, newdata = data_xg)) # Only integers are allowed.\ntable(pred, data$quality)\n\n    \npred   3   4   5   6   7   8\n   3   2   0   0   0   0   0\n   4   0  10   0   0   0   0\n   5   0   0 136   0   0   0\n   6   0   0   0 113   1   0\n   7   0   0   0   0  34   0\n   8   0   0   0   0   0   3\n\n(accuracy = mean(pred == data$quality)) # Fits very well (on the training data...)\n\n[1] 0.9966555\n\n# For submission:\n#write.csv(round(predict(rf, submission)), file = \"wine_RF.csv\")\n\n# Look at variable importance:\nxgboost::xgb.importance(model = brt)\n\n                 Feature       Gain      Cover  Frequency\n 1:              alcohol 0.28363854 0.13667721 0.07073509\n 2:            sulphates 0.11331809 0.07015405 0.06657420\n 3:        fixed.acidity 0.09844424 0.11359510 0.19278779\n 4:     volatile.acidity 0.09582787 0.07098397 0.12760055\n 5: total.sulfur.dioxide 0.09207959 0.09147259 0.07212205\n 6:              density 0.07374571 0.14910006 0.08321775\n 7:            chlorides 0.06025507 0.07972405 0.08876560\n 8:       residual.sugar 0.05307941 0.07202137 0.08044383\n 9:  free.sulfur.dioxide 0.04602735 0.04743503 0.06518724\n10:                   pH 0.04477571 0.12562892 0.07489598\n11:          citric.acid 0.03880842 0.04320764 0.07766990\n\n\nEvery method yields slightly different results, but the main ingredient is alcohol (and sulphates).\n\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nIf you’re done with the previous tasks and have still time and appetite, improve the submissions for our competition, in particular for the Wine data set. Possible ideas:\n\nUse MLR framework (section Section 4.5.1).\nTry Transfer learning (section Section 10.4.2). The winner from last years used transfer learning to win the flower competition\nSearch on kaggle for more ideas / try to copy the ideas. This was the winner two years ago.\n\nA minimal example for the (unbalanced!) Wine data set:\n\n\nClick here to see the solution\n\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(123L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nreadin = function(percentageTest = 0.2, aggregate = 0){\n    # Parameter \"aggregate\" packs the classes with very low abundances into one.\n    # If \"aggregate\" equals to NA, NaN, Null, 0 or FALSE, no aggregation is performed.\n    # Else, the given number is the boundary.\n    # Every class with less elements than the boundary is aggregated into one.\n    \n    # WARNING: These classes cannot be distinguished from then on!\n    # Using the predictions for submission needs further processing!\n    \n    # Just for random selection of features, independent of the amount of function calls.\n    set.seed(12345)\n    \n    train = as.data.frame(EcoData::wine)\n    indicesTrain = which(!is.na(train$quality))\n    labelsTrain = train$quality[indicesTrain]\n    labelsTrain = labelsTrain - min(labelsTrain)  # Start at 0 (for softmax).\n    train = train[, -which(colnames(train) == \"quality\")]\n    \n    if(!is.na(aggregate) & aggregate){\n        indices = names(table(labelsTrain)[\n            table(labelsTrain) &lt; aggregate & table(labelsTrain) &gt; 0\n        ])\n        if(length(indices)){\n            labelsTrain[labelsTrain %in% indices] = -1\n            labelsTrain = as.factor(labelsTrain)\n            levels(labelsTrain) = 1:length(levels(labelsTrain)) - 1\n            labelsTrain = as.integer(labelsTrain)\n        }\n    }\n    \n    # Impute missing values (before any splitting, to get the highest power):\n    train = missRanger::missRanger(\n        data = train,\n        maxiter = 10L,\n        seed = 123,\n        num.trees = 200L\n    )\n    \n    # Separate submission data (mind scaling!):\n    submission = scale(train[-indicesTrain,])\n    train = scale(train[indicesTrain,])\n    \n    # Very asymmetric training data:\n    cat(paste0(\"Size of training set: \", length(labelsTrain), \"\\n\"))\n    print(table(labelsTrain))\n    \n    if(percentageTest == 0){\n      return(list(\n        \"labelsTrain\" = labelsTrain,\n        \"labelsTest\" = list(),\n        \"train\" = train,\n        \"test\" = list(),\n        \"submission\" = submission\n      ))\n    }\n    \n    # Split into training and test set:\n    len = nrow(train)\n    indicesTest = sample(x = 1:len, size = percentageTest * len, replace = FALSE)\n    test = as.data.frame(train[indicesTest,])\n    labelsTest = labelsTrain[indicesTest]\n    train = as.data.frame(train[-indicesTest,])\n    labelsTrain = labelsTrain[-indicesTest]\n    \n    return(list(\n        \"labelsTrain\" = labelsTrain,\n        \"labelsTest\" = labelsTest,\n        \"train\" = train,\n        \"test\" = test,\n        \"submission\" = submission\n    ))\n}\n\nretVal = readin(aggregate = 0)\nlabelsTrain = retVal[[\"labelsTrain\"]]\nlabelsTest = retVal[[\"labelsTest\"]]\ntrain = retVal[[\"train\"]]\ntest = retVal[[\"test\"]]\nsubmission = retVal[[\"submission\"]]\nrm(retVal)\n\nclassNumber = length(table(labelsTrain))\n\nmodel = keras_model_sequential()\nmodel %&gt;%\n    layer_dense(units = 200L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    input_shape = ncol(train)) %&gt;%\n    layer_dropout(0.45) %&gt;%\n    layer_dense(units = 100L, activation = \"relu\",\n    bias_regularizer = regularizer_l1_l2(0.5)) %&gt;%\n    layer_dropout(0.2) %&gt;%\n    layer_dense(units = 100L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    bias_regularizer = regularizer_l1_l2(0.1)) %&gt;%\n    layer_dropout(0.25) %&gt;%\n    layer_dense(units = 50L, activation = \"gelu\") %&gt;%\n    layer_dense(units = 25L, activation = \"elu\") %&gt;%\n    layer_dropout(0.35) %&gt;%\n    # We need probabilities. So we use the softmax function.\n    # Remember, the labels MUST start at 0!\n    layer_dense(units = classNumber, activation = \"softmax\")\n\nmodel %&gt;%\n    keras::compile(loss = loss_binary_crossentropy,\n                   optimizer = optimizer_adamax(learning_rate = 0.015))\n\nmodel_history = \n    model %&gt;% # Mind the matrix property (no data.frame)!\n        fit(x = as.matrix(train), y = k_one_hot(labelsTrain, classNumber),\n            epochs = 80L, batch = 12L, shuffle = TRUE)\n\nplot(model_history)\n\n# Accuracy on training set (!)\npred = predict(model, as.matrix(train)) %&gt;% apply(1, which.max) - 1\nMetrics::accuracy(pred, labelsTrain)\ntable(pred, labelsTrain)\n\n# Accuracy on test set\npred = predict(model, as.matrix(test)) %&gt;% apply(1, which.max) - 1\nMetrics::accuracy(pred, labelsTest)\ntable(pred, labelsTest)\n\nRecognize overfitting of your model selection strategy by changing the seed few times (while keeping the model constant) and increase the percentage of test data. Furthermore, consider fitting a random forest for good quality as well.\nFor the final predictions, we use the whole data set without holdouts:\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nretVal = readin(percentageTest = 0, aggregate = 0)\nlabelsTrain = retVal[[\"labelsTrain\"]]\nlabelsTest = retVal[[\"labelsTest\"]]\ntrain = retVal[[\"train\"]]\ntest = retVal[[\"test\"]]\nsubmission = retVal[[\"submission\"]]\nrm(retVal)\n\nclassNumber = length(table(labelsTrain))\n\nmodel = keras_model_sequential()\nmodel %&gt;%\n    layer_dense(units = 200L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    input_shape = ncol(train)) %&gt;%\n    layer_dropout(0.45) %&gt;%\n    layer_dense(units = 100L, activation = \"relu\",\n    bias_regularizer = regularizer_l1_l2(0.5)) %&gt;%\n    layer_dropout(0.2) %&gt;%\n    layer_dense(units = 100L, activation = \"leaky_relu\",\n    kernel_regularizer = regularizer_l2(0.00035),\n    bias_regularizer = regularizer_l1_l2(0.1)) %&gt;%\n    layer_dropout(0.25) %&gt;%\n    layer_dense(units = 50L, activation = \"gelu\") %&gt;%\n    layer_dense(units = 25L, activation = \"elu\") %&gt;%\n    layer_dropout(0.35) %&gt;%\n    # We need probabilities. So we use the softmax function.\n    # Remember, the labels MUST start at 0!\n    layer_dense(units = classNumber, activation = \"softmax\")\n\nmodel %&gt;%\n    keras::compile(loss = loss_binary_crossentropy,\n                   optimizer = optimizer_adamax(learning_rate = 0.015))\n\nmodel_history = \n    model %&gt;% # Mind the matrix property (no data.frame)!\n        fit(x = as.matrix(train), y = k_one_hot(labelsTrain, classNumber),\n            epochs = 80L, batch = 12L, shuffle = TRUE)\n\nplot(model_history)\n\n# Accuracy on training set (!)\npred = predict(model, as.matrix(train)) %&gt;% apply(1, which.max) - 1\nMetrics::accuracy(pred, labelsTrain)\ntable(pred, labelsTrain)\n\n# Reverse subtraction (for start at 0) and create submission file.\nwrite.csv(pred + min(as.data.frame(EcoData::wine)$quality, na.rm = TRUE),\n          file = \"wine_NN.csv\")"
  },
  {
    "objectID": "E1-Autoencoder.html#autoencoder---deep-neural-network-mnist",
    "href": "E1-Autoencoder.html#autoencoder---deep-neural-network-mnist",
    "title": "15  Autoencoder",
    "section": "15.1 Autoencoder - Deep Neural Network MNIST",
    "text": "15.1 Autoencoder - Deep Neural Network MNIST\nWe now will write an autoencoder for the MNIST data set.\nLet’s start with the (usual) MNIST example:\nlibrary(keras)\nlibrary(tensorflow)\n\ndata = keras::dataset_mnist()\nWe don’t need the labels here, our images will be the inputs and at the same time the outputs of our final autoencoder.\nrotate = function(x){ t(apply(x, 2, rev)) }\n\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  if(title != \"\"){ main = paste0(\"Label: \", as.character(title)) }\n  else{ main = \"\" }\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE, main = main)\n}\n\ntrain = data[[1]]\ntest = data[[2]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]])[1], 784L))\ntest_x = array(test[[1]]/255, c(dim(test[[1]])[1], 784L))\nOur encoder: image (784 dimensions) \\(\\rightarrow\\) 2 dimensions\ndown_size_model = keras_model_sequential()\ndown_size_model %&gt;% \n  layer_dense(units = 100L, input_shape = c(784L), activation = \"relu\") %&gt;% \n  layer_dense(units = 20L, activation = \"relu\") %&gt;% \n  layer_dense(units = 2L, activation = \"linear\")\nOur decoder: 2 dimensions \\(\\rightarrow\\) 784 dimensions (our image)\nup_size_model = keras_model_sequential()\nup_size_model %&gt;% \n  layer_dense(units = 20L, input_shape = c(2L), activation = \"relu\") %&gt;% \n  layer_dense(units = 100L, activation = \"relu\") %&gt;% \n  layer_dense(units = 784L, activation = \"sigmoid\")\nWe can use the non-sequential model type to connect the two models. (We did the same in the transfer learning chapter.)\nautoencoder = keras_model(inputs = down_size_model$input, \n                          outputs = up_size_model(down_size_model$output))\nautoencoder$compile(loss = loss_binary_crossentropy,\n                    optimizer = optimizer_adamax(0.01))\nsummary(autoencoder)\n#&gt; Model: \"model\"\n#&gt; __________________________________________________________________________________________\n#&gt;  Layer (type)                           Output Shape                        Param #       \n#&gt; ==========================================================================================\n#&gt;  dense_2_input (InputLayer)             [(None, 784)]                       0             \n#&gt;  dense_2 (Dense)                        (None, 100)                         78500         \n#&gt;  dense_1 (Dense)                        (None, 20)                          2020          \n#&gt;  dense (Dense)                          (None, 2)                           42            \n#&gt;  sequential_1 (Sequential)              (None, 784)                         81344         \n#&gt; ==========================================================================================\n#&gt; Total params: 161,906\n#&gt; Trainable params: 161,906\n#&gt; Non-trainable params: 0\n#&gt; __________________________________________________________________________________________\nWe will now show an example of an image before and after the unfitted autoencoder, so we see that we have to train the autoencoder.\nimage = autoencoder(train_x[1,,drop = FALSE])\noldpar = par(mfrow = c(1, 2))\nimgPlot(array(train_x[1,,drop = FALSE], c(28, 28)), title = \"Before\")\nimgPlot(array(image$numpy(), c(28, 28)), title = \"After\")\n\npar(oldpar)\nFit the autoencoder (inputs == outputs!):\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(123L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nautoencoder %&gt;% \n  fit(x = train_x, y = train_x, epochs = 5L, batch_size = 128L)\nVisualization of the latent variables:\npred_dim = down_size_model(test_x)\nreconstr_pred = up_size_model(pred_dim)\nimgPlot(array(reconstr_pred[10,]$numpy(), dim = c(28L, 28L)), title = \"\")\n\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\noldpar = par(mfrow = c(1, 1))\nplot(pred_dim$numpy()[,1], pred_dim$numpy()[,2], col = ownColors[test[[2]]+1L])\n\npar(oldpar)\nThe picture above shows the 2-dimensional encoded values of the numbers in the MNIST data set and the number they are depicting via the respective color."
  },
  {
    "objectID": "E1-Autoencoder.html#autoencoder---mnist-convolutional-neural-networks",
    "href": "E1-Autoencoder.html#autoencoder---mnist-convolutional-neural-networks",
    "title": "15  Autoencoder",
    "section": "15.2 Autoencoder - MNIST Convolutional Neural Networks",
    "text": "15.2 Autoencoder - MNIST Convolutional Neural Networks\nWe can also use convolutional neural networks instead or on the side of deep neural networks: Prepare data:\ndata = tf$keras$datasets$mnist$load_data()\ntrain = data[[1]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]]), 1L))\ntest_x = array(data[[2]][[1]]/255, c(dim(data[[2]][[1]]/255), 1L))\nThen define the downsize model:\ndown_size_model = keras_model_sequential()\ndown_size_model %&gt;% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L, 3L), input_shape = c(28L, 28L, 1L), padding = \"same\") %&gt;% \n  layer_max_pooling_2d(, padding = \"same\") %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_max_pooling_2d(, padding = \"same\") %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_max_pooling_2d(, padding = \"same\") %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 2L, activation = \"linear\")\nDefine the upsize model:\nup_size_model = keras_model_sequential()\nup_size_model %&gt;% \n  layer_dense(units = 128L, activation = \"relu\", input_shape = c(2L)) %&gt;% \n  layer_reshape(target_shape = c(4L, 4L, 8L)) %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_upsampling_2d() %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_upsampling_2d() %&gt;% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L,3L)) %&gt;% \n  layer_upsampling_2d() %&gt;% \n  layer_conv_2d(filters = 1, activation = \"sigmoid\", kernel_size = c(3L,3L), padding = \"same\")\nCombine the two models and fit it:\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nautoencoder = tf$keras$models$Model(inputs = down_size_model$input,\n                                    outputs = up_size_model(down_size_model$output))\n\nautoencoder %&gt;% compile(loss = loss_binary_crossentropy,\n                    optimizer = optimizer_rmsprop(0.001))\n\nautoencoder %&gt;%  fit(x = tf$constant(train_x), y = tf$constant(train_x),\n                      epochs = 50L, batch_size = 64L)\nTest it:\npred_dim = down_size_model(tf$constant(test_x, \"float32\"))\nreconstr_pred = autoencoder(tf$constant(test_x, \"float32\"))\nimgPlot(reconstr_pred[10,,,]$numpy()[,,1])\n\n\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\nplot(pred_dim[,1]$numpy(), pred_dim[,2]$numpy(), col = ownColors[test[[2]]+1L])\n\n\n## Generate new images!\nnew = matrix(c(10, 10), 1, 2)\nimgPlot(array(up_size_model(new)$numpy(), c(28L, 28L)))\n\n\nnew = matrix(c(5, 5), 1, 2)\nimgPlot(array(up_size_model(new)$numpy(), c(28L, 28L)))"
  },
  {
    "objectID": "E1-Autoencoder.html#sec-VAE",
    "href": "E1-Autoencoder.html#sec-VAE",
    "title": "15  Autoencoder",
    "section": "15.3 Variational Autoencoder (VAE)",
    "text": "15.3 Variational Autoencoder (VAE)\nThe difference between a variational and a normal autoencoder is that a variational autoencoder assumes a distribution for the latent variables (latent variables cannot be observed and are composed of other variables) and the parameters of this distribution are learned. Thus new objects can be generated by inserting valid (!) (with regard to the assumed distribution) “seeds” to the decoder. To achieve the property that more or less randomly chosen points in the low dimensional latent space are meaningful and yield suitable results after decoding, the latent space/training process must be regularized. In this process, the input to the VAE is encoded to a distribution in the latent space rather than a single point.\nFor building variational autoencoders, we will use TensorFlow probability, but first, we need to split the data again.\nlibrary(tfprobability)\n\ndata = tf$keras$datasets$mnist$load_data()\ntrain = data[[1]]\ntrain_x = array(train[[1]]/255, c(dim(train[[1]]), 1L))\nWe will use TensorFlow probability to define priors for our latent variables.\nlibrary(tfprobability)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ntfp = reticulate::import(\"tensorflow_probability\")\nBuild the two networks:\nencoded = 2L\nprior = tfd_independent(tfd_normal(c(0.0, 0.0), 1.0), 1L)\n\nup_size_model = keras_model_sequential()\nup_size_model %&gt;% \n  layer_dense(units = 128L, activation = \"relu\", input_shape = c(2L)) %&gt;% \n  layer_reshape(target_shape = c(4L, 4L, 8L)) %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_upsampling_2d() %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_upsampling_2d() %&gt;% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L,3L)) %&gt;% \n  layer_upsampling_2d() %&gt;% \n  layer_conv_2d(filters = 1, activation = \"sigmoid\", kernel_size = c(3L,3L), padding = \"same\")\n\ndown_size_model = keras_model_sequential()\ndown_size_model %&gt;% \n  layer_conv_2d(filters = 16L, activation = \"relu\", kernel_size = c(3L, 3L), input_shape = c(28L, 28L, 1L), padding = \"same\") %&gt;% \n  layer_max_pooling_2d(, padding = \"same\") %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_max_pooling_2d(, padding = \"same\") %&gt;% \n  layer_conv_2d(filters = 8L, activation = \"relu\", kernel_size = c(3L,3L), padding = \"same\") %&gt;% \n  layer_max_pooling_2d(, padding = \"same\") %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 4L, activation = \"linear\") %&gt;% \n  layer_independent_normal(2L,\n                           activity_regularizer =\n                             tfp$layers$KLDivergenceRegularizer(distribution_b = prior))\n\nVAE = keras_model(inputs = down_size_model$inputs,\n                  outputs = up_size_model(down_size_model$outputs))\nCompile and fit model:\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nloss_binary = function(true, pred){\n  return(loss_binary_crossentropy(true, pred) * 28.0 * 28.0)\n}\nVAE %&gt;% compile(loss = loss_binary, optimizer = optimizer_adamax())\n\nVAE %&gt;% fit(train_x, train_x, epochs = 50L)\nAnd show that it works:\ndist = down_size_model(train_x[1:2000,,,,drop = FALSE])\nimages = up_size_model(dist$sample()[1:5,])\n\nownColors = c(\"limegreen\", \"purple\", \"yellow\", \"grey\", \"orange\",\n              \"black\", \"red\", \"navy\", \"sienna\", \"springgreen\")\noldpar = par(mfrow = c(1, 1))\nimgPlot(images[1,,,1]$numpy())\n\nplot(dist$mean()$numpy()[,1], dist$mean()$numpy()[,2], col = ownColors[train[[2]]+1L])\n\npar(oldpar)"
  },
  {
    "objectID": "E1-Autoencoder.html#exercise",
    "href": "E1-Autoencoder.html#exercise",
    "title": "15  Autoencoder",
    "section": "15.4 Exercise",
    "text": "15.4 Exercise\n\n\n\n\n\n\nQuestion\n\n\n\nRead section Section 15.3 on variational autoencoders and try to transfer the examples with MNIST to our flower data set (so from black-white images to colored images).\n\n\nClick here to see the solution\n\nSplit the data:\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(tfprobability)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\ndata = EcoData::dataset_flower()\ntest = data$test/255\ntrain = data$train/255\nrm(data)\nBuild the variational autoencoder:\nencoded = 10L\nprior = tfp$distributions$Independent(\n  tfp$distributions$Normal(loc=tf$zeros(encoded), scale = 1.),\n  reinterpreted_batch_ndims = 1L\n)\n\ndown_size_model = tf$keras$models$Sequential(list(\n  tf$keras$layers$InputLayer(input_shape = c(80L, 80L, 3L)),\n  tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 1L),\n  tf$keras$layers$Conv2D(filters = 32L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 2L),\n  tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 1L),\n  tf$keras$layers$Conv2D(filters = 64L, activation = tf$nn$leaky_relu,\n                         kernel_size = 5L, strides = 2L),\n  tf$keras$layers$Conv2D(filters = 128L, activation = tf$nn$leaky_relu,\n                         kernel_size = 7L, strides = 1L),\n  tf$keras$layers$Flatten(),\n  tf$keras$layers$Dense(units = tfp$layers$MultivariateNormalTriL$params_size(encoded),\n                        activation = NULL),\n  tfp$layers$MultivariateNormalTriL(\n    encoded, \n    activity_regularizer = tfp$layers$KLDivergenceRegularizer(prior, weight = 0.0002)\n  )\n))\n\nup_size_model = tf$keras$models$Sequential(list(\n  tf$keras$layers$InputLayer(input_shape = encoded),\n  tf$keras$layers$Dense(units = 8192L, activation = \"relu\"),\n  tf$keras$layers$Reshape(target_shape =  c(8L, 8L, 128L)),\n  tf$keras$layers$Conv2DTranspose(filters = 128L, kernel_size = 7L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 2L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 64L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 2L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 32L, kernel_size = 5L,\n                                  activation = tf$nn$leaky_relu, strides = 1L,\n                                  use_bias = FALSE),\n  tf$keras$layers$Conv2DTranspose(filters = 3L, kernel_size = c(4L, 4L),\n                                  activation = \"sigmoid\", strides = c(1L, 1L),\n                                  use_bias = FALSE)\n))\n\nVAE = tf$keras$models$Model(inputs = down_size_model$inputs, \n                            outputs = up_size_model(down_size_model$outputs))\nsummary(VAE)\n#&gt; Model: \"model_3\"\n#&gt; __________________________________________________________________________________________\n#&gt;  Layer (type)                           Output Shape                        Param #       \n#&gt; ==========================================================================================\n#&gt;  input_1 (InputLayer)                   [(None, 80, 80, 3)]                 0             \n#&gt;  conv2d_14 (Conv2D)                     (None, 76, 76, 32)                  2432          \n#&gt;  conv2d_15 (Conv2D)                     (None, 36, 36, 32)                  25632         \n#&gt;  conv2d_16 (Conv2D)                     (None, 32, 32, 64)                  51264         \n#&gt;  conv2d_17 (Conv2D)                     (None, 14, 14, 64)                  102464        \n#&gt;  conv2d_18 (Conv2D)                     (None, 8, 8, 128)                   401536        \n#&gt;  flatten_2 (Flatten)                    (None, 8192)                        0             \n#&gt;  dense_10 (Dense)                       (None, 65)                          532545        \n#&gt;  multivariate_normal_tri_l (Multivariat  ((None, 10),                       0             \n#&gt;  eNormalTriL)                            (None, 10))                                      \n#&gt;  sequential_7 (Sequential)              (None, 80, 80, 3)                   1278464       \n#&gt; ==========================================================================================\n#&gt; Total params: 2,394,337\n#&gt; Trainable params: 2,394,337\n#&gt; Non-trainable params: 0\n#&gt; __________________________________________________________________________________________\nCompile and train model:\nbe = function(true, pred){\n  return(tf$losses$binary_crossentropy(true, pred) * 80.0 * 80.0)\n}\n\nVAE$compile(loss = be,\n            optimizer = tf$keras$optimizers$Adamax(learning_rate = 0.0003))\nVAE$fit(x = train, y = train, epochs = 50L, shuffle = TRUE, batch_size = 20L)\n#&gt; &lt;keras.callbacks.History object at 0x7fcefde64b90&gt;\n\ndist = down_size_model(train[1:10,,,])\nimages = up_size_model( dist$sample()[1:5,] )\n\noldpar = par(mfrow = c(3, 1), mar = rep(1, 4))\nscales::rescale(images[1,,,]$numpy(), to = c(0, 255)) %&gt;% \n  image_to_array() %&gt;%\n  `/`(., 255) %&gt;%\n  as.raster() %&gt;%\n  plot()\n\nscales::rescale(images[2,,,]$numpy(), to = c(0, 255)) %&gt;% \n  image_to_array() %&gt;%\n  `/`(., 255) %&gt;%\n  as.raster() %&gt;%\n  plot()\n\nscales::rescale(images[3,,,]$numpy(), to = c(0, 255)) %&gt;% \n  image_to_array() %&gt;%\n  `/`(., 255) %&gt;%\n  as.raster() %&gt;%\n  plot()\n\npar(oldpar)"
  },
  {
    "objectID": "E2-GAN.html#mnist---generative-adversarial-networks-based-on-deep-neural-networks",
    "href": "E2-GAN.html#mnist---generative-adversarial-networks-based-on-deep-neural-networks",
    "title": "16  Generative Adversarial Networks (GANs)",
    "section": "16.1 MNIST - Generative Adversarial Networks Based on Deep Neural Networks",
    "text": "16.1 MNIST - Generative Adversarial Networks Based on Deep Neural Networks\nWe will now explore this on the MNIST data set.\nlibrary(keras)\nlibrary(tensorflow)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nrotate = function(x){ t(apply(x, 2, rev)) }\nimgPlot = function(img, title = \"\"){\n  col = grey.colors(255)\n  image(rotate(img), col = col, xlab = \"\", ylab = \"\", axes = FALSE,\n        main = paste0(\"Label: \", as.character(title)))\n}\nWe don’t need the test set here.\ndata = dataset_mnist()\ntrain = data$train\ntrain_x = array((train$x-127.5)/127.5, c(dim(train$x)[1], 784L))\nWe need a function to sample images for the discriminator.\nbatch_size = 32L\ndataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, \"float32\"))\ndataset$batch(batch_size)\n#&gt; &lt;BatchDataset element_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name=None)&gt;\nDefine generator model:\nget_generator = function(){\n  generator = keras_model_sequential()\n  generator %&gt;% \n  layer_dense(units = 200L, input_shape = c(100L)) %&gt;% \n  layer_activation_leaky_relu() %&gt;% \n  layer_dense(units = 200L) %&gt;% \n  layer_activation_leaky_relu() %&gt;% \n  layer_dense(units = 784L, activation = \"tanh\")\n  \n  return(generator)\n}\nAnd we also test the generator model:\ngenerator = get_generator()\nsample = tf$random$normal(c(1L, 100L))\nimgPlot(array(generator(sample)$numpy(), c(28L, 28L)))\n\nIn the discriminator, noise (random vector with 100 values) is passed through the network such that the output corresponds to the number of pixels of one MNIST image (784). We therefore define the discriminator function now.\nget_discriminator = function(){\n  discriminator = keras_model_sequential()\n  discriminator %&gt;% \n  layer_dense(units = 200L, input_shape = c(784L)) %&gt;% \n  layer_activation_leaky_relu() %&gt;% \n  layer_dense(units = 100L) %&gt;% \n  layer_activation_leaky_relu() %&gt;% \n  layer_dense(units = 1L, activation = \"sigmoid\")\n  \n  return(discriminator)\n}\nAnd we also test the discriminator function.\ndiscriminator = get_discriminator()\ndiscriminator(generator(tf$random$normal(c(1L, 100L))))\n#&gt; tf.Tensor([[0.5089391]], shape=(1, 1), dtype=float32)\nWe also have to define the loss functions for both networks.We use the already known binary cross entropy. However, we have to encode the real and predicted values for the two networks individually.\nThe discriminator will get two losses - one for identifying fake images as fake, and one for identifying real MNIST images as real images.\nThe generator will just get one loss - was it able to deceive the discriminator?\nce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE)\n\nloss_discriminator = function(real, fake){\n  real_loss = ce(tf$ones_like(real), real)\n  fake_loss = ce(tf$zeros_like(fake), fake)\n  return(real_loss + fake_loss)\n}\n\nloss_generator = function(fake){\n  return(ce(tf$ones_like(fake), fake))\n}\nEach network will get its own optimizer (in a GAN the networks are treated independently):\ngen_opt = tf$keras$optimizers$RMSprop(1e-4)\ndisc_opt = tf$keras$optimizers$RMSprop(1e-4)\nWe have to write our own training loop here (we cannot use the fit function). In each iteration (for each batch) we will do the following (the GradientTape records computations to do automatic differentiation):\n\nSample noise.\nGenerator creates images from the noise.\nDiscriminator makes predictions for fake images and real images (response is a probability between [0,1]).\nCalculate loss for generator.\nCalculate loss for discriminator.\nCalculate gradients for weights and the loss.\nUpdate weights of generator.\nUpdate weights of discriminator.\nReturn losses.\n\ngenerator = get_generator()\ndiscriminator = get_discriminator()\n\ntrain_step = function(images){\n  noise = tf$random$normal(c(128L, 100L))\n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      gen_images = generator(noise)\n      fake_output = discriminator(gen_images)\n      real_output = discriminator(images)\n      gen_loss = loss_generator(fake_output)\n      disc_loss = loss_discriminator(real_output, fake_output)\n    }\n  )\n  \n  gen_grads = tape$gradient(gen_loss, generator$weights)\n  disc_grads = tape$gradient(disc_loss, discriminator$weights)\n  rm(tape)\n  gen_opt$apply_gradients(purrr::transpose(list(gen_grads, generator$weights)))\n  disc_opt$apply_gradients(purrr::transpose(list(disc_grads, discriminator$weights)))\n  \n  return(c(gen_loss, disc_loss))\n}\n\ntrain_step = tf$`function`(reticulate::py_func(train_step))\nNow we can finally train our networks in a training loop:\n\nCreate networks.\nGet batch of images.\nRun train_step function.\nPrint losses.\nRepeat step 2-4 for number of epochs.\n\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nbatch_size = 128L\nepochs = 20L\nsteps = as.integer(nrow(train_x)/batch_size)\ncounter = 1\ngen_loss = c()\ndisc_loss = c()\n\ndataset2 = dataset$prefetch(tf$data$AUTOTUNE)\n\nfor(e in 1:epochs){\n  dat = reticulate::as_iterator(dataset2$batch(batch_size))\n  \n  coro::loop(\n    for(images in dat){\n      losses = train_step(images)\n      gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy())\n      disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy())\n    }\n  )\n   \n  if(e %% 5 == 0){ #Print output every 5 steps.\n    cat(\"Gen: \", mean(gen_loss), \" Disc: \", mean(disc_loss), \" \\n\")\n  }\n  noise = tf$random$normal(c(1L, 100L))\n  if(e %% 10 == 0){  #Plot image every 10 steps.\n    imgPlot(array(generator(noise)$numpy(), c(28L, 28L)), \"Gen\")\n  }\n}\n#&gt; Gen:  0.8095555  Disc:  1.10533  \n#&gt; Gen:  0.8928918  Disc:  1.287504\n\n#&gt; Gen:  0.9071119  Disc:  1.314586  \n#&gt; Gen:  0.9514963  Disc:  1.31548"
  },
  {
    "objectID": "E2-GAN.html#flower---gan",
    "href": "E2-GAN.html#flower---gan",
    "title": "16  Generative Adversarial Networks (GANs)",
    "section": "16.2 Flower - GAN",
    "text": "16.2 Flower - GAN\nWe can now also do the same for the flower data set. We will write this completely on our own following the steps also done for the MNIST data set.\nlibrary(keras)\nlibrary(tidyverse)\n#&gt; ── Attaching packages ───────────────────────────────────────────────── tidyverse 1.3.1 ──\n#&gt; ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#&gt; ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n#&gt; ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#&gt; ✔ readr   2.1.2     ✔ forcats 0.5.1\n#&gt; ── Conflicts ──────────────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\nlibrary(tensorflow)\nlibrary(EcoData)\n\ndata = EcoData::dataset_flower()\ntrain = (data$train-127.5)/127.5\ntest = (data$test-127.5)/127.5\ntrain_x = abind::abind(list(train, test), along = 1L)\ndataset = tf$data$Dataset$from_tensor_slices(tf$constant(train_x, \"float32\"))\nDefine the generator model and test it:\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nget_generator = function(){\n  generator = keras_model_sequential()\n  generator %&gt;% \n    layer_dense(units = 20L*20L*128L, input_shape = c(100L),\n                use_bias = FALSE) %&gt;% \n    layer_activation_leaky_relu() %&gt;% \n    layer_reshape(c(20L, 20L, 128L)) %&gt;% \n    layer_dropout(0.3) %&gt;% \n    layer_conv_2d_transpose(filters = 256L, kernel_size = c(3L, 3L),\n                            padding = \"same\", strides = c(1L, 1L),\n                            use_bias = FALSE) %&gt;% \n    layer_activation_leaky_relu() %&gt;% \n    layer_dropout(0.3) %&gt;% \n    layer_conv_2d_transpose(filters = 128L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(1L, 1L),\n                            use_bias = FALSE) %&gt;% \n    layer_activation_leaky_relu() %&gt;% \n    layer_dropout(0.3) %&gt;% \n    layer_conv_2d_transpose(filters = 64L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(2L, 2L),\n                            use_bias = FALSE) %&gt;%\n    layer_activation_leaky_relu() %&gt;% \n    layer_dropout(0.3) %&gt;% \n    layer_conv_2d_transpose(filters = 3L, kernel_size = c(5L, 5L),\n                            padding = \"same\", strides = c(2L, 2L),\n                            activation = \"tanh\", use_bias = FALSE)\n  return(generator)\n}\n\ngenerator = get_generator()\nimage = generator(tf$random$normal(c(1L,100L)))$numpy()[1,,,]\nimage = scales::rescale(image, to = c(0, 255))\nimage %&gt;% \n  image_to_array() %&gt;%\n  `/`(., 255) %&gt;%\n  as.raster() %&gt;%\n  plot()\n\nDefine the discriminator and test it:\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nget_discriminator = function(){\n  discriminator = keras_model_sequential()\n  discriminator %&gt;% \n    layer_conv_2d(filters = 64L, kernel_size = c(5L, 5L),\n                  strides = c(2L, 2L), padding = \"same\",\n                  input_shape = c(80L, 80L, 3L)) %&gt;%\n    layer_activation_leaky_relu() %&gt;% \n    layer_dropout(0.3) %&gt;% \n    layer_conv_2d(filters = 128L, kernel_size = c(5L, 5L),\n                  strides = c(2L, 2L), padding = \"same\") %&gt;% \n    layer_activation_leaky_relu() %&gt;% \n    layer_dropout(0.3) %&gt;% \n    layer_conv_2d(filters = 256L, kernel_size = c(3L, 3L),\n                  strides = c(2L, 2L), padding = \"same\") %&gt;% \n    layer_activation_leaky_relu() %&gt;% \n    layer_dropout(0.3) %&gt;% \n    layer_flatten() %&gt;% \n    layer_dense(units = 1L, activation = \"sigmoid\")\n  return(discriminator)\n}\n\ndiscriminator = get_discriminator()\ndiscriminator\n#&gt; Model: \"sequential_13\"\n#&gt; __________________________________________________________________________________________\n#&gt;  Layer (type)                           Output Shape                        Param #       \n#&gt; ==========================================================================================\n#&gt;  conv2d_21 (Conv2D)                     (None, 40, 40, 64)                  4864          \n#&gt;  leaky_re_lu_14 (LeakyReLU)             (None, 40, 40, 64)                  0             \n#&gt;  dropout_6 (Dropout)                    (None, 40, 40, 64)                  0             \n#&gt;  conv2d_20 (Conv2D)                     (None, 20, 20, 128)                 204928        \n#&gt;  leaky_re_lu_13 (LeakyReLU)             (None, 20, 20, 128)                 0             \n#&gt;  dropout_5 (Dropout)                    (None, 20, 20, 128)                 0             \n#&gt;  conv2d_19 (Conv2D)                     (None, 10, 10, 256)                 295168        \n#&gt;  leaky_re_lu_12 (LeakyReLU)             (None, 10, 10, 256)                 0             \n#&gt;  dropout_4 (Dropout)                    (None, 10, 10, 256)                 0             \n#&gt;  flatten_3 (Flatten)                    (None, 25600)                       0             \n#&gt;  dense_25 (Dense)                       (None, 1)                           25601         \n#&gt; ==========================================================================================\n#&gt; Total params: 530,561\n#&gt; Trainable params: 530,561\n#&gt; Non-trainable params: 0\n#&gt; __________________________________________________________________________________________\ndiscriminator(generator(tf$random$normal(c(1L, 100L))))\n#&gt; tf.Tensor([[0.49996078]], shape=(1, 1), dtype=float32)\nDefine the loss functions:\nce = tf$keras$losses$BinaryCrossentropy(from_logits = TRUE,\n                                        label_smoothing = 0.1)\n\nloss_discriminator = function(real, fake){\n  real_loss = ce(tf$ones_like(real), real)\n  fake_loss = ce(tf$zeros_like(fake), fake)\n  return(real_loss+fake_loss)\n}\n\nloss_generator = function(fake){\n  return(ce(tf$ones_like(fake), fake))\n}\nDefine the optimizers and the batch function:\ngen_opt = tf$keras$optimizers$RMSprop(1e-4)\ndisc_opt = tf$keras$optimizers$RMSprop(1e-4)\nDefine functions for the generator and discriminator:\ngenerator = get_generator()\ndiscriminator = get_discriminator()\n\ntrain_step = function(images){\n  noise = tf$random$normal(c(32L, 100L))\n  \n  with(tf$GradientTape(persistent = TRUE) %as% tape,\n    {\n      gen_images = generator(noise)\n      \n      real_output = discriminator(images)\n      fake_output = discriminator(gen_images)\n      \n      gen_loss = loss_generator(fake_output)\n      disc_loss = loss_discriminator(real_output, fake_output)\n    }\n  )\n  \n  gen_grads = tape$gradient(gen_loss, generator$weights)\n  disc_grads = tape$gradient(disc_loss, discriminator$weights)\n  rm(tape)\n  \n  gen_opt$apply_gradients(purrr::transpose(list(gen_grads,\n                                                generator$weights)))\n  disc_opt$apply_gradients(purrr::transpose(list(disc_grads,\n                                                 discriminator$weights)))\n  \n  return(c(gen_loss, disc_loss))\n}\n\ntrain_step = tf$`function`(reticulate::py_func(train_step))\nDo the training:\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nbatch_size = 32L\nepochs = 30L\nsteps = as.integer(dim(train_x)[1]/batch_size)\ncounter = 1\ngen_loss = c()\ndisc_loss = c()\n\ndataset = dataset$prefetch(tf$data$AUTOTUNE)\n\nfor(e in 1:epochs){\n  dat = reticulate::as_iterator(dataset$batch(batch_size))\n  \n  coro::loop(\n    for(images in dat){\n      losses = train_step(images)\n      gen_loss = c(gen_loss, tf$reduce_sum(losses[[1]])$numpy())\n      disc_loss = c(disc_loss, tf$reduce_sum(losses[[2]])$numpy())\n    }\n  )\n   \n  noise = tf$random$normal(c(1L, 100L))\n  image = generator(noise)$numpy()[1,,,]\n  image = scales::rescale(image, to = c(0, 255))\n  if(e %% 15 == 0){\n    image %&gt;% \n      image_to_array() %&gt;%\n        `/`(., 255) %&gt;%\n        as.raster() %&gt;%\n        plot()\n  }\n   \n  if(e %% 10 == 0) cat(\"Gen: \", mean(gen_loss), \" Disc: \", mean(disc_loss), \" \\n\")\n}\n#&gt; Gen:  1.651127  Disc:  0.8720699\n\n#&gt; Gen:  1.303061  Disc:  1.037192\n\n#&gt; Gen:  1.168868  Disc:  1.100166\nlibrary(tensorflow)\nlibrary(keras)\nset_random_seed(321L, disable_gpu = FALSE)  # Already sets R's random seed.\n\nnoise = tf$random$normal(c(1L, 100L))\nimage = generator(noise)$numpy()[1,,,]\nimage = scales::rescale(image, to = c(0, 255))\nimage %&gt;% \n  image_to_array() %&gt;%\n  `/`(., 255) %&gt;%\n  as.raster() %&gt;%\n  plot()\n\nMore images:"
  },
  {
    "objectID": "E2-GAN.html#exercise",
    "href": "E2-GAN.html#exercise",
    "title": "16  Generative Adversarial Networks (GANs)",
    "section": "16.3 Exercise",
    "text": "16.3 Exercise\n\n\n\n\n\n\nQuestion\n\n\n\nGo through the R examples on generative adversarial networks (Chapter 16) and compare the flower example with the MNIST example - where are the differences - and why?\nThe MNIST example uses a “simple” deep neural network which is sufficient for a classification that easy. The flower example uses a much more expensive convolutional neural network to classify the images."
  },
  {
    "objectID": "Appendix-Datasets.html#titanic",
    "href": "Appendix-Datasets.html#titanic",
    "title": "Appendix A — Datasets",
    "section": "A.1 Titanic",
    "text": "A.1 Titanic\nThe data set is a collection of Titanic passengers with information about their age, class, sex, and their survival status. The competition is simple here: Train a machine learning model and predict the survival probability.\nThe Titanic data set is very well explored and serves as a stepping stone in many machine learning careers. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “survived”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(titanic_ml)\ntitanic = titanic_ml\nsummary(titanic)\n\n     pclass         survived          name               sex     \n Min.   :1.000   Min.   :0.0000   Length:1309        female:466  \n 1st Qu.:2.000   1st Qu.:0.0000   Class :character   male  :843  \n Median :3.000   Median :0.0000   Mode  :character               \n Mean   :2.295   Mean   :0.3853                                  \n 3rd Qu.:3.000   3rd Qu.:1.0000                                  \n Max.   :3.000   Max.   :1.0000                                  \n                 NA's   :655                                     \n      age              sibsp            parch            ticket    \n Min.   : 0.1667   Min.   :0.0000   Min.   :0.000   CA. 2343:  11  \n 1st Qu.:21.0000   1st Qu.:0.0000   1st Qu.:0.000   1601    :   8  \n Median :28.0000   Median :0.0000   Median :0.000   CA 2144 :   8  \n Mean   :29.8811   Mean   :0.4989   Mean   :0.385   3101295 :   7  \n 3rd Qu.:39.0000   3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7  \n Max.   :80.0000   Max.   :8.0000   Max.   :9.000   347082  :   7  \n NA's   :263                                        (Other) :1261  \n      fare                     cabin      embarked      boat    \n Min.   :  0.000                  :1014    :  2           :823  \n 1st Qu.:  7.896   C23 C25 C27    :   6   C:270    13     : 39  \n Median : 14.454   B57 B59 B63 B66:   5   Q:123    C      : 38  \n Mean   : 33.295   G6             :   5   S:914    15     : 37  \n 3rd Qu.: 31.275   B96 B98        :   4            14     : 33  \n Max.   :512.329   C22 C26        :   4            4      : 31  \n NA's   :1         (Other)        : 271            (Other):308  \n      body                      home.dest  \n Min.   :  1.0                       :564  \n 1st Qu.: 72.0   New York, NY        : 64  \n Median :155.0   London              : 14  \n Mean   :160.8   Montreal, PQ        : 10  \n 3rd Qu.:256.0   Cornwall / Akron, OH:  9  \n Max.   :328.0   Paris, France       :  9  \n NA's   :1188    (Other)             :639  \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\ntitanic_imputed = titanic %&gt;% select(-name, -ticket, -cabin, -boat, -home.dest)\ntitanic_imputed = missRanger::missRanger(data = titanic_imputed %&gt;%\n                                           select(-survived), verbose = 0)\ntitanic_imputed$survived = titanic$survived\n\n\nSplit into training and test set:\n\n\ntrain = titanic_imputed[!is.na(titanic$survived), ]\ntest = titanic_imputed[is.na(titanic$survived), ]\n\n\nTrain model:\n\n\nmodel = glm(survived~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, data = test, type = \"response\")\nhead(preds)\n\n       561        321       1177       1098       1252       1170 \n0.79095923 0.30597519 0.01400693 0.12310859 0.14099292 0.11768284 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")\n\nAnd submit the csv on http://rhsbio7.uni-regensburg.de:8500."
  },
  {
    "objectID": "Appendix-Datasets.html#sec-plantpoll",
    "href": "Appendix-Datasets.html#sec-plantpoll",
    "title": "Appendix A — Datasets",
    "section": "A.2 Plant-pollinator Database",
    "text": "A.2 Plant-pollinator Database\nThe plant-pollinator database is a collection of plant-pollinator interactions with traits for plants and pollinators. The idea is pollinators interact with plants when their traits fit (e.g. the tongue of a bee needs to match the shape of a flower). We explored the advantage of machine learning algorithms over traditional statistical models in predicting species interactions in our paper. If you are interested you can have a look here.\n\n\n\n\n\nResponse variable: “interaction”\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(plantPollinator_df)\nplant_poll = plantPollinator_df\nsummary(plant_poll)\n\n                   crop                       insect          type          \n Vaccinium_corymbosum:  256   Andrena_wilkella   :   80   Length:20480      \n Brassica_napus      :  256   Andrena_barbilabris:   80   Class :character  \n Carum_carvi         :  256   Andrena_cineraria  :   80   Mode  :character  \n Coriandrum_sativum  :  256   Andrena_flavipes   :   80                     \n Daucus_carota       :  256   Andrena_gravida    :   80                     \n Malus_domestica     :  256   Andrena_haemorrhoa :   80                     \n (Other)             :18944   (Other)            :20000                     \n    season             diameter        corolla             colour         \n Length:20480       Min.   :  2.00   Length:20480       Length:20480      \n Class :character   1st Qu.:  5.00   Class :character   Class :character  \n Mode  :character   Median : 19.00   Mode  :character   Mode  :character  \n                    Mean   : 27.03                                        \n                    3rd Qu.: 25.00                                        \n                    Max.   :150.00                                        \n                    NA's   :9472                                          \n    nectar            b.system         s.pollination      inflorescence     \n Length:20480       Length:20480       Length:20480       Length:20480      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  composite            guild               tongue            body      \n Length:20480       Length:20480       Min.   : 2.000   Min.   : 2.00  \n Class :character   Class :character   1st Qu.: 4.800   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median : 6.600   Median :10.50  \n                                       Mean   : 8.104   Mean   :10.66  \n                                       3rd Qu.:10.500   3rd Qu.:13.00  \n                                       Max.   :26.400   Max.   :25.00  \n                                       NA's   :17040    NA's   :6160   \n  sociality           feeding          interaction \n Length:20480       Length:20480       0   :14095  \n Class :character   Class :character   1   :  595  \n Mode  :character   Mode  :character   NA's: 5790  \n                                                   \n                                                   \n                                                   \n                                                   \n\n\n\nImpute missing values (not our response variable!) We will select only a few predictors here (you can work with all predictors of course).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nplant_poll_imputed = plant_poll %&gt;% select(diameter,\n                                           corolla,\n                                           tongue,\n                                           body,\n                                           interaction)\nplant_poll_imputed = missRanger::missRanger(data = plant_poll_imputed %&gt;%\n                                              select(-interaction), verbose = 0)\nplant_poll_imputed$interaction = plant_poll$interaction\n\n\nSplit into training and test set:\n\n\ntrain = plant_poll_imputed[!is.na(plant_poll_imputed$interaction), ]\ntest = plant_poll_imputed[is.na(plant_poll_imputed$interaction), ]\n\n\nTrain model:\n\n\nmodel = glm(interaction~., data = train, family = binomial())\n\n\nPredictions:\n\n\npreds = predict(model, newdata = test, type = \"response\")\nhead(preds)\n\n         1          2          3          4          5          6 \n0.02942746 0.05063489 0.03780247 0.03780247 0.02651142 0.04130643 \n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"glm.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#wine",
    "href": "Appendix-Datasets.html#wine",
    "title": "Appendix A — Datasets",
    "section": "A.3 Wine",
    "text": "A.3 Wine\nThe data set is a collection of wines of different quality. The aim is to predict the quality of the wine based on physiochemical predictors.\nFor inspiration and data exploration notebooks, check out this kaggle competition. For instance, check out this very nice notebook which removes a few problems from the data.\nResponse variable: “quality”\nWe could theoretically use a regression model for this task but we will stick with a classification model.\nA minimal working example:\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(wine)\nsummary(wine)\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900  \n 1st Qu.: 7.100   1st Qu.:0.3900   1st Qu.:0.0900   1st Qu.: 1.900  \n Median : 7.900   Median :0.5200   Median :0.2600   Median : 2.200  \n Mean   : 8.335   Mean   :0.5284   Mean   :0.2705   Mean   : 2.533  \n 3rd Qu.: 9.300   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600  \n Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500  \n NA's   :70       NA's   :48       NA's   :41       NA's   :60      \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00       1st Qu.:0.9956  \n Median :0.07900   Median :14.00       Median : 38.00       Median :0.9968  \n Mean   :0.08747   Mean   :15.83       Mean   : 46.23       Mean   :0.9968  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00       3rd Qu.:0.9979  \n Max.   :0.61100   Max.   :72.00       Max.   :289.00       Max.   :1.0037  \n NA's   :37        NA's   :78          NA's   :78           NA's   :78      \n       pH          sulphates         alcohol         quality     \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n 1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000  \n Mean   :3.311   Mean   :0.6572   Mean   :10.42   Mean   :5.596  \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n NA's   :25      NA's   :51                       NA's   :905    \n\n\n\nImpute missing values (not our response variable!).\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nwine_imputed = missRanger::missRanger(data = wine %&gt;% select(-quality), verbose = 0)\nwine_imputed$quality = wine$quality\n\n\nSplit into training and test set:\n\n\ntrain = wine_imputed[!is.na(wine$quality), ]\ntest = wine_imputed[is.na(wine$quality), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(quality~., data = train, classification = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions\nhead(preds)\n\n[1] 6 5 5 7 6 6\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#nasa",
    "href": "Appendix-Datasets.html#nasa",
    "title": "Appendix A — Datasets",
    "section": "A.4 Nasa",
    "text": "A.4 Nasa\nA collection about asteroids and their characteristics from kaggle. The aim is to predict whether the asteroids are hazardous or not. For inspiration and data exploration notebooks, check out this kaggle competition.\nResponse variable: “Hazardous”\n\nLoad data set:\n\n\nlibrary(EcoData)\n\ndata(nasa)\nsummary(nasa)\n\n Neo.Reference.ID       Name         Absolute.Magnitude Est.Dia.in.KM.min.\n Min.   :2000433   Min.   :2000433   Min.   :11.16      Min.   : 0.00101  \n 1st Qu.:3102682   1st Qu.:3102683   1st Qu.:20.10      1st Qu.: 0.03346  \n Median :3514800   Median :3514800   Median :21.90      Median : 0.11080  \n Mean   :3272675   Mean   :3273113   Mean   :22.27      Mean   : 0.20523  \n 3rd Qu.:3690987   3rd Qu.:3690385   3rd Qu.:24.50      3rd Qu.: 0.25384  \n Max.   :3781897   Max.   :3781897   Max.   :32.10      Max.   :15.57955  \n NA's   :53        NA's   :57        NA's   :36         NA's   :60        \n Est.Dia.in.KM.max. Est.Dia.in.M.min.   Est.Dia.in.M.max. \n Min.   : 0.00226   Min.   :    1.011   Min.   :    2.26  \n 1st Qu.: 0.07482   1st Qu.:   33.462   1st Qu.:   74.82  \n Median : 0.24777   Median :  110.804   Median :  247.77  \n Mean   : 0.45754   Mean   :  204.649   Mean   :  458.45  \n 3rd Qu.: 0.56760   3rd Qu.:  253.837   3rd Qu.:  567.60  \n Max.   :34.83694   Max.   :15579.552   Max.   :34836.94  \n NA's   :23         NA's   :29          NA's   :46        \n Est.Dia.in.Miles.min. Est.Dia.in.Miles.max. Est.Dia.in.Feet.min.\n Min.   :0.00063       Min.   : 0.00140      Min.   :    3.32    \n 1st Qu.:0.02079       1st Qu.: 0.04649      1st Qu.:  109.78    \n Median :0.06885       Median : 0.15395      Median :  363.53    \n Mean   :0.12734       Mean   : 0.28486      Mean   :  670.44    \n 3rd Qu.:0.15773       3rd Qu.: 0.35269      3rd Qu.:  832.80    \n Max.   :9.68068       Max.   :21.64666      Max.   :51114.02    \n NA's   :42            NA's   :50            NA's   :21          \n Est.Dia.in.Feet.max. Close.Approach.Date Epoch.Date.Close.Approach\n Min.   :     7.41    2016-07-22:  18     Min.   :7.889e+11        \n 1st Qu.:   245.49    2015-01-15:  17     1st Qu.:1.016e+12        \n Median :   812.88    2015-02-15:  16     Median :1.203e+12        \n Mean   :  1500.77    2007-11-08:  15     Mean   :1.180e+12        \n 3rd Qu.:  1862.19    2012-01-15:  15     3rd Qu.:1.356e+12        \n Max.   :114294.42    (Other)   :4577     Max.   :1.473e+12        \n NA's   :46           NA's      :  29     NA's   :43               \n Relative.Velocity.km.per.sec Relative.Velocity.km.per.hr Miles.per.hour   \n Min.   : 0.3355              Min.   :  1208              Min.   :  750.5  \n 1st Qu.: 8.4497              1st Qu.: 30399              1st Qu.:18846.7  \n Median :12.9370              Median : 46532              Median :28893.7  \n Mean   :13.9848              Mean   : 50298              Mean   :31228.0  \n 3rd Qu.:18.0774              3rd Qu.: 65068              3rd Qu.:40436.9  \n Max.   :44.6337              Max.   :160681              Max.   :99841.2  \n NA's   :27                   NA's   :28                  NA's   :38       \n Miss.Dist..Astronomical. Miss.Dist..lunar.   Miss.Dist..kilometers.\n Min.   :0.00018          Min.   :  0.06919   Min.   :   26610      \n 1st Qu.:0.13341          1st Qu.: 51.89874   1st Qu.:19964907      \n Median :0.26497          Median :103.19415   Median :39685408      \n Mean   :0.25690          Mean   : 99.91366   Mean   :38436154      \n 3rd Qu.:0.38506          3rd Qu.:149.59244   3rd Qu.:57540318      \n Max.   :0.49988          Max.   :194.45491   Max.   :74781600      \n NA's   :60               NA's   :30          NA's   :56            \n Miss.Dist..miles.  Orbiting.Body    Orbit.ID     \n Min.   :   16535   Earth:4665    Min.   :  1.00  \n 1st Qu.:12454813   NA's :  22    1st Qu.:  9.00  \n Median :24662435                 Median : 16.00  \n Mean   :23885560                 Mean   : 28.34  \n 3rd Qu.:35714721                 3rd Qu.: 31.00  \n Max.   :46467132                 Max.   :611.00  \n NA's   :27                       NA's   :33      \n        Orbit.Determination.Date Orbit.Uncertainity Minimum.Orbit.Intersection\n 2017-06-21 06:17:20:   9        Min.   :0.000      Min.   :0.00000           \n 2017-04-06 08:57:13:   8        1st Qu.:0.000      1st Qu.:0.01435           \n 2017-04-06 09:24:24:   8        Median :3.000      Median :0.04653           \n 2017-04-06 08:24:13:   7        Mean   :3.521      Mean   :0.08191           \n 2017-04-06 08:26:19:   7        3rd Qu.:6.000      3rd Qu.:0.12150           \n (Other)            :4622        Max.   :9.000      Max.   :0.47789           \n NA's               :  26        NA's   :49         NA's   :137               \n Jupiter.Tisserand.Invariant Epoch.Osculation   Eccentricity    \n Min.   :2.196               Min.   :2450164   Min.   :0.00752  \n 1st Qu.:4.047               1st Qu.:2458000   1st Qu.:0.24086  \n Median :5.071               Median :2458000   Median :0.37251  \n Mean   :5.056               Mean   :2457723   Mean   :0.38267  \n 3rd Qu.:6.017               3rd Qu.:2458000   3rd Qu.:0.51256  \n Max.   :9.025               Max.   :2458020   Max.   :0.96026  \n NA's   :56                  NA's   :60        NA's   :39       \n Semi.Major.Axis   Inclination       Asc.Node.Longitude Orbital.Period  \n Min.   :0.6159   Min.   : 0.01451   Min.   :  0.0019   Min.   : 176.6  \n 1st Qu.:1.0012   1st Qu.: 4.93290   1st Qu.: 83.1849   1st Qu.: 365.9  \n Median :1.2422   Median :10.27694   Median :172.6347   Median : 504.9  \n Mean   :1.4009   Mean   :13.36159   Mean   :172.1717   Mean   : 635.5  \n 3rd Qu.:1.6782   3rd Qu.:19.47848   3rd Qu.:254.8804   3rd Qu.: 793.1  \n Max.   :5.0720   Max.   :75.40667   Max.   :359.9059   Max.   :4172.2  \n NA's   :53       NA's   :42         NA's   :60         NA's   :46      \n Perihelion.Distance Perihelion.Arg     Aphelion.Dist    Perihelion.Time  \n Min.   :0.08074     Min.   :  0.0069   Min.   :0.8038   Min.   :2450100  \n 1st Qu.:0.63038     1st Qu.: 95.6430   1st Qu.:1.2661   1st Qu.:2457815  \n Median :0.83288     Median :189.7729   Median :1.6182   Median :2457972  \n Mean   :0.81316     Mean   :184.0185   Mean   :1.9864   Mean   :2457726  \n 3rd Qu.:0.99718     3rd Qu.:271.9535   3rd Qu.:2.4497   3rd Qu.:2458108  \n Max.   :1.29983     Max.   :359.9931   Max.   :8.9839   Max.   :2458839  \n NA's   :22          NA's   :48         NA's   :38       NA's   :59       \n  Mean.Anomaly       Mean.Motion       Equinox       Hazardous    \n Min.   :  0.0032   Min.   :0.08628   J2000:4663   Min.   :0.000  \n 1st Qu.: 87.0069   1st Qu.:0.45147   NA's :  24   1st Qu.:0.000  \n Median :186.0219   Median :0.71137                Median :0.000  \n Mean   :181.2882   Mean   :0.73732                Mean   :0.176  \n 3rd Qu.:276.6418   3rd Qu.:0.98379                3rd Qu.:0.000  \n Max.   :359.9180   Max.   :2.03900                Max.   :1.000  \n NA's   :40         NA's   :48                     NA's   :4187   \n\n\n\nImpute missing values (not our response variable!):\n\n\nlibrary(missRanger)\nlibrary(dplyr)\nset.seed(123)\n\nnasa_imputed = missRanger::missRanger(data = nasa %&gt;% select(-Hazardous),\n                                      maxiter = 1, num.trees = 5L, verbose = 0)\nnasa_imputed$Hazardous = nasa$Hazardous\n\n\nSplit into training and test set:\n\n\ntrain = nasa_imputed[!is.na(nasa$Hazardous), ]\ntest = nasa_imputed[is.na(nasa$Hazardous), ]\n\n\nTrain model:\n\n\nlibrary(ranger)\nset.seed(123)\n\nrf = ranger(Hazardous~., data = train, classification = TRUE,\n            probability = TRUE)\n\n\nPredictions:\n\n\npreds = predict(rf, data = test)$predictions[,2]\nhead(preds)\n\n[1] 0.6348055556 0.7525960317 0.0008444444 0.7733373016 0.1404333333\n[6] 0.1509190476\n\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = preds), file = \"rf.csv\")"
  },
  {
    "objectID": "Appendix-Datasets.html#flower",
    "href": "Appendix-Datasets.html#flower",
    "title": "Appendix A — Datasets",
    "section": "A.5 Flower",
    "text": "A.5 Flower\nA collection of over 4000 flower images of 5 plant species. The data set is from kaggle but we downsampled the images from \\(320*240\\) to \\(80*80\\) pixels. You can a) download the data set here or b) get it via the EcoData package.\nNotes:\n\nCheck out convolutional neural network notebooks on kaggle (they are often written in Python but you can still copy the architectures), e.g. this one.\nLast year’s winners have used a transfer learning approach (they achieved around 70% accuracy), check out this notebook, see also the section about transfer learning @ref(transfer).\n\nResponse variable: “Plant species”\n\nLoad data set:\n\n\nlibrary(tensorflow)\nlibrary(keras)\n\ntrain = EcoData::dataset_flower()$train/255\ntest = EcoData::dataset_flower()$test/255\nlabels = EcoData::dataset_flower()$labels\n\nLet’s visualize a flower:\n\ntrain[100,,,] %&gt;%\n  image_to_array() %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\n\n\nBuild and train model:\n\n\nmodel = keras_model_sequential()\nmodel %&gt;% \n  layer_conv_2d(filters = 4L, kernel_size = 2L,\n                input_shape = list(80L, 80L, 3L)) %&gt;% \n  layer_max_pooling_2d() %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 5L, activation = \"softmax\")\n\n### Model fitting ###\n\nmodel %&gt;% \n  compile(loss = loss_categorical_crossentropy, \n          optimizer = optimizer_adamax(learning_rate = 0.01))\n\nmodel %&gt;% \n  fit(x = train, y = keras::k_one_hot(labels, 5L))\n\n\nPredictions:\n\n\n# Prediction on training data:\npred = apply(model %&gt;% predict(train), 1, which.max)\nMetrics::accuracy(pred - 1L, labels)\ntable(pred)\n\n# Prediction for the submission server:\npred = model %&gt;% predict(test) %&gt;% apply(1, which.max) - 1L\ntable(pred)\n\n\nCreate submission csv:\n\n\nwrite.csv(data.frame(y = pred), file = \"cnn.csv\")"
  }
]